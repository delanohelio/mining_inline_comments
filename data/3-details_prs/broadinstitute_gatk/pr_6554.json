{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1MzM1OTMz", "number": 6554, "title": "(all of) gCNV exome joint calling", "bodyText": "I thought I'd PR this before it gets too big.\nThe idea is to do defragmentation and breakpoint clustering on the exome CNV variants and output the new coordinated with the copy number for each sample.  This is sort of like the CombineGVCFs step.  The next step, which is the GenotypeGVCFs equivalent, will be updating the quality scores for each variant.  Since we changed the bounds, we have to recalculate QS, QA, QSS, QSE.  I think that should be possible using similar code to PostprocessGCNVCalls and using the clustered breakpoints instead of the viterbi segmentation.  I guess we'll see.", "createdAt": "2020-04-17T20:54:42Z", "url": "https://github.com/broadinstitute/gatk/pull/6554", "merged": true, "mergeCommit": {"oid": "31df35bb9204b5551cc1a3ee7468e2b0e577215d"}, "closed": true, "closedAt": "2020-12-23T01:28:15Z", "author": {"login": "ldgauthier"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcYnvw6AFqTM5NTc2ODYzNw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdoxFQrABqjQxNDE3NjY0NDY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzY4NjM3", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395768637", "createdAt": "2020-04-17T20:58:44Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMDo1ODo0NFrOGHc5SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMDo1ODo0NFrOGHc5SQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw==", "bodyText": "This doesn't go so well with just two events that have the minimal overlap.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410466633", "createdAt": "2020-04-17T20:58:44Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzY4ODY0", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395768864", "createdAt": "2020-04-17T20:59:10Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMDo1OToxMFrOGHc6Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMDo1OToxMFrOGHc6Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2Njg1NQ==", "bodyText": "I'm not sure what role the evidence plays in the WGS pipeline.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410466855", "createdAt": "2020-04-17T20:59:10Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzY5MDk1", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395769095", "createdAt": "2020-04-17T20:59:34Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMDo1OTozNVrOGHc64Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMDo1OTozNVrOGHc64Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzA0MQ==", "bodyText": "Same here -- are they identical if the evidence doesn't match?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410467041", "createdAt": "2020-04-17T20:59:35Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+    }\n+\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzY5Nzc4", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395769778", "createdAt": "2020-04-17T21:00:47Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowMDo0N1rOGHc8_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowMDo0N1rOGHc8_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzU4Mw==", "bodyText": "I modified the math here to better reflect my idea of how we compute reciprocal overlap, but I'm open to discussion.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410467583", "createdAt": "2020-04-17T21:00:47Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzcwNDc2", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395770476", "createdAt": "2020-04-17T21:02:04Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowMjowNVrOGHc_DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowMjowNVrOGHc_DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODEwOQ==", "bodyText": "Should we be checking the copy number here?  I don't think I want to merge a CN1 deletion with a CN0 deletion, for example.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410468109", "createdAt": "2020-04-17T21:02:05Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzcxMTE5", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395771119", "createdAt": "2020-04-17T21:03:18Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowMzoxOFrOGHdBGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowMzoxOFrOGHdBGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODYzMw==", "bodyText": "Is the length inclusive?  That's typically the GATK convention, in which case this needs a +1.  Unless it comes as the above compensatory +1?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410468633", "createdAt": "2020-04-17T21:03:18Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzcyMjU1", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395772255", "createdAt": "2020-04-17T21:05:27Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowNToyN1rOGHdEuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowNToyN1rOGHdEuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2OTU2MQ==", "bodyText": "This is a more strict check compared with itemsAreIdentical -- I use this in the tests", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410469561", "createdAt": "2020-04-17T21:05:27Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecordWithEvidence.java", "diffHunk": "@@ -0,0 +1,82 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+\n+public class SVCallRecordWithEvidence extends SVCallRecord {\n+\n+    private final List<SplitReadSite> startSplitReadSites;\n+    private final List<SplitReadSite> endSplitReadSites;\n+    private final List<DiscordantPairEvidence> discordantPairs;\n+\n+    public SVCallRecordWithEvidence(final SVCallRecord record) {\n+        super(record.getContig(), record.getStart(), record.getStartStrand(), record.getEndContig(), record.getEnd(),\n+                record.getEndStrand(), record.getType(), record.getLength(), record.getAlgorithms(), record.getGenotypes());\n+        this.startSplitReadSites = Collections.emptyList();\n+        this.endSplitReadSites = Collections.emptyList();\n+        this.discordantPairs = Collections.emptyList();\n+    }\n+\n+    public SVCallRecordWithEvidence(final String startContig,\n+                                    final int start,\n+                                    final boolean startStrand,\n+                                    final String endContig,\n+                                    final int end,\n+                                    final boolean endStrand,\n+                                    final StructuralVariantType type,\n+                                    final int length,\n+                                    final List<String> algorithms,\n+                                    final List<Genotype> genotypes,\n+                                    final List<SplitReadSite> startSplitReadSites,\n+                                    final List<SplitReadSite> endSplitReadSites,\n+                                    final List<DiscordantPairEvidence> discordantPairs) {\n+        super(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, genotypes);\n+        Utils.nonNull(startSplitReadSites);\n+        Utils.nonNull(endSplitReadSites);\n+        Utils.nonNull(discordantPairs);\n+        Utils.containsNoNull(startSplitReadSites, \"Encountered null in start split reads\");\n+        Utils.containsNoNull(endSplitReadSites, \"Encountered null in end split reads\");\n+        Utils.containsNoNull(discordantPairs, \"Encountered null in discordant pairs\");\n+        this.startSplitReadSites = startSplitReadSites;\n+        this.endSplitReadSites = endSplitReadSites;\n+        this.discordantPairs = discordantPairs;\n+    }\n+\n+    public List<DiscordantPairEvidence> getDiscordantPairs() {\n+        return discordantPairs;\n+    }\n+\n+    public List<SplitReadSite> getStartSplitReadSites() {\n+        return startSplitReadSites;\n+    }\n+\n+    public List<SplitReadSite> getEndSplitReadSites() {\n+        return endSplitReadSites;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzcyNTE3", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395772517", "createdAt": "2020-04-17T21:05:56Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowNTo1N1rOGHdFig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowNTo1N1rOGHdFig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2OTc3MA==", "bodyText": "I don't have any insertion tests yet.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410469770", "createdAt": "2020-04-17T21:05:57Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NzczMDA3", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-395773007", "createdAt": "2020-04-17T21:06:54Z", "commit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowNjo1NFrOGHdG8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTowNjo1NFrOGHdG8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3MDEyOA==", "bodyText": "Would it be better to throw an IllegalStateException if we shouldn't reach here?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410470128", "createdAt": "2020-04-17T21:06:54Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)\n+                .overlaps(getClusteringInterval(b, null));\n+    }\n+\n+\n+    /**\n+     * Determine an overlap interval for clustering using {@value #PADDING_FRACTION} padding\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param currentClusterInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval currentClusterInterval) {\n+        Utils.nonNull(call);\n+        final SimpleInterval callInterval = getCallInterval(call);\n+        final int paddedCallStart = (int) (callInterval.getStart() - PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final int paddedCallEnd = (int) (callInterval.getEnd() + PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final String currentContig = getCurrentContig();\n+        final int contigLength = dictionary.getSequence(currentContig).getSequenceLength();\n+        if (currentClusterInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, paddedCallStart, paddedCallEnd, contigLength);\n+        }\n+        //NOTE: this is an approximation -- padding should be based on the length of the call plus currentClusterIntervals\n+        final int newMinStart = Math.min(paddedCallStart, currentClusterInterval.getStart());\n+        final int newMaxEnd = Math.max(paddedCallEnd, currentClusterInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxEnd, contigLength);\n+    }\n+\n+    // Not used for single-linkage clustering\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 95}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4MzYxNjU2", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-408361656", "createdAt": "2020-05-08T16:31:15Z", "commit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxNjozMToxNVrOGSrlzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMTo0ODozNVrOGXz3fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0MTc0Mw==", "bodyText": "What happened here?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422241743", "createdAt": "2020-05-08T16:31:15Z", "author": {"login": "mwalker174"}, "path": "README.md", "diffHunk": "@@ -1,620 +1,6 @@\n-[![Build Status](https://travis-ci.com/broadinstitute/gatk.svg?branch=master)](https://travis-ci.com/broadinstitute/gatk)\n-[![Maven Central](https://img.shields.io/maven-central/v/org.broadinstitute/gatk.svg)](https://maven-badges.herokuapp.com/maven-central/org.broadinstitute/gatk)\n-[![License (3-Clause BSD)](https://img.shields.io/badge/license-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n-\n-***Please see the [GATK website](http://www.broadinstitute.org/gatk), where you can download a precompiled executable, read documentation, ask questions, and receive technical support. For GitHub basics, see [here](https://software.broadinstitute.org/gatk/documentation/article?id=23405).***\n-\n-### GATK 4\n-\n-This repository contains the next generation of the Genome Analysis Toolkit (GATK). The contents\n-of this repository are 100% open source and released under the BSD 3-Clause license (see [LICENSE.TXT](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT)).\n-\n-GATK4 aims to bring together well-established tools from the [GATK](http://www.broadinstitute.org/gatk) and\n-[Picard](http://broadinstitute.github.io/picard/) codebases under a streamlined framework,\n-and to enable selected tools to be run in a massively parallel way on local clusters or in the cloud using\n-[Apache Spark](http://spark.apache.org/). It also contains many newly developed tools not present in earlier\n-releases of the toolkit.\n-\n-## Table of Contents\n-* [Requirements](#requirements)\n-* [Quick Start Guide](#quickstart)\n-* [Downloading GATK4](#downloading)\n-* [Building GATK4](#building)\n-* [Running GATK4](#running)\n-    * [Passing JVM options to gatk](#jvmoptions)\n-    * [Passing a configuration file to gatk](#configFileOptions)\n-    * [Running GATK4 with inputs on Google Cloud Storage](#gcs)\n-    * [Running GATK4 Spark tools on a Spark cluster](#sparkcluster)\n-    * [Running GATK4 Spark tools on Google Cloud Dataproc](#dataproc)\n-    * [Using R to generate plots](#R)\n-    * [GATK Tab Completion for Bash](#tab_completion)\n-* [For GATK Developers](#developers)\n-    * [General guidelines for GATK4 developers](#dev_guidelines)\n-    * [Testing GATK4](#testing)\n-    * [Using Git LFS to download and track large test data](#lfs)\n-    * [Creating a GATK project in the IntelliJ IDE](#intellij)\n-    * [Setting up debugging in IntelliJ](#debugging)\n-    * [Updating the Intellij project when dependencies change](#intellij_gradle_refresh)\n-    * [Setting up profiling using JProfiler](#jprofiler)\n-    * [Uploading Archives to Sonatype](#sonatype)\n-    * [Building GATK4 Docker images](#docker_building)\n-    * [Releasing GATK4](#releasing_gatk)\n-    * [Generating GATK4 documentation](#gatkdocs)\n-    * [Using Zenhub to track github issues](#zenhub)\n-* [Further Reading on Spark](#spark_further_reading)\n-* [How to contribute to GATK](#contribute)\n-* [Discussions](#discussions)\n-* [Authors](#authors)\n-* [License](#license)\n-\n-## <a name=\"requirements\">Requirements</a>\n-* To run GATK:\n-    * Java 8 is needed to run or build GATK. \n-    We recommend either of the following:\n-        * OpenJDK 8 with Hotspot from [AdoptOpenJdk](https://adoptopenjdk.net/)\n-        * [OracleJDK 8](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\n-    which requires an Oracle account to download and comes with restrictive [license conditions](https://www.oracle.com/downloads/licenses/javase-license1.html).\n-    * Python 2.6 or greater (required to run the `gatk` frontend script)\n-    * Python 3.6.2, along with a set of additional Python packages, is required to run some tools and workflows.\n-      See [Python Dependencies](#python) for more information.\n-    * R 3.2.5 (needed for producing plots in certain tools)\n-* To build GATK:\n-    * A Java 8 JDK\n-    * Git 2.5 or greater\n-    * [git-lfs](https://git-lfs.github.com/) 1.1.0 or greater. Required to download the large files used to build GATK, and\n-      test files required to run the test suite. Run `git lfs install` after downloading, followed by `git lfs pull` from\n-      the root of your git clone to download all of the large files, including those required to run the test suite. The\n-      full download is approximately 2 gigabytes. Alternatively, if you are just building GATK and not running the test\n-      suite, you can skip this step since the build itself will use git-lfs to download the minimal set of large `lfs`\n-      resource files required to complete the build. The test resources will not be downloaded, but this greatly reduces\n-      the size of the download.\n-    * Gradle 5.6. We recommend using the `./gradlew` script which will\n-      download and use an appropriate gradle version automatically (see examples below).\n-    * R 3.2.5 (needed for running the test suite)\n-* Pre-packaged Docker images with all needed dependencies installed can be found on\n-  [our dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk/). This requires a recent version of the\n-   docker client, which can be found on the [docker website](https://www.docker.com/get-docker).\n-* Python Dependencies:<a name=\"python\"></a>\n-    * GATK4 uses the [Conda](https://conda.io/docs/index.html) package manager to establish and manage the\n-      Python environment and dependencies required by GATK tools that have a Python dependency. The ```gatk``` environment, \n-      requires hardware with AVX support for tools that depend on TensorFlow (e.g. CNNScoreVariant). The GATK Docker image \n-      comes with the ```gatk``` environment pre-configured.\n-    * To establish the  environment when not using the Docker image, a conda environment must first be \"created\", and\n-      then \"activated\":\n-        * First, make sure [Miniconda or Conda](https://conda.io/docs/index.html) is installed (Miniconda is sufficient).\n-        * To \"create\" the conda environment:\n-            * If running from a zip or tar distribution, run the command ```conda env create -f gatkcondaenv.yml``` to\n-              create the ```gatk``` environment.\n-            * If running from a cloned repository, run ```./gradlew localDevCondaEnv```. This generates the Python\n-              package archive and conda yml dependency file(s) in the build directory, and also creates (or updates)\n-              the local  ```gatk``` conda environment.\n-        * To \"activate\" the conda environment (the conda environment must be activated within the same shell from which\n-          GATK is run):\n-             * Execute the shell command ```source activate gatk``` to activate the ```gatk``` environment.\n-        * See the [Conda](https://conda.io/docs/user-guide/tasks/manage-environments.html) documentation for\n-          additional information about using and managing Conda environments.\n-\n-## <a name=\"quickstart\">Quick Start Guide</a>\n-\n-* Build the GATK: `./gradlew bundle` (creates `gatk-VERSION.zip` in `build/`)\n-* Get help on running the GATK: `./gatk --help`\n-* Get a list of available tools: `./gatk --list`\n-* Run a tool: `./gatk PrintReads -I src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O output.bam`\n-* Get help on a particular tool: `./gatk PrintReads --help`\n-\n-## <a name=\"downloading\">Downloading GATK4</a>\n-\n-You can download and run pre-built versions of GATK4 from the following places:\n-\n-* A zip archive with everything you need to run GATK4 can be downloaded for each release from the [github releases page](https://github.com/broadinstitute/gatk/releases). We also host unstable archives generated nightly in the Google bucket gs://gatk-nightly-builds.\n-\n-* You can download a GATK4 docker image from [our dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk/). We also host unstable nightly development builds on [this dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk-nightly/).\n-    * Within the docker image, run gatk commands as usual from the default startup directory (/gatk).\n-\n-## <a name=\"building\">Building GATK4</a>\n-\n-* **To do a full build of GATK4, first clone the GATK repository using \"git clone\", then run:**\n-\n-        ./gradlew bundle\n-        \n-  Equivalently, you can just type:\n-  \n-        ./gradlew\n-        \n-    * This creates a zip archive in the `build/` directory with a name like `gatk-VERSION.zip` containing a complete standalone GATK distribution, including our launcher `gatk`, both the local and spark jars, and this README.    \n-    * You can also run GATK commands directly from the root of your git clone after running this command.\n-    * Note that you *must* have a full git clone in order to build GATK, including the git-lfs files in src/main/resources. The zipped source code alone is not buildable.\n-\n-* **Other ways to build:**\n-    * `./gradlew installDist`  \n-        * Does a *fast* build that only lets you run GATK tools from inside your git clone, and locally only (not on a cluster). Good for developers! \n-    * `./gradlew installAll`\n-        * Does a *semi-fast* build that only lets you run GATK tools from inside your git clone, but works both locally and on a cluster. Good for developers!\n-    * `./gradlew localJar`\n-        * Builds *only* the GATK jar used for running tools locally (not on a Spark cluster). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-local.jar`, and can be used outside of your git clone.\n-    * `./gradlew sparkJar`\n-        * Builds *only* the GATK jar used for running tools on a Spark cluster (rather than locally). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-spark.jar`, and can be used outside of your git clone. \n-        * This jar will not include Spark and Hadoop libraries, in order to allow the versions of Spark and Hadoop installed on your cluster to be used.\n-\n-* **To remove previous builds, run:** \n-\n-        ./gradlew clean\n-\n-* For faster gradle operations, add `org.gradle.daemon=true` to your `~/.gradle/gradle.properties` file.\n-  This will keep a gradle daemon running in the background and avoid the ~6s gradle start up time on every command.\n-\n-* Gradle keeps a cache of dependencies used to build GATK.  By default this goes in `~/.gradle`.  If there is insufficient free space in your home directory, you can change the location of the cache by setting the `GRADLE_USER_HOME` environment variable.\n-\n-* The version number is automatically derived from the git history using `git describe`, you can override it by setting the `versionOverride` property.\n-  ( `./gradlew -DversionOverride=my_weird_version printVersion` )\n-\n-## <a name=\"running\">Running GATK4</a>\n-\n-* The standard way to run GATK4 tools is via the **`gatk`** wrapper script located in the root directory of a clone of this repository.\n-    * Requires Python 2.6 or greater (this includes Python 3.x)\n-    * You need to have built the GATK as described in the [Building GATK4](#building) section above before running this script.\n-    * There are several ways `gatk` can be run:\n-        * Directly from the root of your git clone after building\n-        * By extracting the zip archive produced by `./gradlew bundle` to a directory, and running `gatk` from there\n-        * Manually putting the `gatk` script within the same directory as fully-packaged GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar`\n-        * Defining the environment variables `GATK_LOCAL_JAR` and `GATK_SPARK_JAR`, and setting them to the paths to the GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar` \n-    * `gatk` can run non-Spark tools as well as Spark tools, and can run Spark tools locally, on a Spark cluster, or on Google Cloud Dataproc.\n-    * ***Note:*** running with `java -jar` directly and bypassing `gatk` causes several important system properties to not get set, including htsjdk compression level!\n-    \n-* For help on using `gatk` itself, run **`./gatk --help`**\n-\n-* To print a list of available tools, run **`./gatk --list`**.\n-    * Spark-based tools will have a name ending in `Spark` (eg., `BaseRecalibratorSpark`). Most other tools are non-Spark-based.\n-\n-* To print help for a particular tool, run **`./gatk ToolName --help`**.\n-\n-* To run a non-Spark tool, or to run a Spark tool locally, the syntax is: **`./gatk ToolName toolArguments`**.\n-\n-* Tool arguments that allow multiple values, such as -I, can be supplied on the command line using a file with the extension \".args\". Each line of the file should contain a\n-  single value for the argument.\n-\n-* Examples:\n-\n-  ```\n-  ./gatk PrintReads -I input.bam -O output.bam\n-  ```\n-\n-  ```\n-  ./gatk PrintReadsSpark -I input.bam -O output.bam\n-  ```\n-\n-#### <a name=\"jvmoptions\">Passing JVM options to gatk</a>\n-\n-* To pass JVM arguments to GATK, run `gatk` with the `--java-options` argument: \n-\n-    ```\n-    ./gatk --java-options \"-Xmx4G\" <rest of command>\n-     \n-    ./gatk --java-options \"-Xmx4G -XX:+PrintGCDetails\" <rest of command>\n-    ```\n-#### <a name=\"configFileOptions\">Passing a configuration file to gatk</a>\n-\n-* To pass a configuration file to GATK, run `gatk` with the `--gatk-config-file` argument: \n-\n-\t```\n-\t./gatk --gatk-config-file GATKProperties.config <rest of command>\n-\t```\n-\n-\tAn example GATK configuration file is packaged with each release as `GATKConfig.EXAMPLE.properties`\n-\tThis example file contains all current options that are used by GATK and their default values.\n-\n-#### <a name=\"gcs\">Running GATK4 with inputs on Google Cloud Storage:</a>\n-\n-* Many GATK4 tools can read BAM or VCF inputs from a Google Cloud Storage bucket. Just use the \"gs://\" prefix:\n-  ```\n-  ./gatk PrintReads -I gs://mybucket/path/to/my.bam -L 1:10000-20000 -O output.bam\n-  ```\n-* ***Important:*** You must set up your credentials first for this to work! There are three options:\n-    * Option (a): run in a Google Cloud Engine VM\n-        * If you are running in a Google VM then your credentials are already in the VM and will be picked up by GATK, you don't need to do anything special.\n-    * Option (b): use your own account\n-        * Install [Google Cloud SDK](https://cloud.google.com/sdk/)\n-        * Log into your account:\n-        ```\n-        gcloud auth application-default login\n-        ```\n-        * Done! GATK will use the application-default credentials you set up there.\n-    * Option (c): use a service account\n-        * Create a new service account on the Google Cloud web page and download the JSON key file\n-        * Install [Google Cloud SDK](https://cloud.google.com/sdk/)\n-        * Tell gcloud about the key file:\n-        ```\n-        gcloud auth activate-service-account --key-file \"$PATH_TO_THE_KEY_FILE\"\n-        ```\n-        * Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the file\n-        ```\n-        export GOOGLE_APPLICATION_CREDENTIALS=\"$PATH_TO_THE_KEY_FILE\"\n-        ```\n-        * Done! GATK will pick up the service account. You can also do this in a VM if you'd like to override the default credentials.\n-\n-#### <a name=\"sparkcluster\">Running GATK4 Spark tools on a Spark cluster:</a>\n-\n-**`./gatk ToolName toolArguments -- --spark-runner SPARK --spark-master <master_url> additionalSparkArguments`**\n-* Examples:\n-\n-  ```\n-  ./gatk PrintReadsSpark -I hdfs://path/to/input.bam -O hdfs://path/to/output.bam \\\n-      -- \\\n-      --spark-runner SPARK --spark-master <master_url>\n-  ```\n-\n-    ```\n-    ./gatk PrintReadsSpark -I hdfs://path/to/input.bam -O hdfs://path/to/output.bam \\\n-      -- \\\n-      --spark-runner SPARK --spark-master <master_url> \\\n-      --num-executors 5 --executor-cores 2 --executor-memory 4g \\\n-      --conf spark.executor.memoryOverhead=600\n-    ```\n-\n-* You can also omit the \"--num-executors\" argument to enable [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) if you configure the cluster properly (see the Spark website for instructions).\n-* Note that the Spark-specific arguments are separated from the tool-specific arguments by a `--`.\n-* Running a Spark tool on a cluster requires Spark to have been installed from http://spark.apache.org/, since\n-   `gatk` invokes the `spark-submit` tool behind-the-scenes.\n-* Note that the examples above use YARN but we have successfully run GATK4 on Mesos as well.\n-\n-#### <a name=\"dataproc\">Running GATK4 Spark tools on Google Cloud Dataproc:</a>\n-  * You must have a [Google cloud services](https://cloud.google.com/) account, and have spun up a Dataproc cluster\n-    in the [Google Developer's console](https://console.developers.google.com). You may need to have the \"Allow API access to all Google Cloud services in the same project\" option enabled (settable when you create a cluster).\n-  * You need to have installed the Google Cloud SDK from [here](https://cloud.google.com/sdk/), since\n-    `gatk` invokes the `gcloud` tool behind-the-scenes. As part of the installation, be sure\n-      that you follow the `gcloud` setup instructions [here](https://cloud.google.com/sdk/gcloud/). As this library is frequently updated by Google, we recommend updating your copy regularly to avoid any version-related difficulties.\n-  * Your inputs to the GATK when running on dataproc are typically in Google Cloud Storage buckets, and should be specified on\n-    your GATK command line using the syntax `gs://my-gcs-bucket/path/to/my-file`\n-  * You can run GATK4 jobs on Dataproc from your local computer or from the VM (master node) on the cloud.\n-\n-  Once you're set up, you can run a Spark tool on your Dataproc cluster using a command of the form:\n-\n-  **`./gatk ToolName toolArguments -- --spark-runner GCS --cluster myGCSCluster additionalSparkArguments`**\n-\n-  * Examples:\n-\n-      ```      \n-      ./gatk PrintReadsSpark \\\n-          -I gs://my-gcs-bucket/path/to/input.bam \\\n-          -O gs://my-gcs-bucket/path/to/output.bam \\\n-          -- \\\n-          --spark-runner GCS --cluster myGCSCluster\n-      ```\n-\n-      ```\n-      ./gatk PrintReadsSpark \\\n-          -I gs://my-gcs-bucket/path/to/input.bam \\\n-          -O gs://my-gcs-bucket/path/to/output.bam \\\n-          -- \\\n-          --spark-runner GCS --cluster myGCSCluster \\\n-          --num-executors 5 --executor-cores 2 --executor-memory 4g \\\n-          --conf spark.yarn.executor.memoryOverhead=600\n-      ```\n-  * When using Dataproc you can access the web interfaces for YARN, Hadoop and HDFS by opening an SSH tunnel and connecting with your browser.  This can be done easily using included `gcs-cluster-ui` script.\n-  \n-    ```\n-    scripts/dataproc-cluster-ui myGCSCluster\n-    ```\n-    Or see these [these instructions](https://cloud.google.com/dataproc/cluster-web-interfaces) for more details.\n-  * Note that the spark-specific arguments are separated from the tool-specific arguments by a `--`.\n-  * If you want to avoid uploading the GATK jar to GCS on every run, set the `GATK_GCS_STAGING`\n-    environment variable to a bucket you have write access to (eg., `export GATK_GCS_STAGING=gs://<my_bucket>/`)\n-  * Dataproc Spark clusters are configured with [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) so you can omit the \"--num-executors\" argument and let YARN handle it automatically.\n-\n-#### <a name=\"R\">Using R to generate plots</a>\n-Certain GATK tools may optionally generate plots if R is installed.  We recommend **R v3.2.5** if you want to produce plots.  If you are uninterested in plotting, R is still required by several of the unit tests.  Plotting is currently untested and should be viewed as a convenience rather than a primary output.\n-\n-R installation is not part of the gradle build.  See http://cran.r-project.org/ for general information on installing R for your system.\n-* for ubuntu see these [ubuntu specific instructions](http://cran.r-project.org/bin/linux/ubuntu/README)\n-* for OSX we recommend installation through [homebrew](http://brew.sh/)\n-```\n-brew install R\n-```\n-\n-The plotting R scripts require certain R packages to be installed. You can install these by running `scripts/docker/gatkbase/install_R_packages.R`.  Either run it as superuser to force installation into the sites library or run interactively and create a local library.\n-```\n-sudo Rscript scripts/docker/gatkbase/install_R_packages.R\n-```\n-**or**\n-```\n-R \n-source(\"scripts/docker/gatkbase/install_R_packages.R\")\n-```\n-\n-#### <a name=\"tab_completion\">Bash Command-line Tab Completion (BETA)</a>\n-\n-* A tab completion bootstrap file for the bash shell is now included in releases.  This file allows the command-line shell to complete GATK run options in a manner equivalent to built-in command-line tools (e.g. grep).  \n-\n-* This tab completion functionality has only been tested in the bash shell, and is released as a beta feature.\n-\n-* To enable tab completion for the GATK, open a terminal window and source the included tab completion script:\n-\n-```\n-source gatk-completion.sh\n-```\n-\n-* Sourcing this file will allow you to press the tab key twice to get a list of options available to add to your current GATK command.  By default you will have to source this file once in each command-line session, then for the rest of the session the GATK tab completion functionality will be available.  GATK tab completion will be available in that current command-line session only.\n-\n-* Note that you must have already started typing an invocation of the GATK (using gatk) for tab completion to initiate:\n-\n-```\n-./gatk <TAB><TAB>\n-```\n-\n-* We recommend adding a line to your bash settings file (i.e. your ~/.bashrc file) that sources the tab completion script.  To add this line to your bash settings / bashrc file you can use the following command:\n-\n-```\n-echo \"source <PATH_TO>/gatk-completion.sh\" >> ~/.bashrc\n-```\n-\n-* Where ```<PATH_TO>``` is the fully qualified path to the ```gatk-completion.sh``` script.\n-\n-## <a name=\"developers\">For GATK Developers</a>\n-\n-#### <a name=\"dev_guidelines\">General guidelines for GATK4 developers</a>\n-\n-* **Do not put private or restricted data into the repo.**\n-\n-* **Try to keep datafiles under 100kb in size.** Larger test files should go into `src/test/resources/large` (and subdirectories) so that they'll be stored and tracked by git-lfs as described [above](#lfs).\n-\n-* GATK4 is BSD licensed.  The license is in the top level LICENSE.TXT file.  Do not add any additional license text or accept files with a license included in them.\n-\n-* Each tool should have at least one good end-to-end integration test with a check for expected output, plus high-quality unit tests for all non-trivial utility methods/classes used by the tool. Although we have no specific coverage target, coverage should be extensive enough that if tests pass, the tool is guaranteed to be in a usable state.\n-\n-* All newly written code must have good test coverage (>90%).\n-\n-* All bug fixes must be accompanied by a regression test.\n-\n-* All pull requests must be reviewed before merging to master (even documentation changes).\n-\n-* Don't issue or accept pull requests that introduce warnings. Warnings must be addressed or suppressed.\n-\n-* Don't issue or accept pull requests that significantly decrease coverage (less than 1% decrease is sort of tolerable). \n-\n-* Don't use `toString()` for anything other than human consumption (ie. don't base the logic of your code on results of `toString()`.)\n-\n-* Don't override `clone()` unless you really know what you're doing. If you do override it, document thoroughly. Otherwise, prefer other means of making copies of objects.\n-\n-* For logging, use [org.apache.logging.log4j.Logger](https://logging.apache.org/log4j/2.0/log4j-api/apidocs/org/apache/logging/log4j/Logger.html)\n-\n-* We mostly follow the [Google Java Style guide](https://google.github.io/styleguide/javaguide.html)\n-\n-* Git: Don't push directly to master - make a pull request instead. \n-\n-* Git: Rebase and squash commits when merging.\n-\n-* If you push to master or mess up the commit history, you owe us 1 growler or tasty snacks at happy hour. If you break the master build, you owe 3 growlers (or lots of tasty snacks). Beer may be replaced by wine (in the color and vintage of buyer's choosing) in proportions of 1 growler = 1 bottle. \n-\n-#### <a name=\"testing\">Testing GATK</a>\n-\n-* Before running the test suite, be sure that you've installed `git lfs` and downloaded the large test data, following the [git lfs setup instructions](#lfs)\n-\n-* To run the test suite, run **`./gradlew test`**.\n-    * Test report is in `build/reports/tests/test/index.html`.\n-    * What will happen depends on the value of the `TEST_TYPE` environment variable: \n-       * unset or any other value         : run non-cloud unit and integration tests, this is the default\n-       * `cloud`, `unit`, `integration`, `spark`, `python`   : run only the cloud, unit, integration, python, or Spark tests\n-       * `all`                            : run the entire test suite\n-    * Cloud tests require being logged into `gcloud` and authenticated with a project that has access\n-      to the cloud test data.  They also require setting several certain environment variables.\n-      * `HELLBENDER_JSON_SERVICE_ACCOUNT_KEY` : path to a local JSON file with [service account credentials](https://cloud.google.com/storage/docs/authentication#service_accounts) \n-      * `HELLBENDER_TEST_PROJECT` : your google cloud project \n-      * `HELLBENDER_TEST_STAGING` : a gs:// path to a writable location\n-      * `HELLBENDER_TEST_INPUTS` : path to cloud test data, ex: gs://hellbender/test/resources/ \n-    * Setting the environment variable `TEST_VERBOSITY=minimal` will produce much less output from the test suite \n-\n-* To run a subset of tests, use gradle's test filtering (see [gradle doc](https://docs.gradle.org/current/userguide/java_plugin.html)):\n-    * You can use `test.single` when you just want to run a specific test class:\n-        * `./gradlew test -Dtest.single=SomeSpecificTestClass`\n-    * You can also use `--tests` with a wildcard to run a specific test class, method, or to select multiple test classes:\n-        * `./gradlew test --tests *SomeSpecificTestClass`\n-        * `./gradlew test --tests *SomeTest.someSpecificTestMethod`\n-        * `./gradlew test --tests all.in.specific.package*`\n-\n-* To run tests and compute coverage reports, run **`./gradlew jacocoTestReport`**. The report is then in `build/reports/jacoco/test/html/index.html`.\n-  (IntelliJ has a good coverage tool that is preferable for development).\n-\n-* We use [Travis-CI](https://travis-ci.org/broadinstitute/gatk) as our continuous integration provider.\n-\n-    * Before merging any branch make sure that all required tests pass on travis.\n-    * Every travis build will upload the test results to our GATK Google Cloud Storage bucket.\n-      A link to the uploaded report will appear at the very bottom of the travis log.\n-      Look for the line that says `See the test report at`.\n-      If TestNG itself crashes there will be no report generated.\n-\n-* We use [Broad Jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) for our long-running tests and performance tests.\n-    * To add a performance test (requires Broad-ID), you need to make a \"new item\" in Jenkins and make it a \"copy\" instead of a blank project. You need to base it on either the \"-spark-\" jobs or the other kind of jobs and alter the commandline. \n-\n-* To output stack traces for `UserException` set the environment variable `GATK_STACKTRACE_ON_USER_EXCEPTION=true`\n-\n-#### <a name=\"lfs\">Using Git LFS to download and track large test data</a>\n-\n-We use [git-lfs](https://git-lfs.github.com/) to version and distribute test data that is too large to check into our repository directly. You must install and configure it in order to be able to run our test suite.\n-\n-* After installing [git-lfs](https://git-lfs.github.com/), run `git lfs install`\n-    * This adds hooks to your git configuration that will cause git-lfs files to be checked out for you automatically in the future.\n-    \n-* To manually retrieve the large test data, run `git lfs pull` from the root of your GATK git clone.\n-    * The download is several hundred megabytes.\n-    \n-* To add a new large file to be tracked by git-lfs, simply:\n-    * Put the new file(s) in `src/test/resources/large` (or a subdirectory)\n-    * `git add` the file(s), then `git commit -a`\n-    * That's it! Do ***not*** run `git lfs track` on the files manually: all files in `src/test/resources/large` are tracked by git-lfs automatically. \n-\n-#### <a name=\"intellij\">Creating a GATK project in the IntelliJ IDE (last tested with version 2016.2.4):</a>\n-\n-* Ensure that you have `gradle` and the Java 8 JDK installed\n-\n-* You may need to install the TestNG and Gradle plugins (in preferences)\n-\n-* Clone the GATK repository using git\n-\n-* In IntelliJ, click on \"Import Project\" in the home screen or go to File -> New... -> Project From Existing Sources...\n-\n-* Select the root directory of your GATK clone, then click on \"OK\"\n-\n-* Select \"Import project from external model\", then \"Gradle\", then click on \"Next\"\n-\n-* Ensure that \"Gradle project\" points to the build.gradle file in the root of your GATK clone\n-\n-* Select \"Use auto-import\" and \"Use default gradle wrapper\".\n-\n-* Make sure the Gradle JVM points to Java 1.8. You may need to set this manually after creating the project, to do so find the gradle settings by clicking the wrench icon in the gradle tab on the right bar, from there edit \"Gradle JVM\" argument to point to Java 1.8.\n-\n-* Click \"Finish\"\n-\n-* After downloading project dependencies, IntelliJ should open a new window with your GATK project\n-\n-* Make sure that the Java version is set correctly by going to File -> \"Project Structure\" -> \"Project\". Check that the \"Project SDK\" is set to your Java 1.8 JDK, and \"Project language level\" to 8 (you may need to add your Java 8 JDK under \"Platform Settings\" -> SDKs if it isn't there already). Then click \"Apply\"/\"Ok\".\n-\n-#### <a name=\"debugging\">Setting up debugging in IntelliJ</a>\n-\n-* Follow the instructions above for creating an IntelliJ project for GATK\n-\n-* Go to Run -> \"Edit Configurations\", then click \"+\" and add a new \"Application\" configuration\n-\n-* Set the name of the new configuration to something like \"GATK debug\"\n-\n-* For \"Main class\", enter `org.broadinstitute.hellbender.Main`\n-\n-* Ensure that \"Use classpath of module:\" is set to use the \"gatk\" module's classpath\n-\n-* Enter the arguments for the command you want to debug in \"Program Arguments\"\n-\n-* Click \"Apply\"/\"Ok\"\n-\n-* Set breakpoints, etc., as desired, then select \"Run\" -> \"Debug\" -> \"GATK debug\" to start your debugging session\n-\n-* In future debugging sessions, you can simply adjust the \"Program Arguments\" in the \"GATK debug\" configuration as needed\n-\n-#### <a name=\"intellij_gradle_refresh\">Updating the Intellij project when dependencies change</a>\n-If there are dependency changes in `build.gradle` it is necessary to refresh the gradle project. This is easily done with the following steps.\n-\n-* Open the gradle tool window  ( \"View\" -> \"Tool Windows\" -> \"Gradle\" )\n-* Click the refresh button in the Gradle tool window.  It is in the top left of the gradle view and is represented by two blue arrows.\n-\n-#### <a name=\"jprofiler\">Setting up profiling using JProfiler</a>\n-\n-   * Running JProfiler standalone:\n-       * Build a full GATK4 jar using `./gradlew localJar`\n-       * In the \"Session Settings\" window, select the GATK4 jar, eg. `~/gatk/build/libs/gatk-package-4.alpha-196-gb542813-SNAPSHOT-local.jar` for \"Main class or executable JAR\" and enter the right \"Arguments\"\n-       * Under \"Profiling Settings\", select \"sampling\" as the \"Method call recording\" method.\n-\n-   * Running JProfiler from within IntelliJ:\n-       * JProfiler has great integration with IntelliJ (we're using IntelliJ Ultimate edition) so the setup is trivial.   \n-       * Follow the instructions [above](#intellij) for creating an IntelliJ project for GATK  \n-       * Right click on a test method/class/package and select \"Profile\" \n-\n-#### <a name=\"sonatype\">Uploading Archives to Sonatype (to make them available via maven central)</a>\n-To upload snapshots to Sonatype you'll need the following:\n-\n-* You must have a registered account on the sonatype JIRA (and be approved as a gatk uploader)\n-* You need to configure several additional properties in your `/~.gradle/gradle.properties` file\n-\n-* If you want to upload a release instead of a snapshot you will additionally need to have access to the gatk signing key and password\n-\n-```\n-#needed for snapshot upload\n-sonatypeUsername=<your sonatype username>\n-sonatypePassword=<your sonatype password>\n-\n-#needed for signing a release\n-signing.keyId=<gatk key id>\n-signing.password=<gatk key password>\n-signing.secretKeyRingFile=/Users/<username>/.gnupg/secring.gpg\n-```\n-\n-To perform an upload, use\n-```\n-./gradlew uploadArchives\n-```\n-\n-Builds are considered snapshots by default.  You can mark a build as a release build by setting `-Drelease=true`.  \n-The archive name is based off of `git describe`.\n-\n-#### <a name=\"docker_building\">Building GATK4 Docker images</a>\n-\n-Please see the [the Docker README](scripts/docker/README.md) in ``scripts/docker``.  This has instructions for the Dockerfile in the root directory.\n-\n-#### <a name=\"releasing_gatk\">Releasing GATK4</a>\n-\n-Please see the [How to release GATK4](https://github.com/broadinstitute/gatk/wiki/How-to-release-GATK4) wiki article for instructions on releasing GATK4.\n-\n-#### <a name=\"gatkdocs\">Generating GATK4 documentation</a>\n-\n-To generate GATK documentation, run `./gradlew gatkDoc`\n-\n-* Generated docs will be in the `build/docs/gatkdoc` directory.\n-\n-#### <a name=\"zenhub\">Using Zenhub to track github issues</a>\n-\n-We use [Zenhub](https://www.zenhub.com/) to organize and track github issues.\n-\n-* To add Zenhub to github, go to the [Zenhub home page](https://www.zenhub.com/) while logged in to github, and click \"Add Zenhub to Github\"\n-\n-* Zenhub allows the GATK development team to assign time estimates to issues, and to mark issues as Triaged/In Progress/In Review/Blocked/etc.\n-\n-## <a name=\"spark_further_reading\">Further Reading on Spark</a>\n-\n-[Apache Spark](https://spark.apache.org/) is a fast and general engine for large-scale data processing.\n-GATK4 can run on any Spark cluster, such as an on-premise Hadoop cluster with HDFS storage and the Spark\n-runtime, as well as on the cloud using Google Dataproc.\n-\n-In a cluster scenario, your input and output files reside on HDFS, and Spark will run in a distributed fashion on the cluster.\n-The Spark documentation has a good [overview of the architecture](https://spark.apache.org/docs/latest/cluster-overview.html).\n-\n-Note that if you don't have a dedicated cluster you can run Spark in\n-[standalone mode](https://spark.apache.org/docs/latest/spark-standalone.html) on a single machine, which exercises\n-the distributed code paths, albeit on a single node.\n-\n-While your Spark job is running, the [Spark UI](http://spark.apache.org/docs/latest/monitoring.html) is an excellent place to monitor the  progress.\n-Additionally, if you're running tests, then by adding `-Dgatk.spark.debug=true` you can run a single Spark test and\n-look at the Spark UI (on [http://localhost:4040/](http://localhost:4040/)) as it runs.\n-\n-You can find more information about tuning Spark and choosing good values for important settings such as the number\n-of executors and memory settings at the following:\n-\n-* [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html)\n-* [How-to: Tune Your Apache Spark Jobs (Part 1)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)\n-* [How-to: Tune Your Apache Spark Jobs (Part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n-\n-## <a name=\"contribute\">How to contribute to GATK</a>\n-(Note: section inspired by, and some text copied from, [Apache Parquet](https://github.com/apache/parquet-mr))\n- \n-We welcome all contributions to the GATK project. The contribution can be a [issue report]( https://github.com/broadinstitute/gatk/issues) \n-or a [pull request](https://github.com/broadinstitute/gatk/pulls). If you're not a committer, you will \n-need to [make a fork](https://help.github.com/articles/fork-a-repo/) of the gatk repository \n-and [issue a pull request](https://help.github.com/articles/be-social/) from your fork.\n-\n-For ideas on what to contribute, check issues labeled [\"Help wanted (Community)\"](https://github.com/broadinstitute/gatk/issues?q=is%3Aopen+is%3Aissue+label%3A%22Help+Wanted+%28Community%29%22). Comment on the issue to indicate you're interested in contibuting code and for sharing your questions and ideas.\n-\n-To contribute a patch:\n-* Break your work into small, single-purpose patches if possible. It\u2019s much harder to merge in a large change with a lot of disjoint features.\n-* Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on [forking a repo](https://help.github.com/articles/fork-a-repo/) and [sending a pull request](https://help.github.com/articles/be-social/). If applicable, include the issue number in the pull request name.\n-* Make sure that your code passes all our tests. You can run the tests with `./gradlew test` in the root directory.\n-* Add tests for all new code you've written. We prefer unit tests but high quality integration tests that use small amounts of data are acceptable.\n-* Follow the [**General guidelines for GATK4 developers**](https://github.com/broadinstitute/gatk#general-guidelines-for-gatk4-developers).\n-\n-We tend to do fairly close readings of pull requests, and you may get a lot of comments. Some things to consider:\n-* Write tests for all new code.\n-* Document all classes and public methods.\n-* For all public methods, check validity of the arguments and throw `IllegalArgumentException` if invalid.\n-* Use braces for control constructs, `if`, `for` etc.\n-* Make classes, variables, parameters etc `final` unless there is a strong reason not to.\n-* Give your operators some room. Not `a+b` but `a + b` and not `foo(int a,int b)` but `foo(int a, int b)`.\n-* Generally speaking, stick to the [Google Java Style guide](https://google.github.io/styleguide/javaguide.html)\n-\n-Thank you for getting involved!\n-\n-## <a name=\"discussions\">Discussions</a>\n-* [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics) for general discussions on how to use the GATK and support questions.\n-* [Issue tracker](https://github.com/broadinstitute/gatk/issues) to report errors and enhancement ideas. \n-* Discussions also take place in [GATK pull requests](https://github.com/broadinstitute/gatk/pulls)\n-\n-## <a name=\"authors\">Authors</a>\n-The authors list is maintained in the [AUTHORS](https://github.com/broadinstitute/gatk/edit/master/AUTHORS) file. \n-See also the [Contributors](https://github.com/broadinstitute/gatk/graphs/contributors) list at github. \n-\n-## <a name=\"license\">License</a>\n-Licensed under the BSD License. See the [LICENSE.txt](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT) file.\n+Python packages to be used in the GATK should be placed in this directory.  \n+Each package should be contained in its own subdirectory.  If the package \n+can be installed as a standalone package, a corresponding `setup_<PACKAGE_NAME>.py` \n+file may be placed in this directory.  However, during creation of the common \n+GATK conda environment, all packages will be combined and pip-installed as a \n+single package named ``gatkpythonpackages`` by `setup.py`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 626}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0MjQ3OQ==", "bodyText": "Looks like a bracket typo", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422242479", "createdAt": "2020-05-08T16:32:47Z", "author": {"login": "mwalker174"}, "path": "scripts/cnv_wdl/germline/joint_call_exome_cnvs.wdl", "diffHunk": "@@ -0,0 +1,261 @@\n+version 1.0\n+\n+workflow JointCallExomeCNVs {\n+\n+    ##################################\n+    #### required basic arguments ####\n+    ##################################\n+    input {\n+      File intervals\n+      File? blacklist_intervals\n+      Array[File]+ segments_vcfs\n+      Array[File]+ segments_vcf_indexes\n+      Array[File]+ intervals_vcf\n+      Array[File]+ intervals_vcf_indexes\n+      Array[Array[File]] gcnv_calls_tars\n+      Array[File] gcnv_model_tars\n+      Array[File] calling_configs\n+      Array[File] denoising_configs\n+      Array[File] gcnvkernel_version\n+      Array[File] sharded_interval_lists\n+      File contig_ploidy_calls_tar\n+      Array[String]? allosomal_contigs\n+      Int ref_copy_number_autosomal_contigs\n+      File ref_fasta_dict\n+      File ref_fasta_fai\n+      File ref_fasta\n+      String gatk_docker\n+    }\n+\n+    call JointSegmentation {\n+      input:\n+        segments_vcfs = segments_vcfs,\n+        segments_vcf_indexes = segments_vcf_indexes,\n+        ref_fasta = ref_fasta,\n+        ref_fasta_fai = ref_fasta_fai,\n+        ref_fasta_dict = ref_fasta_dict,\n+        gatk_docker = gatk_docker\n+    }\n+\n+    Array[Array[File] ]gcnv_calls_tars_T = transpose(gcnv_calls_tars)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0NjExMg==", "bodyText": "Unfortunate that we haven't ported this to gatk4 yet. I would suggest bcftools merge but I'll leave this up to you.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422246112", "createdAt": "2020-05-08T16:39:54Z", "author": {"login": "mwalker174"}, "path": "scripts/cnv_wdl/germline/joint_call_exome_cnvs.wdl", "diffHunk": "@@ -0,0 +1,261 @@\n+version 1.0\n+\n+workflow JointCallExomeCNVs {\n+\n+    ##################################\n+    #### required basic arguments ####\n+    ##################################\n+    input {\n+      File intervals\n+      File? blacklist_intervals\n+      Array[File]+ segments_vcfs\n+      Array[File]+ segments_vcf_indexes\n+      Array[File]+ intervals_vcf\n+      Array[File]+ intervals_vcf_indexes\n+      Array[Array[File]] gcnv_calls_tars\n+      Array[File] gcnv_model_tars\n+      Array[File] calling_configs\n+      Array[File] denoising_configs\n+      Array[File] gcnvkernel_version\n+      Array[File] sharded_interval_lists\n+      File contig_ploidy_calls_tar\n+      Array[String]? allosomal_contigs\n+      Int ref_copy_number_autosomal_contigs\n+      File ref_fasta_dict\n+      File ref_fasta_fai\n+      File ref_fasta\n+      String gatk_docker\n+    }\n+\n+    call JointSegmentation {\n+      input:\n+        segments_vcfs = segments_vcfs,\n+        segments_vcf_indexes = segments_vcf_indexes,\n+        ref_fasta = ref_fasta,\n+        ref_fasta_fai = ref_fasta_fai,\n+        ref_fasta_dict = ref_fasta_dict,\n+        gatk_docker = gatk_docker\n+    }\n+\n+    Array[Array[File] ]gcnv_calls_tars_T = transpose(gcnv_calls_tars)\n+\n+    scatter (scatter_index in range(length(segments_vcfs))) {\n+      call PostprocessGermlineCNVCalls as RecalcQual {\n+        input:\n+              entity_id = sub(sub(intervals_vcf[scatter_index], \".vcf.gz\", \"\"), \"intervals_output_\", \"\"),\n+              gcnv_calls_tars = gcnv_calls_tars_T[scatter_index],\n+              gcnv_model_tars = gcnv_model_tars,\n+              calling_configs = calling_configs,\n+              denoising_configs = denoising_configs,\n+              gcnvkernel_version = gcnvkernel_version,\n+              sharded_interval_lists = sharded_interval_lists,\n+              contig_ploidy_calls_tar = contig_ploidy_calls_tar,\n+              allosomal_contigs = allosomal_contigs,\n+              ref_copy_number_autosomal_contigs = ref_copy_number_autosomal_contigs,\n+              sample_index = scatter_index,\n+              intervals_vcf = intervals_vcf[scatter_index],\n+              intervals_vcf_index = intervals_vcf_indexes[scatter_index],\n+              clustered_vcf = JointSegmentation.clustered_vcf,\n+              clustered_vcf_index = JointSegmentation.clustered_vcf_index,\n+              gatk_docker = gatk_docker\n+      }\n+    }\n+\n+    call CombineVariants {\n+      input:\n+        input_vcfs = RecalcQual.genotyped_segments_vcf,\n+        ref_fasta = ref_fasta,\n+        ref_fasta_fai = ref_fasta_fai,\n+        ref_fasta_dict = ref_fasta_dict\n+    }\n+    output {\n+      File combined_calls = CombineVariants.combined_vcf\n+      File combined_calls_index = CombineVariants.combined_vcf_index\n+    }\n+}\n+\n+task JointSegmentation {\n+  input {\n+    Array[File] segments_vcfs\n+    Array[File] segments_vcf_indexes\n+    File ref_fasta_dict\n+    File ref_fasta_fai\n+    File ref_fasta\n+\n+     # Runtime parameters\n+    String gatk_docker\n+    Int? mem_gb\n+    Int? disk_space_gb\n+    Boolean use_ssd = false\n+    Int? cpu\n+    Int? preemptible_attempts\n+    }\n+\n+    parameter_meta {\n+      segments_vcfs: {localization_optional: true}\n+      segments_vcf_indexes: {localization_optional: true}\n+    }\n+\n+    Int machine_mem_mb = select_first([mem_gb, 2]) * 1000\n+    Int command_mem_mb = machine_mem_mb - 500\n+\n+  #NOTE: output has to be gzipped to be read in by pyvcf in the next step\n+  command <<<\n+    set -e\n+    gatk --java-options \"-Xmx~{command_mem_mb}m\" JointCNVSegmentation \\\n+    -R ~{ref_fasta} -O clustered.vcf.gz -V ~{sep=' -V ' segments_vcfs} --disable-sequence-dictionary-validation\n+    >>>\n+\n+    output {\n+      File clustered_vcf = \"clustered.vcf.gz\"\n+      File clustered_vcf_index = \"clustered.vcf.gz.tbi\"\n+    }\n+\n+    runtime {\n+      docker: gatk_docker\n+      memory: machine_mem_mb + \" MB\"\n+      disks: \"local-disk \" + select_first([disk_space_gb, 40]) + if use_ssd then \" SSD\" else \" HDD\"\n+      cpu: select_first([cpu, 1])\n+      preemptible: select_first([preemptible_attempts, 2])\n+    }\n+}\n+\n+task CombineVariants {\n+  input {\n+    Array[File] input_vcfs\n+    File ref_fasta\n+    File ref_fasta_fai\n+    File ref_fasta_dict\n+    Int? preemptible_tries\n+    Int? disk_size\n+  }\n+\n+  command <<<\n+    java -jar -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xms2000m \\\n+          -jar /usr/gitc/GATK35.jar \\\n+          -T CombineVariants -R ~{ref_fasta} \\\n+          -o combined.vcf", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0NjYyMw==", "bodyText": "No longer - can you import this now?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422246623", "createdAt": "2020-05-08T16:40:53Z", "author": {"login": "mwalker174"}, "path": "scripts/cnv_wdl/germline/joint_call_exome_cnvs.wdl", "diffHunk": "@@ -0,0 +1,261 @@\n+version 1.0\n+\n+workflow JointCallExomeCNVs {\n+\n+    ##################################\n+    #### required basic arguments ####\n+    ##################################\n+    input {\n+      File intervals\n+      File? blacklist_intervals\n+      Array[File]+ segments_vcfs\n+      Array[File]+ segments_vcf_indexes\n+      Array[File]+ intervals_vcf\n+      Array[File]+ intervals_vcf_indexes\n+      Array[Array[File]] gcnv_calls_tars\n+      Array[File] gcnv_model_tars\n+      Array[File] calling_configs\n+      Array[File] denoising_configs\n+      Array[File] gcnvkernel_version\n+      Array[File] sharded_interval_lists\n+      File contig_ploidy_calls_tar\n+      Array[String]? allosomal_contigs\n+      Int ref_copy_number_autosomal_contigs\n+      File ref_fasta_dict\n+      File ref_fasta_fai\n+      File ref_fasta\n+      String gatk_docker\n+    }\n+\n+    call JointSegmentation {\n+      input:\n+        segments_vcfs = segments_vcfs,\n+        segments_vcf_indexes = segments_vcf_indexes,\n+        ref_fasta = ref_fasta,\n+        ref_fasta_fai = ref_fasta_fai,\n+        ref_fasta_dict = ref_fasta_dict,\n+        gatk_docker = gatk_docker\n+    }\n+\n+    Array[Array[File] ]gcnv_calls_tars_T = transpose(gcnv_calls_tars)\n+\n+    scatter (scatter_index in range(length(segments_vcfs))) {\n+      call PostprocessGermlineCNVCalls as RecalcQual {\n+        input:\n+              entity_id = sub(sub(intervals_vcf[scatter_index], \".vcf.gz\", \"\"), \"intervals_output_\", \"\"),\n+              gcnv_calls_tars = gcnv_calls_tars_T[scatter_index],\n+              gcnv_model_tars = gcnv_model_tars,\n+              calling_configs = calling_configs,\n+              denoising_configs = denoising_configs,\n+              gcnvkernel_version = gcnvkernel_version,\n+              sharded_interval_lists = sharded_interval_lists,\n+              contig_ploidy_calls_tar = contig_ploidy_calls_tar,\n+              allosomal_contigs = allosomal_contigs,\n+              ref_copy_number_autosomal_contigs = ref_copy_number_autosomal_contigs,\n+              sample_index = scatter_index,\n+              intervals_vcf = intervals_vcf[scatter_index],\n+              intervals_vcf_index = intervals_vcf_indexes[scatter_index],\n+              clustered_vcf = JointSegmentation.clustered_vcf,\n+              clustered_vcf_index = JointSegmentation.clustered_vcf_index,\n+              gatk_docker = gatk_docker\n+      }\n+    }\n+\n+    call CombineVariants {\n+      input:\n+        input_vcfs = RecalcQual.genotyped_segments_vcf,\n+        ref_fasta = ref_fasta,\n+        ref_fasta_fai = ref_fasta_fai,\n+        ref_fasta_dict = ref_fasta_dict\n+    }\n+    output {\n+      File combined_calls = CombineVariants.combined_vcf\n+      File combined_calls_index = CombineVariants.combined_vcf_index\n+    }\n+}\n+\n+task JointSegmentation {\n+  input {\n+    Array[File] segments_vcfs\n+    Array[File] segments_vcf_indexes\n+    File ref_fasta_dict\n+    File ref_fasta_fai\n+    File ref_fasta\n+\n+     # Runtime parameters\n+    String gatk_docker\n+    Int? mem_gb\n+    Int? disk_space_gb\n+    Boolean use_ssd = false\n+    Int? cpu\n+    Int? preemptible_attempts\n+    }\n+\n+    parameter_meta {\n+      segments_vcfs: {localization_optional: true}\n+      segments_vcf_indexes: {localization_optional: true}\n+    }\n+\n+    Int machine_mem_mb = select_first([mem_gb, 2]) * 1000\n+    Int command_mem_mb = machine_mem_mb - 500\n+\n+  #NOTE: output has to be gzipped to be read in by pyvcf in the next step\n+  command <<<\n+    set -e\n+    gatk --java-options \"-Xmx~{command_mem_mb}m\" JointCNVSegmentation \\\n+    -R ~{ref_fasta} -O clustered.vcf.gz -V ~{sep=' -V ' segments_vcfs} --disable-sequence-dictionary-validation\n+    >>>\n+\n+    output {\n+      File clustered_vcf = \"clustered.vcf.gz\"\n+      File clustered_vcf_index = \"clustered.vcf.gz.tbi\"\n+    }\n+\n+    runtime {\n+      docker: gatk_docker\n+      memory: machine_mem_mb + \" MB\"\n+      disks: \"local-disk \" + select_first([disk_space_gb, 40]) + if use_ssd then \" SSD\" else \" HDD\"\n+      cpu: select_first([cpu, 1])\n+      preemptible: select_first([preemptible_attempts, 2])\n+    }\n+}\n+\n+task CombineVariants {\n+  input {\n+    Array[File] input_vcfs\n+    File ref_fasta\n+    File ref_fasta_fai\n+    File ref_fasta_dict\n+    Int? preemptible_tries\n+    Int? disk_size\n+  }\n+\n+  command <<<\n+    java -jar -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xms2000m \\\n+          -jar /usr/gitc/GATK35.jar \\\n+          -T CombineVariants -R ~{ref_fasta} \\\n+          -o combined.vcf\n+    >>>\n+\n+  runtime {\n+    docker: \"us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.4.3-1564508330\"\n+    preemptible: select_first([preemptible_tries, 2])\n+    memory: \"3.5 GiB\"\n+    cpu: \"1\"\n+    disks: \"local-disk \" + select_first([disk_size, 50]) + \" HDD\"\n+  }\n+\n+  output {\n+    File combined_vcf = \"combined.vcf\"\n+    File combined_vcf_index = \"combined.vcf.idx\"\n+  }\n+}\n+\n+#copied here instead of imported because common tasks are still on draft-3", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0NzA0Ng==", "bodyText": "Are these changes intentional?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422247046", "createdAt": "2020-05-08T16:41:41Z", "author": {"login": "mwalker174"}, "path": "scripts/gatkcondaenv.yml.template", "diffHunk": "@@ -4,17 +4,14 @@ name: $condaEnvName\n channels:\n - defaults\n dependencies:\n-- certifi=2016.2.28=py36_0\n-- intel-openmp=2018.0.0\n - mkl=2018.0.1\n - mkl-service=1.1.2\n-- openssl=1.0.2l=0\n - pip=9.0.1=py36_1\n - python=3.6.2=0\n - readline=6.2=2\n - setuptools=36.4.0=py36_1\n - sqlite=3.13.0=0\n-- anaconda::tensorflow=1.12.0=mkl_py36h69b6ba0_0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0OTk5MA==", "bodyText": "Unfortunate that it's come to this, but I suppose this is our best option at this point given all of the gCNV vcfs that have been generated already. Alternatively we could implement a task to detect missing sequence dictionaries and fix the headers. @droazen Any concerns?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422249990", "createdAt": "2020-05-08T16:47:32Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private CloseableIterator<VariantContext> currentIterator;\n     private SortedSet<String> mergedSamples;\n \n+    private boolean skipDictionaryValidation = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI1NTMwOA==", "bodyText": "These aren't needed are they?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422255308", "createdAt": "2020-05-08T16:57:13Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/utils/GATKSVVCFConstants.java", "diffHunk": "@@ -74,6 +74,14 @@\n     public static final String DUP_TAN_EXPANSION_INTERNAL_ID_START_STRING = \"INS-DUPLICATION-TANDEM-EXPANSION\";\n     public static final String DUP_INV_INTERNAL_ID_START_STRING = \"INS-DUPLICATION-INVERTED-EXPANSION\";\n \n+    // for breakpoint segmentation\n+    public static final String ALGORITHMS_ATTRIBUTE = \"ALGORITHMS\";\n+    public static final String STRANDS_ATTRIBUTE = \"STRANDS\";\n+    public static final String DEPTH_ALGORITHM = \"depth\";\n+    public static final String END_CONTIG_ATTRIBUTE = \"CHR2\";\n+    public static String START_SPLIT_READ_COUNT_ATTRIBUTE = \"SSR\";\n+    public static String END_SPLIT_READ_COUNT_ATTRIBUTE = \"ESR\";\n+    public static String DISCORDANT_PAIR_COUNT_ATTRIBUTE = \"PE\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMxNDYyMw==", "bodyText": "This seems to do a lot of unneeded work if the compared objects are not equal. I think you should short-circuit the checks like this:\nif (!this.getContig().equals(b.getContig())) return false;\nif (this.getStart() != b.getStart()) return false;\n...\n\nIt's a little less readable but IMO the efficiency costs are worth it.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422314623", "createdAt": "2020-05-08T18:56:38Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;\n+        return new SVCallRecord(startContig, start, startStrand, startContig, end, endStrand, type, length, algorithms, passing);\n+    }\n+\n+    public SVCallRecord(final String startContig,\n+                        final int start,\n+                        final boolean startStrand,\n+                        final String endContig,\n+                        final int end,\n+                        final boolean endStrand,\n+                        final StructuralVariantType type,\n+                        final int length,\n+                        final List<String> algorithms,\n+                        final List<Genotype> genotypes) {\n+        Utils.nonNull(startContig);\n+        Utils.nonNull(endContig);\n+        Utils.nonNull(type);\n+        Utils.nonNull(algorithms);\n+        Utils.nonNull(genotypes);\n+        Utils.nonEmpty(algorithms);\n+        Utils.nonEmpty(genotypes);\n+        Utils.containsNoNull(algorithms, \"Encountered null algorithm\");\n+        Utils.containsNoNull(genotypes, \"Encountered null genotype\");\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+        this.type = type;\n+        this.length = length;\n+        this.algorithms = algorithms;\n+        this.genotypes = genotypes;\n+        this.samples = genotypes.stream()\n+                .filter(Genotype::isCalled)\n+                .map(Genotype::getSampleName)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+    }\n+\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        return end;\n+    }\n+\n+    public boolean getEndStrand() {\n+        return endStrand;\n+    }\n+\n+    public StructuralVariantType getType() {\n+        return type;\n+    }\n+\n+    public int getLength() {\n+        return length;\n+    }\n+\n+    public List<String> getAlgorithms() {\n+        return algorithms;\n+    }\n+\n+    public Set<String> getSamples() {\n+        return samples;\n+    }\n+\n+    public List<Genotype> getGenotypes() {\n+        return genotypes;\n+    }\n+\n+    public SimpleInterval getStartAsInterval() {\n+        return new SimpleInterval(startContig, start, start + 1);\n+    }\n+\n+    public SimpleInterval getEndAsInterval() {\n+        return new SimpleInterval(endContig, end, end + 1);\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {\n+        if (obj == null) {\n+            return false;\n+        }\n+        if (this.getClass() != obj.getClass()) {\n+            return false;\n+        }\n+        final SVCallRecord b = (SVCallRecord) obj;\n+        boolean areEqual = this.getContig().equals(b.getContig());\n+        areEqual &= this.getStart() == b.getStart();\n+        areEqual &= this.getStartStrand() == b.getStartStrand();\n+\n+        areEqual &= this.getEndContig() == b.getEndContig();\n+        areEqual &= this.getEnd() == b.getEnd();\n+        areEqual &= this.getEndStrand() == b.getEndStrand();\n+\n+        areEqual &= this.getType() == b.getType();\n+        areEqual &= this.getLength() == b.getLength();\n+\n+        areEqual &= this.getAlgorithms().containsAll(b.getAlgorithms());\n+        areEqual &= b.getAlgorithms().containsAll(this.getAlgorithms());\n+\n+        areEqual &= this.getSamples().containsAll(b.getSamples());\n+        areEqual &= b.getSamples().containsAll(this.getSamples());\n+\n+        areEqual &= this.getGenotypes().containsAll(b.getGenotypes());\n+        areEqual &= b.getGenotypes().containsAll(this.getGenotypes());\n+\n+        return areEqual;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyNjE2OQ==", "bodyText": "Let's go with the GATK convention", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422326169", "createdAt": "2020-05-08T19:20:35Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODYzMw=="}, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyODA1OQ==", "bodyText": "I'm fine with merging this as is, but we should open a ticket to keep track of this", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422328059", "createdAt": "2020-05-08T19:24:23Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyOTcwOQ==", "bodyText": "Remove + 1's", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422329709", "createdAt": "2020-05-08T19:27:49Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;\n+        return new SVCallRecord(startContig, start, startStrand, startContig, end, endStrand, type, length, algorithms, passing);\n+    }\n+\n+    public SVCallRecord(final String startContig,\n+                        final int start,\n+                        final boolean startStrand,\n+                        final String endContig,\n+                        final int end,\n+                        final boolean endStrand,\n+                        final StructuralVariantType type,\n+                        final int length,\n+                        final List<String> algorithms,\n+                        final List<Genotype> genotypes) {\n+        Utils.nonNull(startContig);\n+        Utils.nonNull(endContig);\n+        Utils.nonNull(type);\n+        Utils.nonNull(algorithms);\n+        Utils.nonNull(genotypes);\n+        Utils.nonEmpty(algorithms);\n+        Utils.nonEmpty(genotypes);\n+        Utils.containsNoNull(algorithms, \"Encountered null algorithm\");\n+        Utils.containsNoNull(genotypes, \"Encountered null genotype\");\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+        this.type = type;\n+        this.length = length;\n+        this.algorithms = algorithms;\n+        this.genotypes = genotypes;\n+        this.samples = genotypes.stream()\n+                .filter(Genotype::isCalled)\n+                .map(Genotype::getSampleName)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+    }\n+\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        return end;\n+    }\n+\n+    public boolean getEndStrand() {\n+        return endStrand;\n+    }\n+\n+    public StructuralVariantType getType() {\n+        return type;\n+    }\n+\n+    public int getLength() {\n+        return length;\n+    }\n+\n+    public List<String> getAlgorithms() {\n+        return algorithms;\n+    }\n+\n+    public Set<String> getSamples() {\n+        return samples;\n+    }\n+\n+    public List<Genotype> getGenotypes() {\n+        return genotypes;\n+    }\n+\n+    public SimpleInterval getStartAsInterval() {\n+        return new SimpleInterval(startContig, start, start + 1);\n+    }\n+\n+    public SimpleInterval getEndAsInterval() {\n+        return new SimpleInterval(endContig, end, end + 1);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMzNTE4NA==", "bodyText": "Fair point, although I think there are downsides to any simple rule. The solution may be to incorporate evidence to make a more intelligent decision. For depth-only calls where the breakpoints are imprecise, it seems reasonable to use a mean value. Other options would be min start / max end or max start / min end. It would be great if all these could be chosen by the user with another commandline arg.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422335184", "createdAt": "2020-05-08T19:39:30Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw=="}, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0OTI1MQ==", "bodyText": "By GATK convention, should be just return start;", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424649251", "createdAt": "2020-05-13T18:34:43Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java", "diffHunk": "@@ -0,0 +1,62 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+\n+import htsjdk.tribble.Feature;\n+\n+public final class DiscordantPairEvidence implements Feature {\n+\n+    final String sample;\n+    final String startContig;\n+    final String endContig;\n+    final int start;\n+    final int end;\n+    final boolean startStrand;\n+    final boolean endStrand;\n+\n+    public DiscordantPairEvidence(final String sample, final String startContig, final int start, final boolean startStrand,\n+                                  final String endContig, final int end, final boolean endStrand) {\n+        this.sample = sample;\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+    }\n+\n+    public String getSample() {\n+        return sample;\n+    }\n+\n+    // For purposes of indexing, we will return the start position\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        if (startContig.equals(endContig)) {\n+            return end;\n+        } else {\n+            return start + 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1MTcyOA==", "bodyText": "Have you been excluding allosomes when testing?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424651728", "createdAt": "2020-05-13T18:39:07Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyODA1OQ=="}, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDQzNQ==", "bodyText": "Don't need it for gCNV but I think you can leave it in", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424664435", "createdAt": "2020-05-13T19:00:31Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2Njg1NQ=="}, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0MTQzNA==", "bodyText": "Hm this looks correct! Not sure what I was doing before...", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424741434", "createdAt": "2020-05-13T21:27:46Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzU4Mw=="}, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0ODI5MA==", "bodyText": "I think it should be 1 + validClusterIds.size()", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424748290", "createdAt": "2020-05-13T21:43:02Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java", "diffHunk": "@@ -0,0 +1,294 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import scala.Tuple2;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public abstract class LocatableClusterEngine<T extends Locatable> {\n+\n+    public enum CLUSTERING_TYPE {\n+        SINGLE_LINKAGE,\n+        MAX_CLIQUE\n+    }\n+\n+    protected final SAMSequenceDictionary dictionary;\n+    private final List<Tuple2<SimpleInterval, List<Long>>> currentClusters; // Pairs of cluster start interval with item IDs\n+    private final Map<Long,T> idToItemMap;\n+    private final List<T> outputBuffer;\n+    private final CLUSTERING_TYPE clusteringType;\n+    private long currentItemId;\n+    private String currentContig;\n+\n+\n+    public LocatableClusterEngine(final SAMSequenceDictionary dictionary, final CLUSTERING_TYPE clusteringType) {\n+        this.dictionary = dictionary;\n+        this.clusteringType = clusteringType;\n+        this.currentClusters = new LinkedList<>();\n+        this.idToItemMap = new HashMap<>();\n+        this.outputBuffer = new ArrayList<>();\n+        currentItemId = 0;\n+        currentContig = null;\n+    }\n+\n+    abstract protected boolean clusterTogether(final T a, final T b);\n+    abstract protected SimpleInterval getClusteringInterval(final T item, final SimpleInterval currentClusterInterval);\n+    abstract protected T deduplicateIdenticalItems(final Collection<T> items);\n+    abstract protected boolean itemsAreIdentical(final T a, final T b);\n+    abstract protected T flattenCluster(final Collection<T> cluster);\n+\n+    public List<T> getOutput() {\n+        flushClusters();\n+        final List<T> output = deduplicateItems(outputBuffer);\n+        outputBuffer.clear();\n+        return output;\n+    }\n+\n+    private void resetItemIds() {\n+        Utils.validate(currentClusters.isEmpty(), \"Current cluster collection not empty\");\n+        currentItemId = 0;\n+        idToItemMap.clear();\n+    }\n+\n+    public boolean isEmpty() {\n+        return currentContig == null;\n+    }\n+\n+    public void add(final T item) {\n+\n+        // Start a new cluster if on a new contig\n+        if (!item.getContig().equals(currentContig)) {\n+            flushClusters();\n+            currentContig = item.getContig();\n+            idToItemMap.put(currentItemId, item);\n+            seedCluster(currentItemId);\n+            currentItemId++;\n+            return;\n+        }\n+\n+        // Keep track of a unique id for each item\n+        idToItemMap.put(currentItemId, item);\n+        final List<Integer> clusterIdsToProcess = cluster(item);\n+        processFinalizedClusters(clusterIdsToProcess);\n+        deleteRedundantClusters();\n+        currentItemId++;\n+    }\n+\n+    public String getCurrentContig() {\n+        return currentContig;\n+    }\n+\n+    public List<T> deduplicateItems(final List<T> items) {\n+        final List<T> sortedItems = IntervalUtils.sortLocatablesBySequenceDictionary(items, dictionary);\n+        final List<T> deduplicatedList = new ArrayList<>();\n+        int i = 0;\n+        while (i < sortedItems.size()) {\n+            final T record = sortedItems.get(i);\n+            int j = i + 1;\n+            final Collection<Integer> identicalItemIndexes = new ArrayList<>();\n+            while (j < sortedItems.size() && record.getStart() == sortedItems.get(j).getStart()) {\n+                final T other = sortedItems.get(j);\n+                if (itemsAreIdentical(record, other)) {\n+                    identicalItemIndexes.add(j);\n+                }\n+                j++;\n+            }\n+            if (identicalItemIndexes.isEmpty()) {\n+                deduplicatedList.add(record);\n+                i++;\n+            } else {\n+                identicalItemIndexes.add(i);\n+                final List<T> identicalItems = identicalItemIndexes.stream().map(sortedItems::get).collect(Collectors.toList());\n+                deduplicatedList.add(deduplicateIdenticalItems(identicalItems));\n+                i = j;\n+            }\n+        }\n+        return deduplicatedList;\n+    }\n+\n+    /**\n+     * Add a new {@param <T>} to the current clusters and determine which are complete\n+     * @param item to be added\n+     * @return the IDs for clusters that are complete and ready for processing\n+     */\n+    private List<Integer> cluster(final T item) {\n+        // Get list of item IDs from active clusters that cluster with this item\n+        final Set<Long> linkedItemIds = idToItemMap.entrySet().stream()\n+                .filter(other -> other.getKey().intValue() != currentItemId && clusterTogether(item, other.getValue()))\n+                .map(Map.Entry::getKey)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+\n+        // Find clusters to which this item belongs, and which active clusters we're definitely done with\n+        int clusterIndex = 0;\n+        final List<Integer> clusterIdsToProcess = new ArrayList<>();\n+        final List<Integer> clustersToAdd = new ArrayList<>();\n+        final List<Integer> clustersToSeedWith = new ArrayList<>();\n+        for (final Tuple2<SimpleInterval, List<Long>> cluster : currentClusters) {\n+            final SimpleInterval clusterInterval = cluster._1;\n+            final List<Long> clusterItemIds = cluster._2;\n+            if (item.getStart() > clusterInterval.getEnd()) {\n+                clusterIdsToProcess.add(clusterIndex);  //this cluster is complete -- process it when we're done\n+            } else {\n+                if (clusteringType.equals(CLUSTERING_TYPE.SINGLE_LINKAGE)) {\n+                    final int n = (int) clusterItemIds.stream().filter(linkedItemIds::contains).count();\n+                    if (n == clusterItemIds.size()) {\n+                        clustersToAdd.add(clusterIndex);\n+                    } else if (n > 0) {\n+                        clustersToSeedWith.add(clusterIndex);\n+                    }\n+                } else if (clusteringType.equals(CLUSTERING_TYPE.MAX_CLIQUE)) {\n+                    final boolean matchesCluster = clusterItemIds.stream().anyMatch(linkedItemIds::contains);\n+                    if (matchesCluster) {\n+                        clustersToAdd.add(clusterIndex);\n+                    }\n+                } else {\n+                    throw new IllegalArgumentException(\"Clustering algorithm for type \" + clusteringType.name() + \" not implemented\");\n+                }\n+            }\n+            clusterIndex++;\n+        }\n+\n+        // Add to item clusters\n+        for (final int index : clustersToAdd) {\n+            addToCluster(index, currentItemId);\n+        }\n+        // Create new clusters/cliques\n+        for (final int index : clustersToSeedWith) {\n+            seedWithExistingCluster(currentItemId, index, linkedItemIds);\n+        }\n+        // If there weren't any matches, create a new singleton cluster\n+        if (clustersToAdd.isEmpty() && clustersToSeedWith.isEmpty()) {\n+            seedCluster(currentItemId);\n+        }\n+        return clusterIdsToProcess;\n+    }\n+\n+    private void processCluster(final int clusterIndex) {\n+        final Tuple2<SimpleInterval, List<Long>> cluster = validateClusterIndex(clusterIndex);\n+        final List<Long> clusterItemIds = cluster._2;\n+        currentClusters.remove(clusterIndex);\n+        final List<T> clusterItems = clusterItemIds.stream().map(idToItemMap::get).collect(Collectors.toList());\n+        outputBuffer.add(flattenCluster(clusterItems));\n+    }\n+\n+    private void processFinalizedClusters(final List<Integer> clusterIdsToProcess) {\n+        final Set<Integer> activeClusterIds = IntStream.range(0, currentClusters.size()).boxed().collect(Collectors.toSet());\n+        activeClusterIds.removeAll(clusterIdsToProcess);\n+        final Set<Long> activeClusterItemIds = activeClusterIds.stream().flatMap(i -> currentClusters.get(i)._2.stream()).collect(Collectors.toSet());\n+        final Set<Long> finalizedItemIds = clusterIdsToProcess.stream()\n+                .flatMap(i -> currentClusters.get(i)._2.stream())\n+                .filter(i -> !activeClusterItemIds.contains(i))\n+                .collect(Collectors.toSet());\n+        for (int i = clusterIdsToProcess.size() - 1; i >= 0; i--) {\n+            processCluster(clusterIdsToProcess.get(i));\n+        }\n+        finalizedItemIds.stream().forEach(idToItemMap::remove);\n+    }\n+\n+    private void deleteRedundantClusters() {\n+        final Set<Integer> redundantClusterSet = new HashSet<>();\n+        for (int i = 0; i < currentClusters.size(); i++) {\n+            final Set<Long> clusterSetA = new HashSet<>(currentClusters.get(i)._2);\n+            for (int j = 0; j < i; j++) {\n+                final Set<Long> clusterSetB = new HashSet<>(currentClusters.get(j)._2);\n+                if (clusterSetA.containsAll(clusterSetB)) {\n+                    redundantClusterSet.add(j);\n+                } else if (clusterSetA.size() != clusterSetB.size() && clusterSetB.containsAll(clusterSetA)) {\n+                    redundantClusterSet.add(i);\n+                }\n+            }\n+        }\n+        final List<Integer> redundantClustersList = new ArrayList<>(redundantClusterSet);\n+        redundantClustersList.sort(Comparator.naturalOrder());\n+        for (int i = redundantClustersList.size() - 1; i >= 0; i--) {\n+            currentClusters.remove((int)redundantClustersList.get(i));\n+        }\n+    }\n+\n+    private void flushClusters() {\n+        while (!currentClusters.isEmpty()) {\n+            processCluster(0);\n+        }\n+        resetItemIds();\n+    }\n+\n+    private void seedCluster(final long seedId) {\n+        final T seed = validateItemIndex(seedId);\n+        final List<Long> newCluster = new ArrayList<>(1);\n+        newCluster.add(seedId);\n+        currentClusters.add(new Tuple2<>(getClusteringInterval(seed, null), newCluster));\n+    }\n+\n+    /**\n+     * Create a new cluster\n+     * @param seedId    itemId\n+     * @param existingClusterIndex\n+     * @param clusteringIds\n+     */\n+    private void seedWithExistingCluster(final Long seedId, final int existingClusterIndex, final Set<Long> clusteringIds) {\n+        final T seed = validateItemIndex(seedId);\n+        final List<Long> existingCluster = currentClusters.get(existingClusterIndex)._2;\n+        final List<Long> validClusterIds = existingCluster.stream().filter(clusteringIds::contains).collect(Collectors.toList());\n+        final List<Long> newCluster = new ArrayList<>(1 + existingCluster.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc5OTg4NQ==", "bodyText": "Uh ohhhh, this should be CLUSTERING_TYPE.MAX_CLIQUE and vice versa below. This is my mistake.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424799885", "createdAt": "2020-05-14T00:09:08Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java", "diffHunk": "@@ -0,0 +1,294 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import scala.Tuple2;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public abstract class LocatableClusterEngine<T extends Locatable> {\n+\n+    public enum CLUSTERING_TYPE {\n+        SINGLE_LINKAGE,\n+        MAX_CLIQUE\n+    }\n+\n+    protected final SAMSequenceDictionary dictionary;\n+    private final List<Tuple2<SimpleInterval, List<Long>>> currentClusters; // Pairs of cluster start interval with item IDs\n+    private final Map<Long,T> idToItemMap;\n+    private final List<T> outputBuffer;\n+    private final CLUSTERING_TYPE clusteringType;\n+    private long currentItemId;\n+    private String currentContig;\n+\n+\n+    public LocatableClusterEngine(final SAMSequenceDictionary dictionary, final CLUSTERING_TYPE clusteringType) {\n+        this.dictionary = dictionary;\n+        this.clusteringType = clusteringType;\n+        this.currentClusters = new LinkedList<>();\n+        this.idToItemMap = new HashMap<>();\n+        this.outputBuffer = new ArrayList<>();\n+        currentItemId = 0;\n+        currentContig = null;\n+    }\n+\n+    abstract protected boolean clusterTogether(final T a, final T b);\n+    abstract protected SimpleInterval getClusteringInterval(final T item, final SimpleInterval currentClusterInterval);\n+    abstract protected T deduplicateIdenticalItems(final Collection<T> items);\n+    abstract protected boolean itemsAreIdentical(final T a, final T b);\n+    abstract protected T flattenCluster(final Collection<T> cluster);\n+\n+    public List<T> getOutput() {\n+        flushClusters();\n+        final List<T> output = deduplicateItems(outputBuffer);\n+        outputBuffer.clear();\n+        return output;\n+    }\n+\n+    private void resetItemIds() {\n+        Utils.validate(currentClusters.isEmpty(), \"Current cluster collection not empty\");\n+        currentItemId = 0;\n+        idToItemMap.clear();\n+    }\n+\n+    public boolean isEmpty() {\n+        return currentContig == null;\n+    }\n+\n+    public void add(final T item) {\n+\n+        // Start a new cluster if on a new contig\n+        if (!item.getContig().equals(currentContig)) {\n+            flushClusters();\n+            currentContig = item.getContig();\n+            idToItemMap.put(currentItemId, item);\n+            seedCluster(currentItemId);\n+            currentItemId++;\n+            return;\n+        }\n+\n+        // Keep track of a unique id for each item\n+        idToItemMap.put(currentItemId, item);\n+        final List<Integer> clusterIdsToProcess = cluster(item);\n+        processFinalizedClusters(clusterIdsToProcess);\n+        deleteRedundantClusters();\n+        currentItemId++;\n+    }\n+\n+    public String getCurrentContig() {\n+        return currentContig;\n+    }\n+\n+    public List<T> deduplicateItems(final List<T> items) {\n+        final List<T> sortedItems = IntervalUtils.sortLocatablesBySequenceDictionary(items, dictionary);\n+        final List<T> deduplicatedList = new ArrayList<>();\n+        int i = 0;\n+        while (i < sortedItems.size()) {\n+            final T record = sortedItems.get(i);\n+            int j = i + 1;\n+            final Collection<Integer> identicalItemIndexes = new ArrayList<>();\n+            while (j < sortedItems.size() && record.getStart() == sortedItems.get(j).getStart()) {\n+                final T other = sortedItems.get(j);\n+                if (itemsAreIdentical(record, other)) {\n+                    identicalItemIndexes.add(j);\n+                }\n+                j++;\n+            }\n+            if (identicalItemIndexes.isEmpty()) {\n+                deduplicatedList.add(record);\n+                i++;\n+            } else {\n+                identicalItemIndexes.add(i);\n+                final List<T> identicalItems = identicalItemIndexes.stream().map(sortedItems::get).collect(Collectors.toList());\n+                deduplicatedList.add(deduplicateIdenticalItems(identicalItems));\n+                i = j;\n+            }\n+        }\n+        return deduplicatedList;\n+    }\n+\n+    /**\n+     * Add a new {@param <T>} to the current clusters and determine which are complete\n+     * @param item to be added\n+     * @return the IDs for clusters that are complete and ready for processing\n+     */\n+    private List<Integer> cluster(final T item) {\n+        // Get list of item IDs from active clusters that cluster with this item\n+        final Set<Long> linkedItemIds = idToItemMap.entrySet().stream()\n+                .filter(other -> other.getKey().intValue() != currentItemId && clusterTogether(item, other.getValue()))\n+                .map(Map.Entry::getKey)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+\n+        // Find clusters to which this item belongs, and which active clusters we're definitely done with\n+        int clusterIndex = 0;\n+        final List<Integer> clusterIdsToProcess = new ArrayList<>();\n+        final List<Integer> clustersToAdd = new ArrayList<>();\n+        final List<Integer> clustersToSeedWith = new ArrayList<>();\n+        for (final Tuple2<SimpleInterval, List<Long>> cluster : currentClusters) {\n+            final SimpleInterval clusterInterval = cluster._1;\n+            final List<Long> clusterItemIds = cluster._2;\n+            if (item.getStart() > clusterInterval.getEnd()) {\n+                clusterIdsToProcess.add(clusterIndex);  //this cluster is complete -- process it when we're done\n+            } else {\n+                if (clusteringType.equals(CLUSTERING_TYPE.SINGLE_LINKAGE)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgyMDM1Mw==", "bodyText": "Actually, as it's written now, it appears that this code does get called but will always be false for single-linkage clustering. The code path is getOutput() -> deduplicateItems() -> itemsAreIdentical(), so I think the better solution would be to check if the class algorithm type is single-linkage or max-clique and skip deduplicateItems() if it's single-linkage. In that case, we could throw a NeverReachHere exception.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424820353", "createdAt": "2020-05-14T01:27:12Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)\n+                .overlaps(getClusteringInterval(b, null));\n+    }\n+\n+\n+    /**\n+     * Determine an overlap interval for clustering using {@value #PADDING_FRACTION} padding\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param currentClusterInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval currentClusterInterval) {\n+        Utils.nonNull(call);\n+        final SimpleInterval callInterval = getCallInterval(call);\n+        final int paddedCallStart = (int) (callInterval.getStart() - PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final int paddedCallEnd = (int) (callInterval.getEnd() + PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final String currentContig = getCurrentContig();\n+        final int contigLength = dictionary.getSequence(currentContig).getSequenceLength();\n+        if (currentClusterInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, paddedCallStart, paddedCallEnd, contigLength);\n+        }\n+        //NOTE: this is an approximation -- padding should be based on the length of the call plus currentClusterIntervals\n+        final int newMinStart = Math.min(paddedCallStart, currentClusterInterval.getStart());\n+        final int newMaxEnd = Math.max(paddedCallEnd, currentClusterInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxEnd, contigLength);\n+    }\n+\n+    // Not used for single-linkage clustering\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        return false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3MDEyOA=="}, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0Njc3Mw==", "bodyText": "Thanks it looks like you fixed this from my version where I had min/max switched, but I'm not sure I follow how this is an approximation. The purpose of the cluster interval is to determine whether a given item is \"past\" the cluster, ie the item's start position guarantees that it cannot be linked to any members of the cluster. To guarantee this, don't we just take the largest end position of all the items in the cluster?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427446773", "createdAt": "2020-05-19T16:43:59Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,209 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary, boolean depthOnly) {\n+        super(dictionary, depthOnly ? CLUSTERING_TYPE.SINGLE_LINKAGE : CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ1NDQzOA==", "bodyText": "Yes I would like to keep the flexibility to be able to compare items without using equals() for the purposes of deduplication. (I'll comment elsewhere on whether we should be using SVCallRecordWithEvidence).", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427454438", "createdAt": "2020-05-19T16:55:42Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+    }\n+\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzA0MQ=="}, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ1Njk5MA==", "bodyText": "I'm seeing this again and don't like it. Can you add a TODO here stating that we need to put more thought into this?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427456990", "createdAt": "2020-05-19T16:59:30Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,209 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary, boolean depthOnly) {\n+        super(dictionary, depthOnly ? CLUSTERING_TYPE.SINGLE_LINKAGE : CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+    }\n+\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        return a.getContig().equals(b.getContig())\n+                && a.getStart() == b.getStart()\n+                && a.getEndContig().equals(b.getEndContig())\n+                && a.getEnd() == b.getEnd()\n+                && a.getType().equals(b.getType())\n+                && a.getStartStrand() == b.getStartStrand()\n+                && a.getEndStrand() == b.getEndStrand();\n+    }\n+\n+    /**\n+     *  Merge genotypes and algorithms for multiple calls describing the same event\n+     * @param items all entries are assumed to describe the same event, i.e. satisfy {@link #itemsAreIdentical}\n+     * @return  a single representative call\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence deduplicateIdenticalItems(final Collection<SVCallRecordWithEvidence> items) {\n+        if (items.isEmpty()) {\n+            return null;\n+        }\n+        final List<Genotype> genotypes = items.stream()\n+                .map(SVCallRecordWithEvidence::getGenotypes)\n+                .flatMap(Collection::stream)\n+                .collect(Collectors.toList());\n+        final List<String> algorithms = items.stream()\n+                .map(SVCallRecordWithEvidence::getAlgorithms)\n+                .flatMap(Collection::stream)\n+                .distinct()\n+                .collect(Collectors.toList());\n+        final SVCallRecordWithEvidence example = items.iterator().next();\n+        return new SVCallRecordWithEvidence(\n+                example.getContig(),\n+                example.getStart(),\n+                example.getStartStrand(),\n+                example.getEndContig(),\n+                example.getEnd(),\n+                example.getEndStrand(),\n+                example.getType(),\n+                example.getLength(),\n+                algorithms,\n+                genotypes,\n+                example.getStartSplitReadSites(),\n+                example.getEndSplitReadSites(),\n+                example.getDiscordantPairs());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUzODU3Mg==", "bodyText": "I'm looking back at my code and I believe I used SVCallRecordWithEvidence rather than SVCallRecord out of convenience for my clustering tool. However, I think it would be better to refactor using the latter so we can avoid issues like this one here.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427538572", "createdAt": "2020-05-19T19:12:35Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MTk2Mg==", "bodyText": "Thanks for pointing that out - we should check genotypes. I think easiest way would be to add a genotype to SVCallRecord.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427541962", "createdAt": "2020-05-19T19:18:45Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODEwOQ=="}, "originalCommit": {"oid": "34b71a1aba60653bf73cebae2bd0cf49b728a701"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MjcyMw==", "bodyText": "I think this and DiscordantPairEvidence can be omitted if you stick to SVCallRecord", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427542723", "createdAt": "2020-05-19T19:20:07Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SplitReadSite.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+final class SplitReadSite {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MzQwNQ==", "bodyText": "20 seems low to me. Did you consult with jfu on this?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427543405", "createdAt": "2020-05-19T19:21:15Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0NTgxNQ==", "bodyText": "I feel like this should get built into the engine with a requiresDictionary() method", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427545815", "createdAt": "2020-05-19T19:25:31Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Override\n+    public void onTraversalStart() {\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (dictionary == null) {\n+            throw new UserException(\"Reference sequence dictionary required\");\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwNzc2OA==", "bodyText": "I think the end will now be at call.getEnd() + 1", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427607768", "createdAt": "2020-05-19T21:22:16Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Override\n+    public void onTraversalStart() {\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (dictionary == null) {\n+            throw new UserException(\"Reference sequence dictionary required\");\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.0);\n+        clusterEngine = new SVClusterEngine(dictionary, true);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                defragmenter.add(new SVCallRecordWithEvidence(record));\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();  //Don't forget to do the last cluster!!!\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+        defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        //defragmented calls may still be overlapping, so run the clustering engine to combine based on reciprocal overlap\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        calls.stream()\n+                .sorted(Comparator.comparing(c -> c.getStartAsInterval(), IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(this::buildVariantContext)\n+                .forEachOrdered(vcfWriter::add);\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call) {\n+        Utils.nonNull(call);\n+        final Allele altAllele = Allele.create(\"<\" + call.getType().name() + \">\", false);\n+        final Allele refAllele = Allele.REF_N;\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                Lists.newArrayList(refAllele, altAllele));\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMDU3Mg==", "bodyText": "What was the impetus for these changes?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427610572", "createdAt": "2020-05-19T21:27:39Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java", "diffHunk": "@@ -26,16 +31,17 @@ private IndexUtils(){}\n      * Load a Tribble .idx index from disk, checking for out of date indexes and old versions\n      * @return an Index, or null if we're unable to load\n      */\n-    public static Index loadTribbleIndex(final File featureFile) {\n+    public static Index loadTribbleIndex(final Path featureFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMTczMg==", "bodyText": "This class will probably evolve later, but it would be best to change this one line now. Let's make the suffix \".sv_calls.tsv.gz\" otherwise we could create trouble with feature codec collisions.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427611732", "createdAt": "2020-05-19T21:30:12Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/codecs/SVCallRecordCodec.java", "diffHunk": "@@ -0,0 +1,76 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import com.google.common.base.Splitter;\n+import htsjdk.tribble.AsciiFeatureCodec;\n+import htsjdk.tribble.index.tabix.TabixFormat;\n+import htsjdk.tribble.readers.LineIterator;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+public class SVCallRecordCodec extends AsciiFeatureCodec<SVCallRecord> {\n+\n+    public static final String FORMAT_SUFFIX = \".tsv.gz\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMjY2OQ==", "bodyText": "Thanks, I'm not a fan of using assert except for testing, but at least you've added an error message.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427612669", "createdAt": "2020-05-19T21:32:10Z", "author": {"login": "mwalker174"}, "path": "src/main/python/org/broadinstitute/hellbender/gcnvkernel/postprocess/segment_quality_utils.py", "diffHunk": "@@ -145,7 +145,8 @@ def get_log_constrained_posterior_prob(self,\n         \"\"\"\n         assert start_index >= 0\n         assert end_index < self.num_sites\n-        assert end_index >= start_index\n+        assert end_index >= start_index, \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxNDE2Mg==", "bodyText": "I think an info message should be emitted with this", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427614162", "createdAt": "2020-05-19T21:35:21Z", "author": {"login": "mwalker174"}, "path": "src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_ploidy.py", "diffHunk": "@@ -121,6 +123,8 @@ def __call__(self):\n             log_q_ploidy_jk = self.ploidy_workspace.log_q_ploidy_sjk.get_value(borrow=True)[si, :, :]\n             for j in range(self.ploidy_workspace.num_contigs):\n                 ploidy_j[j], ploidy_genotyping_quality_j[j] = model_commons.perform_genotyping(log_q_ploidy_jk[j, :])\n+                if ploidy_genotyping_quality_j[j] < self.ploidy_gq_filter:\n+                    ploidy_j[j] = np.argmax(self.ploidy_config.contig_ploidy_prior_map.get(j))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxODE2MA==", "bodyText": "Ideally IMO we shouldn't be parsing anything in python except streams from java", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427618160", "createdAt": "2020-05-19T21:44:07Z", "author": {"login": "mwalker174"}, "path": "src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_vcf_parsing.py", "diffHunk": "@@ -0,0 +1,111 @@\n+import logging\n+import vcf", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYyMDIyMg==", "bodyText": "Fun fact: argparse would convert the dashes to underscores in the object members automatically, allowing us to use snake-case. But leave it this way in solidarity", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427620222", "createdAt": "2020-05-19T21:48:35Z", "author": {"login": "mwalker174"}, "path": "src/main/resources/org/broadinstitute/hellbender/tools/copynumber/cohort_determine_ploidy_and_depth.py", "diffHunk": "@@ -54,6 +54,12 @@\n                    help=\"Output path to write posteriors\")\n \n # optional arguments\n+group.add_argument(\"--ploidy_correction_gq_threshold\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "505d571eb5c09fdb8b1619f3d0a8bab69067a203"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzODQ1MDU4", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-423845058", "createdAt": "2020-06-03T19:02:52Z", "commit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxOTowMjo1MlrOGepa6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxOTowMjo1MlrOGepa6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4OTA5OA==", "bodyText": "Now that I'm looking at this again, I don't understand this logic.  Inversions, sure, but dupes?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r434789098", "createdAt": "2020-06-03T19:02:52Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 80}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "db499098290dc0c62f78224d0e5dc82b69be7069", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/db499098290dc0c62f78224d0e5dc82b69be7069", "committedDate": "2020-06-12T19:50:07Z", "message": "I bet you a dollar against a donut I forgot to add some file"}, "afterCommit": {"oid": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/4f8b4b099475adf375c72df9e7f1096c194a1bed", "committedDate": "2020-06-15T20:32:52Z", "message": "More tests to check qual score calculation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5NTEyMjIx", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-439512221", "createdAt": "2020-06-29T21:25:47Z", "commit": {"oid": "6cd8471380a722a009cf80fd81446c8000ebd115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQyMToyNTo0OFrOGqizDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQyMToyNTo0OFrOGqizDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg==", "bodyText": "Note that use of non-overlapping intervals for all CNV locatable collections was enforced by using the base AbstractLocatableCollection class---see documentation of that class and subclasses. In practice, PreprocessIntervals creates valid non-overlapping bins, which remain valid as they move through the pipelines.\nIt might be kind of a nightmare to go through and change all the relevant documentation---and even more of a nightmare to define behavior for algorithms/procedures (such as segmentation, segment merging, identifying unique site/bin overlaps, etc.), which rely on this rather fundamental property (part of why we enforce this in the first place).\nI haven't looked closely at the code to see why you absolutely need to use the CNV collections classes to represent overlapping intervals, but I'd strongly suggest refactoring to avoid this if possible. Perhaps instead convert to VCF, IntervalList, etc.?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r447263502", "createdAt": "2020-06-29T21:25:48Z", "author": {"login": "samuelklee"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java", "diffHunk": "@@ -48,17 +48,24 @@ public static void validateIntervalArgumentCollection(final IntervalArgumentColl\n     }\n \n     /**\n-     * Validate that a list of locatables is valid and sorted according to a sequence dictionary and contains no duplicates or overlaps.\n+     * Validate that a list of locatables is valid and sorted according to a sequence dictionary", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6cd8471380a722a009cf80fd81446c8000ebd115"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7b42ed9447e3832dcf393feaa32422585fa4509f", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/7b42ed9447e3832dcf393feaa32422585fa4509f", "committedDate": "2020-10-14T17:54:24Z", "message": "Trusting ALTs fixes chrY problems"}, "afterCommit": {"oid": "854619b80a1516622859192909557b831afe7fc9", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/854619b80a1516622859192909557b831afe7fc9", "committedDate": "2020-10-19T17:15:55Z", "message": "Use call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDon't fill in ref GTs until we look at upstream calls\nBig refactor to allow overlap in segments for joint calling\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nContinue AC annotation.  Add some checks for single-sample VCFs\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "854619b80a1516622859192909557b831afe7fc9", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/854619b80a1516622859192909557b831afe7fc9", "committedDate": "2020-10-19T17:15:55Z", "message": "Use call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDon't fill in ref GTs until we look at upstream calls\nBig refactor to allow overlap in segments for joint calling\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nContinue AC annotation.  Add some checks for single-sample VCFs\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up"}, "afterCommit": {"oid": "0d6bb314543637a02636795e07be1ba7345f023c", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/0d6bb314543637a02636795e07be1ba7345f023c", "committedDate": "2020-10-19T20:31:54Z", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDon't fill in ref GTs until we look at upstream calls\nBig refactor to allow overlap in segments for joint calling\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nContinue AC annotation.  Add some checks for single-sample VCFs\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwNzYyMDA4", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-520762008", "createdAt": "2020-10-30T15:09:48Z", "commit": {"oid": "0d6bb314543637a02636795e07be1ba7345f023c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowOTo0OFrOHrTZOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowOTo0OFrOHrTZOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2ODU2OA==", "bodyText": "Looks like this isn't used yet, but if it's something you'll add, perhaps change the argument name to input-denoised-copy-ratios to be consistent with output-denoised-copy-ratios.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r515168568", "createdAt": "2020-10-30T15:09:48Z", "author": {"login": "samuelklee"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -128,6 +129,15 @@\n     public static final String OUTPUT_DENOISED_COPY_RATIOS_LONG_NAME = \"output-denoised-copy-ratios\";\n     public static final String AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME = \"autosomal-ref-copy-number\";\n     public static final String ALLOSOMAL_CONTIG_LONG_NAME = \"allosomal-contig\";\n+    public static final String INPUT_INTERVALS_LONG_NAME = \"input-intervals-vcf\";\n+    public static final String INPUT_DENOISED_COPY_RATIO_LONG_NAME = \"input-denoised-copy-ratio\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d6bb314543637a02636795e07be1ba7345f023c"}, "originalPosition": 13}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4e23e916804d82b7b7202b93bb78687bbc8ae6ac", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/4e23e916804d82b7b7202b93bb78687bbc8ae6ac", "committedDate": "2020-11-16T21:53:33Z", "message": "Python unit test runner looks good\nAddress overlapping interval collection refactor"}, "afterCommit": {"oid": "874848454bbd012d995a7c879927ff4491174b34", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/874848454bbd012d995a7c879927ff4491174b34", "committedDate": "2020-11-17T15:44:45Z", "message": "Python unit test runner looks good\nAddress overlapping interval collection refactor"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c8bfcfe4a75881608b76818c6e15e91239a3c59b", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/c8bfcfe4a75881608b76818c6e15e91239a3c59b", "committedDate": "2020-11-19T14:45:20Z", "message": "Fix tests?"}, "afterCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/62b655c720bdea7621392a194861c8366a641fae", "committedDate": "2020-11-19T17:03:31Z", "message": "I thought we would need this...."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0NzQwMjEw", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-534740210", "createdAt": "2020-11-19T18:37:26Z", "commit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "state": "COMMENTED", "comments": {"totalCount": 68, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxODozNzoyNlrOH2sYqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMzowNTo1OVrOH21onw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjM2Mw==", "bodyText": "optional=true", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527112363", "createdAt": "2020-11-19T18:37:26Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng==", "bodyText": "This was a little confusing to me, and I would think that WGS would need this as well. Suggestion: gCNV model intervals created with the FilterIntervals tool.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527112546", "createdAt": "2020-11-19T18:37:47Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMzcwMw==", "bodyText": "Also if this is truly optional, add optional = true", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527113703", "createdAt": "2020-11-19T18:39:49Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNjg3OQ==", "bodyText": "optional = true. I didn't realize barclay supported enum type inputs. Should you list the options here or do they pop out somewhere in the docs?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527116879", "createdAt": "2020-11-19T18:45:04Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNzE4OQ==", "bodyText": "Clarify that this is a vcf. Also can remove optional=false since that is default.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527117189", "createdAt": "2020-11-19T18:45:31Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExODAxOA==", "bodyText": "Can you make them all end in _LONG_NAME or just _NAME?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527118018", "createdAt": "2020-11-19T18:47:00Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExOTAzMw==", "bodyText": "What's the reasoning for this? Maybe we should make disabling this an option.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527119033", "createdAt": "2020-11-19T18:48:41Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMTMzMg==", "bodyText": "I've written some machinery to read in gCNV contig ploidy calls that we can use to deal with inferring ploidy. We can put a pin in that for now.\nAlso, I do think it's nice to have the ped file option although my understanding is the sex assignments tend to be unreliable in practice.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527121332", "createdAt": "2020-11-19T18:52:19Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjAwOQ==", "bodyText": "For readability, can you put these validation blocks into a separate function?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122009", "createdAt": "2020-11-19T18:53:29Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjEwNQ==", "bodyText": "Extra newline here", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122105", "createdAt": "2020-11-19T18:53:38Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjMyMA==", "bodyText": "Missing indent", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122320", "createdAt": "2020-11-19T18:53:57Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMzEwNw==", "bodyText": "Should make 0.8 a tool parameter", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527123107", "createdAt": "2020-11-19T18:55:14Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMzk4Mw==", "bodyText": "callIntervals is null by default already, can reduce code here:\nif (modelCallIntervalList != null) {\n    final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n    final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n    callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n}", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527123983", "createdAt": "2020-11-19T18:56:40Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNjc0OA==", "bodyText": "There is probably something I don't understand here, but why not just final VCFHeader vcfHeader = new VCFHeader(headerLines, samples);?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527126748", "createdAt": "2020-11-19T19:01:09Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNzU2OA==", "bodyText": "Can make all parameters final", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527127568", "createdAt": "2020-11-19T19:02:29Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyODIwNw==", "bodyText": "This is very neat", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527128207", "createdAt": "2020-11-19T19:03:40Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyOTQxOA==", "bodyText": "Should add some minimal documentation here", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527129418", "createdAt": "2020-11-19T19:05:41Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzMDc1Mw==", "bodyText": "vc.getNSamples() would be better", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527130753", "createdAt": "2020-11-19T19:08:09Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzMjE4MQ==", "bodyText": "Is this behavior documented somewhere?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527132181", "createdAt": "2020-11-19T19:10:30Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzNTQ3MA==", "bodyText": "This variable was a little confusing at first. I think this should be called isMultiSampleInput and assigned using the header in onTraversalStart, rather than checking the number of samples for every variant.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527135470", "createdAt": "2020-11-19T19:16:00Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0MjQxMQ==", "bodyText": "DELs are +/- (true/false) and DUPs and -/+. Inversions are -/- and +/+.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527142411", "createdAt": "2020-11-19T19:27:23Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4OTA5OA=="}, "originalCommit": {"oid": "acc7db5effdd87b4828b4182b1255bd8a069128e"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0Mzc4Ng==", "bodyText": "Someday we will be able to just check the alt alleles.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527143786", "createdAt": "2020-11-19T19:29:27Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))\n+                    || Integer.valueOf((String) g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) < minQuality\n+                    || isNullCall(g)) {\n+                return null;\n+            }\n+        }\n+\n+\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        boolean isDel = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0NzMwOA==", "bodyText": "Should make this a class variable and initialize this in onTraversalStart()", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527147308", "createdAt": "2020-11-19T19:35:10Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODE2OA==", "bodyText": "Can be private", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527148168", "createdAt": "2020-11-19T19:36:38Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 433}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MDIxMw==", "bodyText": "Since this is a high-use function, we should assign this list its final size here", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527150213", "createdAt": "2020-11-19T19:40:13Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 436}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MTc0NA==", "bodyText": "Avoid running this twice and make it a variable, boolean isCnv = call.getType().equals(StructuralVariantType.CNV). This will also help initializing the alleles list size.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527151744", "createdAt": "2020-11-19T19:42:46Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 439}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MjQ3MQ==", "bodyText": "We've moved away from MCNV in gatk-sv, is there a specific reason here? This should probably be defined as a static String somewhere.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527152471", "createdAt": "2020-11-19T19:44:06Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 451}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MzA4Ng==", "bodyText": "final and initial size g.getAlleles().size()", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527153086", "createdAt": "2020-11-19T19:45:05Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");\n+        } else {\n+            builder.attribute(VCFConstants.SVTYPE, call.getType());\n+        }\n+        final List<Genotype> genotypes = new ArrayList<>();\n+        for (final Genotype g : call.getGenotypes()) {\n+            final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(g);\n+            //update reference alleles\n+            List<Allele> newGenotypeAlleles = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 459}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1OTg0Ng==", "bodyText": "Initial size call.getGenotypes().size()", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527159846", "createdAt": "2020-11-19T19:56:31Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");\n+        } else {\n+            builder.attribute(VCFConstants.SVTYPE, call.getType());\n+        }\n+        final List<Genotype> genotypes = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 455}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2MjY3Mw==", "bodyText": "final", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527162673", "createdAt": "2020-11-19T20:01:48Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2NTYxMQ==", "bodyText": "Suggestion: resolvedVCs.forEach(vcfWriter::add);", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527165611", "createdAt": "2020-11-19T20:07:15Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 261}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2NzgwMA==", "bodyText": "I think you can simplify here by just having this:\nint clusterEnd = 0;\nString clusterContig = null;\n\nand then maybe have a check at the beginning of resolveVariantContexts() for empty input.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527167800", "createdAt": "2020-11-19T20:11:23Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2OTQyMQ==", "bodyText": "Give initial size", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527169421", "createdAt": "2020-11-19T20:14:21Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3MTQwOQ==", "bodyText": "Avoid parsing g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) twice by assigning to a variable, and make sure it exists first.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527171409", "createdAt": "2020-11-19T20:17:49Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 290}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3MzI4Nw==", "bodyText": "Can this be combined with the previous loop to be more efficient?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527173287", "createdAt": "2020-11-19T20:21:08Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 301}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQwOA==", "bodyText": "Assign initial size with SVUtils.hashMapCapacity()", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527174408", "createdAt": "2020-11-19T20:23:12Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 284}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQ3Nw==", "bodyText": "Assign initial size with SVUtils.hashMapCapacity()", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527174477", "createdAt": "2020-11-19T20:23:21Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 316}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NTU5MA==", "bodyText": "When is this the case?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527175590", "createdAt": "2020-11-19T20:25:24Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 323}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NDc3OQ==", "bodyText": "Should make this into a Set", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527184779", "createdAt": "2020-11-19T20:41:40Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 402}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NDkzOA==", "bodyText": "Extra newline", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527184938", "createdAt": "2020-11-19T20:41:59Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 412}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTUwMQ==", "bodyText": "The if clause is redundant, and can combine with previous line -> } else {", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527185501", "createdAt": "2020-11-19T20:43:05Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 414}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NzQ2MA==", "bodyText": "Should this be Samples missing from pedigree and without VCF genotypes assumed to have ploidy 1 on allosomes. ?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527187460", "createdAt": "2020-11-19T20:46:36Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 410}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4ODY2Nw==", "bodyText": "I think you could eliminate the samplePloidy variable and just have return statements wherever it is assigned and at the default-case warning message.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527188667", "createdAt": "2020-11-19T20:49:01Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 430}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4OTk2NA==", "bodyText": "It seems confusing to allow users to specify a list of allosomal contigs but hard-code X and Y. Could we instead have separate \"X\" and \"Y\" tool input lists and use them here?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527189964", "createdAt": "2020-11-19T20:51:17Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 416}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5MjU1NA==", "bodyText": "Suggest rewriting as: final int copyNumber = g != null && g.hasAnyAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()) : samplePloidy;", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527192554", "createdAt": "2020-11-19T20:55:59Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NDk0MQ==", "bodyText": "Can you add a doc section for this?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527194941", "createdAt": "2020-11-19T21:00:14Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NTYzNA==", "bodyText": "Can avoid allele variable by replacing assignments with return statements", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527195634", "createdAt": "2020-11-19T21:01:32Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));\n+        //can't determine counts per haplotypes if there is a duplication\n+        } else if (genotypeAllele.equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+            return GATKVariantContextUtils.noCallAlleles(refCopyNumber);\n+        //for homDels, hetDels or homRefs\n+        } else if (refCopyNumber == 2) {\n+            if (copyNumberCall == 0) {\n+                returnAlleles.add(genotypeAllele);\n+            } else {\n+                returnAlleles.add(refAllele);\n+            }\n+            returnAlleles.add(genotypeAllele);\n+            return returnAlleles;\n+        //multiploid dels\n+        } else {\n+            for (int i = 0; i < copyNumberCall; i++) {\n+                returnAlleles.add(refAllele);\n+            }\n+            for (int i = copyNumberCall; i < refCopyNumber; i++) {\n+                returnAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            }\n+            return returnAlleles;\n+        }\n+    }\n+\n+    /**\n+     *\n+     * @param copyNumberCall\n+     * @param refCopyNumber\n+     * @param refAllele\n+     * @return variant allele if copyNumberCall != refCopyNumber, else refAllele\n+     */\n+    public static Allele getAlleleForCopyNumber(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final Allele allele;\n+        if (copyNumberCall > refCopyNumber) {\n+            allele = GATKSVVCFConstants.DUP_ALLELE;\n+        } else if (copyNumberCall < refCopyNumber) {\n+            allele = GATKSVVCFConstants.DEL_ALLELE;\n+        } else {\n+            allele = refAllele;\n+        }\n+        return allele;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NjY3MA==", "bodyText": "Also maybe rename to make it clear that this is for DEL/DUP/CNV only", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527196670", "createdAt": "2020-11-19T21:03:28Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NDk0MQ=="}, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NzQzMg==", "bodyText": "return Collections.singletonList(genotypeAllele);", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527197432", "createdAt": "2020-11-19T21:04:50Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5Nzk0NQ==", "bodyText": "All of these else's are unnecessary (only need if)", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527197945", "createdAt": "2020-11-19T21:05:41Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));\n+        //can't determine counts per haplotypes if there is a duplication\n+        } else if (genotypeAllele.equals(GATKSVVCFConstants.DUP_ALLELE)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMTMzNA==", "bodyText": "Could avoid potentially computing vc.hasGenotype(sample) twice using nested if's", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527201334", "createdAt": "2020-11-19T21:11:48Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 346}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE1Mg==", "bodyText": "temp -> count (and below)", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527203152", "createdAt": "2020-11-19T21:15:11Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 344}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ4OQ==", "bodyText": "(copyNumber > samplePloidy) -> copyNumber > samplePloidy", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527203489", "createdAt": "2020-11-19T21:15:41Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMTMzNA=="}, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 346}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTEzOA==", "bodyText": "Can you wrap this into a nested class? The getLeft()'s and getRights()'s can be hard to follow", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527205138", "createdAt": "2020-11-19T21:18:41Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQwOA=="}, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 284}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNjkyMg==", "bodyText": "Seems like this should be built into the initialization of copyNumber above. It's a bit confusing this way.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527206922", "createdAt": "2020-11-19T21:21:58Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 355}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNDU1OQ==", "bodyText": "I think this could be consolidated into a one-liner: final long AC = alleleCountMap.get(vc.getAlternateAllele(0));", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527214559", "createdAt": "2020-11-19T21:35:15Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNTI0MQ==", "bodyText": "Give initial sizes", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527215241", "createdAt": "2020-11-19T21:35:59Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 374}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNzE2MA==", "bodyText": "When would we expect a non-ref allele to not be in the count map? We should probably throw an error", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527217160", "createdAt": "2020-11-19T21:38:05Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 384}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyMTA5OQ==", "bodyText": "What happened here? Are the proper alleles set elsewhere?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527221099", "createdAt": "2020-11-19T21:42:20Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 377}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzI3MQ==", "bodyText": "Mention this can be generated with the JointGermlineCNVSegmentation tool", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527233271", "createdAt": "2020-11-19T22:05:11Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -171,6 +179,20 @@\n     )\n     private List<String> allosomalContigList;\n \n+    @Argument(\n+            doc = \"Input VCF with combined intervals for all samples\",\n+            fullName = INPUT_INTERVALS_LONG_NAME,\n+            optional = true\n+    )\n+    private File combinedIntervalsVCFFile = null;\n+\n+    @Argument(\n+            doc = \"VCF with clustered breakpoints and copy number calls for all samples\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzc0Nw==", "bodyText": "Also the functionality you've added should be added to the tool documentation", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527233747", "createdAt": "2020-11-19T22:06:04Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -171,6 +179,20 @@\n     )\n     private List<String> allosomalContigList;\n \n+    @Argument(\n+            doc = \"Input VCF with combined intervals for all samples\",\n+            fullName = INPUT_INTERVALS_LONG_NAME,\n+            optional = true\n+    )\n+    private File combinedIntervalsVCFFile = null;\n+\n+    @Argument(\n+            doc = \"VCF with clustered breakpoints and copy number calls for all samples\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzI3MQ=="}, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MTg1NA==", "bodyText": "What differentiates GATKSVVCFConstants.COPY_NUMBER_FORMAT and GermlineCNVSegmentVariantComposer.CN? Maybe we should only have one", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527241854", "createdAt": "2020-11-19T22:18:55Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MzI3NQ==", "bodyText": "Switch to using the VariantContextGetters utils throughout for getting genotype and variant attributes.", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527243275", "createdAt": "2020-11-19T22:21:45Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))\n+                    || Integer.valueOf((String) g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) < minQuality", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0Njg4MQ==", "bodyText": "It seems like setRight() and setLeft() are switched with respect to the use of getLeft() and getRight():\ngenotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n\nif (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527246881", "createdAt": "2020-11-19T22:29:02Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 291}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI1MjI3OA==", "bodyText": "Suggested edit:\n        //if we supply a breakpoints file, then allow overlapping segments\n        final AbstractRecordCollection integerCopyNumberSegmentCollection;\n        final String sampleNameFromSegmentCollection;\n        if (clusteredBreakpointsVCFFile == null) {\n            final IntegerCopyNumberSegmentCollection collection = new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n            sampleNameFromSegmentCollection = collection.getMetadata().getSampleName();\n            integerCopyNumberSegmentCollection = collection;\n        } else {\n            final OverlappingIntegerCopyNumberSegmentCollection collection = new OverlappingIntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n            sampleNameFromSegmentCollection = collection.getMetadata().getSampleName();\n            integerCopyNumberSegmentCollection = collection;\n        }\n        Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n                String.format(\"Sample name found in the header of copy-number segments file is \" +\n                                \"different from the expected sample name (found: %s, expected: %s).\",\n                        sampleNameFromSegmentCollection, sampleName));\n...\n        germlineCNVSegmentVariantComposer.writeAll(integerCopyNumberSegmentCollection.getRecords());", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527252278", "createdAt": "2020-11-19T22:40:43Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -387,36 +442,56 @@ private void generateIntervalsVCFFileFromAllShards() {\n     }\n \n     private void generateSegmentsVCFFileFromAllShards() {\n-        logger.info(\"Generating segments VCF file...\");\n+        logger.info(\"Generating segments...\");\n \n         /* perform segmentation */\n         final File pythonScriptOutputPath = IOUtils.createTempDir(\"gcnv-segmented-calls\");\n         final boolean pythonScriptSucceeded = executeSegmentGermlineCNVCallsPythonScript(\n                 sampleIndex, inputContigPloidyCallsPath, sortedCallsShardPaths, sortedModelShardPaths,\n-                pythonScriptOutputPath);\n+                combinedIntervalsVCFFile, clusteredBreakpointsVCFFile, pythonScriptOutputPath);\n         if (!pythonScriptSucceeded) {\n             throw new UserException(\"Python return code was non-zero.\");\n         }\n \n         /* parse segments */\n+        logger.info(\"Parsing Python output...\");\n         final File copyNumberSegmentsFile = getCopyNumberSegmentsFile(pythonScriptOutputPath, sampleIndex);\n-        final IntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection =\n-                new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n-        final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n-                .getMetadata().getSampleName();\n-        Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n-                String.format(\"Sample name found in the header of copy-number segments file is \" +\n-                                \"different from the expected sample name (found: %s, expected: %s).\",\n-                        sampleNameFromSegmentCollection, sampleName));\n+\n+        final List<IntegerCopyNumberSegment> records;\n+        //if we supply a breakpoints file, then allow overlapping segments\n+        if (clusteredBreakpointsVCFFile == null) {\n+            final IntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+                    = new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n+            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+                    .getMetadata().getSampleName();\n+            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n+                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n+                                    \"different from the expected sample name (found: %s, expected: %s).\",\n+                            sampleNameFromSegmentCollection, sampleName));\n+            records = integerCopyNumberSegmentCollection.getRecords();\n+        } else {\n+            final OverlappingIntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+                    = new OverlappingIntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n+            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+                    .getMetadata().getSampleName();\n+            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n+                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n+                                    \"different from the expected sample name (found: %s, expected: %s).\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI1NDU4OA==", "bodyText": "Can you break both lambdas out into their own functions?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527254588", "createdAt": "2020-11-19T22:45:37Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -284,23 +288,24 @@ private void validateAllSequenceDictionaries() {\n                 if (dictionary == null) {\n                     logger.warn(\n                             \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                            \" not be obtained for feature input: \" + ds.getName() +\n-                            \". The input may not exist or may not have a valid header\");\n+                                    \" not be obtained for feature input: \" + ds.getName() +\n+                                    \". The input may not exist or may not have a valid header\");\n                 } else {\n+                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts\n                     dictionary.getSequences().forEach(\n-                        sourceSequence -> {\n-                            final String sourceSequenceName = sourceSequence.getSequenceName();\n-                            final FeatureDataSource<VariantContext> previousDataSource = contigMap.getOrDefault(sourceSequenceName, null);\n-                            if (previousDataSource != null) {\n-                                final SAMSequenceDictionary previousDictionary = previousDataSource.getSequenceDictionary();\n-                                final SAMSequenceRecord previousSequence = previousDictionary.getSequence(sourceSequenceName);\n-                                validateSequenceDictionaryRecords(\n-                                        ds.getName(), dictionary, sourceSequence,\n-                                        previousDataSource.getName(), previousDictionary, previousSequence);\n-                            } else {\n-                                contigMap.put(sourceSequenceName, ds);\n+                            sourceSequence -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MDM5Mg==", "bodyText": "It does seem complicated. Do you know how it affects runtime now?", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527260392", "createdAt": "2020-11-19T22:58:14Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -284,23 +288,24 @@ private void validateAllSequenceDictionaries() {\n                 if (dictionary == null) {\n                     logger.warn(\n                             \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                            \" not be obtained for feature input: \" + ds.getName() +\n-                            \". The input may not exist or may not have a valid header\");\n+                                    \" not be obtained for feature input: \" + ds.getName() +\n+                                    \". The input may not exist or may not have a valid header\");\n                 } else {\n+                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MzkwMw==", "bodyText": "File inputs should be GATKPaths (also in JointGermlineCNVSegmentation)", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527263903", "createdAt": "2020-11-19T23:05:59Z", "author": {"login": "mwalker174"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -171,6 +179,20 @@\n     )\n     private List<String> allosomalContigList;\n \n+    @Argument(\n+            doc = \"Input VCF with combined intervals for all samples\",\n+            fullName = INPUT_INTERVALS_LONG_NAME,\n+            optional = true\n+    )\n+    private File combinedIntervalsVCFFile = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62b655c720bdea7621392a194861c8366a641fae"}, "originalPosition": 38}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "858fb477f44dcc16a4cdcea8b7417aa04b64abf2", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/858fb477f44dcc16a4cdcea8b7417aa04b64abf2", "committedDate": "2020-12-08T21:03:22Z", "message": "More integration tests and other review responses"}, "afterCommit": {"oid": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/61759e542a67b74ae02f728f6ca1cbac3ba0a857", "committedDate": "2020-12-08T21:13:29Z", "message": "More integration tests and other review responses"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNzA1Njk4", "url": "https://github.com/broadinstitute/gatk/pull/6554#pullrequestreview-552705698", "createdAt": "2020-12-15T17:37:20Z", "commit": {"oid": "2d6a877f48dcf5b7d28caddc77834eeab0dedfbd"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fb519a96dadd3bd2f58cef9affb235854073a0c7", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/fb519a96dadd3bd2f58cef9affb235854073a0c7", "committedDate": "2020-12-22T17:05:47Z", "message": "Last WDL fixes?"}, "afterCommit": {"oid": "c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "author": {"user": {"login": "mwalker174", "name": "Mark Walker"}}, "url": "https://github.com/broadinstitute/gatk/commit/c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "committedDate": "2020-12-22T18:58:48Z", "message": "Add clustering code (from mw_gatk_sv_v2) and modify for gCNV exome joint calling\ngCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up\nNew Python unit test runner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7175045562c85450dedbc8ecb909edf4bb1b4c6", "author": {"user": {"login": "mwalker174", "name": "Mark Walker"}}, "url": "https://github.com/broadinstitute/gatk/commit/b7175045562c85450dedbc8ecb909edf4bb1b4c6", "committedDate": "2020-12-22T20:54:33Z", "message": "Add clustering code (from mw_gatk_sv_v2) and modify for gCNV exome joint calling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c41239987b118546daa78533256216fcee3378c0", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/c41239987b118546daa78533256216fcee3378c0", "committedDate": "2020-12-22T20:54:36Z", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nFilter by raw calls and filtered calls\nNew Python unit test runner"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "author": {"user": {"login": "mwalker174", "name": "Mark Walker"}}, "url": "https://github.com/broadinstitute/gatk/commit/c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "committedDate": "2020-12-22T18:58:48Z", "message": "Add clustering code (from mw_gatk_sv_v2) and modify for gCNV exome joint calling\ngCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up\nNew Python unit test runner"}, "afterCommit": {"oid": "c41239987b118546daa78533256216fcee3378c0", "author": {"user": {"login": "ldgauthier", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/c41239987b118546daa78533256216fcee3378c0", "committedDate": "2020-12-22T20:54:36Z", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nFilter by raw calls and filtered calls\nNew Python unit test runner"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2857, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}