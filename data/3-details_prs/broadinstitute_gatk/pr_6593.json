{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE1ODQ0NTM3", "number": 6593, "title": "Fixed bugs and simplified AlleleLikelihoods evidence-to-index cache", "bodyText": "Closes #6586.  @droazen\nAlleleLikelihoods caches the evidence-to-index Map.  The previous implementation tried to update this map on the fly whenever evidence was removed.  The new approach is to simply invalidate the cache and allow the existing code to generate it to run later.\nI don't expect this to cause performance problems for a few reasons:\n\nIt only applies when we're doing contamination downsampling.\nIt may save time whenever evidence is removed and we don't need the evidence-to-index map later.\nRegenerating the cache is O(N), but so is updating on-the-fly even when only one read is removed.", "createdAt": "2020-05-11T03:25:15Z", "url": "https://github.com/broadinstitute/gatk/pull/6593", "merged": true, "mergeCommit": {"oid": "ee56f2779e35920ed8cac3f47463d5c9343a7f40"}, "closed": true, "closedAt": "2020-05-26T13:11:17Z", "author": {"login": "davidbenjamin"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcgGu1tgH2gAyNDE1ODQ0NTM3OmZiMDA2ZmQ5ZmMyZThjZTllZTU0OWZkYWZhMDc2MjJmMDUzMTA3NjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABclEY5vAFqTQxODI3ODcxMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "fb006fd9fc2e8ce9ee549fdafa07622f05310765", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/fb006fd9fc2e8ce9ee549fdafa07622f05310765", "committedDate": "2020-05-11T03:02:15Z", "message": "Fixed bugs and simplified implementation of AlleleLikelihoods evidence-to-index cache"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5MzQxNzE3", "url": "https://github.com/broadinstitute/gatk/pull/6593#pullrequestreview-409341717", "createdAt": "2020-05-11T16:38:54Z", "commit": {"oid": "fb006fd9fc2e8ce9ee549fdafa07622f05310765"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxNjozODo1NVrOGTkTYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxOTozNTozOVrOGTqeTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE3MDkxMg==", "bodyText": "Make a method .invalidateCache() that gets called here and everywhere else we edit the evidences lists.", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r423170912", "createdAt": "2020-05-11T16:38:55Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1129,60 +1129,45 @@ private void removeEvidence(final int sampleIndex, final Collection<EVIDENCE> ev\n         removeEvidenceByIndex(sampleIndex, indexesToRemove);\n     }\n \n+    // remove evidence and unset the {@code evidenceIndexBySampleIndex} Map for this sample\n     // assumes that evidencesToRemove is sorted and without duplicates.\n     private void removeEvidenceByIndex(final int sampleIndex, final int[] evidencesToRemove) {\n-        if (evidencesToRemove.length == 0) {\n+        final int numToRemove = evidencesToRemove.length;\n+        if (numToRemove == 0) {\n             return;\n-        } else {\n-            final Object2IntMap<EVIDENCE> evidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n-            final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n-            final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n-            if (newEvidenceCount < 0) {\n-                throw new IllegalStateException(\"attempt to remove non-existent evidence or repeated evidence\");\n-            } else if (newEvidenceCount == 0) { // taking in consideration the assumption we simply remove\n-                // all evidence.\n-                evidenceIndex.clear();\n-                numberOfEvidences[sampleIndex] = 0;\n+        }\n+        final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n+        final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n+        Utils.validate(newEvidenceCount >= 0, \"attempted to remove non-existent evidence or repeated evidence\");\n+\n+        // update the list of evidence and evidence count\n+        final List<EVIDENCE> oldEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final List<EVIDENCE> newEvidence = new ArrayList<>(newEvidenceCount);\n+        for (int n = 0, numRemoved = 0; n < oldEvidenceCount; n++) {\n+            if (numRemoved < numToRemove && n == evidencesToRemove[numRemoved]) {\n+                numRemoved++;\n             } else {\n-                final List<EVIDENCE> evidences = evidenceBySampleIndex.get(sampleIndex);\n-                final double[][] values = valuesBySampleIndex[sampleIndex];\n-\n-                int nextIndexToRemove = evidencesToRemove[0];\n-                if (nextIndexToRemove < 0) {\n-                    throw new IllegalStateException(\"invalid input index array as it contains negatives\");\n-                }\n-                evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {\n-                    if (etrIndex < evidencesToRemove.length) {\n-                        nextIndexToRemove = evidencesToRemove[etrIndex];\n-                        if (nextIndexToRemove < from) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        } else if (nextIndexToRemove >= oldEvidenceCount) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        }\n-                        evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                    } else {\n-                        nextIndexToRemove = oldEvidenceCount;\n-                    }\n-                    for (; from < nextIndexToRemove; from++) {\n-                        final EVIDENCE evidence = evidences.get(from);\n-                        evidences.set(to, evidence);\n-                        evidenceIndex.put(evidence, to++);\n-                    }\n-                }\n-                Utils.truncate(evidences, newEvidenceCount);\n-                // now we do the likelihood values, we save ourselves all the checks:\n-                for (final double[] alleleValues : values) {\n-                    for (int etrIndex = 1, to = evidencesToRemove[0], from = to + 1; to < newEvidenceCount; from++, etrIndex++) {\n-                        nextIndexToRemove = etrIndex < evidencesToRemove.length ? evidencesToRemove[etrIndex] : oldEvidenceCount;\n-                        for (; from < nextIndexToRemove; from++) {\n-                            alleleValues[to++] = alleleValues[from];\n-                        }\n-                    }\n+                newEvidence.add(oldEvidence.get(n));\n+            }\n+        }\n+        Utils.validate(newEvidence.size() == newEvidenceCount, \"Indices to remove contained duplicates, was not ordered, or contained out-of-range indices.\");\n+        evidenceBySampleIndex.set(sampleIndex, newEvidence);\n+        numberOfEvidences[sampleIndex] = newEvidenceCount;\n+\n+        //  invalidate the cached evidence to index map\n+        evidenceIndexBySampleIndex.set(sampleIndex, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fb006fd9fc2e8ce9ee549fdafa07622f05310765"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzI3MTU3NQ==", "bodyText": "Furthermore we should call invalidateCache() for all operations that mutate the sample arrays (so for adding samples and removing samples)", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r423271575", "createdAt": "2020-05-11T19:34:49Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1129,60 +1129,45 @@ private void removeEvidence(final int sampleIndex, final Collection<EVIDENCE> ev\n         removeEvidenceByIndex(sampleIndex, indexesToRemove);\n     }\n \n+    // remove evidence and unset the {@code evidenceIndexBySampleIndex} Map for this sample\n     // assumes that evidencesToRemove is sorted and without duplicates.\n     private void removeEvidenceByIndex(final int sampleIndex, final int[] evidencesToRemove) {\n-        if (evidencesToRemove.length == 0) {\n+        final int numToRemove = evidencesToRemove.length;\n+        if (numToRemove == 0) {\n             return;\n-        } else {\n-            final Object2IntMap<EVIDENCE> evidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n-            final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n-            final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n-            if (newEvidenceCount < 0) {\n-                throw new IllegalStateException(\"attempt to remove non-existent evidence or repeated evidence\");\n-            } else if (newEvidenceCount == 0) { // taking in consideration the assumption we simply remove\n-                // all evidence.\n-                evidenceIndex.clear();\n-                numberOfEvidences[sampleIndex] = 0;\n+        }\n+        final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n+        final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n+        Utils.validate(newEvidenceCount >= 0, \"attempted to remove non-existent evidence or repeated evidence\");\n+\n+        // update the list of evidence and evidence count\n+        final List<EVIDENCE> oldEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final List<EVIDENCE> newEvidence = new ArrayList<>(newEvidenceCount);\n+        for (int n = 0, numRemoved = 0; n < oldEvidenceCount; n++) {\n+            if (numRemoved < numToRemove && n == evidencesToRemove[numRemoved]) {\n+                numRemoved++;\n             } else {\n-                final List<EVIDENCE> evidences = evidenceBySampleIndex.get(sampleIndex);\n-                final double[][] values = valuesBySampleIndex[sampleIndex];\n-\n-                int nextIndexToRemove = evidencesToRemove[0];\n-                if (nextIndexToRemove < 0) {\n-                    throw new IllegalStateException(\"invalid input index array as it contains negatives\");\n-                }\n-                evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {\n-                    if (etrIndex < evidencesToRemove.length) {\n-                        nextIndexToRemove = evidencesToRemove[etrIndex];\n-                        if (nextIndexToRemove < from) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        } else if (nextIndexToRemove >= oldEvidenceCount) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        }\n-                        evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                    } else {\n-                        nextIndexToRemove = oldEvidenceCount;\n-                    }\n-                    for (; from < nextIndexToRemove; from++) {\n-                        final EVIDENCE evidence = evidences.get(from);\n-                        evidences.set(to, evidence);\n-                        evidenceIndex.put(evidence, to++);\n-                    }\n-                }\n-                Utils.truncate(evidences, newEvidenceCount);\n-                // now we do the likelihood values, we save ourselves all the checks:\n-                for (final double[] alleleValues : values) {\n-                    for (int etrIndex = 1, to = evidencesToRemove[0], from = to + 1; to < newEvidenceCount; from++, etrIndex++) {\n-                        nextIndexToRemove = etrIndex < evidencesToRemove.length ? evidencesToRemove[etrIndex] : oldEvidenceCount;\n-                        for (; from < nextIndexToRemove; from++) {\n-                            alleleValues[to++] = alleleValues[from];\n-                        }\n-                    }\n+                newEvidence.add(oldEvidence.get(n));\n+            }\n+        }\n+        Utils.validate(newEvidence.size() == newEvidenceCount, \"Indices to remove contained duplicates, was not ordered, or contained out-of-range indices.\");\n+        evidenceBySampleIndex.set(sampleIndex, newEvidence);\n+        numberOfEvidences[sampleIndex] = newEvidenceCount;\n+\n+        //  invalidate the cached evidence to index map\n+        evidenceIndexBySampleIndex.set(sampleIndex, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE3MDkxMg=="}, "originalCommit": {"oid": "fb006fd9fc2e8ce9ee549fdafa07622f05310765"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzI3MjAxMg==", "bodyText": "Add some comments to evidenceIndexBySampleIndex.set(sampleIndex, null); and the class javadocs explaining how this cache works and what it accomplishes.", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r423272012", "createdAt": "2020-05-11T19:35:39Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1129,60 +1129,45 @@ private void removeEvidence(final int sampleIndex, final Collection<EVIDENCE> ev\n         removeEvidenceByIndex(sampleIndex, indexesToRemove);\n     }\n \n+    // remove evidence and unset the {@code evidenceIndexBySampleIndex} Map for this sample", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fb006fd9fc2e8ce9ee549fdafa07622f05310765"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432", "committedDate": "2020-05-25T02:54:22Z", "message": "Review edits"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3NDQ2Mzk3", "url": "https://github.com/broadinstitute/gatk/pull/6593#pullrequestreview-417446397", "createdAt": "2020-05-25T04:09:25Z", "commit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwNDowOToyNVrOGZ0PtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwNDo1NDowMVrOGZ0rEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyMzU3Mw==", "bodyText": "You just calculated that values when initializing previousEvidenceCount why not simply reuse it here:\nint nextIndex = previousEvidenceCount;", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429723573", "createdAt": "2020-05-25T04:09:25Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -758,46 +759,28 @@ private void extendsLikelihoodArrays(final double initialLikelihood, final int s\n         }\n     }\n \n-    // Append the new evidence reference into the structure per-sample.\n-    private List<EVIDENCE> appendEvidence(final List<EVIDENCE> newSampleEvidence, final int sampleIndex) {\n+    // Append the new evidence reference into the structure per-sample, returning the count of evidence actually added (duplicates are not added)\n+    // NOTE: the evidence-to-index cache is updated in place and not invalidated via {@link #invalidateEvidenceToIndexCache(int)} because adding new evidence\n+    // to the cache, as opposed to removing evidence, is just a matter of appending entries\n+    private int appendEvidence(final List<EVIDENCE> newSampleEvidence, final int sampleIndex) {\n \n         final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n         final Object2IntMap<EVIDENCE> sampleEvidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n+        final int previousEvidenceCount = sampleEvidence.size();\n \n-        // actually-added will have the list of evidence added at the end of this method.\n-        // this won't include those that were already in the table.\n-        // being optimistic we assume that there is no repeats in the input new evidence so we set it to\n-        // the input list but if we found something we then start a new list.\n-        List<EVIDENCE> actuallyAdded = newSampleEvidence;\n-\n-        int i, nextIndex = sampleEvidence.size();\n-        final int stop = newSampleEvidence.size();\n-        for (i = 0; i < stop; i++) {\n-            final EVIDENCE newEvidence = newSampleEvidence.get(i);\n-            final int previousValue = sampleEvidenceIndex.put(newEvidence, nextIndex);\n-            if (previousValue == MISSING_INDEX) {\n-                nextIndex++;\n-                sampleEvidence.add(newEvidence);\n-            } else {\n-                actuallyAdded = new ArrayList<>(newSampleEvidence.subList(0, i));\n-                i++; // skip the repeated element.\n-                break;\n-            }\n-        }\n-        // second for below only use if we encounter some evidence that is already in the table:\n-        for (; i < stop; i++) {\n-            final EVIDENCE newEvidence = newSampleEvidence.get(i);\n+        int nextIndex = sampleEvidence.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyMzg1OQ==", "bodyText": "nextIndex contains the value you need here.\nperhaps it would be better to call this variable currentSize", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429723859", "createdAt": "2020-05-25T04:11:29Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -758,46 +759,28 @@ private void extendsLikelihoodArrays(final double initialLikelihood, final int s\n         }\n     }\n \n-    // Append the new evidence reference into the structure per-sample.\n-    private List<EVIDENCE> appendEvidence(final List<EVIDENCE> newSampleEvidence, final int sampleIndex) {\n+    // Append the new evidence reference into the structure per-sample, returning the count of evidence actually added (duplicates are not added)\n+    // NOTE: the evidence-to-index cache is updated in place and not invalidated via {@link #invalidateEvidenceToIndexCache(int)} because adding new evidence\n+    // to the cache, as opposed to removing evidence, is just a matter of appending entries\n+    private int appendEvidence(final List<EVIDENCE> newSampleEvidence, final int sampleIndex) {\n \n         final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n         final Object2IntMap<EVIDENCE> sampleEvidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n+        final int previousEvidenceCount = sampleEvidence.size();\n \n-        // actually-added will have the list of evidence added at the end of this method.\n-        // this won't include those that were already in the table.\n-        // being optimistic we assume that there is no repeats in the input new evidence so we set it to\n-        // the input list but if we found something we then start a new list.\n-        List<EVIDENCE> actuallyAdded = newSampleEvidence;\n-\n-        int i, nextIndex = sampleEvidence.size();\n-        final int stop = newSampleEvidence.size();\n-        for (i = 0; i < stop; i++) {\n-            final EVIDENCE newEvidence = newSampleEvidence.get(i);\n-            final int previousValue = sampleEvidenceIndex.put(newEvidence, nextIndex);\n-            if (previousValue == MISSING_INDEX) {\n-                nextIndex++;\n-                sampleEvidence.add(newEvidence);\n-            } else {\n-                actuallyAdded = new ArrayList<>(newSampleEvidence.subList(0, i));\n-                i++; // skip the repeated element.\n-                break;\n-            }\n-        }\n-        // second for below only use if we encounter some evidence that is already in the table:\n-        for (; i < stop; i++) {\n-            final EVIDENCE newEvidence = newSampleEvidence.get(i);\n+        int nextIndex = sampleEvidence.size();\n+        for (final EVIDENCE newEvidence : newSampleEvidence) {\n             final int previousValue = sampleEvidenceIndex.put(newEvidence, nextIndex);\n             if (previousValue == MISSING_INDEX) {\n                 nextIndex++;\n                 sampleEvidence.add(newEvidence);\n-                actuallyAdded.add(newEvidence);\n             } else {\n                 sampleEvidenceIndex.put(newEvidence, previousValue); // revert\n             }\n         }\n+\n         numberOfEvidences[sampleIndex] = sampleEvidence.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyMzk2NQ==", "bodyText": "return nextIndex - previousEvidenceCount;", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429723965", "createdAt": "2020-05-25T04:12:17Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -758,46 +759,28 @@ private void extendsLikelihoodArrays(final double initialLikelihood, final int s\n         }\n     }\n \n-    // Append the new evidence reference into the structure per-sample.\n-    private List<EVIDENCE> appendEvidence(final List<EVIDENCE> newSampleEvidence, final int sampleIndex) {\n+    // Append the new evidence reference into the structure per-sample, returning the count of evidence actually added (duplicates are not added)\n+    // NOTE: the evidence-to-index cache is updated in place and not invalidated via {@link #invalidateEvidenceToIndexCache(int)} because adding new evidence\n+    // to the cache, as opposed to removing evidence, is just a matter of appending entries\n+    private int appendEvidence(final List<EVIDENCE> newSampleEvidence, final int sampleIndex) {\n \n         final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n         final Object2IntMap<EVIDENCE> sampleEvidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n+        final int previousEvidenceCount = sampleEvidence.size();\n \n-        // actually-added will have the list of evidence added at the end of this method.\n-        // this won't include those that were already in the table.\n-        // being optimistic we assume that there is no repeats in the input new evidence so we set it to\n-        // the input list but if we found something we then start a new list.\n-        List<EVIDENCE> actuallyAdded = newSampleEvidence;\n-\n-        int i, nextIndex = sampleEvidence.size();\n-        final int stop = newSampleEvidence.size();\n-        for (i = 0; i < stop; i++) {\n-            final EVIDENCE newEvidence = newSampleEvidence.get(i);\n-            final int previousValue = sampleEvidenceIndex.put(newEvidence, nextIndex);\n-            if (previousValue == MISSING_INDEX) {\n-                nextIndex++;\n-                sampleEvidence.add(newEvidence);\n-            } else {\n-                actuallyAdded = new ArrayList<>(newSampleEvidence.subList(0, i));\n-                i++; // skip the repeated element.\n-                break;\n-            }\n-        }\n-        // second for below only use if we encounter some evidence that is already in the table:\n-        for (; i < stop; i++) {\n-            final EVIDENCE newEvidence = newSampleEvidence.get(i);\n+        int nextIndex = sampleEvidence.size();\n+        for (final EVIDENCE newEvidence : newSampleEvidence) {\n             final int previousValue = sampleEvidenceIndex.put(newEvidence, nextIndex);\n             if (previousValue == MISSING_INDEX) {\n                 nextIndex++;\n                 sampleEvidence.add(newEvidence);\n-                actuallyAdded.add(newEvidence);\n             } else {\n                 sampleEvidenceIndex.put(newEvidence, previousValue); // revert\n             }\n         }\n+\n         numberOfEvidences[sampleIndex] = sampleEvidence.size();\n-        return actuallyAdded;\n+        return sampleEvidence.size() - previousEvidenceCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyNDkwOQ==", "bodyText": "This method is private do we really need to check this, can we simply assume the input is correct?", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429724909", "createdAt": "2020-05-25T04:17:23Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1129,60 +1112,50 @@ private void removeEvidence(final int sampleIndex, final Collection<EVIDENCE> ev\n         removeEvidenceByIndex(sampleIndex, indexesToRemove);\n     }\n \n+    // remove evidence and unset the {@code evidenceIndexBySampleIndex} cache for this sample\n     // assumes that evidencesToRemove is sorted and without duplicates.\n     private void removeEvidenceByIndex(final int sampleIndex, final int[] evidencesToRemove) {\n-        if (evidencesToRemove.length == 0) {\n+        final int numToRemove = evidencesToRemove.length;\n+        if (numToRemove == 0) {\n             return;\n-        } else {\n-            final Object2IntMap<EVIDENCE> evidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n-            final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n-            final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n-            if (newEvidenceCount < 0) {\n-                throw new IllegalStateException(\"attempt to remove non-existent evidence or repeated evidence\");\n-            } else if (newEvidenceCount == 0) { // taking in consideration the assumption we simply remove\n-                // all evidence.\n-                evidenceIndex.clear();\n-                numberOfEvidences[sampleIndex] = 0;\n+        }\n+        final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n+        final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n+        Utils.validate(newEvidenceCount >= 0, \"attempted to remove non-existent evidence or repeated evidence\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyNjU0Mw==", "bodyText": "You can be a bit more efficient by putting the next to skip \"on-deck\":\nfor (int n = 0, numRemoved = 0, nextToRemove = evidencesToRemove[0]; n < oldEvidenceCount; n++) {\n     if (n == nextToRemove) {\n         nextToRemove = ++numRemoved < evidencesToRemove.length ? evidencesToRemove[numRemoved] : -1;\n     } else {\n         newEvidence.add(oldEvidence.get(n));\n     }\n}", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429726543", "createdAt": "2020-05-25T04:27:38Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1129,60 +1112,50 @@ private void removeEvidence(final int sampleIndex, final Collection<EVIDENCE> ev\n         removeEvidenceByIndex(sampleIndex, indexesToRemove);\n     }\n \n+    // remove evidence and unset the {@code evidenceIndexBySampleIndex} cache for this sample\n     // assumes that evidencesToRemove is sorted and without duplicates.\n     private void removeEvidenceByIndex(final int sampleIndex, final int[] evidencesToRemove) {\n-        if (evidencesToRemove.length == 0) {\n+        final int numToRemove = evidencesToRemove.length;\n+        if (numToRemove == 0) {\n             return;\n-        } else {\n-            final Object2IntMap<EVIDENCE> evidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n-            final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n-            final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n-            if (newEvidenceCount < 0) {\n-                throw new IllegalStateException(\"attempt to remove non-existent evidence or repeated evidence\");\n-            } else if (newEvidenceCount == 0) { // taking in consideration the assumption we simply remove\n-                // all evidence.\n-                evidenceIndex.clear();\n-                numberOfEvidences[sampleIndex] = 0;\n+        }\n+        final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n+        final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n+        Utils.validate(newEvidenceCount >= 0, \"attempted to remove non-existent evidence or repeated evidence\");\n+\n+        // update the list of evidence and evidence count\n+        final List<EVIDENCE> oldEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final List<EVIDENCE> newEvidence = new ArrayList<>(newEvidenceCount);\n+        for (int n = 0, numRemoved = 0; n < oldEvidenceCount; n++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyNjgyNQ==", "bodyText": "Again a private does not need to perform this checks... This is something that must be cached by unit tests, perhaps you should increase testing coverage if you are worried that this is a real issue.", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429726825", "createdAt": "2020-05-25T04:29:36Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1129,60 +1112,50 @@ private void removeEvidence(final int sampleIndex, final Collection<EVIDENCE> ev\n         removeEvidenceByIndex(sampleIndex, indexesToRemove);\n     }\n \n+    // remove evidence and unset the {@code evidenceIndexBySampleIndex} cache for this sample\n     // assumes that evidencesToRemove is sorted and without duplicates.\n     private void removeEvidenceByIndex(final int sampleIndex, final int[] evidencesToRemove) {\n-        if (evidencesToRemove.length == 0) {\n+        final int numToRemove = evidencesToRemove.length;\n+        if (numToRemove == 0) {\n             return;\n-        } else {\n-            final Object2IntMap<EVIDENCE> evidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n-            final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n-            final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n-            if (newEvidenceCount < 0) {\n-                throw new IllegalStateException(\"attempt to remove non-existent evidence or repeated evidence\");\n-            } else if (newEvidenceCount == 0) { // taking in consideration the assumption we simply remove\n-                // all evidence.\n-                evidenceIndex.clear();\n-                numberOfEvidences[sampleIndex] = 0;\n+        }\n+        final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n+        final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n+        Utils.validate(newEvidenceCount >= 0, \"attempted to remove non-existent evidence or repeated evidence\");\n+\n+        // update the list of evidence and evidence count\n+        final List<EVIDENCE> oldEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final List<EVIDENCE> newEvidence = new ArrayList<>(newEvidenceCount);\n+        for (int n = 0, numRemoved = 0; n < oldEvidenceCount; n++) {\n+            if (numRemoved < numToRemove && n == evidencesToRemove[numRemoved]) {\n+                numRemoved++;\n             } else {\n-                final List<EVIDENCE> evidences = evidenceBySampleIndex.get(sampleIndex);\n-                final double[][] values = valuesBySampleIndex[sampleIndex];\n-\n-                int nextIndexToRemove = evidencesToRemove[0];\n-                if (nextIndexToRemove < 0) {\n-                    throw new IllegalStateException(\"invalid input index array as it contains negatives\");\n-                }\n-                evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {\n-                    if (etrIndex < evidencesToRemove.length) {\n-                        nextIndexToRemove = evidencesToRemove[etrIndex];\n-                        if (nextIndexToRemove < from) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        } else if (nextIndexToRemove >= oldEvidenceCount) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        }\n-                        evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                    } else {\n-                        nextIndexToRemove = oldEvidenceCount;\n-                    }\n-                    for (; from < nextIndexToRemove; from++) {\n-                        final EVIDENCE evidence = evidences.get(from);\n-                        evidences.set(to, evidence);\n-                        evidenceIndex.put(evidence, to++);\n-                    }\n-                }\n-                Utils.truncate(evidences, newEvidenceCount);\n-                // now we do the likelihood values, we save ourselves all the checks:\n-                for (final double[] alleleValues : values) {\n-                    for (int etrIndex = 1, to = evidencesToRemove[0], from = to + 1; to < newEvidenceCount; from++, etrIndex++) {\n-                        nextIndexToRemove = etrIndex < evidencesToRemove.length ? evidencesToRemove[etrIndex] : oldEvidenceCount;\n-                        for (; from < nextIndexToRemove; from++) {\n-                            alleleValues[to++] = alleleValues[from];\n-                        }\n-                    }\n+                newEvidence.add(oldEvidence.get(n));\n+            }\n+        }\n+        Utils.validate(newEvidence.size() == newEvidenceCount, \"Indices to remove contained duplicates, was not ordered, or contained out-of-range indices.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyNzg2OQ==", "bodyText": "why we have two for loops that seem to do pretty much the same. Can we simply move this line of code to the for loop above?... I mean:\nfor (...) {\n   if (n == nextToRemove) {\n     ...\n   } else {\n     ...\n     for (final double[] alleleValues : valuesBySampleIndex[sampleIndex]) {\n          alleleValues[n - numRemoved] = alleleValues[n];\n     }\n   }\n\nOtherwise if you want to keep it this way, you could start the loop on the first removed-index:\n   for (int n = evidencesToRemove[0] + 1, ...; n < oldEvidenceCount; n++) {\n       ...\n   }", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429727869", "createdAt": "2020-05-25T04:36:26Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1129,60 +1112,50 @@ private void removeEvidence(final int sampleIndex, final Collection<EVIDENCE> ev\n         removeEvidenceByIndex(sampleIndex, indexesToRemove);\n     }\n \n+    // remove evidence and unset the {@code evidenceIndexBySampleIndex} cache for this sample\n     // assumes that evidencesToRemove is sorted and without duplicates.\n     private void removeEvidenceByIndex(final int sampleIndex, final int[] evidencesToRemove) {\n-        if (evidencesToRemove.length == 0) {\n+        final int numToRemove = evidencesToRemove.length;\n+        if (numToRemove == 0) {\n             return;\n-        } else {\n-            final Object2IntMap<EVIDENCE> evidenceIndex = evidenceIndexBySampleIndex(sampleIndex);\n-            final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n-            final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n-            if (newEvidenceCount < 0) {\n-                throw new IllegalStateException(\"attempt to remove non-existent evidence or repeated evidence\");\n-            } else if (newEvidenceCount == 0) { // taking in consideration the assumption we simply remove\n-                // all evidence.\n-                evidenceIndex.clear();\n-                numberOfEvidences[sampleIndex] = 0;\n+        }\n+        final int oldEvidenceCount = numberOfEvidences[sampleIndex];\n+        final int newEvidenceCount = oldEvidenceCount - evidencesToRemove.length;\n+        Utils.validate(newEvidenceCount >= 0, \"attempted to remove non-existent evidence or repeated evidence\");\n+\n+        // update the list of evidence and evidence count\n+        final List<EVIDENCE> oldEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final List<EVIDENCE> newEvidence = new ArrayList<>(newEvidenceCount);\n+        for (int n = 0, numRemoved = 0; n < oldEvidenceCount; n++) {\n+            if (numRemoved < numToRemove && n == evidencesToRemove[numRemoved]) {\n+                numRemoved++;\n             } else {\n-                final List<EVIDENCE> evidences = evidenceBySampleIndex.get(sampleIndex);\n-                final double[][] values = valuesBySampleIndex[sampleIndex];\n-\n-                int nextIndexToRemove = evidencesToRemove[0];\n-                if (nextIndexToRemove < 0) {\n-                    throw new IllegalStateException(\"invalid input index array as it contains negatives\");\n-                }\n-                evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {\n-                    if (etrIndex < evidencesToRemove.length) {\n-                        nextIndexToRemove = evidencesToRemove[etrIndex];\n-                        if (nextIndexToRemove < from) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        } else if (nextIndexToRemove >= oldEvidenceCount) {\n-                            throw new IllegalStateException(\"invalid input index array contains indexes out of order\");\n-                        }\n-                        evidenceIndex.remove(evidences.get(nextIndexToRemove));\n-                    } else {\n-                        nextIndexToRemove = oldEvidenceCount;\n-                    }\n-                    for (; from < nextIndexToRemove; from++) {\n-                        final EVIDENCE evidence = evidences.get(from);\n-                        evidences.set(to, evidence);\n-                        evidenceIndex.put(evidence, to++);\n-                    }\n-                }\n-                Utils.truncate(evidences, newEvidenceCount);\n-                // now we do the likelihood values, we save ourselves all the checks:\n-                for (final double[] alleleValues : values) {\n-                    for (int etrIndex = 1, to = evidencesToRemove[0], from = to + 1; to < newEvidenceCount; from++, etrIndex++) {\n-                        nextIndexToRemove = etrIndex < evidencesToRemove.length ? evidencesToRemove[etrIndex] : oldEvidenceCount;\n-                        for (; from < nextIndexToRemove; from++) {\n-                            alleleValues[to++] = alleleValues[from];\n-                        }\n-                    }\n+                newEvidence.add(oldEvidence.get(n));\n+            }\n+        }\n+        Utils.validate(newEvidence.size() == newEvidenceCount, \"Indices to remove contained duplicates, was not ordered, or contained out-of-range indices.\");\n+        evidenceBySampleIndex.set(sampleIndex, newEvidence);\n+        numberOfEvidences[sampleIndex] = newEvidenceCount;\n+\n+        invalidateEvidenceToIndexCache(sampleIndex);\n+\n+        // update the likelihoods arrays in place\n+        for (final double[] alleleValues : valuesBySampleIndex[sampleIndex]) {\n+            for (int n = 0, numRemoved = 0; n < oldEvidenceCount; n++) {\n+                if (numRemoved < numToRemove && n == evidencesToRemove[numRemoved]) {\n+                    numRemoved++;\n+                } else {\n+                    alleleValues[n - numRemoved] = alleleValues[n];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyODU2MA==", "bodyText": "You can avoid the double get if the index map is not null, but is just a direct get acces to an array-list so perhaps not worth the trouble.", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429728560", "createdAt": "2020-05-25T04:40:55Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1216,18 +1189,26 @@ public void filterPoorlyModeledEvidence(final ToDoubleFunction<EVIDENCE> log10Mi\n \n     private Object2IntMap<EVIDENCE> evidenceIndexBySampleIndex(final int sampleIndex) {\n         if (evidenceIndexBySampleIndex.get(sampleIndex) == null) {\n-            final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n-            final int sampleEvidenceCount = sampleEvidence.size();\n-            final Object2IntMap<EVIDENCE> index = new Object2IntOpenHashMap<>(sampleEvidenceCount);\n-            index.defaultReturnValue(MISSING_INDEX);\n-            evidenceIndexBySampleIndex.set(sampleIndex, index);\n-            for (int r = 0; r < sampleEvidenceCount; r++) {\n-                index.put(sampleEvidence.get(r), r);\n-            }\n-            return index;\n-        } else {\n-            return evidenceIndexBySampleIndex.get(sampleIndex);\n+            fillEvidenceToIndexCache(sampleIndex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyODkwMg==", "bodyText": "We are not multithread safe in general but as a matter of principle I think you should only set the list once the map is filled and not before.", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429728902", "createdAt": "2020-05-25T04:43:12Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1216,18 +1189,26 @@ public void filterPoorlyModeledEvidence(final ToDoubleFunction<EVIDENCE> log10Mi\n \n     private Object2IntMap<EVIDENCE> evidenceIndexBySampleIndex(final int sampleIndex) {\n         if (evidenceIndexBySampleIndex.get(sampleIndex) == null) {\n-            final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n-            final int sampleEvidenceCount = sampleEvidence.size();\n-            final Object2IntMap<EVIDENCE> index = new Object2IntOpenHashMap<>(sampleEvidenceCount);\n-            index.defaultReturnValue(MISSING_INDEX);\n-            evidenceIndexBySampleIndex.set(sampleIndex, index);\n-            for (int r = 0; r < sampleEvidenceCount; r++) {\n-                index.put(sampleEvidence.get(r), r);\n-            }\n-            return index;\n-        } else {\n-            return evidenceIndexBySampleIndex.get(sampleIndex);\n+            fillEvidenceToIndexCache(sampleIndex);\n         }\n+        return evidenceIndexBySampleIndex.get(sampleIndex);\n+    }\n+\n+    @VisibleForTesting\n+    void fillEvidenceToIndexCache(int sampleIndex) {\n+        final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final int sampleEvidenceCount = sampleEvidence.size();\n+        final Object2IntMap<EVIDENCE> index = new Object2IntOpenHashMap<>(sampleEvidenceCount);\n+        index.defaultReturnValue(MISSING_INDEX);\n+        evidenceIndexBySampleIndex.set(sampleIndex, index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyOTI4MQ==", "bodyText": "what about using a foreach:\nint nextIdx = 0;\nfor( final EVIDENCE evi : sampleEvidence) {\n    index.put(evi, nextIdx++);\n}", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429729281", "createdAt": "2020-05-25T04:45:39Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1216,18 +1189,26 @@ public void filterPoorlyModeledEvidence(final ToDoubleFunction<EVIDENCE> log10Mi\n \n     private Object2IntMap<EVIDENCE> evidenceIndexBySampleIndex(final int sampleIndex) {\n         if (evidenceIndexBySampleIndex.get(sampleIndex) == null) {\n-            final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n-            final int sampleEvidenceCount = sampleEvidence.size();\n-            final Object2IntMap<EVIDENCE> index = new Object2IntOpenHashMap<>(sampleEvidenceCount);\n-            index.defaultReturnValue(MISSING_INDEX);\n-            evidenceIndexBySampleIndex.set(sampleIndex, index);\n-            for (int r = 0; r < sampleEvidenceCount; r++) {\n-                index.put(sampleEvidence.get(r), r);\n-            }\n-            return index;\n-        } else {\n-            return evidenceIndexBySampleIndex.get(sampleIndex);\n+            fillEvidenceToIndexCache(sampleIndex);\n         }\n+        return evidenceIndexBySampleIndex.get(sampleIndex);\n+    }\n+\n+    @VisibleForTesting\n+    void fillEvidenceToIndexCache(int sampleIndex) {\n+        final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final int sampleEvidenceCount = sampleEvidence.size();\n+        final Object2IntMap<EVIDENCE> index = new Object2IntOpenHashMap<>(sampleEvidenceCount);\n+        index.defaultReturnValue(MISSING_INDEX);\n+        evidenceIndexBySampleIndex.set(sampleIndex, index);\n+        for (int r = 0; r < sampleEvidenceCount; r++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTcyOTYzOQ==", "bodyText": "I like more ...IsPresent rather than ...isFilled, but my mother's tongle is not English.", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429729639", "createdAt": "2020-05-25T04:47:48Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -1216,18 +1189,26 @@ public void filterPoorlyModeledEvidence(final ToDoubleFunction<EVIDENCE> log10Mi\n \n     private Object2IntMap<EVIDENCE> evidenceIndexBySampleIndex(final int sampleIndex) {\n         if (evidenceIndexBySampleIndex.get(sampleIndex) == null) {\n-            final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n-            final int sampleEvidenceCount = sampleEvidence.size();\n-            final Object2IntMap<EVIDENCE> index = new Object2IntOpenHashMap<>(sampleEvidenceCount);\n-            index.defaultReturnValue(MISSING_INDEX);\n-            evidenceIndexBySampleIndex.set(sampleIndex, index);\n-            for (int r = 0; r < sampleEvidenceCount; r++) {\n-                index.put(sampleEvidence.get(r), r);\n-            }\n-            return index;\n-        } else {\n-            return evidenceIndexBySampleIndex.get(sampleIndex);\n+            fillEvidenceToIndexCache(sampleIndex);\n         }\n+        return evidenceIndexBySampleIndex.get(sampleIndex);\n+    }\n+\n+    @VisibleForTesting\n+    void fillEvidenceToIndexCache(int sampleIndex) {\n+        final List<EVIDENCE> sampleEvidence = evidenceBySampleIndex.get(sampleIndex);\n+        final int sampleEvidenceCount = sampleEvidence.size();\n+        final Object2IntMap<EVIDENCE> index = new Object2IntOpenHashMap<>(sampleEvidenceCount);\n+        index.defaultReturnValue(MISSING_INDEX);\n+        evidenceIndexBySampleIndex.set(sampleIndex, index);\n+        for (int r = 0; r < sampleEvidenceCount; r++) {\n+            index.put(sampleEvidence.get(r), r);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    boolean evidenceToIndexCacheIsFilled(final int sampleIndex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTczMDU3Nw==", "bodyText": "IMO this cache is a implementation business and the using code is not supposed to know or care about it. I don't think there is the need to check the inner state of the likelihood collection this way which results in exposing its workings for the sake of this testing.\nInstead you could focus that the index are consistent after several mutating operations.", "url": "https://github.com/broadinstitute/gatk/pull/6593#discussion_r429730577", "createdAt": "2020-05-25T04:54:01Z", "author": {"login": "vruano"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoodsUnitTest.java", "diffHunk": "@@ -174,11 +174,19 @@ public void testFilterPoorlyModeledReads(final String[] samples, final Allele[]\n         final AlleleLikelihoods<GATKRead, Allele> original = makeGoodAndBadLikelihoods(samples, alleles, reads);\n \n         final AlleleLikelihoods<GATKRead, Allele> result = makeGoodAndBadLikelihoods(samples, alleles, reads);\n+\n+        // fill the evidence-to-index cache now to check that it is invalidated below", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72f6f4fe5fd3b91a6a097fa1931ce9f7d18c7432"}, "originalPosition": 23}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9494c4ea231f5f8feef18679bcc6de2eba12145d", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/9494c4ea231f5f8feef18679bcc6de2eba12145d", "committedDate": "2020-05-26T05:56:39Z", "message": "Valentin edits"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4Mjc4NzEx", "url": "https://github.com/broadinstitute/gatk/pull/6593#pullrequestreview-418278711", "createdAt": "2020-05-26T13:08:06Z", "commit": {"oid": "9494c4ea231f5f8feef18679bcc6de2eba12145d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2573, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}