{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2ODgxNDYx", "number": 6860, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxMjo1MVrOEp3tuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDoxNToxMVrOEqVntQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzM5ODk5OnYy", "diffSide": "RIGHT", "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxMjo1MVrOHbzfag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNzoyNzoyM1rOHb1ujg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODkxNzIyNg==", "bodyText": "id++ should be changed to ++id\nthis was in a recent PR of mine, but the rest looks correct.\nEventually we should add a test that verifies 1. there is no probe 0 and 2. that 2 different runs to this code yield the same probe->id map (these were both bugs in the past. and don't feel like you have to do that for this PR)", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r498917226", "createdAt": "2020-10-02T16:12:51Z", "author": {"login": "ahaessly"}, "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "diffHunk": "@@ -0,0 +1,150 @@\n+version 1.0\n+\n+workflow ImportArrayManifest {\n+\n+  input {\n+    File extended_manifest_csv\n+    File manifest_schema_json\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+ \n+    Int? preemptible_tries\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+ \n+  call CreateManifestCsv {\n+    input:\n+      extended_manifest_csv = extended_manifest_csv,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+\n+  call LoadManifest {\n+    input:\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      table_name = table_name,\n+      manifest_schema_json = manifest_schema_json,\n+      manifest_csv = CreateManifestCsv.manifest_csv,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+  output {\n+    File manifest_csv = CreateManifestCsv.manifest_csv\n+    File manifest_ingest_csv = CreateManifestCsv.manifest_ingest_csv\n+    File manifest_sub_csv = CreateManifestCsv.manifest_sub_csv\n+    File manifest_proc_csv = CreateManifestCsv.manifest_proc_csv\n+  }\n+}\n+\n+task LoadManifest {\n+  input {\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+    File manifest_csv\n+    File manifest_schema_json\n+    # runtime\n+    Int? preemptible_tries\n+    String docker\n+    # String to add command for testing only. Can be ignored otherwise.\n+    String? for_testing_only\n+  }\n+\n+  String ingest_table = dataset_name + \".\" + select_first([table_name, \"probe_info\"])\n+\n+  parameter_meta {\n+    manifest_schema_json: {\n+      localization_optional: false\n+    }\n+  }\n+   \n+    command <<<\n+      set +e\n+      ~{for_testing_only}\n+      bq ls --project_id ~{project_id} ~{dataset_name} > /dev/null\n+      if [ $? -ne 0 ]; then\n+        echo \"making dataset ~{project_id}.~{dataset_name}\"\n+        bq mk --project_id=~{project_id} ~{dataset_name}\n+      fi\n+      bq show --project_id ~{project_id} ~{ingest_table} > /dev/null\n+      if [ $? -ne 0 ]; then\n+        echo \"making table ~{ingest_table}\"\n+        # create a site info table and load - schema and TSV header need to be the same order\n+        bq --location=US mk --project_id=~{project_id} ~{ingest_table} ~{manifest_schema_json}\n+      fi\n+      set -e\n+\n+      bq load --location=US --project_id=~{project_id} --null_marker \"null\" --source_format=CSV ~{ingest_table} ~{manifest_csv} ~{manifest_schema_json}\n+    >>>\n+    runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + 20 + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+\n+}\n+\n+task CreateManifestCsv {\n+  input {\n+    File extended_manifest_csv\n+\n+    # runtime\n+    Int? preemptible_tries\n+    String docker\n+  }\n+\n+  Int disk_size = ceil(size(extended_manifest_csv, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    extended_manifest_csv: {\n+      localization_optional: false\n+    }\n+  }\n+  command <<<\n+    set -e\n+\n+    TMP_SORTED=\"manifest_ingest_sorted.csv\"\n+    TMP_SUB=\"manifest_ingest_sub.csv\"\n+    TMP_PROC=\"manifest_ingest_processed.csv\"\n+    TMP=\"manifest_ingest.csv\"\n+\n+    # put integers in front of the chromosomes that are not numbered so that they end up ordered by X, Y and MT\n+    sed 's/,X,/,23X,/g; s/,Y,/,24Y,/g; s/,MT,/,25MT,/g' ~{extended_manifest_csv} > $TMP_SUB\n+\n+    # sort the probes by chrom, position and then name so there is a specific ordering when we assign integers\n+    sort -t , -k23n,23 -k24n,24 -k2,2 $TMP_SUB > $TMP_SORTED\n+\n+    # checking for != \"build37Flag\" skips the header row (we don't want that numbered)\n+    # only process rows with 29 fields - this skips some header info fields\n+    # also skip entries that are flagged, not matched or have index conflict\n+    awk -F ',' 'NF==29 && ($29!=\"ILLUMINA_FLAGGED\" && $29!=\"INDEL_NOT_MATCHED\" && $29!=\"INDEL_CONFLICT\" && $29!=\"build37Flag\") { flag=$29; if ($29==\"PASS\") flag=\"null\"; print id++\",\"$2\",\"$9\",\"$23\",\"$24\",\"$25\",\"$26\",\"$27\",\"flag }' $TMP_SORTED > $TMP_PROC", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "feeaa1cb1dee552ad6966455524aee222a98cf71"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk1Mzg3MA==", "bodyText": "Definitely considering it a TODO to add tests that actually make sure the data that was imported is correct. I haven't figured out the best way to do this though. Maybe I need a GATK tool that pulls down a table and compares to an expected file? Or maybe by looking at the extracted VCF from the full \"end-to-end\" test we'll have enough coverage? I'm not sure if that part of the pipeline pulls down information in the manifest.", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r498953870", "createdAt": "2020-10-02T17:27:23Z", "author": {"login": "meganshand"}, "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "diffHunk": "@@ -0,0 +1,150 @@\n+version 1.0\n+\n+workflow ImportArrayManifest {\n+\n+  input {\n+    File extended_manifest_csv\n+    File manifest_schema_json\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+ \n+    Int? preemptible_tries\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+ \n+  call CreateManifestCsv {\n+    input:\n+      extended_manifest_csv = extended_manifest_csv,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+\n+  call LoadManifest {\n+    input:\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      table_name = table_name,\n+      manifest_schema_json = manifest_schema_json,\n+      manifest_csv = CreateManifestCsv.manifest_csv,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+  output {\n+    File manifest_csv = CreateManifestCsv.manifest_csv\n+    File manifest_ingest_csv = CreateManifestCsv.manifest_ingest_csv\n+    File manifest_sub_csv = CreateManifestCsv.manifest_sub_csv\n+    File manifest_proc_csv = CreateManifestCsv.manifest_proc_csv\n+  }\n+}\n+\n+task LoadManifest {\n+  input {\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+    File manifest_csv\n+    File manifest_schema_json\n+    # runtime\n+    Int? preemptible_tries\n+    String docker\n+    # String to add command for testing only. Can be ignored otherwise.\n+    String? for_testing_only\n+  }\n+\n+  String ingest_table = dataset_name + \".\" + select_first([table_name, \"probe_info\"])\n+\n+  parameter_meta {\n+    manifest_schema_json: {\n+      localization_optional: false\n+    }\n+  }\n+   \n+    command <<<\n+      set +e\n+      ~{for_testing_only}\n+      bq ls --project_id ~{project_id} ~{dataset_name} > /dev/null\n+      if [ $? -ne 0 ]; then\n+        echo \"making dataset ~{project_id}.~{dataset_name}\"\n+        bq mk --project_id=~{project_id} ~{dataset_name}\n+      fi\n+      bq show --project_id ~{project_id} ~{ingest_table} > /dev/null\n+      if [ $? -ne 0 ]; then\n+        echo \"making table ~{ingest_table}\"\n+        # create a site info table and load - schema and TSV header need to be the same order\n+        bq --location=US mk --project_id=~{project_id} ~{ingest_table} ~{manifest_schema_json}\n+      fi\n+      set -e\n+\n+      bq load --location=US --project_id=~{project_id} --null_marker \"null\" --source_format=CSV ~{ingest_table} ~{manifest_csv} ~{manifest_schema_json}\n+    >>>\n+    runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + 20 + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+\n+}\n+\n+task CreateManifestCsv {\n+  input {\n+    File extended_manifest_csv\n+\n+    # runtime\n+    Int? preemptible_tries\n+    String docker\n+  }\n+\n+  Int disk_size = ceil(size(extended_manifest_csv, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    extended_manifest_csv: {\n+      localization_optional: false\n+    }\n+  }\n+  command <<<\n+    set -e\n+\n+    TMP_SORTED=\"manifest_ingest_sorted.csv\"\n+    TMP_SUB=\"manifest_ingest_sub.csv\"\n+    TMP_PROC=\"manifest_ingest_processed.csv\"\n+    TMP=\"manifest_ingest.csv\"\n+\n+    # put integers in front of the chromosomes that are not numbered so that they end up ordered by X, Y and MT\n+    sed 's/,X,/,23X,/g; s/,Y,/,24Y,/g; s/,MT,/,25MT,/g' ~{extended_manifest_csv} > $TMP_SUB\n+\n+    # sort the probes by chrom, position and then name so there is a specific ordering when we assign integers\n+    sort -t , -k23n,23 -k24n,24 -k2,2 $TMP_SUB > $TMP_SORTED\n+\n+    # checking for != \"build37Flag\" skips the header row (we don't want that numbered)\n+    # only process rows with 29 fields - this skips some header info fields\n+    # also skip entries that are flagged, not matched or have index conflict\n+    awk -F ',' 'NF==29 && ($29!=\"ILLUMINA_FLAGGED\" && $29!=\"INDEL_NOT_MATCHED\" && $29!=\"INDEL_CONFLICT\" && $29!=\"build37Flag\") { flag=$29; if ($29==\"PASS\") flag=\"null\"; print id++\",\"$2\",\"$9\",\"$23\",\"$24\",\"$25\",\"$26\",\"$27\",\"flag }' $TMP_SORTED > $TMP_PROC", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODkxNzIyNg=="}, "originalCommit": {"oid": "feeaa1cb1dee552ad6966455524aee222a98cf71"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyODI2OTY1OnYy", "diffSide": "RIGHT", "path": "scripts/variantstore_cromwell_tests/import_array_manifest_test.json", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDowODozM1rOHcew5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDoxNzozOFrOHcfJ0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyNjIxNA==", "bodyText": "Is this just copied from elsewhere?  Seems like there should be a gatk-test google project...", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499626214", "createdAt": "2020-10-05T14:08:33Z", "author": {"login": "kcibul"}, "path": "scripts/variantstore_cromwell_tests/import_array_manifest_test.json", "diffHunk": "@@ -0,0 +1,8 @@\n+{\n+  \"ImportArrayManifest.extended_manifest_csv\":\"/home/travis/build/broadinstitute/gatk/src/test/resources/org/broadinstitute/hellbender/tools/variantdb/arrays/tiny_manifest.csv\",\n+  \"ImportArrayManifest.manifest_schema_json\":\"/home/travis/build/broadinstitute/gatk/scripts/variantstore_wdl/schemas/manifest_schema.json\",\n+  \"ImportArrayManifest.project_id\":\"broad-dsde-dev\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df29cd95aeb3322ff7dbe78d29f28529217f4b0a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMjU5Mg==", "bodyText": "This is the GATK test project AFAIK. It's used in the BigQueryUtils tests (in GATK).", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499632592", "createdAt": "2020-10-05T14:17:38Z", "author": {"login": "meganshand"}, "path": "scripts/variantstore_cromwell_tests/import_array_manifest_test.json", "diffHunk": "@@ -0,0 +1,8 @@\n+{\n+  \"ImportArrayManifest.extended_manifest_csv\":\"/home/travis/build/broadinstitute/gatk/src/test/resources/org/broadinstitute/hellbender/tools/variantdb/arrays/tiny_manifest.csv\",\n+  \"ImportArrayManifest.manifest_schema_json\":\"/home/travis/build/broadinstitute/gatk/scripts/variantstore_wdl/schemas/manifest_schema.json\",\n+  \"ImportArrayManifest.project_id\":\"broad-dsde-dev\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyNjIxNA=="}, "originalCommit": {"oid": "df29cd95aeb3322ff7dbe78d29f28529217f4b0a"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyODI5ODc3OnYy", "diffSide": "RIGHT", "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDoxNToxMVrOHcfDEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDoxOTozMlrOHcfPGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMDg2Ng==", "bodyText": "Are you overriding this?  Or are we always testing with this static GATK version?", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499630866", "createdAt": "2020-10-05T14:15:11Z", "author": {"login": "kcibul"}, "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "diffHunk": "@@ -0,0 +1,150 @@\n+version 1.0\n+\n+workflow ImportArrayManifest {\n+\n+  input {\n+    File extended_manifest_csv\n+    File manifest_schema_json\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+ \n+    Int? preemptible_tries\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df29cd95aeb3322ff7dbe78d29f28529217f4b0a"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMzk0Ng==", "bodyText": "I'm not overriding it in this case because we're not using any GATK tools in this WDL. But maybe we should still be testing the current branch regardless? I know we'll need to for future WDLs (Ingest, calculate metrics, and extract will all use GATK tools that will need to be in the docker from the current branch)", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499633946", "createdAt": "2020-10-05T14:19:32Z", "author": {"login": "meganshand"}, "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "diffHunk": "@@ -0,0 +1,150 @@\n+version 1.0\n+\n+workflow ImportArrayManifest {\n+\n+  input {\n+    File extended_manifest_csv\n+    File manifest_schema_json\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+ \n+    Int? preemptible_tries\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMDg2Ng=="}, "originalCommit": {"oid": "df29cd95aeb3322ff7dbe78d29f28529217f4b0a"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 805, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}