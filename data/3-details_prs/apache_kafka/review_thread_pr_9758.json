{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQwOTk4NTcx", "number": 9758, "reviewThreads": {"totalCount": 62, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwNzowODoyNlrOFHbK0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwNjozMzozM1rOFhYYag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMzI5NDkxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwNzowODoyNlrOII5pCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwNzowODoyNlrOII5pCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjIwMzkxNQ==", "bodyText": "@ijuma This is the only case that we create LazyDownConversionRecords in production. Through this PR, this case can get rid of generic FetchResponse.PartitionData. Hence, we can remove generic from FetchResponse.PartitionData after this PR goes in trunk.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r546203915", "createdAt": "2020-12-19T07:08:26Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -777,12 +775,12 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n \n     def maybeConvertFetchedData(tp: TopicPartition,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bae931be1b6b558ae70498ec5f25412361d12e5a"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDI4OTUyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxMToxMVrOIN3hqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxMToxMVrOIN3hqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMjEzOQ==", "bodyText": "These changes seem unrelated?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551412139", "createdAt": "2021-01-04T16:11:11Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -733,11 +733,11 @@ void resetOffsetIfNeeded(TopicPartition partition, OffsetResetStrategy requested\n     }\n \n     private void resetOffsetsAsync(Map<TopicPartition, Long> partitionResetTimestamps) {\n-        Map<Node, Map<TopicPartition, ListOffsetsPartition>> timestampsToSearchByNode =\n+        Map<Node, Map<TopicPartition, ListOffsetsRequestData.ListOffsetsPartition>> timestampsToSearchByNode =\n                 groupListOffsetRequests(partitionResetTimestamps, new HashSet<>());\n-        for (Map.Entry<Node, Map<TopicPartition, ListOffsetsPartition>> entry : timestampsToSearchByNode.entrySet()) {\n+        for (Map.Entry<Node, Map<TopicPartition, ListOffsetsRequestData.ListOffsetsPartition>> entry : timestampsToSearchByNode.entrySet()) {\n             Node node = entry.getKey();\n-            final Map<TopicPartition, ListOffsetsPartition> resetTimestamps = entry.getValue();\n+            final Map<TopicPartition, ListOffsetsRequestData.ListOffsetsPartition> resetTimestamps = entry.getValue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDMwMDExOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxMzoyMlrOIN3ojQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxNjo0Mjo1M1rOIOepCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzkwMQ==", "bodyText": "Can we remove this altogether?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551413901", "createdAt": "2021-01-04T16:13:22Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ0MTc4Mg==", "bodyText": "partitionData can be removed but responseDataMap is still a response type and it is used by production code. I did not refactor that to avoid big patch.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551441782", "createdAt": "2021-01-04T16:57:48Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzkwMQ=="}, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjA0Mzc2Nw==", "bodyText": "I understand. It's a bit hard to understand the end state for this class as it stands. We've taken a few intermediate steps to this point. Can we just go to the end state now?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552043767", "createdAt": "2021-01-05T16:28:32Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzkwMQ=="}, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjA1MzAwMw==", "bodyText": "You are right. Will address it in next commit.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552053003", "createdAt": "2021-01-05T16:42:53Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzkwMQ=="}, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDMwNzU1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxNDo1N1rOIN3s0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxODo1MTowN1rOIOjHjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxNDk5Mg==", "bodyText": "Do we need this class? What does it add over FetchResponseData.FetchablePartitionRespons?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551414992", "createdAt": "2021-01-04T16:14:57Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n+    /**\n+     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n+     * `FetchRequest.fetchData`.\n+     *\n+     * @param error             The top-level error code.\n+     * @param responseData      The fetched data grouped by partition.\n+     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n+     * @param sessionId         The fetch session id.\n+     */\n+    public FetchResponse(Errors error,\n+                         LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n+                         int throttleTimeMs,\n+                         int sessionId) {\n+        this(error, throttleTimeMs, sessionId, responseData.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey,\n+            entry -> entry.getValue().partitionResponse, (o1, o2) -> {\n+                throw new RuntimeException(\"this is impossible\");\n+            }, LinkedHashMap::new)));\n+    }\n \n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n+    public FetchResponse(Errors error,\n+                         int throttleTimeMs,\n+                         int sessionId,\n+                         LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = new FetchResponseData()\n+                .setSessionId(sessionId)\n+                .setErrorCode(error.code())\n+                .setThrottleTimeMs(throttleTimeMs);\n+        responseData.forEach((tp, tpData) -> data.responses().add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(tp.topic())\n+            .setPartitionResponses(Collections.singletonList(tpData.setPartition(tp.partition())))));\n+        this.partitionData = responseData;\n+    }\n \n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = fetchResponseData;\n+        this.partitionData = new LinkedHashMap<>();\n+        fetchResponseData.responses().forEach(topicResponse ->\n+            topicResponse.partitionResponses().forEach(partitionResponse ->\n+                partitionData.put(new TopicPartition(topicResponse.topic(), partitionResponse.partition()), partitionResponse))\n+        );\n+    }\n \n-            AbortedTransaction that = (AbortedTransaction) o;\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n \n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n+    public LinkedHashMap<TopicPartition, PartitionData<T>> responseData() {\n+        if (responseDataMap == null) {\n+            responseDataMap = new LinkedHashMap<>(partitionData.size());\n+            partitionData.forEach((tp, d) -> responseDataMap.put(tp, new PartitionData<>(d)));\n         }\n+        return responseDataMap;\n+    }\n \n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n+    @Override\n+    public int throttleTimeMs() {\n+        return data.throttleTimeMs();\n+    }\n \n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n+    public int sessionId() {\n+        return data.sessionId();\n+    }\n \n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        Map<Errors, Integer> errorCounts = new HashMap<>();\n+        updateErrorCounts(errorCounts, error());\n+        partitionData.values().forEach(response ->\n+            updateErrorCounts(errorCounts, Errors.forCode(response.errorCode()))\n+        );\n+        return errorCounts;\n     }\n \n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n+    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n+        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n+    }\n \n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n+    /**\n+     * Convenience method to find the size of a response.\n+     *\n+     * @param version       The version of the response to use.\n+     * @param partIterator  The partition iterator.\n+     * @return              The response size in bytes.\n+     */\n+    public static <T extends Records> int sizeOf(short version,\n+                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+        // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n+        // use arbitrary values here without affecting the result.\n+        LinkedHashMap<TopicPartition, PartitionData<T>> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        return 4 + new FetchResponse<>(Errors.NONE, data, 0, INVALID_SESSION_ID).data.size(cache, version);\n+    }\n \n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n+    @Override\n+    public boolean shouldClientThrottle(short version) {\n+        return version >= 8;\n+    }\n \n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n \n-            this.partitionResponse = partitionResponse;\n-        }\n+    public static final class PartitionData<T extends BaseRecords> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ0MTgyMg==", "bodyText": "What does it add over FetchResponseData.FetchablePartitionRespons?\n\nIt offers methods to transfer \"non-defined value\" to Optional type.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551441822", "createdAt": "2021-01-04T16:57:51Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n+    /**\n+     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n+     * `FetchRequest.fetchData`.\n+     *\n+     * @param error             The top-level error code.\n+     * @param responseData      The fetched data grouped by partition.\n+     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n+     * @param sessionId         The fetch session id.\n+     */\n+    public FetchResponse(Errors error,\n+                         LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n+                         int throttleTimeMs,\n+                         int sessionId) {\n+        this(error, throttleTimeMs, sessionId, responseData.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey,\n+            entry -> entry.getValue().partitionResponse, (o1, o2) -> {\n+                throw new RuntimeException(\"this is impossible\");\n+            }, LinkedHashMap::new)));\n+    }\n \n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n+    public FetchResponse(Errors error,\n+                         int throttleTimeMs,\n+                         int sessionId,\n+                         LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = new FetchResponseData()\n+                .setSessionId(sessionId)\n+                .setErrorCode(error.code())\n+                .setThrottleTimeMs(throttleTimeMs);\n+        responseData.forEach((tp, tpData) -> data.responses().add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(tp.topic())\n+            .setPartitionResponses(Collections.singletonList(tpData.setPartition(tp.partition())))));\n+        this.partitionData = responseData;\n+    }\n \n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = fetchResponseData;\n+        this.partitionData = new LinkedHashMap<>();\n+        fetchResponseData.responses().forEach(topicResponse ->\n+            topicResponse.partitionResponses().forEach(partitionResponse ->\n+                partitionData.put(new TopicPartition(topicResponse.topic(), partitionResponse.partition()), partitionResponse))\n+        );\n+    }\n \n-            AbortedTransaction that = (AbortedTransaction) o;\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n \n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n+    public LinkedHashMap<TopicPartition, PartitionData<T>> responseData() {\n+        if (responseDataMap == null) {\n+            responseDataMap = new LinkedHashMap<>(partitionData.size());\n+            partitionData.forEach((tp, d) -> responseDataMap.put(tp, new PartitionData<>(d)));\n         }\n+        return responseDataMap;\n+    }\n \n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n+    @Override\n+    public int throttleTimeMs() {\n+        return data.throttleTimeMs();\n+    }\n \n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n+    public int sessionId() {\n+        return data.sessionId();\n+    }\n \n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        Map<Errors, Integer> errorCounts = new HashMap<>();\n+        updateErrorCounts(errorCounts, error());\n+        partitionData.values().forEach(response ->\n+            updateErrorCounts(errorCounts, Errors.forCode(response.errorCode()))\n+        );\n+        return errorCounts;\n     }\n \n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n+    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n+        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n+    }\n \n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n+    /**\n+     * Convenience method to find the size of a response.\n+     *\n+     * @param version       The version of the response to use.\n+     * @param partIterator  The partition iterator.\n+     * @return              The response size in bytes.\n+     */\n+    public static <T extends Records> int sizeOf(short version,\n+                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+        // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n+        // use arbitrary values here without affecting the result.\n+        LinkedHashMap<TopicPartition, PartitionData<T>> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        return 4 + new FetchResponse<>(Errors.NONE, data, 0, INVALID_SESSION_ID).data.size(cache, version);\n+    }\n \n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n+    @Override\n+    public boolean shouldClientThrottle(short version) {\n+        return version >= 8;\n+    }\n \n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n \n-            this.partitionResponse = partitionResponse;\n-        }\n+    public static final class PartitionData<T extends BaseRecords> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxNDk5Mg=="}, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjA0NzEzOQ==", "bodyText": "It seems like we do it for a couple of cases. It doesn't seem worth it given that fetch is one of the hot paths. We could have static helper methods instead if helpful.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552047139", "createdAt": "2021-01-05T16:33:30Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n+    /**\n+     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n+     * `FetchRequest.fetchData`.\n+     *\n+     * @param error             The top-level error code.\n+     * @param responseData      The fetched data grouped by partition.\n+     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n+     * @param sessionId         The fetch session id.\n+     */\n+    public FetchResponse(Errors error,\n+                         LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n+                         int throttleTimeMs,\n+                         int sessionId) {\n+        this(error, throttleTimeMs, sessionId, responseData.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey,\n+            entry -> entry.getValue().partitionResponse, (o1, o2) -> {\n+                throw new RuntimeException(\"this is impossible\");\n+            }, LinkedHashMap::new)));\n+    }\n \n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n+    public FetchResponse(Errors error,\n+                         int throttleTimeMs,\n+                         int sessionId,\n+                         LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = new FetchResponseData()\n+                .setSessionId(sessionId)\n+                .setErrorCode(error.code())\n+                .setThrottleTimeMs(throttleTimeMs);\n+        responseData.forEach((tp, tpData) -> data.responses().add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(tp.topic())\n+            .setPartitionResponses(Collections.singletonList(tpData.setPartition(tp.partition())))));\n+        this.partitionData = responseData;\n+    }\n \n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = fetchResponseData;\n+        this.partitionData = new LinkedHashMap<>();\n+        fetchResponseData.responses().forEach(topicResponse ->\n+            topicResponse.partitionResponses().forEach(partitionResponse ->\n+                partitionData.put(new TopicPartition(topicResponse.topic(), partitionResponse.partition()), partitionResponse))\n+        );\n+    }\n \n-            AbortedTransaction that = (AbortedTransaction) o;\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n \n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n+    public LinkedHashMap<TopicPartition, PartitionData<T>> responseData() {\n+        if (responseDataMap == null) {\n+            responseDataMap = new LinkedHashMap<>(partitionData.size());\n+            partitionData.forEach((tp, d) -> responseDataMap.put(tp, new PartitionData<>(d)));\n         }\n+        return responseDataMap;\n+    }\n \n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n+    @Override\n+    public int throttleTimeMs() {\n+        return data.throttleTimeMs();\n+    }\n \n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n+    public int sessionId() {\n+        return data.sessionId();\n+    }\n \n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        Map<Errors, Integer> errorCounts = new HashMap<>();\n+        updateErrorCounts(errorCounts, error());\n+        partitionData.values().forEach(response ->\n+            updateErrorCounts(errorCounts, Errors.forCode(response.errorCode()))\n+        );\n+        return errorCounts;\n     }\n \n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n+    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n+        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n+    }\n \n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n+    /**\n+     * Convenience method to find the size of a response.\n+     *\n+     * @param version       The version of the response to use.\n+     * @param partIterator  The partition iterator.\n+     * @return              The response size in bytes.\n+     */\n+    public static <T extends Records> int sizeOf(short version,\n+                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+        // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n+        // use arbitrary values here without affecting the result.\n+        LinkedHashMap<TopicPartition, PartitionData<T>> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        return 4 + new FetchResponse<>(Errors.NONE, data, 0, INVALID_SESSION_ID).data.size(cache, version);\n+    }\n \n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n+    @Override\n+    public boolean shouldClientThrottle(short version) {\n+        return version >= 8;\n+    }\n \n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n \n-            this.partitionResponse = partitionResponse;\n-        }\n+    public static final class PartitionData<T extends BaseRecords> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxNDk5Mg=="}, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyNjM1MQ==", "bodyText": "Nice suggestions. will address it.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552126351", "createdAt": "2021-01-05T18:51:07Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n+    /**\n+     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n+     * `FetchRequest.fetchData`.\n+     *\n+     * @param error             The top-level error code.\n+     * @param responseData      The fetched data grouped by partition.\n+     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n+     * @param sessionId         The fetch session id.\n+     */\n+    public FetchResponse(Errors error,\n+                         LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n+                         int throttleTimeMs,\n+                         int sessionId) {\n+        this(error, throttleTimeMs, sessionId, responseData.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey,\n+            entry -> entry.getValue().partitionResponse, (o1, o2) -> {\n+                throw new RuntimeException(\"this is impossible\");\n+            }, LinkedHashMap::new)));\n+    }\n \n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n+    public FetchResponse(Errors error,\n+                         int throttleTimeMs,\n+                         int sessionId,\n+                         LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = new FetchResponseData()\n+                .setSessionId(sessionId)\n+                .setErrorCode(error.code())\n+                .setThrottleTimeMs(throttleTimeMs);\n+        responseData.forEach((tp, tpData) -> data.responses().add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(tp.topic())\n+            .setPartitionResponses(Collections.singletonList(tpData.setPartition(tp.partition())))));\n+        this.partitionData = responseData;\n+    }\n \n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = fetchResponseData;\n+        this.partitionData = new LinkedHashMap<>();\n+        fetchResponseData.responses().forEach(topicResponse ->\n+            topicResponse.partitionResponses().forEach(partitionResponse ->\n+                partitionData.put(new TopicPartition(topicResponse.topic(), partitionResponse.partition()), partitionResponse))\n+        );\n+    }\n \n-            AbortedTransaction that = (AbortedTransaction) o;\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n \n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n+    public LinkedHashMap<TopicPartition, PartitionData<T>> responseData() {\n+        if (responseDataMap == null) {\n+            responseDataMap = new LinkedHashMap<>(partitionData.size());\n+            partitionData.forEach((tp, d) -> responseDataMap.put(tp, new PartitionData<>(d)));\n         }\n+        return responseDataMap;\n+    }\n \n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n+    @Override\n+    public int throttleTimeMs() {\n+        return data.throttleTimeMs();\n+    }\n \n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n+    public int sessionId() {\n+        return data.sessionId();\n+    }\n \n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        Map<Errors, Integer> errorCounts = new HashMap<>();\n+        updateErrorCounts(errorCounts, error());\n+        partitionData.values().forEach(response ->\n+            updateErrorCounts(errorCounts, Errors.forCode(response.errorCode()))\n+        );\n+        return errorCounts;\n     }\n \n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n+    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n+        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n+    }\n \n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n+    /**\n+     * Convenience method to find the size of a response.\n+     *\n+     * @param version       The version of the response to use.\n+     * @param partIterator  The partition iterator.\n+     * @return              The response size in bytes.\n+     */\n+    public static <T extends Records> int sizeOf(short version,\n+                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+        // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n+        // use arbitrary values here without affecting the result.\n+        LinkedHashMap<TopicPartition, PartitionData<T>> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        return 4 + new FetchResponse<>(Errors.NONE, data, 0, INVALID_SESSION_ID).data.size(cache, version);\n+    }\n \n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n+    @Override\n+    public boolean shouldClientThrottle(short version) {\n+        return version >= 8;\n+    }\n \n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n \n-            this.partitionResponse = partitionResponse;\n-        }\n+    public static final class PartitionData<T extends BaseRecords> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxNDk5Mg=="}, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 230}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3Njc3MzAxOnYy", "diffSide": "LEFT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNzowMjoxMlrOIO0BHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMzo0NToyM1rOItKUtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQwMzIzMQ==", "bodyText": "I don't batch the partitions again in this PR as it create a new FetchResponseData.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552403231", "createdAt": "2021-01-06T07:02:12Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -304,58 +115,12 @@ public int sessionId() {\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n         updateErrorCounts(errorCounts, error());\n-        responseDataMap.values().forEach(response ->\n-            updateErrorCounts(errorCounts, response.error())\n-        );\n+        dataByTopicPartition.values().forEach(response -> updateErrorCounts(errorCounts, Errors.forCode(response.errorCode())));\n         return errorCounts;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n-        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static <T extends BaseRecords> LinkedHashMap<TopicPartition, PartitionData<T>> toResponseDataMap(\n-            FetchResponseData message) {\n-        LinkedHashMap<TopicPartition, PartitionData<T>> responseMap = new LinkedHashMap<>();\n-        message.responses().forEach(topicResponse -> {\n-            topicResponse.partitionResponses().forEach(partitionResponse -> {\n-                TopicPartition tp = new TopicPartition(topicResponse.topic(), partitionResponse.partition());\n-                PartitionData<T> partitionData = new PartitionData<>(partitionResponse);\n-                responseMap.put(tp, partitionData);\n-            });\n-        });\n-        return responseMap;\n-    }\n-\n-    private static <T extends BaseRecords> FetchResponseData toMessage(int throttleTimeMs, Errors error,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjI5Mg==", "bodyText": "Can you clarify what you mean here?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172292", "createdAt": "2021-02-27T18:31:43Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -304,58 +115,12 @@ public int sessionId() {\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n         updateErrorCounts(errorCounts, error());\n-        responseDataMap.values().forEach(response ->\n-            updateErrorCounts(errorCounts, response.error())\n-        );\n+        dataByTopicPartition.values().forEach(response -> updateErrorCounts(errorCounts, Errors.forCode(response.errorCode())));\n         return errorCounts;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n-        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static <T extends BaseRecords> LinkedHashMap<TopicPartition, PartitionData<T>> toResponseDataMap(\n-            FetchResponseData message) {\n-        LinkedHashMap<TopicPartition, PartitionData<T>> responseMap = new LinkedHashMap<>();\n-        message.responses().forEach(topicResponse -> {\n-            topicResponse.partitionResponses().forEach(partitionResponse -> {\n-                TopicPartition tp = new TopicPartition(topicResponse.topic(), partitionResponse.partition());\n-                PartitionData<T> partitionData = new PartitionData<>(partitionResponse);\n-                responseMap.put(tp, partitionData);\n-            });\n-        });\n-        return responseMap;\n-    }\n-\n-    private static <T extends BaseRecords> FetchResponseData toMessage(int throttleTimeMs, Errors error,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQwMzIzMQ=="}, "originalCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyNTk3Mw==", "bodyText": "Oh, I planed to remove all usages of this method from production (i.e KafkaApis should generate batched response directly). However, it can produce a big patch so I will keep this method in next commit.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584225973", "createdAt": "2021-02-28T03:45:23Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -304,58 +115,12 @@ public int sessionId() {\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n         updateErrorCounts(errorCounts, error());\n-        responseDataMap.values().forEach(response ->\n-            updateErrorCounts(errorCounts, response.error())\n-        );\n+        dataByTopicPartition.values().forEach(response -> updateErrorCounts(errorCounts, Errors.forCode(response.errorCode())));\n         return errorCounts;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n-        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static <T extends BaseRecords> LinkedHashMap<TopicPartition, PartitionData<T>> toResponseDataMap(\n-            FetchResponseData message) {\n-        LinkedHashMap<TopicPartition, PartitionData<T>> responseMap = new LinkedHashMap<>();\n-        message.responses().forEach(topicResponse -> {\n-            topicResponse.partitionResponses().forEach(partitionResponse -> {\n-                TopicPartition tp = new TopicPartition(topicResponse.topic(), partitionResponse.partition());\n-                PartitionData<T> partitionData = new PartitionData<>(partitionResponse);\n-                responseMap.put(tp, partitionData);\n-            });\n-        });\n-        return responseMap;\n-    }\n-\n-    private static <T extends BaseRecords> FetchResponseData toMessage(int throttleTimeMs, Errors error,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQwMzIzMQ=="}, "originalCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "originalPosition": 306}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3ODA3Njc2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FetchSession.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxMzoyNToyMlrOIPAbrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxMzoyNToyMlrOIPAbrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjYwNjYzOQ==", "bodyText": "Maybe we could have a isPreferredReplica method since there are at least two places where we just call isPresent on the result.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552606639", "createdAt": "2021-01-06T13:25:22Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/FetchSession.scala", "diffHunk": "@@ -142,19 +141,20 @@ class CachedPartition(val topic: String,\n       if (updateResponseData)\n         localLogStartOffset = respData.logStartOffset\n     }\n-    if (respData.preferredReadReplica.isPresent) {\n+    if (FetchResponse.preferredReadReplica(respData).isPresent) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTMyNzc4OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxMzo0Mzo1MlrOIqIJSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDozNzoxMVrOIqK1qQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA0NDU1Mg==", "bodyText": "FetchablePartitionResponse is a bit long and redundant. Could we find a shorter name for it now that PartitionData is gone?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581044552", "createdAt": "2021-02-23T13:43:52Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -290,7 +291,7 @@ public void onSuccess(ClientResponse resp) {\n                             Set<TopicPartition> partitions = new HashSet<>(response.responseData().keySet());\n                             FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);\n \n-                            for (Map.Entry<TopicPartition, FetchResponse.PartitionData<Records>> entry : response.responseData().entrySet()) {\n+                            for (Map.Entry<TopicPartition, FetchResponseData.FetchablePartitionResponse> entry : response.responseData().entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA4ODY4MQ==", "bodyText": "sure. I reuse the name PartitionData", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581088681", "createdAt": "2021-02-23T14:37:11Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -290,7 +291,7 @@ public void onSuccess(ClientResponse resp) {\n                             Set<TopicPartition> partitions = new HashSet<>(response.responseData().keySet());\n                             FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);\n \n-                            for (Map.Entry<TopicPartition, FetchResponse.PartitionData<Records>> entry : response.responseData().entrySet()) {\n+                            for (Map.Entry<TopicPartition, FetchResponseData.FetchablePartitionResponse> entry : response.responseData().entrySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA0NDU1Mg=="}, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTM4ODcxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxMzo1NjoyNFrOIqIuVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxMzo1NjoyNFrOIqIuVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA1NDAzOA==", "bodyText": "Could we encapsulate this cast in a utility method?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581054038", "createdAt": "2021-02-23T13:56:24Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1257,7 +1259,7 @@ private CompletedFetch initializeCompletedFetch(CompletedFetch nextCompletedFetc\n \n                 log.trace(\"Preparing to read {} bytes of data for partition {} with offset {}\",\n                         partition.records().sizeInBytes(), tp, position);\n-                Iterator<? extends RecordBatch> batches = partition.records().batches().iterator();\n+                Iterator<? extends RecordBatch> batches = ((Records) partition.records()).batches().iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTQxMzgwOnYy", "diffSide": "RIGHT", "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowMTo0NlrOIqI-cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowMTo0NlrOIqI-cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA1ODE2Mg==", "bodyText": "Maybe we can remove the defaults from this and every other place where we build FetchablePartitionResponse", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581058162", "createdAt": "2021-02-23T14:01:46Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java", "diffHunk": "@@ -70,24 +70,26 @@ public void setUp() {\n         handler = new FetchSessionHandler(LOG_CONTEXT, 1);\n         FetchSessionHandler.Builder builder = handler.newBuilder();\n \n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> respMap = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> respMap = new LinkedHashMap<>();\n         for (int i = 0; i < partitionCount; i++) {\n             TopicPartition tp = new TopicPartition(\"foo\", i);\n             FetchRequest.PartitionData partitionData = new FetchRequest.PartitionData(0, 0, 200,\n                     Optional.empty());\n             fetches.put(tp, partitionData);\n             builder.add(tp, partitionData);\n-            respMap.put(tp, new FetchResponse.PartitionData<>(\n-                    Errors.NONE,\n-                    0L,\n-                    0L,\n-                    0,\n-                    null,\n-                    null));\n+            respMap.put(tp, new FetchResponseData.FetchablePartitionResponse()\n+                            .setPartition(tp.partition())\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setHighWatermark(0)\n+                            .setLastStableOffset(0)\n+                            .setLogStartOffset(0)\n+                            .setAbortedTransactions(null)\n+                            .setRecords(null)\n+                            .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTQyMDUxOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/LogOffsetTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowMjo1N1rOIqJCTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowMjo1N1rOIqJCTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA1OTE0OQ==", "bodyText": "Similar to elsewhere, it would be useful to have a utility method to avoid unsafe operations all over the code.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581059149", "createdAt": "2021-02-23T14:02:57Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/LogOffsetTest.scala", "diffHunk": "@@ -127,7 +124,7 @@ class LogOffsetTest extends BaseRequestTest {\n       Map(topicPartition -> new FetchRequest.PartitionData(consumerOffsets.head, FetchRequest.INVALID_LOG_START_OFFSET,\n         300 * 1024, Optional.empty())).asJava).build()\n     val fetchResponse = sendFetchRequest(fetchRequest)\n-    assertFalse(fetchResponse.responseData.get(topicPartition).records.batches.iterator.hasNext)\n+    assertFalse(fetchResponse.responseData.get(topicPartition).records.asInstanceOf[Records].batches.iterator.hasNext)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTQ0NjU0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowNzo0OFrOIqJSgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowNzo0OFrOIqJSgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA2MzI5Ng==", "bodyText": "Could this be FetchData too? Are there other places like it?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581063296", "createdAt": "2021-02-23T14:07:48Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala", "diffHunk": "@@ -110,7 +113,7 @@ class ReplicaAlterLogDirsThread(name: String,\n   // process fetched data\n   override def processPartitionData(topicPartition: TopicPartition,\n                                     fetchOffset: Long,\n-                                    partitionData: PartitionData[Records]): Option[LogAppendInfo] = {\n+                                    partitionData: FetchResponseData.FetchablePartitionResponse): Option[LogAppendInfo] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTQ1NTQxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowOToxNVrOIqJX3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDowOToxNVrOIqJX3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA2NDY2OQ==", "bodyText": "I think it would be better to make this a static factory method and keep the constructor for the case where we receive FetchResponseData.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581064669", "createdAt": "2021-02-23T14:09:15Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +55,51 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n-\n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            AbortedTransaction that = (AbortedTransaction) o;\n-\n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n-\n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n-    }\n-\n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n-\n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n-\n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n-\n-            this.partitionResponse = partitionResponse;\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, preferredReadReplica,\n-                abortedTransactions, Optional.empty(), records);\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, Optional.empty(), abortedTransactions, records);\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            PartitionData that = (PartitionData) o;\n-\n-            return this.partitionResponse.equals(that.partitionResponse);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return this.partitionResponse.hashCode();\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(error=\" + error() +\n-                    \", highWaterMark=\" + highWatermark() +\n-                    \", lastStableOffset = \" + lastStableOffset() +\n-                    \", logStartOffset = \" + logStartOffset() +\n-                    \", preferredReadReplica = \" + preferredReadReplica().map(Object::toString).orElse(\"absent\") +\n-                    \", abortedTransactions = \" + abortedTransactions() +\n-                    \", divergingEpoch =\" + divergingEpoch() +\n-                    \", recordsSizeInBytes=\" + records().sizeInBytes() + \")\";\n-        }\n-\n-        public Errors error() {\n-            return error;\n-        }\n-\n-        public long highWatermark() {\n-            return partitionResponse.highWatermark();\n-        }\n-\n-        public long lastStableOffset() {\n-            return partitionResponse.lastStableOffset();\n-        }\n-\n-        public long logStartOffset() {\n-            return partitionResponse.logStartOffset();\n-        }\n-\n-        public Optional<Integer> preferredReadReplica() {\n-            return preferredReplica;\n-        }\n-\n-        public List<AbortedTransaction> abortedTransactions() {\n-            return abortedTransactions;\n-        }\n-\n-        public Optional<FetchResponseData.EpochEndOffset> divergingEpoch() {\n-            FetchResponseData.EpochEndOffset epochEndOffset = partitionResponse.divergingEpoch();\n-            if (epochEndOffset.epoch() < 0) {\n-                return Optional.empty();\n-            } else {\n-                return Optional.of(epochEndOffset);\n-            }\n-        }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        public T records() {\n-            return (T) partitionResponse.recordSet();\n-        }\n-    }\n-\n-    /**\n-     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n-     * `FetchRequest.fetchData`.\n-     *\n-     * @param error             The top-level error code.\n-     * @param responseData      The fetched data grouped by partition.\n-     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n-     * @param sessionId         The fetch session id.\n-     */\n     public FetchResponse(Errors error,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTQ4MjY1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDoxNDoyOFrOIqJpDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDoxNDoyOFrOIqJpDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA2OTA3MQ==", "bodyText": "This isn't needed when we return a fetch from the broker, right? If this is true, can we remove it from the fetch response and build it on the client when needed?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581069071", "createdAt": "2021-02-23T14:14:28Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +55,51 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n-\n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            AbortedTransaction that = (AbortedTransaction) o;\n-\n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n-\n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n-    }\n-\n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n-\n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n-\n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n-\n-            this.partitionResponse = partitionResponse;\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, preferredReadReplica,\n-                abortedTransactions, Optional.empty(), records);\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, Optional.empty(), abortedTransactions, records);\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            PartitionData that = (PartitionData) o;\n-\n-            return this.partitionResponse.equals(that.partitionResponse);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return this.partitionResponse.hashCode();\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(error=\" + error() +\n-                    \", highWaterMark=\" + highWatermark() +\n-                    \", lastStableOffset = \" + lastStableOffset() +\n-                    \", logStartOffset = \" + logStartOffset() +\n-                    \", preferredReadReplica = \" + preferredReadReplica().map(Object::toString).orElse(\"absent\") +\n-                    \", abortedTransactions = \" + abortedTransactions() +\n-                    \", divergingEpoch =\" + divergingEpoch() +\n-                    \", recordsSizeInBytes=\" + records().sizeInBytes() + \")\";\n-        }\n-\n-        public Errors error() {\n-            return error;\n-        }\n-\n-        public long highWatermark() {\n-            return partitionResponse.highWatermark();\n-        }\n-\n-        public long lastStableOffset() {\n-            return partitionResponse.lastStableOffset();\n-        }\n-\n-        public long logStartOffset() {\n-            return partitionResponse.logStartOffset();\n-        }\n-\n-        public Optional<Integer> preferredReadReplica() {\n-            return preferredReplica;\n-        }\n-\n-        public List<AbortedTransaction> abortedTransactions() {\n-            return abortedTransactions;\n-        }\n-\n-        public Optional<FetchResponseData.EpochEndOffset> divergingEpoch() {\n-            FetchResponseData.EpochEndOffset epochEndOffset = partitionResponse.divergingEpoch();\n-            if (epochEndOffset.epoch() < 0) {\n-                return Optional.empty();\n-            } else {\n-                return Optional.of(epochEndOffset);\n-            }\n-        }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        public T records() {\n-            return (T) partitionResponse.recordSet();\n-        }\n-    }\n-\n-    /**\n-     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n-     * `FetchRequest.fetchData`.\n-     *\n-     * @param error             The top-level error code.\n-     * @param responseData      The fetched data grouped by partition.\n-     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n-     * @param sessionId         The fetch session id.\n-     */\n     public FetchResponse(Errors error,\n-                         LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n                          int throttleTimeMs,\n-                         int sessionId) {\n-        super(ApiKeys.FETCH);\n-        this.data = toMessage(throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n-        this.responseDataMap = responseData;\n+                         int sessionId,\n+                         LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData) {\n+        this(new FetchResponseData()\n+            .setSessionId(sessionId)\n+            .setErrorCode(error.code())\n+            .setThrottleTimeMs(throttleTimeMs)\n+            .setResponses(responseData.entrySet().stream().map(entry -> new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(entry.getKey().topic())\n+                .setPartitionResponses(Collections.singletonList(entry.getValue().setPartition(entry.getKey().partition()))))\n+                .collect(Collectors.toList())));\n     }\n \n     public FetchResponse(FetchResponseData fetchResponseData) {\n         super(ApiKeys.FETCH);\n         this.data = fetchResponseData;\n-        this.responseDataMap = toResponseDataMap(fetchResponseData);\n+        this.responseData = new LinkedHashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY2NTg2NDM5OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/FetchResponse.json", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNToyMTozNFrOIqNQug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNToyNDoyMlrOIqNbGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTEyODM3OA==", "bodyText": "We typically call this PartitionIndex in other requests/responses. Is that right?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581128378", "createdAt": "2021-02-23T15:21:34Z", "author": {"login": "ijuma"}, "path": "clients/src/main/resources/common/message/FetchResponse.json", "diffHunk": "@@ -53,9 +53,9 @@\n       \"about\": \"The response topics.\", \"fields\": [\n       { \"name\": \"Topic\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n         \"about\": \"The topic name.\" },\n-      { \"name\": \"PartitionResponses\", \"type\": \"[]FetchablePartitionResponse\", \"versions\": \"0+\",\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionData\", \"versions\": \"0+\",\n         \"about\": \"The topic partitions.\", \"fields\": [\n-        { \"name\": \"Partition\", \"type\": \"int32\", \"versions\": \"0+\",\n+        { \"name\": \"Index\", \"type\": \"int32\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eddfffd349a34cc26ddc5db284b5526c40f5fcc8"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTEzMTAzMg==", "bodyText": "We typically call this PartitionIndex in other requests/responses. Is that right?\n\nYou are right!", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581131032", "createdAt": "2021-02-23T15:24:22Z", "author": {"login": "chia7712"}, "path": "clients/src/main/resources/common/message/FetchResponse.json", "diffHunk": "@@ -53,9 +53,9 @@\n       \"about\": \"The response topics.\", \"fields\": [\n       { \"name\": \"Topic\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n         \"about\": \"The topic name.\" },\n-      { \"name\": \"PartitionResponses\", \"type\": \"[]FetchablePartitionResponse\", \"versions\": \"0+\",\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionData\", \"versions\": \"0+\",\n         \"about\": \"The topic partitions.\", \"fields\": [\n-        { \"name\": \"Partition\", \"type\": \"int32\", \"versions\": \"0+\",\n+        { \"name\": \"Index\", \"type\": \"int32\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTEyODM3OA=="}, "originalCommit": {"oid": "eddfffd349a34cc26ddc5db284b5526c40f5fcc8"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTE5MDAxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxNTozMjoxN1rOItFMPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxNzo0MDozNlrOItGmqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MTg4Nw==", "bodyText": "This is not thread-safe and requests are typically thread-safe. What's the thinking here?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584141887", "createdAt": "2021-02-27T15:32:17Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +56,39 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    // we build responseData when needed.\n+    private LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MjA3NA==", "bodyText": "Looks like this method is used only Fetcher.sendFetches outside of tests, benchmaks, etc.. Also once in errorCounts, but we can change that code. Maybe we can remove this altogether. Thoughts?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584142074", "createdAt": "2021-02-27T15:34:49Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +56,39 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    // we build responseData when needed.\n+    private LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MTg4Nw=="}, "originalCommit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MjE4Ng==", "bodyText": "I think I would move the method to a test utility so that tests can use that instead.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584142186", "createdAt": "2021-02-27T15:35:36Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +56,39 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    // we build responseData when needed.\n+    private LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MTg4Nw=="}, "originalCommit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0ODgxNw==", "bodyText": "It is used by FetchSessionHandler also. I prefer to rewrite that code by another PR to avoid big patch :(\nIt seems to me using synchronization block can resolve the thread issue. Also, I will file a jira as follow-up. WDYT?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584148817", "createdAt": "2021-02-27T15:58:31Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +56,39 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    // we build responseData when needed.\n+    private LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MTg4Nw=="}, "originalCommit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE1MTcxNg==", "bodyText": "Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584151716", "createdAt": "2021-02-27T16:18:32Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +56,39 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    // we build responseData when needed.\n+    private LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MTg4Nw=="}, "originalCommit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE2NTAzMw==", "bodyText": "jira: https://issues.apache.org/jira/browse/KAFKA-12385\n\nyou can use a volatile field and synchronize on the assignment if still null.\n\nupdated", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584165033", "createdAt": "2021-02-27T17:40:36Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +56,39 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    // we build responseData when needed.\n+    private LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MTg4Nw=="}, "originalCommit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQxMjMwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozMDozNVrOItHCkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozMDozNVrOItHCkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjE3Nw==", "bodyText": "Can we update this not to use responseData? Then we at least have the right behavior for the broker and we can fix the clients in the subsequent PR.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172177", "createdAt": "2021-02-27T18:30:35Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -304,58 +108,12 @@ public int sessionId() {\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n         updateErrorCounts(errorCounts, error());\n-        responseDataMap.values().forEach(response ->\n-            updateErrorCounts(errorCounts, response.error())\n-        );\n+        responseData().values().forEach(response -> updateErrorCounts(errorCounts, Errors.forCode(response.errorCode())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 281}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQxNDM3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozMzoyNVrOItHDnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMjo1MDo1M1rOItKD3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjQ0NQ==", "bodyText": "Aren't many of these set automatically by the generated classes?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172445", "createdAt": "2021-02-27T18:33:25Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +123,70 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+                .setPartitionIndex(partition)\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 386}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMTY2MQ==", "bodyText": "you are right. Except for HighWatermark, other args have default value. remove duplicate assignment!", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584221661", "createdAt": "2021-02-28T02:50:53Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +123,70 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+                .setPartitionIndex(partition)\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjQ0NQ=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 386}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQxNTEwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozNDoxNlrOItHD8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMjo0Mzo0OFrOItKBbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjUzMA==", "bodyText": "Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also \"This is used to eliminate duplicate code of type casting.\" seems a bit redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172530", "createdAt": "2021-02-27T18:34:16Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +123,70 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+                .setPartitionIndex(partition)\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. This is used to eliminate duplicate code of type casting.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 390}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMTAzNg==", "bodyText": "the data from KRPC always use MemoryRecords so it should never fail if the data is from KRPC. I will add more comments for this case.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584221036", "createdAt": "2021-02-28T02:43:48Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +123,70 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+                .setPartitionIndex(partition)\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. This is used to eliminate duplicate code of type casting.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjUzMA=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 390}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQxNTcwOnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozNToxMFrOItHEPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozNToxMFrOItHEPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjYwNg==", "bodyText": "Nit: indenting seems excessive.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172606", "createdAt": "2021-02-27T18:35:10Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java", "diffHunk": "@@ -150,22 +150,23 @@ private static void assertListEquals(List<TopicPartition> expected, List<TopicPa\n \n     private static final class RespEntry {\n         final TopicPartition part;\n-        final FetchResponse.PartitionData<MemoryRecords> data;\n+        final FetchResponseData.PartitionData data;\n \n         RespEntry(String topic, int partition, long highWatermark, long lastStableOffset) {\n             this.part = new TopicPartition(topic, partition);\n-            this.data = new FetchResponse.PartitionData<>(\n-                Errors.NONE,\n-                highWatermark,\n-                lastStableOffset,\n-                0,\n-                null,\n-                null);\n+\n+            this.data = new FetchResponseData.PartitionData()\n+                        .setErrorCode(Errors.NONE.code())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQxODY5OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozNzo0MVrOItHFiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozNzo0MVrOItHFiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjkzNw==", "bodyText": "Are some of these redundant? (eg setAbortedTransactions(null))", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172937", "createdAt": "2021-02-27T18:37:41Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java", "diffHunk": "@@ -2377,14 +2378,19 @@ private ListOffsetsResponse listOffsetsResponse(Map<TopicPartition, Long> partit\n                     builder.append(0L, (\"key-\" + i).getBytes(), (\"value-\" + i).getBytes());\n                 records = builder.build();\n             }\n-            tpResponses.put(partition, new FetchResponse.PartitionData<>(\n-                    Errors.NONE, highWatermark, FetchResponse.INVALID_LAST_STABLE_OFFSET,\n-                    logStartOffset, null, records));\n+            tpResponses.put(partition,\n+                    new FetchResponseData.PartitionData()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setHighWatermark(highWatermark)\n+                            .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                            .setLogStartOffset(logStartOffset)\n+                            .setAbortedTransactions(null)\n+                            .setRecords(records));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQxOTA4OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozODowMVrOItHFuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozODowMVrOItHFuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3Mjk4NA==", "bodyText": "Are some of these redundant? (eg setAbortedTransactions(null))", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172984", "createdAt": "2021-02-27T18:38:01Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java", "diffHunk": "@@ -150,22 +150,23 @@ private static void assertListEquals(List<TopicPartition> expected, List<TopicPa\n \n     private static final class RespEntry {\n         final TopicPartition part;\n-        final FetchResponse.PartitionData<MemoryRecords> data;\n+        final FetchResponseData.PartitionData data;\n \n         RespEntry(String topic, int partition, long highWatermark, long lastStableOffset) {\n             this.part = new TopicPartition(topic, partition);\n-            this.data = new FetchResponse.PartitionData<>(\n-                Errors.NONE,\n-                highWatermark,\n-                lastStableOffset,\n-                0,\n-                null,\n-                null);\n+\n+            this.data = new FetchResponseData.PartitionData()\n+                        .setErrorCode(Errors.NONE.code())\n+                        .setHighWatermark(highWatermark)\n+                        .setLastStableOffset(lastStableOffset)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(null)\n+                        .setRecords(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQyMDU5OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozOTowOVrOItHGZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozOTowOVrOItHGZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzE1Ng==", "bodyText": "Are some of these redundant? (eg setAbortedTransactions(null)). Other examples in the same file.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173156", "createdAt": "2021-02-27T18:39:09Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -1270,13 +1271,24 @@ public void testFetchPositionAfterException() {\n \n         assertEquals(1, fetcher.sendFetches());\n \n-        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new LinkedHashMap<>();\n-        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n-            FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, records));\n-        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.OFFSET_OUT_OF_RANGE, 100,\n-            FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));\n-        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n-            0, INVALID_SESSION_ID));\n+\n+        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new LinkedHashMap<>();\n+        partitions.put(tp1, new FetchResponseData.PartitionData()\n+                .setErrorCode(Errors.NONE.code())\n+                .setHighWatermark(100)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(records));\n+        partitions.put(tp0, new FetchResponseData.PartitionData()\n+                .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n+                .setHighWatermark(100)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQyMTk2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODo0MDoxMFrOItHG-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODo0MDoxMFrOItHG-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzMwNw==", "bodyText": "Nit: indenting seems wrong.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173307", "createdAt": "2021-02-27T18:40:10Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -364,7 +361,7 @@ abstract class AbstractFetcherThread(name: String,\n                       }\n                     }\n                     if (isTruncationOnFetchSupported) {\n-                      partitionData.divergingEpoch.ifPresent { divergingEpoch =>\n+                     FetchResponse.divergingEpoch(partitionData).ifPresent { divergingEpoch =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQyMzA1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODo0MTo0MVrOItHHgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMjo1MzozMlrOItKEbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzQ0MQ==", "bodyText": "Not clear why we need this val. Seems like we can introduce a variable in the case _ instead.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173441", "createdAt": "2021-02-27T18:41:41Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -340,7 +336,8 @@ abstract class AbstractFetcherThread(name: String,\n             // the current offset is the same as the offset requested.\n             val fetchPartitionData = sessionPartitions.get(topicPartition)\n             if (fetchPartitionData != null && fetchPartitionData.fetchOffset == currentFetchState.fetchOffset && currentFetchState.isReadyForFetch) {\n-              partitionData.error match {\n+              val partitionError = Errors.forCode(partitionData.errorCode)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMTgwNQ==", "bodyText": "Good catch! updated", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584221805", "createdAt": "2021-02-28T02:53:32Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -340,7 +336,8 @@ abstract class AbstractFetcherThread(name: String,\n             // the current offset is the same as the offset requested.\n             val fetchPartitionData = sessionPartitions.get(topicPartition)\n             if (fetchPartitionData != null && fetchPartitionData.fetchOffset == currentFetchState.fetchOffset && currentFetchState.isReadyForFetch) {\n-              partitionData.error match {\n+              val partitionError = Errors.forCode(partitionData.errorCode)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzQ0MQ=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTQyMzg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODo0MjozNFrOItHH3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMjo1NTo1MFrOItKFHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzUzMg==", "bodyText": "Do we have to update the matching inside the method to handle other potential records types? Or do we want to avoid changing this method signature instead?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173532", "createdAt": "2021-02-27T18:42:34Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -734,7 +730,7 @@ abstract class AbstractFetcherThread(name: String,\n     Option(partitionStates.stateValue(topicPartition))\n   }\n \n-  protected def toMemoryRecords(records: Records): MemoryRecords = {\n+  protected def toMemoryRecords(records: BaseRecords): MemoryRecords = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMTk4Mw==", "bodyText": "This change can be reverted as we have helper method FetchResponse#records now.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584221983", "createdAt": "2021-02-28T02:55:50Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -734,7 +730,7 @@ abstract class AbstractFetcherThread(name: String,\n     Option(partitionStates.stateValue(topicPartition))\n   }\n \n-  protected def toMemoryRecords(records: Records): MemoryRecords = {\n+  protected def toMemoryRecords(records: BaseRecords): MemoryRecords = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzUzMg=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUyNTcwOnYy", "diffSide": "RIGHT", "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyNDo1MFrOItH6xQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMzowMTowOFrOItKG8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjU2NQ==", "bodyText": "We can remove some redundant set calls?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186565", "createdAt": "2021-02-27T20:24:50Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java", "diffHunk": "@@ -70,24 +70,25 @@ public void setUp() {\n         handler = new FetchSessionHandler(LOG_CONTEXT, 1);\n         FetchSessionHandler.Builder builder = handler.newBuilder();\n \n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> respMap = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> respMap = new LinkedHashMap<>();\n         for (int i = 0; i < partitionCount; i++) {\n             TopicPartition tp = new TopicPartition(\"foo\", i);\n             FetchRequest.PartitionData partitionData = new FetchRequest.PartitionData(0, 0, 200,\n                     Optional.empty());\n             fetches.put(tp, partitionData);\n             builder.add(tp, partitionData);\n-            respMap.put(tp, new FetchResponse.PartitionData<>(\n-                    Errors.NONE,\n-                    0L,\n-                    0L,\n-                    0,\n-                    null,\n-                    null));\n+            respMap.put(tp, new FetchResponseData.PartitionData()\n+                            .setPartitionIndex(tp.partition())\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setHighWatermark(0)\n+                            .setLastStableOffset(0)\n+                            .setLogStartOffset(0)\n+                            .setAbortedTransactions(null)\n+                            .setRecords(null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMjQ0OQ==", "bodyText": "updated", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584222449", "createdAt": "2021-02-28T03:01:08Z", "author": {"login": "chia7712"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java", "diffHunk": "@@ -70,24 +70,25 @@ public void setUp() {\n         handler = new FetchSessionHandler(LOG_CONTEXT, 1);\n         FetchSessionHandler.Builder builder = handler.newBuilder();\n \n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> respMap = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> respMap = new LinkedHashMap<>();\n         for (int i = 0; i < partitionCount; i++) {\n             TopicPartition tp = new TopicPartition(\"foo\", i);\n             FetchRequest.PartitionData partitionData = new FetchRequest.PartitionData(0, 0, 200,\n                     Optional.empty());\n             fetches.put(tp, partitionData);\n             builder.add(tp, partitionData);\n-            respMap.put(tp, new FetchResponse.PartitionData<>(\n-                    Errors.NONE,\n-                    0L,\n-                    0L,\n-                    0,\n-                    null,\n-                    null));\n+            respMap.put(tp, new FetchResponseData.PartitionData()\n+                            .setPartitionIndex(tp.partition())\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setHighWatermark(0)\n+                            .setLastStableOffset(0)\n+                            .setLogStartOffset(0)\n+                            .setAbortedTransactions(null)\n+                            .setRecords(null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjU2NQ=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUyNjE1OnYy", "diffSide": "RIGHT", "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyNToxMFrOItH6-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMjo1Nzo1NlrOItKF6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjYxOQ==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186619", "createdAt": "2021-02-27T20:25:10Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java", "diffHunk": "@@ -174,8 +173,14 @@ public int sizeInBytes() {\n                     return null;\n                 }\n             };\n-            initialFetched.put(tp, new FetchResponse.PartitionData<>(Errors.NONE, 0, 0, 0,\n-                    new LinkedList<>(), fetched));\n+            initialFetched.put(tp, new FetchResponseData.PartitionData()\n+                    .setPartitionIndex(tp.partition())\n+                    .setErrorCode(Errors.NONE.code())\n+                    .setHighWatermark(0)\n+                    .setLastStableOffset(0)\n+                    .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMjE4Ng==", "bodyText": "I will remove redundant setter setHighWatermark(0). Other values are different from default value in KRPC.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584222186", "createdAt": "2021-02-28T02:57:56Z", "author": {"login": "chia7712"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java", "diffHunk": "@@ -174,8 +173,14 @@ public int sizeInBytes() {\n                     return null;\n                 }\n             };\n-            initialFetched.put(tp, new FetchResponse.PartitionData<>(Errors.NONE, 0, 0, 0,\n-                    new LinkedList<>(), fetched));\n+            initialFetched.put(tp, new FetchResponseData.PartitionData()\n+                    .setPartitionIndex(tp.partition())\n+                    .setErrorCode(Errors.NONE.code())\n+                    .setHighWatermark(0)\n+                    .setLastStableOffset(0)\n+                    .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjYxOQ=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUyNjc2OnYy", "diffSide": "RIGHT", "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyNToyOFrOItH7Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMzowMDozOFrOItKGyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjY4Nw==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186687", "createdAt": "2021-02-27T20:25:28Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java", "diffHunk": "@@ -78,19 +78,25 @@ public void setup() {\n         for (int topicIdx = 0; topicIdx < topicCount; topicIdx++) {\n             String topic = UUID.randomUUID().toString();\n             for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n-                FetchResponse.PartitionData<MemoryRecords> partitionData = new FetchResponse.PartitionData<>(\n-                    Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records);\n+                FetchResponseData.PartitionData partitionData = new FetchResponseData.PartitionData()\n+                                .setPartitionIndex(partitionId)\n+                                .setErrorCode(Errors.NONE.code())\n+                                .setHighWatermark(0)\n+                                .setLastStableOffset(0)\n+                                .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMjQwOQ==", "bodyText": "updated", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584222409", "createdAt": "2021-02-28T03:00:38Z", "author": {"login": "chia7712"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java", "diffHunk": "@@ -78,19 +78,25 @@ public void setup() {\n         for (int topicIdx = 0; topicIdx < topicCount; topicIdx++) {\n             String topic = UUID.randomUUID().toString();\n             for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n-                FetchResponse.PartitionData<MemoryRecords> partitionData = new FetchResponse.PartitionData<>(\n-                    Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records);\n+                FetchResponseData.PartitionData partitionData = new FetchResponseData.PartitionData()\n+                                .setPartitionIndex(partitionId)\n+                                .setErrorCode(Errors.NONE.code())\n+                                .setHighWatermark(0)\n+                                .setLastStableOffset(0)\n+                                .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjY4Nw=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUyNzA2OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyNTo1NFrOItH7ZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyNTo1NFrOItH7ZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjcyNA==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186724", "createdAt": "2021-02-27T20:25:54Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -963,9 +966,13 @@ class ReplicaFetcherThreadTest {\n \n     val records = MemoryRecords.withRecords(CompressionType.NONE,\n       new SimpleRecord(1000, \"foo\".getBytes(StandardCharsets.UTF_8)))\n-\n-    val partitionData: thread.FetchData = new FetchResponse.PartitionData[Records](\n-      Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records)\n+    val partitionData: thread.FetchData = new FetchResponseData.PartitionData()\n+        .setErrorCode(Errors.NONE.code)\n+        .setHighWatermark(0)\n+        .setLastStableOffset(0)\n+        .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUyNzcyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyNjozNFrOItH7sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMzowMDo0NFrOItKG5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjgwMA==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186800", "createdAt": "2021-02-27T20:26:34Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -155,13 +155,28 @@ class FetchSessionTest {\n     assertEquals(Optional.of(1), epochs1(tp1))\n     assertEquals(Optional.of(2), epochs1(tp2))\n \n-    val response = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    response.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100, 100,\n-      100, null, null))\n-    response.put(tp1, new FetchResponse.PartitionData(\n-      Errors.NONE, 10, 10, 10, null, null))\n-    response.put(tp2, new FetchResponse.PartitionData(\n-      Errors.NONE, 5, 5, 5, null, null))\n+    val response = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    response.put(tp0, new FetchResponseData.PartitionData()\n+      .setErrorCode(Errors.NONE.code)\n+      .setHighWatermark(100)\n+      .setLastStableOffset(100)\n+      .setLogStartOffset(100)\n+      .setAbortedTransactions(null)\n+      .setRecords(null))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyMjQzOQ==", "bodyText": "updated", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584222439", "createdAt": "2021-02-28T03:00:44Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -155,13 +155,28 @@ class FetchSessionTest {\n     assertEquals(Optional.of(1), epochs1(tp1))\n     assertEquals(Optional.of(2), epochs1(tp2))\n \n-    val response = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    response.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100, 100,\n-      100, null, null))\n-    response.put(tp1, new FetchResponse.PartitionData(\n-      Errors.NONE, 10, 10, 10, null, null))\n-    response.put(tp2, new FetchResponse.PartitionData(\n-      Errors.NONE, 5, 5, 5, null, null))\n+    val response = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    response.put(tp0, new FetchResponseData.PartitionData()\n+      .setErrorCode(Errors.NONE.code)\n+      .setHighWatermark(100)\n+      .setLastStableOffset(100)\n+      .setLogStartOffset(100)\n+      .setAbortedTransactions(null)\n+      .setRecords(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjgwMA=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUyOTk2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyODoyMFrOItH8tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDoyODoyMFrOItH8tA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzA2MA==", "bodyText": "Nit: remove () twice.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187060", "createdAt": "2021-02-27T20:28:20Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala", "diffHunk": "@@ -403,25 +401,24 @@ private class ReplicaFetcher(name: String, sourceBroker: Node, topicPartitions:\n \n     debug(\"Issuing fetch request \")\n \n-    var fetchResponse: FetchResponse[MemoryRecords] = null\n+    var fetchResponse: FetchResponse = null\n     try {\n       val clientResponse = fetchEndpoint.sendRequest(fetchRequestBuilder)\n-      fetchResponse = clientResponse.responseBody.asInstanceOf[FetchResponse[MemoryRecords]]\n+      fetchResponse = clientResponse.responseBody.asInstanceOf[FetchResponse]\n     } catch {\n       case t: Throwable =>\n         if (!isRunning)\n           throw t\n     }\n \n     if (fetchResponse != null) {\n-      fetchResponse.responseData.forEach { (tp, partitionData) =>\n-        replicaBuffer.addFetchedData(tp, sourceBroker.id, partitionData)\n-      }\n+      fetchResponse.data.responses().forEach(topicResponse =>\n+        topicResponse.partitions().forEach(partitionResponse =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUzMjY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDozMToxNFrOItH95g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMzozMzoyMVrOItKQ1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzM2Ng==", "bodyText": "Do we have to copy like this or can we mutate the response?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187366", "createdAt": "2021-02-27T20:31:14Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyNDY5Mw==", "bodyText": "It created copy before so I don't change the behavior in this PR. We can investigate it in separate PR :)", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584224693", "createdAt": "2021-02-28T03:28:41Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzM2Ng=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyNDk4MQ==", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-12387", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584224981", "createdAt": "2021-02-28T03:33:21Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzM2Ng=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUzMjkxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDozMTozOFrOItH-BQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMzoyOTozNVrOItKP7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzM5Nw==", "bodyText": "Similar question, is the copy required?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187397", "createdAt": "2021-02-27T20:31:38Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyNDc1MA==", "bodyText": "same to #9758 (comment)", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584224750", "createdAt": "2021-02-28T03:29:35Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzM5Nw=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 175}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4NTUzMzI3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDozMjowNFrOItH-LA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yOFQwMzozMDo0M1rOItKQVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzQzNg==", "bodyText": "Similar question, is the copy required?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187436", "createdAt": "2021-02-27T20:32:04Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        partitions.put(tp, new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp.partition)\n+            .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+            .setHighWatermark(data.highWatermark)\n+            .setLastStableOffset(lastStableOffset)\n+            .setLogStartOffset(data.logStartOffset)\n+            .setAbortedTransactions(abortedTransactions)\n+            .setRecords(data.records)\n+            .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+            .setDivergingEpoch(data.divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDIyNDg1NQ==", "bodyText": "same to #9758 (comment)", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584224855", "createdAt": "2021-02-28T03:30:43Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        partitions.put(tp, new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp.partition)\n+            .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+            .setHighWatermark(data.highWatermark)\n+            .setLastStableOffset(lastStableOffset)\n+            .setLogStartOffset(data.logStartOffset)\n+            .setAbortedTransactions(abortedTransactions)\n+            .setRecords(data.records)\n+            .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+            .setDivergingEpoch(data.divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzQzNg=="}, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY4OTk5ODk2OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTozNjo0N1rOItuICA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTozNjo0N1rOItuICA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxMjU1Mg==", "bodyText": "Aborted transactions is empty by default.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584812552", "createdAt": "2021-03-01T15:36:47Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -791,15 +791,18 @@ public void produceRequestGetErrorResponseTest() {\n \n     @Test\n     public void fetchResponseVersionTest() {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n \n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(\n-                Errors.NONE, 1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET,\n-                0L, Optional.empty(), Collections.emptyList(), records));\n-\n-        FetchResponse<MemoryRecords> v0Response = new FetchResponse<>(Errors.NONE, responseData, 0, INVALID_SESSION_ID);\n-        FetchResponse<MemoryRecords> v1Response = new FetchResponse<>(Errors.NONE, responseData, 10, INVALID_SESSION_ID);\n+        responseData.put(new TopicPartition(\"test\", 0),\n+                new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDAxNTM1OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTozOToxM1rOItuSuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTozOToxM1rOItuSuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxNTI4OQ==", "bodyText": "Do we need to set the partition id here? There are a few other cases in this file that are similar.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584815289", "createdAt": "2021-03-01T15:39:13Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -808,22 +811,32 @@ public void fetchResponseVersionTest() {\n \n     @Test\n     public void testFetchResponseV4() {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n \n-        List<FetchResponse.AbortedTransaction> abortedTransactions = asList(\n-                new FetchResponse.AbortedTransaction(10, 100),\n-                new FetchResponse.AbortedTransaction(15, 50)\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = asList(\n+                new FetchResponseData.AbortedTransaction().setProducerId(10).setFirstOffset(100),\n+                new FetchResponseData.AbortedTransaction().setProducerId(15).setFirstOffset(50)\n         );\n-        responseData.put(new TopicPartition(\"bar\", 0), new FetchResponse.PartitionData<>(Errors.NONE, 100000,\n-                FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), abortedTransactions, records));\n-        responseData.put(new TopicPartition(\"bar\", 1), new FetchResponse.PartitionData<>(Errors.NONE, 900000,\n-                5, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, records));\n-        responseData.put(new TopicPartition(\"foo\", 0), new FetchResponse.PartitionData<>(Errors.NONE, 70000,\n-                6, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), emptyList(), records));\n-\n-        FetchResponse<MemoryRecords> response = new FetchResponse<>(Errors.NONE, responseData, 10, INVALID_SESSION_ID);\n-        FetchResponse<MemoryRecords> deserialized = FetchResponse.parse(response.serialize((short) 4), (short) 4);\n+        responseData.put(new TopicPartition(\"bar\", 0),\n+                new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setAbortedTransactions(abortedTransactions)\n+                        .setRecords(records));\n+        responseData.put(new TopicPartition(\"bar\", 1),\n+                new FetchResponseData.PartitionData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDAyODE4OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0MDo0MFrOItubVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0MDo0MFrOItubVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxNzQ5Mg==", "bodyText": "No need to set aborted transactions.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584817492", "createdAt": "2021-03-01T15:40:40Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1159,47 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDAzNjUxOnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0MTozNFrOItug_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0MTozNFrOItug_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxODk0Mw==", "bodyText": "Aborted transactions is empty by default.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584818943", "createdAt": "2021-03-01T15:41:34Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1159,47 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(abortedTransactions));\n+        return FetchResponse.of(Errors.NONE, 25, sessionId, responseData);\n+    }\n+\n+    private FetchResponse createFetchResponse(boolean includeAborted) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA0NjQwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0Mjo1MVrOItun8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQwMzoyMzoyMVrOIuG-zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyMDcyMA==", "bodyText": "We should not have : Errors here as it introduces a type test. What we want is for this to be a catch all.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584820720", "createdAt": "2021-03-01T15:42:51Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -416,9 +412,8 @@ abstract class AbstractFetcherThread(name: String,\n                        \"expected to persist.\")\n                   partitionsWithError += topicPartition\n \n-                case _ =>\n-                  error(s\"Error for partition $topicPartition at offset ${currentFetchState.fetchOffset}\",\n-                    partitionData.error.exception)\n+                case partitionError: Errors =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTIxOTc4OQ==", "bodyText": "you are right. fixed", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585219789", "createdAt": "2021-03-02T03:23:21Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -416,9 +412,8 @@ abstract class AbstractFetcherThread(name: String,\n                        \"expected to persist.\")\n                   partitionsWithError += topicPartition\n \n-                case _ =>\n-                  error(s\"Error for partition $topicPartition at offset ${currentFetchState.fetchOffset}\",\n-                    partitionData.error.exception)\n+                case partitionError: Errors =>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyMDcyMA=="}, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA2MzQxOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NDo0NlrOItu0Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NDo0NlrOItu0Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyMzgyMg==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584823822", "createdAt": "2021-03-01T15:44:46Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "diffHunk": "@@ -1144,8 +1143,14 @@ class AbstractFetcherThreadTest {\n           (Errors.NONE, records)\n         }\n \n-        (partition, new FetchData(error, leaderState.highWatermark, leaderState.highWatermark, leaderState.logStartOffset,\n-          Optional.empty[Integer], List.empty.asJava, divergingEpoch.asJava, records))\n+        (partition, new FetchResponseData.PartitionData()\n+          .setErrorCode(error.code)\n+          .setHighWatermark(leaderState.highWatermark)\n+          .setLastStableOffset(leaderState.highWatermark)\n+          .setLogStartOffset(leaderState.logStartOffset)\n+          .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA2OTE1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NToyM1rOItu4PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQwMzoyODowMVrOIuHEog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNDg5Mg==", "bodyText": "The previous code did asJava, why did we change it?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584824892", "createdAt": "2021-03-01T15:45:23Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "diffHunk": "@@ -1144,8 +1143,14 @@ class AbstractFetcherThreadTest {\n           (Errors.NONE, records)\n         }\n \n-        (partition, new FetchData(error, leaderState.highWatermark, leaderState.highWatermark, leaderState.logStartOffset,\n-          Optional.empty[Integer], List.empty.asJava, divergingEpoch.asJava, records))\n+        (partition, new FetchResponseData.PartitionData()\n+          .setErrorCode(error.code)\n+          .setHighWatermark(leaderState.highWatermark)\n+          .setLastStableOffset(leaderState.highWatermark)\n+          .setLogStartOffset(leaderState.logStartOffset)\n+          .setAbortedTransactions(Collections.emptyList())\n+          .setRecords(records)\n+          .setDivergingEpoch(divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTIyMTI4Mg==", "bodyText": "the type FetchablePartitionResponse is gone and the replacement FetchResponseData.PartitionData (generated data) can't accept Optional type.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585221282", "createdAt": "2021-03-02T03:28:01Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "diffHunk": "@@ -1144,8 +1143,14 @@ class AbstractFetcherThreadTest {\n           (Errors.NONE, records)\n         }\n \n-        (partition, new FetchData(error, leaderState.highWatermark, leaderState.highWatermark, leaderState.logStartOffset,\n-          Optional.empty[Integer], List.empty.asJava, divergingEpoch.asJava, records))\n+        (partition, new FetchResponseData.PartitionData()\n+          .setErrorCode(error.code)\n+          .setHighWatermark(leaderState.highWatermark)\n+          .setLastStableOffset(leaderState.highWatermark)\n+          .setLogStartOffset(leaderState.logStartOffset)\n+          .setAbortedTransactions(Collections.emptyList())\n+          .setRecords(records)\n+          .setDivergingEpoch(divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNDg5Mg=="}, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA3NDI3OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NTo1NFrOItu76w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NTo1NFrOItu76w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNTgzNQ==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584825835", "createdAt": "2021-03-01T15:45:54Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -963,9 +963,11 @@ class ReplicaFetcherThreadTest {\n \n     val records = MemoryRecords.withRecords(CompressionType.NONE,\n       new SimpleRecord(1000, \"foo\".getBytes(StandardCharsets.UTF_8)))\n-\n-    val partitionData: thread.FetchData = new FetchResponse.PartitionData[Records](\n-      Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records)\n+    val partitionData: thread.FetchData = new FetchResponseData.PartitionData()\n+        .setLastStableOffset(0)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA3NTkyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NjowNVrOItu9LA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NjowNVrOItu9LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNjE1Ng==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584826156", "createdAt": "2021-03-01T15:46:05Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -531,10 +529,12 @@ class ReplicaFetcherThreadTest {\n     assertEquals(1, mockNetwork.fetchCount)\n     partitions.foreach { tp => assertEquals(Fetching, thread.fetchState(tp).get.state) }\n \n-    def partitionData(divergingEpoch: FetchResponseData.EpochEndOffset): FetchResponse.PartitionData[Records] = {\n-      new FetchResponse.PartitionData[Records](\n-        Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(),\n-        Optional.of(divergingEpoch), MemoryRecords.EMPTY)\n+    def partitionData(divergingEpoch: FetchResponseData.EpochEndOffset): FetchResponseData.PartitionData = {\n+      new FetchResponseData.PartitionData()\n+          .setLastStableOffset(0)\n+          .setLogStartOffset(0)\n+          .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA3NzkyOnYy", "diffSide": "RIGHT", "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NjoxOFrOItu-lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NjoxOFrOItu-lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNjUxOQ==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584826519", "createdAt": "2021-03-01T15:46:18Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java", "diffHunk": "@@ -78,19 +78,23 @@ public void setup() {\n         for (int topicIdx = 0; topicIdx < topicCount; topicIdx++) {\n             String topic = UUID.randomUUID().toString();\n             for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n-                FetchResponse.PartitionData<MemoryRecords> partitionData = new FetchResponse.PartitionData<>(\n-                    Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records);\n+                FetchResponseData.PartitionData partitionData = new FetchResponseData.PartitionData()\n+                                .setPartitionIndex(partitionId)\n+                                .setLastStableOffset(0)\n+                                .setLogStartOffset(0)\n+                                .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA4MDAxOnYy", "diffSide": "RIGHT", "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NjozMVrOItu_-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0NjozMVrOItu_-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNjg3Mg==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584826872", "createdAt": "2021-03-01T15:46:31Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java", "diffHunk": "@@ -174,8 +173,12 @@ public int sizeInBytes() {\n                     return null;\n                 }\n             };\n-            initialFetched.put(tp, new FetchResponse.PartitionData<>(Errors.NONE, 0, 0, 0,\n-                    new LinkedList<>(), fetched));\n+            initialFetched.put(tp, new FetchResponseData.PartitionData()\n+                    .setPartitionIndex(tp.partition())\n+                    .setLastStableOffset(0)\n+                    .setLogStartOffset(0)\n+                    .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA4Njg3OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0Nzo0M1rOItvEeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0Nzo0M1rOItvEeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODAyNg==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828026", "createdAt": "2021-03-01T15:47:43Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -679,17 +760,24 @@ class FetchSessionTest {\n     assertEquals(Collections.singleton(tp2), resp2.responseData.keySet)\n \n     // All partitions with divergent epoch should be returned.\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)\n+          .setAbortedTransactions(Collections.emptyList())\n+          .setDivergingEpoch(divergingEpoch))\n     val resp3 = context2.updateAndGenerateResponseData(respData)\n     assertEquals(Errors.NONE, resp3.error)\n     assertEquals(resp1.sessionId, resp3.sessionId)\n     assertEquals(Utils.mkSet(tp1, tp2), resp3.responseData.keySet)\n \n     // Partitions that meet other conditions should be returned regardless of whether\n     // divergingEpoch is set or not.\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      110, 110, 0, Optional.empty(), Collections.emptyList(), Optional.empty(), null))\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+        .setHighWatermark(110)\n+        .setLastStableOffset(110)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 498}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA4ODgwOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0ODowMlrOItvFlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0ODowMlrOItvFlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODMwOA==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828308", "createdAt": "2021-03-01T15:48:02Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -679,17 +760,24 @@ class FetchSessionTest {\n     assertEquals(Collections.singleton(tp2), resp2.responseData.keySet)\n \n     // All partitions with divergent epoch should be returned.\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)\n+          .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 483}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA4OTU0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0ODoxMFrOItvF_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0ODoxMFrOItvF_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODQxNA==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828414", "createdAt": "2021-03-01T15:48:10Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -658,12 +732,19 @@ class FetchSessionTest {\n     // Full fetch context returns all partitions in the response\n     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData, EMPTY_PART_LIST, isFollower = false)\n     assertEquals(classOf[FullFetchContext], context1.getClass)\n-    val respData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), Optional.empty(), null))\n-    val divergingEpoch = Optional.of(new FetchResponseData.EpochEndOffset().setEpoch(3).setEndOffset(90))\n-    respData.put(tp2, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    val respData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+        .setHighWatermark(105)\n+        .setLastStableOffset(105)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList()))\n+    val divergingEpoch = new FetchResponseData.EpochEndOffset().setEpoch(3).setEndOffset(90)\n+    respData.put(tp2, new FetchResponseData.PartitionData()\n+        .setHighWatermark(105)\n+        .setLastStableOffset(105)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 468}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA5MDA4OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0ODoxNVrOItvGTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0ODoxNVrOItvGTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODQ5NQ==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828495", "createdAt": "2021-03-01T15:48:15Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -658,12 +732,19 @@ class FetchSessionTest {\n     // Full fetch context returns all partitions in the response\n     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData, EMPTY_PART_LIST, isFollower = false)\n     assertEquals(classOf[FullFetchContext], context1.getClass)\n-    val respData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), Optional.empty(), null))\n-    val divergingEpoch = Optional.of(new FetchResponseData.EpochEndOffset().setEpoch(3).setEndOffset(90))\n-    respData.put(tp2, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    val respData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+        .setHighWatermark(105)\n+        .setLastStableOffset(105)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 462}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDA5NzIyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo0OTozMlrOItvKig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQwMzozODowMVrOIuHQDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyOTU3OA==", "bodyText": "Do we need to set the partition id here and other cases?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584829578", "createdAt": "2021-03-01T15:49:32Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -534,15 +588,21 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session2context = fetchManager.newContext(JFetchMetadata.INITIAL, session1req, EMPTY_PART_LIST, false)\n     assertEquals(classOf[FullFetchContext], session2context.getClass)\n-    val session2RespData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    session2RespData.put(new TopicPartition(\"foo\", 0), new FetchResponse.PartitionData(\n-      Errors.NONE, 100, 100, 100, null, null))\n-    session2RespData.put(new TopicPartition(\"foo\", 1), new FetchResponse.PartitionData(\n-      Errors.NONE, 10, 10, 10, null, null))\n+    val session2RespData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    session2RespData.put(new TopicPartition(\"foo\", 0),\n+      new FetchResponseData.PartitionData()\n+        .setHighWatermark(100)\n+        .setLastStableOffset(100)\n+        .setLogStartOffset(100))\n+    session2RespData.put(new TopicPartition(\"foo\", 1),\n+      new FetchResponseData.PartitionData()\n+        .setHighWatermark(10)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 335}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTIyNDIwNA==", "bodyText": "good catch. will fix all similar issues in next commit.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585224204", "createdAt": "2021-03-02T03:38:01Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -534,15 +588,21 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session2context = fetchManager.newContext(JFetchMetadata.INITIAL, session1req, EMPTY_PART_LIST, false)\n     assertEquals(classOf[FullFetchContext], session2context.getClass)\n-    val session2RespData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    session2RespData.put(new TopicPartition(\"foo\", 0), new FetchResponse.PartitionData(\n-      Errors.NONE, 100, 100, 100, null, null))\n-    session2RespData.put(new TopicPartition(\"foo\", 1), new FetchResponse.PartitionData(\n-      Errors.NONE, 10, 10, 10, null, null))\n+    val session2RespData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    session2RespData.put(new TopicPartition(\"foo\", 0),\n+      new FetchResponseData.PartitionData()\n+        .setHighWatermark(100)\n+        .setLastStableOffset(100)\n+        .setLogStartOffset(100))\n+    session2RespData.put(new TopicPartition(\"foo\", 1),\n+      new FetchResponseData.PartitionData()\n+        .setHighWatermark(10)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyOTU3OA=="}, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 335}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDEyMzA3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo1NDo0MFrOItvadA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQwMzo0MDowN1rOIuHSfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgzMzY1Mg==", "bodyText": "Seems that we could set the diverging offset only if set and leave the default otherwise.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584833652", "createdAt": "2021-03-01T15:54:40Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,84 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        partitions.put(tp, new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp.partition)\n+            .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+            .setHighWatermark(data.highWatermark)\n+            .setLastStableOffset(lastStableOffset)\n+            .setLogStartOffset(data.logStartOffset)\n+            .setAbortedTransactions(abortedTransactions)\n+            .setRecords(data.records)\n+            .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+            .setDivergingEpoch(data.divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTIyNDgyOQ==", "bodyText": "make sense. fixed", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585224829", "createdAt": "2021-03-02T03:40:07Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,84 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        partitions.put(tp, new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp.partition)\n+            .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+            .setHighWatermark(data.highWatermark)\n+            .setLastStableOffset(lastStableOffset)\n+            .setLogStartOffset(data.logStartOffset)\n+            .setAbortedTransactions(abortedTransactions)\n+            .setRecords(data.records)\n+            .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+            .setDivergingEpoch(data.divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgzMzY1Mg=="}, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDEzOTQwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTo1NzoyMFrOItvkDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQwNDoxNTozN1rOIuH8gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgzNjEwOA==", "bodyText": "There is one place in this PR that we check for null when computing the records size, maybe we can use this utility function there.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584836108", "createdAt": "2021-03-01T15:57:20Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 394}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTIzNTU4Ng==", "bodyText": "good point. will copy that", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585235586", "createdAt": "2021-03-02T04:15:37Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgzNjEwOA=="}, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 394}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDE3MTUyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNjowMzoyNVrOItv3_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQwMzo0NjozMlrOIuHZpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDg0MTIxNA==", "bodyText": "Suggestion:\nReturns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`.\n\nIf this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire).", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584841214", "createdAt": "2021-03-01T16:03:25Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 389}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTIyNjY2Mw==", "bodyText": "good one. will copy that", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585226663", "createdAt": "2021-03-02T03:46:32Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDg0MTIxNA=="}, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 389}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDE3MzkzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNjowMzo1N1rOItv5jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQwMzo1Njo1MlrOIuHmJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDg0MTYxNQ==", "bodyText": "Instead of casting blindly, can we include a reasonable error message if the cast fails?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584841615", "createdAt": "2021-03-01T16:03:57Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {\n+        return partition.records() == null ? MemoryRecords.EMPTY : (Records) partition.records();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 395}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTIyOTg2MA==", "bodyText": "will copy that", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585229860", "createdAt": "2021-03-02T03:56:52Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {\n+        return partition.records() == null ? MemoryRecords.EMPTY : (Records) partition.records();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDg0MTYxNQ=="}, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 395}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5NTQ0MTA2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMTozOFrOIuhnLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNTo0MTo0MFrOIuiutA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NjExMQ==", "bodyText": "No else needed since we used return for both other cases. For the exception, I think we can just throw ClassCastException since IllegalStateException doesn't fit very well for this case. I would also make the message a bit more generic to avoid it going stale when we add more Records subtypes. For example:\n\"The record type is \" + partition.records().getClass().getSimpleName() + \", which is not a subtype of \" +\nRecords.class.getSimpleName() + \". This method is only safe to call if the `FetchResponse` was\ndeserialized from bytes.\"", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585656111", "createdAt": "2021-03-02T15:21:38Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,98 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`.\n+     *\n+     * If this response was deserialized after a fetch, this method should never fail. An example where this would\n+     * fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and\n+     * sent on the wire).\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {\n+        if (partition.records() == null) return MemoryRecords.EMPTY;\n+        else if (partition.records() instanceof Records) return (Records) partition.records();\n+        else throw new IllegalStateException(\"the record type is \" + partition.records().getClass().getSimpleName() +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 400}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY2MDk2Mg==", "bodyText": "One more thing, let's call this recordsOrFail to make it clear that the operation is not necessarily safe.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585660962", "createdAt": "2021-03-02T15:26:52Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,98 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`.\n+     *\n+     * If this response was deserialized after a fetch, this method should never fail. An example where this would\n+     * fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and\n+     * sent on the wire).\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {\n+        if (partition.records() == null) return MemoryRecords.EMPTY;\n+        else if (partition.records() instanceof Records) return (Records) partition.records();\n+        else throw new IllegalStateException(\"the record type is \" + partition.records().getClass().getSimpleName() +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NjExMQ=="}, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 400}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY3NDQyMA==", "bodyText": "Will address those nice comments", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585674420", "createdAt": "2021-03-02T15:41:40Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,98 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`.\n+     *\n+     * If this response was deserialized after a fetch, this method should never fail. An example where this would\n+     * fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and\n+     * sent on the wire).\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {\n+        if (partition.records() == null) return MemoryRecords.EMPTY;\n+        else if (partition.records() instanceof Records) return (Records) partition.records();\n+        else throw new IllegalStateException(\"the record type is \" + partition.records().getClass().getSimpleName() +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NjExMQ=="}, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 400}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5NTQ0NTM1OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMjoyMVrOIuhp1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMjoyMVrOIuhp1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1Njc5MQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585656791", "createdAt": "2021-03-02T15:22:21Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -791,15 +791,17 @@ public void produceRequestGetErrorResponseTest() {\n \n     @Test\n     public void fetchResponseVersionTest() {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n \n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(\n-                Errors.NONE, 1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET,\n-                0L, Optional.empty(), Collections.emptyList(), records));\n-\n-        FetchResponse<MemoryRecords> v0Response = new FetchResponse<>(Errors.NONE, responseData, 0, INVALID_SESSION_ID);\n-        FetchResponse<MemoryRecords> v1Response = new FetchResponse<>(Errors.NONE, responseData, 10, INVALID_SESSION_ID);\n+        responseData.put(new TopicPartition(\"test\", 0),\n+                new FetchResponseData.PartitionData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5NTQ0NzM1OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMjo0M1rOIuhrHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMjo0M1rOIuhrHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NzExOQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657119", "createdAt": "2021-03-02T15:22:43Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5NTQ0ODQwOnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMjo1MlrOIuhrvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMjo1MlrOIuhrvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NzI3OQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657279", "createdAt": "2021-03-02T15:22:52Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5NTQ1MDE3OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMzowOFrOIuhs0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMzowOFrOIuhs0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NzU1NQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657555", "createdAt": "2021-03-02T15:23:08Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(abortedTransactions));\n+        return FetchResponse.of(Errors.NONE, 25, sessionId, responseData);\n+    }\n+\n+    private FetchResponse createFetchResponse(boolean includeAborted) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5NTQ1MTU2OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMzoyMFrOIuhtqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMzoyMFrOIuhtqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1Nzc2OA==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657768", "createdAt": "2021-03-02T15:23:20Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(abortedTransactions));\n+        return FetchResponse.of(Errors.NONE, 25, sessionId, responseData);\n+    }\n+\n+    private FetchResponse createFetchResponse(boolean includeAborted) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n \n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-                1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.emptyList();\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.emptyList();\n         if (includeAborted) {\n             abortedTransactions = Collections.singletonList(\n-                    new FetchResponse.AbortedTransaction(234L, 999L));\n+                    new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n         }\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-                1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5NTQ1NzUyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyNDoxOVrOIuhxKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNjowNToyM1rOIukAxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1ODY2NQ==", "bodyText": "We can use the utility method you added to avoid the null check.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585658665", "createdAt": "2021-03-02T15:24:19Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,85 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        val partitionData = new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp.partition)\n+          .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+          .setHighWatermark(data.highWatermark)\n+          .setLastStableOffset(lastStableOffset)\n+          .setLogStartOffset(data.logStartOffset)\n+          .setAbortedTransactions(abortedTransactions)\n+          .setRecords(data.records)\n+          .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+        data.divergingEpoch.foreach(partitionData.setDivergingEpoch)\n+        partitions.put(tp, partitionData)\n       }\n       erroneous.foreach { case (tp, data) => partitions.put(tp, data) }\n \n-      var unconvertedFetchResponse: FetchResponse[Records] = null\n+      var unconvertedFetchResponse: FetchResponse = null\n \n-      def createResponse(throttleTimeMs: Int): FetchResponse[BaseRecords] = {\n+      def createResponse(throttleTimeMs: Int): FetchResponse = {\n         // Down-convert messages for each partition if required\n-        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[BaseRecords]]\n+        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n         unconvertedFetchResponse.responseData.forEach { (tp, unconvertedPartitionData) =>\n-          if (unconvertedPartitionData.error != Errors.NONE)\n+          val error = Errors.forCode(unconvertedPartitionData.errorCode)\n+          if (error != Errors.NONE)\n             debug(s\"Fetch request with correlation id ${request.header.correlationId} from client $clientId \" +\n-              s\"on partition $tp failed due to ${unconvertedPartitionData.error.exceptionName}\")\n+              s\"on partition $tp failed due to ${error.exceptionName}\")\n           convertedData.put(tp, maybeConvertFetchedData(tp, unconvertedPartitionData))\n         }\n \n         // Prepare fetch response from converted data\n-        val response = new FetchResponse(unconvertedFetchResponse.error, convertedData, throttleTimeMs,\n-          unconvertedFetchResponse.sessionId)\n+        val response = FetchResponse.of(unconvertedFetchResponse.error, throttleTimeMs, unconvertedFetchResponse.sessionId, convertedData)\n         // record the bytes out metrics only when the response is being sent\n         response.responseData.forEach { (tp, data) =>\n-          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower, reassigningPartitions.contains(tp), data.records.sizeInBytes)\n+          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower,\n+            reassigningPartitions.contains(tp), if (data.records  == null) 0 else data.records.sizeInBytes)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY4MTgzNA==", "bodyText": "The converted record is LazyDownConversionRecords so casting it to Records can produce error.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585681834", "createdAt": "2021-03-02T15:49:54Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,85 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        val partitionData = new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp.partition)\n+          .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+          .setHighWatermark(data.highWatermark)\n+          .setLastStableOffset(lastStableOffset)\n+          .setLogStartOffset(data.logStartOffset)\n+          .setAbortedTransactions(abortedTransactions)\n+          .setRecords(data.records)\n+          .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+        data.divergingEpoch.foreach(partitionData.setDivergingEpoch)\n+        partitions.put(tp, partitionData)\n       }\n       erroneous.foreach { case (tp, data) => partitions.put(tp, data) }\n \n-      var unconvertedFetchResponse: FetchResponse[Records] = null\n+      var unconvertedFetchResponse: FetchResponse = null\n \n-      def createResponse(throttleTimeMs: Int): FetchResponse[BaseRecords] = {\n+      def createResponse(throttleTimeMs: Int): FetchResponse = {\n         // Down-convert messages for each partition if required\n-        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[BaseRecords]]\n+        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n         unconvertedFetchResponse.responseData.forEach { (tp, unconvertedPartitionData) =>\n-          if (unconvertedPartitionData.error != Errors.NONE)\n+          val error = Errors.forCode(unconvertedPartitionData.errorCode)\n+          if (error != Errors.NONE)\n             debug(s\"Fetch request with correlation id ${request.header.correlationId} from client $clientId \" +\n-              s\"on partition $tp failed due to ${unconvertedPartitionData.error.exceptionName}\")\n+              s\"on partition $tp failed due to ${error.exceptionName}\")\n           convertedData.put(tp, maybeConvertFetchedData(tp, unconvertedPartitionData))\n         }\n \n         // Prepare fetch response from converted data\n-        val response = new FetchResponse(unconvertedFetchResponse.error, convertedData, throttleTimeMs,\n-          unconvertedFetchResponse.sessionId)\n+        val response = FetchResponse.of(unconvertedFetchResponse.error, throttleTimeMs, unconvertedFetchResponse.sessionId, convertedData)\n         // record the bytes out metrics only when the response is being sent\n         response.responseData.forEach { (tp, data) =>\n-          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower, reassigningPartitions.contains(tp), data.records.sizeInBytes)\n+          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower,\n+            reassigningPartitions.contains(tp), if (data.records  == null) 0 else data.records.sizeInBytes)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1ODY2NQ=="}, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY5MTUwNQ==", "bodyText": "Ah, I thought the size method was in FetchResponse. Maybe we should add one (recordsSize or something)? That would always be safe to call.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585691505", "createdAt": "2021-03-02T16:00:35Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,85 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        val partitionData = new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp.partition)\n+          .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+          .setHighWatermark(data.highWatermark)\n+          .setLastStableOffset(lastStableOffset)\n+          .setLogStartOffset(data.logStartOffset)\n+          .setAbortedTransactions(abortedTransactions)\n+          .setRecords(data.records)\n+          .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+        data.divergingEpoch.foreach(partitionData.setDivergingEpoch)\n+        partitions.put(tp, partitionData)\n       }\n       erroneous.foreach { case (tp, data) => partitions.put(tp, data) }\n \n-      var unconvertedFetchResponse: FetchResponse[Records] = null\n+      var unconvertedFetchResponse: FetchResponse = null\n \n-      def createResponse(throttleTimeMs: Int): FetchResponse[BaseRecords] = {\n+      def createResponse(throttleTimeMs: Int): FetchResponse = {\n         // Down-convert messages for each partition if required\n-        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[BaseRecords]]\n+        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n         unconvertedFetchResponse.responseData.forEach { (tp, unconvertedPartitionData) =>\n-          if (unconvertedPartitionData.error != Errors.NONE)\n+          val error = Errors.forCode(unconvertedPartitionData.errorCode)\n+          if (error != Errors.NONE)\n             debug(s\"Fetch request with correlation id ${request.header.correlationId} from client $clientId \" +\n-              s\"on partition $tp failed due to ${unconvertedPartitionData.error.exceptionName}\")\n+              s\"on partition $tp failed due to ${error.exceptionName}\")\n           convertedData.put(tp, maybeConvertFetchedData(tp, unconvertedPartitionData))\n         }\n \n         // Prepare fetch response from converted data\n-        val response = new FetchResponse(unconvertedFetchResponse.error, convertedData, throttleTimeMs,\n-          unconvertedFetchResponse.sessionId)\n+        val response = FetchResponse.of(unconvertedFetchResponse.error, throttleTimeMs, unconvertedFetchResponse.sessionId, convertedData)\n         // record the bytes out metrics only when the response is being sent\n         response.responseData.forEach { (tp, data) =>\n-          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower, reassigningPartitions.contains(tp), data.records.sizeInBytes)\n+          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower,\n+            reassigningPartitions.contains(tp), if (data.records  == null) 0 else data.records.sizeInBytes)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1ODY2NQ=="}, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY5NTQzMA==", "bodyText": "That would always be safe to call.\n\nmake sense. Will copy that", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585695430", "createdAt": "2021-03-02T16:05:23Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,85 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        val partitionData = new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp.partition)\n+          .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+          .setHighWatermark(data.highWatermark)\n+          .setLastStableOffset(lastStableOffset)\n+          .setLogStartOffset(data.logStartOffset)\n+          .setAbortedTransactions(abortedTransactions)\n+          .setRecords(data.records)\n+          .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+        data.divergingEpoch.foreach(partitionData.setDivergingEpoch)\n+        partitions.put(tp, partitionData)\n       }\n       erroneous.foreach { case (tp, data) => partitions.put(tp, data) }\n \n-      var unconvertedFetchResponse: FetchResponse[Records] = null\n+      var unconvertedFetchResponse: FetchResponse = null\n \n-      def createResponse(throttleTimeMs: Int): FetchResponse[BaseRecords] = {\n+      def createResponse(throttleTimeMs: Int): FetchResponse = {\n         // Down-convert messages for each partition if required\n-        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[BaseRecords]]\n+        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n         unconvertedFetchResponse.responseData.forEach { (tp, unconvertedPartitionData) =>\n-          if (unconvertedPartitionData.error != Errors.NONE)\n+          val error = Errors.forCode(unconvertedPartitionData.errorCode)\n+          if (error != Errors.NONE)\n             debug(s\"Fetch request with correlation id ${request.header.correlationId} from client $clientId \" +\n-              s\"on partition $tp failed due to ${unconvertedPartitionData.error.exceptionName}\")\n+              s\"on partition $tp failed due to ${error.exceptionName}\")\n           convertedData.put(tp, maybeConvertFetchedData(tp, unconvertedPartitionData))\n         }\n \n         // Prepare fetch response from converted data\n-        val response = new FetchResponse(unconvertedFetchResponse.error, convertedData, throttleTimeMs,\n-          unconvertedFetchResponse.sessionId)\n+        val response = FetchResponse.of(unconvertedFetchResponse.error, throttleTimeMs, unconvertedFetchResponse.sessionId, convertedData)\n         // record the bytes out metrics only when the response is being sent\n         response.responseData.forEach { (tp, data) =>\n-          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower, reassigningPartitions.contains(tp), data.records.sizeInBytes)\n+          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower,\n+            reassigningPartitions.contains(tp), if (data.records  == null) 0 else data.records.sizeInBytes)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1ODY2NQ=="}, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 241}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNTQ2Nzk0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwNjozMzozM1rOIv_S4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwNzozMjo0MVrOIwA2Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzE5MTAwOA==", "bodyText": "Is this an additional copy compared to previous behavior?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r587191008", "createdAt": "2021-03-04T06:33:33Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,105 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55b35ab5764d8129e23f386229eb350603b7b0aa"}, "originalPosition": 350}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzIxNjQxOA==", "bodyText": "nice question. will revert to previous code :)", "url": "https://github.com/apache/kafka/pull/9758#discussion_r587216418", "createdAt": "2021-03-04T07:32:41Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,105 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzE5MTAwOA=="}, "originalCommit": {"oid": "55b35ab5764d8129e23f386229eb350603b7b0aa"}, "originalPosition": 350}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3520, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}