{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkyNzU4Mjg4", "number": 8337, "reviewThreads": {"totalCount": 40, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjozNjoyM1rODrggtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwNzowNjoyOVrODtZwMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTQ4MDIzOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjozNjoyM1rOF75BLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjozNjoyM1rOF75BLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM0NDQ5NQ==", "bodyText": "Moved this to HighAvailabilityTaskAssignor", "url": "https://github.com/apache/kafka/pull/8337#discussion_r398344495", "createdAt": "2020-03-26T06:36:23Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -858,42 +811,6 @@ private boolean populateClientStatesMap(final Map<UUID, ClientState> clientState\n         return taskEndOffsetSums;\n     }\n \n-    /**\n-     * Rankings are computed as follows, with lower being more caught up:\n-     *      Rank -1: active running task\n-     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n-     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n-     *      Rank 1+: all other tasks are ranked according to their actual total lag\n-     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag\n-     */\n-    static Map<TaskId, SortedSet<RankedClient<UUID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTQ4MTU3OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjozNjo0OFrOF75B3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjozNjo0OFrOF75B3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM0NDY2OQ==", "bodyText": "Moved to HighAvailabilityTaskAssignor", "url": "https://github.com/apache/kafka/pull/8337#discussion_r398344669", "createdAt": "2020-03-26T06:36:48Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -146,53 +144,6 @@ public String toString() {\n         }\n     }\n \n-    public static class RankedClient<ID extends Comparable<? super ID>> implements Comparable<RankedClient<ID>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTQ5NzA5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjo0NDo0NFrOF75LYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjo0NDo0NFrOF75LYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM0NzEwNw==", "bodyText": "Ultimately, I think we can and should refactor the standby task assignment out of StickyTaskAssignor and reuse the same strategy. But for now, I just put in this \"good-enough\" standby assignment approach so we can get the basic structure of the overall assignment algorithm down first.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r398347107", "createdAt": "2020-03-26T06:44:44Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,530 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+            new DefaultBalancedAssignor<ID>().assign(\n+                sortedClients,\n+                statefulTasks,\n+                clientsToNumberOfThreads,\n+                configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements = getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3MzAxNTE5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQyMTo0Mjo0M1rOF8bkeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QwNDo0MTowOFrOF8i-Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxMDU4Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (lagComputationSuccessful && highAvailabilityEnabled) {\n          \n          \n            \n                        taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n          \n          \n            \n                    } else {\n          \n          \n            \n                        taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n          \n          \n            \n                        if (highAvailabilityEnabled) {\n          \n          \n            \n                            // Once high availability is permanently enabled, this will be the default behavior of StickyTaskAssignor\n          \n          \n            \n                            ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    if (highAvailabilityEnabled) {\n          \n          \n            \n                        taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n          \n          \n            \n                    } else {\n          \n          \n            \n                        taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n          \n          \n            \n                        if (!lagComputationSuccessful) {\n          \n          \n            \n                            ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n          \n          \n            \n                        }\n          \n          \n            \n                    }", "url": "https://github.com/apache/kafka/pull/8337#discussion_r398910586", "createdAt": "2020-03-26T21:42:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,23 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor taskAssignor;\n+        if (lagComputationSuccessful && highAvailabilityEnabled) {\n+            taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+            if (highAvailabilityEnabled) {\n+                // Once high availability is permanently enabled, this will be the default behavior of StickyTaskAssignor\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n         }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTAzMTgxMA==", "bodyText": "That's not quite the same (case: highAvailabilityEnabled == true and lagComputationSuccessful == false should fall back on the StickyTaskAssignor and the previous assignment)\nBut I think your point is to have a clean separation of the \"enabled\" and \"disabled\" code flow, which makes sense, so \"ack\"", "url": "https://github.com/apache/kafka/pull/8337#discussion_r399031810", "createdAt": "2020-03-27T04:41:08Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,23 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor taskAssignor;\n+        if (lagComputationSuccessful && highAvailabilityEnabled) {\n+            taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+            if (highAvailabilityEnabled) {\n+                // Once high availability is permanently enabled, this will be the default behavior of StickyTaskAssignor\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n         }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxMDU4Ng=="}, "originalCommit": null, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3MzA0NzA4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQyMTo1MzoxMlrOF8b3lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQyMTo1MzoxMlrOF8b3lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxNTQ3Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static final String HIGH_AVAILABILITY_FLAG_CONFIG = \"high.availability.enabled\";\n          \n          \n            \n                public static final String HIGH_AVAILABILITY_FLAG_CONFIG = \"internal.high.availability.enabled\";\n          \n      \n    \n    \n  \n\nTo document that it's undocumented ;)", "url": "https://github.com/apache/kafka/pull/8337#discussion_r398915477", "createdAt": "2020-03-26T21:53:12Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -38,6 +38,9 @@\n import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n \n public final class AssignorConfiguration {\n+    public static final String HIGH_AVAILABILITY_FLAG_CONFIG = \"high.availability.enabled\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3MzA1MTE0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQyMTo1NDoyN1rOF8b52g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QwNDo0MjoxM1rOF8i_Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxNjA1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (configs.containsKey(HIGH_AVAILABILITY_FLAG_CONFIG)) {\n          \n          \n            \n                        highAvailabilityEnabled = streamsConfig.getBoolean(HIGH_AVAILABILITY_FLAG_CONFIG);\n          \n          \n            \n                    } else {\n          \n          \n            \n                        highAvailabilityEnabled = false;\n          \n          \n            \n                    }\n          \n          \n            \n            highAvailabilityEnabled = configs.containsKey(HIGH_AVAILABILITY_FLAG_CONFIG) && streamsConfig.getBoolean(HIGH_AVAILABILITY_FLAG_CONFIG);\n          \n      \n    \n    \n  \n\nCouldn't resist.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r398916058", "createdAt": "2020-03-26T21:54:27Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -154,6 +157,12 @@ public AssignorConfiguration(final Map<String, ?> configs) {\n         adminClientTimeout = streamsConfig.getInt(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG);\n \n         copartitionedTopicsEnforcer = new CopartitionedTopicsEnforcer(logPrefix);\n+\n+        if (configs.containsKey(HIGH_AVAILABILITY_FLAG_CONFIG)) {\n+            highAvailabilityEnabled = streamsConfig.getBoolean(HIGH_AVAILABILITY_FLAG_CONFIG);\n+        } else {\n+            highAvailabilityEnabled = false;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTAzMjEyNg==", "bodyText": "Fair enough \ud83d\ude04", "url": "https://github.com/apache/kafka/pull/8337#discussion_r399032126", "createdAt": "2020-03-27T04:42:13Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -154,6 +157,12 @@ public AssignorConfiguration(final Map<String, ?> configs) {\n         adminClientTimeout = streamsConfig.getInt(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG);\n \n         copartitionedTopicsEnforcer = new CopartitionedTopicsEnforcer(logPrefix);\n+\n+        if (configs.containsKey(HIGH_AVAILABILITY_FLAG_CONFIG)) {\n+            highAvailabilityEnabled = streamsConfig.getBoolean(HIGH_AVAILABILITY_FLAG_CONFIG);\n+        } else {\n+            highAvailabilityEnabled = false;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxNjA1OA=="}, "originalCommit": null, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3MzA2MTg3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQyMTo1ODozOFrOF8cAjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQxODo0MjozMFrOF_Nyww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxNzc3Mg==", "bodyText": "Oof, this implies an ordering dependency in how you call these methods. Should we enforce that ordering? Or should we push this logic up into the ownedPartitions block, if that's the only place where we might learn that we were previously wrong about which tasks were active?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r398917772", "createdAt": "2020-03-26T21:58:38Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -154,9 +160,21 @@ public int activeTaskCount() {\n         return activeTasks.size();\n     }\n \n+    void addPreviousActiveTask(final TaskId task) {\n+        prevActiveTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n+    void addPreviousStandbyTask(final TaskId task) {\n+        prevStandbyTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n     public void addPreviousActiveTasks(final Set<TaskId> prevTasks) {\n         prevActiveTasks.addAll(prevTasks);\n         prevAssignedTasks.addAll(prevTasks);\n+        // We need to remove from prevStandbyTasks as we may have initially added the task as a standby, before\n+        // learning that it was in fact an active (eg from encoded ownedPartitions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQ0NjI1NA==", "bodyText": "Unfortunately we don't have the partition -> task mapping at the time of setting the ownedPartitions and prevActive/StandbyTasks. But maybe we can just add the task offset sums and owned partitions initially, and then wait to convert both of these to prevActive/Standbytasks` at the same time down here", "url": "https://github.com/apache/kafka/pull/8337#discussion_r399446254", "createdAt": "2020-03-27T17:59:53Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -154,9 +160,21 @@ public int activeTaskCount() {\n         return activeTasks.size();\n     }\n \n+    void addPreviousActiveTask(final TaskId task) {\n+        prevActiveTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n+    void addPreviousStandbyTask(final TaskId task) {\n+        prevStandbyTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n     public void addPreviousActiveTasks(final Set<TaskId> prevTasks) {\n         prevActiveTasks.addAll(prevTasks);\n         prevAssignedTasks.addAll(prevTasks);\n+        // We need to remove from prevStandbyTasks as we may have initially added the task as a standby, before\n+        // learning that it was in fact an active (eg from encoded ownedPartitions)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxNzc3Mg=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTU1NDE0Mg==", "bodyText": "Alright I tried to delay the construction of the prev task sets until we can convert the ownedPartitions into tasks, lmk if this looks more or less clear", "url": "https://github.com/apache/kafka/pull/8337#discussion_r399554142", "createdAt": "2020-03-27T21:49:20Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -154,9 +160,21 @@ public int activeTaskCount() {\n         return activeTasks.size();\n     }\n \n+    void addPreviousActiveTask(final TaskId task) {\n+        prevActiveTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n+    void addPreviousStandbyTask(final TaskId task) {\n+        prevStandbyTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n     public void addPreviousActiveTasks(final Set<TaskId> prevTasks) {\n         prevActiveTasks.addAll(prevTasks);\n         prevAssignedTasks.addAll(prevTasks);\n+        // We need to remove from prevStandbyTasks as we may have initially added the task as a standby, before\n+        // learning that it was in fact an active (eg from encoded ownedPartitions)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxNzc3Mg=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgzMDU5NQ==", "bodyText": "Looks good to me. Thanks!", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401830595", "createdAt": "2020-04-01T18:42:30Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -154,9 +160,21 @@ public int activeTaskCount() {\n         return activeTasks.size();\n     }\n \n+    void addPreviousActiveTask(final TaskId task) {\n+        prevActiveTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n+    void addPreviousStandbyTask(final TaskId task) {\n+        prevStandbyTasks.add(task);\n+        prevAssignedTasks.add(task);\n+    }\n+\n     public void addPreviousActiveTasks(final Set<TaskId> prevTasks) {\n         prevActiveTasks.addAll(prevTasks);\n         prevAssignedTasks.addAll(prevTasks);\n+        // We need to remove from prevStandbyTasks as we may have initially added the task as a standby, before\n+        // learning that it was in fact an active (eg from encoded ownedPartitions)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODkxNzc3Mg=="}, "originalCommit": null, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3Mzc1MTU4OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QwMzo1MzozMFrOF8iUiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMzoxMToxNlrOF-DusA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTAyMTE5NQ==", "bodyText": "None of these tests are removed, just moved to HighAvailabilityTaskAssignorTest", "url": "https://github.com/apache/kafka/pull/8337#discussion_r399021195", "createdAt": "2020-03-27T03:53:30Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -1837,126 +1829,6 @@ public void shouldSetAdminClientTimeout() {\n         assertThat(assignorConfiguration.getAdminClientTimeout(), is(2 * 60 * 1000));\n     }\n \n-    @Test\n-    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxNzEzNg==", "bodyText": "Just leaving a breadcrumb for other reviewers, these are just testing the behavior of the static method buildClientRankingsByTask, which has moved to HighAvailabilityTaskAssignor.\nThis probably indicates that the method (and RankedClient as well IMHO) should be moved to a top-level class and not be nested inside the assignor, but I don't feel strongly enough to insist on it at the moment.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400617136", "createdAt": "2020-03-31T03:11:16Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -1837,126 +1829,6 @@ public void shouldSetAdminClientTimeout() {\n         assertThat(assignorConfiguration.getAdminClientTimeout(), is(2 * 60 * 1000));\n     }\n \n-    @Test\n-    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTAyMTE5NQ=="}, "originalCommit": null, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MzkyMDEyOnYy", "diffSide": "RIGHT", "path": "checkstyle/suppressions.xml", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMzowMjo0NlrOF9_YrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwNDo1NzowOVrOF-w0-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA==", "bodyText": "I assume this is because of private static final TaskId task0_0, why not just call it TASK_0_0, etc?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400545964", "createdAt": "2020-03-30T23:02:46Z", "author": {"login": "vvcephei"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwMDYwNA==", "bodyText": "Hm, I just removed the ConstantName suppression and checkstyle passed. Maybe I was doing something weird that got cleaned up.\nI'm guessing that wasn't really your question, though. I can't speak to why the author of StreamsPartitionAssignor decided to violate/suppress checkstyle back in the day, but I personally find the tests easier to read with the task0_0 naming convention. I'm happy to negotiate though", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400600604", "createdAt": "2020-03-31T02:08:55Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwMzAwNA==", "bodyText": "It's a bit annoying because different tests in the assignment package all need the same set of variables, but have different naming conventions. Some just chose to make them non-static rather than suppress the checkstyle MemberName warning.\nSeeing as every test file starts with a long number of declarations of the same thing, I'd prefer to just move them all (and any shared util methods) to a separate AssignmentTestUtils file. I stopped myself from doing that in this PR, but if/when we do consolidate them we can discuss which of the several naming conventions to go with", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400603004", "createdAt": "2020-03-31T02:18:01Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDc5NjIxNA==", "bodyText": "IMO, suppressing checkstyle rules should be the last thing one should do to get a successful validation. Especially, for new code. Otherwise we need to re-discuss the checkstyle rules.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400796214", "createdAt": "2020-03-31T10:11:35Z", "author": {"login": "cadonna"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE1NTA2NQ==", "bodyText": "Well I guess I personally disagree with the checkstyle rules in this one case, but I'll drop it for now", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401155065", "createdAt": "2020-03-31T19:18:41Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTIzODU5MA==", "bodyText": "As I understand it, the checkstyle rule is enforcing the official code style for the AK project, so we shouldn't just play jazz. But I can get behind your desire to standardize the test style, at least within the module. If you prefer not to uppercase the field names, you can just drop static but keep final, right? In the grand scheme, making it static is just saving a miniscule amount of RAM during tests.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401238590", "createdAt": "2020-03-31T21:54:49Z", "author": {"login": "vvcephei"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM1NDY0OQ==", "bodyText": "Unfortunately I also have gripes about the non-static naming rules as well, as task0_0 is still not allowed (underscore not allowed). Maybe I'm just being unreasonable here...", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401354649", "createdAt": "2020-04-01T04:51:30Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM1NjAyNQ==", "bodyText": "(Not trying to continue the argument, I conformed all names to the oppressive checkstyle rules in the end \ud83d\ude42)", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401356025", "createdAt": "2020-04-01T04:57:09Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -208,8 +208,8 @@\n     <suppress checks=\"MethodLength\"\n               files=\"RocksDBWindowStoreTest.java\"/>\n \n-    <suppress checks=\"MemberName\"\n-              files=\"StreamsPartitionAssignorTest.java\"/>\n+    <suppress checks=\"MemberName|ConstantName\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NTk2NA=="}, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MzkyOTAxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMzowNzowNVrOF9_eaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwNDo1NTozNFrOF-wznw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NzQzMg==", "bodyText": "Thanks, this block looks more understandable to me now.\nNew question: previously, we would set the sticky assignor to preserve previous task assignments if lag computation weren't successful, but now we only do that if we're falling back from the HA task assignor. Did you mean to also include the \"if (!lagComputationSuccessful){ preserve previous }` logic here?\nIt seems a little hairy, and it does seem clear that we should refactor it, but I'm ok with doing that in a follow-up PR, if you are.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400547432", "createdAt": "2020-03-30T23:07:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU5NzY2OA==", "bodyText": "Well, if we define \"previous\" as \"prior to KIP-441\" then the previous behavior was to let the StickyTaskAssignor do its thing regardless of the lag computation (by definition, since there was no lag computation). Is this not the definition of \"previous\" we would apply to the feature flag, ie \"when the flag is disabled, Streams will revert to its previous behavior\"?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400597668", "createdAt": "2020-03-31T01:57:19Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NzQzMg=="}, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg0OTYwMQ==", "bodyText": "Q: Where does if (!lagComputationSuccessful) come from, if StickyTaskAssignor did its thing regardless of the lag computation? I have the feeling I am missing something important here.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400849601", "createdAt": "2020-03-31T11:49:44Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NzQzMg=="}, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2NzI0Ng==", "bodyText": "lagComputationSuccessful was introduced in a previous KIP-441 PR, to address this concern: ableegoldman#2 (comment)", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401167246", "createdAt": "2020-03-31T19:40:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NzQzMg=="}, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE3OTI4Mg==", "bodyText": "I was referring to \"prior\" as in the starting point of this diff, aka current trunk.\nI think it has the lagComputationSuccessful check there because of the way we're inferring active/standby status based on the reported lag, which is not correct if we couldn't actually compute the lags, so we put that \"safety valve\" in to just no-op the assignment and try again next time.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401179282", "createdAt": "2020-03-31T20:02:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NzQzMg=="}, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM1NTY3OQ==", "bodyText": "Just to close the loop for anyone coming back to read this, the lagComputationSuccessful only fails if the end offset fetch fails, ie we will still have the offset sums. We can infer the previous task sets from the offset sums, so even if the lag computation fails, the StickyTaskAssignor can do its thing.\n@vvcephei would it have helped to rename lagComputationSuccessful --> endOffsetFetchSuccessful or the like? Or was it mostly just the partial-KIP-441-ness of the current trunk that threw you off", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401355679", "createdAt": "2020-04-01T04:55:34Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU0NzQzMg=="}, "originalCommit": null, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4Mzk4NTkwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMzozMjo1OVrOF-AAag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMDowNToxMlrOF-mJaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjEzOA==", "bodyText": "I think I might be missing something... The standbyTaskAssignment is empty here, and also on Line 130.\nWhy do we need to add it to these collections?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400556138", "createdAt": "2020-03-30T23:32:59Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwOTM4Mg==", "bodyText": "It's empty when we create the queue, but starts to fill up as we use the queue. The queue needs a reference to all stateful assignment maps so it can verify whether a task can be assigned to that client. The queue also needs a reference to the two standby-type assignment maps (as in Line 130) so it choose the next least loaded client when polled.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400609382", "createdAt": "2020-03-31T02:42:40Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjEzOA=="}, "originalCommit": null, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxMzkzOQ==", "bodyText": "I was worried this part of the code wasn't all that clear, this seems like an indication that may be the case...I'm open to suggestions, or to pushing that to followup work if that makes more sense", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400613939", "createdAt": "2020-03-31T02:59:26Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjEzOA=="}, "originalCommit": null, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4MTAzMg==", "bodyText": "sounds good. Let's put it off for now.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401181032", "createdAt": "2020-03-31T20:05:12Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjEzOA=="}, "originalCommit": null, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDI5MTY5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjoxNzozOVrOF-C3FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwMTowMjo0OVrOF-tYJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwMjkwMQ==", "bodyText": "This is a bit suspicious... If we're polling the queue, we should just loop until the queue is empty, not iterate over another another collection we happen to know has the same number of elements.\nMore specifically, poll might return null, but offer throws an NPE if client is null.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400602901", "createdAt": "2020-03-31T02:17:39Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkxNzA1MA==", "bodyText": "I do not agree. We need to distribute the stateless tasks, therefore the loop is over the stateless tasks. For each task we need to find the client with the least load which is done with the priority queue (i.e. min-heap). Since we poll a client and add the updated client in each iteration, poll() cannot return null.\nMy question would be why we only consider the stateful active tasks assignment and not the assignment of all tasks, i.e., also standby tasks and warm-up replica in the priority queue. Also those tasks contribute to the load of a client.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400917050", "createdAt": "2020-03-31T13:33:54Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwMjkwMQ=="}, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4MTc0Ng==", "bodyText": "fair enough", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401181746", "createdAt": "2020-03-31T20:06:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwMjkwMQ=="}, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI5OTQ5NQ==", "bodyText": "@cadonna I was thinking that we should try to balance the active tasks and standby tasks separately, but maybe a standby task is closer to the workload of a stateful active task than a stateless active task is. So, I think I agree that we should include all tasks here", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401299495", "createdAt": "2020-04-01T01:02:49Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwMjkwMQ=="}, "originalCommit": null, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDMwMTYwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjoyMzo0N1rOF-C9OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjoyMzo0N1rOF-C9OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNDQ3Mw==", "bodyText": "Should this be .equals?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400604473", "createdAt": "2020-03-31T02:23:47Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDMxNTQ3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjozMjowNFrOF-DFow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMDoyMjo1MFrOF-mxdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNjYyNw==", "bodyText": "Please avoid raw types, even when you don't need the type bound.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    for (final RankedClient rankedClient : rankedClients) {\n          \n          \n            \n                    for (final RankedClient<ID> rankedClient : rankedClients) {", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400606627", "createdAt": "2020-03-31T02:32:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE5MTI4NA==", "bodyText": "IDEA will now yell at me for this so you don't have to \ud83d\ude42", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401191284", "createdAt": "2020-03-31T20:22:50Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNjYyNw=="}, "originalCommit": null, "originalPosition": 263}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDMyMDQ0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjozNToyOFrOF-DI5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMTozNDoxNVrOF_Td8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNzQ2MQ==", "bodyText": "I'm starting to lose track of the details... What is the impact of setting these tasks' ranks as -1 instead of 0?\nIf memory serves, we proposed to just treat all caught-up clients as the same for the purpose of assignments.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400607461", "createdAt": "2020-03-31T02:35:28Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0Mzk5Mg==", "bodyText": "I see what you mean. I do not have any heart feelings here. Would be interesting to see in experiments how the two approaches differ.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400943992", "createdAt": "2020-03-31T14:09:41Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNzQ2MQ=="}, "originalCommit": null, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NDMwNg==", "bodyText": "I think we should still prioritize the client that had the active task over one with a caught-up standby. For one thing, with KIP-429 we have to revoke an active task before moving it to a new client in a followup rebalance. This means deadtime for that active task between being closed on one client, waiting for another rebalance, and finally being recreated from a standby on a new client. We also lose the cache, buffers, etc", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401184306", "createdAt": "2020-03-31T20:10:54Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNzQ2MQ=="}, "originalCommit": null, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTIzNDc5OQ==", "bodyText": "That's fair. My concern about the impact was whether it results in non-termination of the probing rebalance cycle, if we always prefer to re-assign the prior active and always propose to move the task to the same caught-up standby, but never consider just giving the active to the caught-up standby, since there is a prior active.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401234799", "createdAt": "2020-03-31T21:46:49Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNzQ2MQ=="}, "originalCommit": null, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM1MDI2MQ==", "bodyText": "Actually I'm pretty concerned about the same thing, but I'm not sure I see how ranking previous active tasks saves us from getting trapped in the rebalancing cycle. Might be worth chatting about offline..", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401350261", "createdAt": "2020-04-01T04:32:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNzQ2MQ=="}, "originalCommit": null, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkyMzU2OQ==", "bodyText": "I suppose that this, among other things, will become clear when we add some integration and system tests.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401923569", "createdAt": "2020-04-01T21:34:15Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwNzQ2MQ=="}, "originalCommit": null, "originalPosition": 302}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDMyODU5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjo0MDowNlrOF-DOAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMDo0NjowM1rOF-nklw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwODc2OA==", "bodyText": "I feel like I might be missing something, but what's the advantage of creating a method reference and passing it in so that we can invoke it with the object that has that method? I.e., why not just:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n          \n          \n            \n                        final Set<TaskId> activeTasks = new HashSet<>(state.prevActiveTasks());\n          \n      \n    \n    \n  \n\nAnd then we don't need the third parameter?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400608768", "createdAt": "2020-03-31T02:40:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 329}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTIwNDM3NQ==", "bodyText": "Refactoring artifact...originally we would also compute the balance factor for the final assignment for some optimization, but I took it out to keep things simple (relatively speaking...). So now this makes no sense, I agree", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401204375", "createdAt": "2020-03-31T20:46:03Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwODc2OA=="}, "originalCommit": null, "originalPosition": 329}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDMzNjIxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjo0NDo0NVrOF-DSqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjoxNjowNFrOF-qN5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwOTk2Mw==", "bodyText": "If I read this right, the current method doesn't actually consider the new proposed assignment. Are we mutating some fields, or could this method actually be invoked at the beginning of the assignment to gate if the current assignment is \"good enough\"?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400609963", "createdAt": "2020-03-31T02:44:45Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 344}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0NzcxNw==", "bodyText": "Another artifact of removing an optimization. I think we can move it up to the beginning now, yes", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401247717", "createdAt": "2020-03-31T22:16:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYwOTk2Mw=="}, "originalCommit": null, "originalPosition": 344}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDM1NTA5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjo1NTozN1rOF-DeLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjo1NTozN1rOF-DeLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxMjkwOA==", "bodyText": "I can't understand why there's no warning about this, but it looks like clientId should always use .equals instead of ==.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400612908", "createdAt": "2020-03-31T02:55:37Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The\n+     * previous assignment will be chosen and returned iff all of the following are true:\n+     *   1) it satisfies the state constraint, ie all tasks with caught up clients are assigned to one of those clients\n+     *   2) it satisfies the balance factor\n+     *   3) there are no unassigned tasks (eg due to a client that dropped out of the group)\n+     */\n+    private boolean shouldUsePreviousAssignment() {\n+        if (previousAssignmentIsValid()) {\n+            final int previousAssignmentBalanceFactor =\n+                computeBalanceFactor(clientStates.values(), statefulTasks, ClientState::prevActiveTasks);\n+            return previousAssignmentBalanceFactor <= configs.balanceFactor;\n+        } else {\n+            return false;\n+        }\n+    }\n+\n+    private Map<ID, List<TaskId>> initializeEmptyTaskAssignmentMap() {\n+        return sortedClients.stream().collect(Collectors.toMap(id -> id, id -> new ArrayList<>()));\n+    }\n+\n+    private void assignActiveTasksToClients(final Map<ID, List<TaskId>> activeTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignActiveTasks(activeTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignStandbyTasksToClients(final Map<ID, List<TaskId>> standbyTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignStandbyTasks(standbyTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignPreviousTasksToClientStates() {\n+        for (final ClientState clientState : clientStates.values()) {\n+            clientState.assignActiveTasks(clientState.prevActiveTasks());\n+            clientState.assignStandbyTasks(clientState.prevStandbyTasks());\n+        }\n+    }\n+\n+    private PriorityQueue<ID> getClientPriorityQueueByTaskLoad(final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        final PriorityQueue<ID> queue = new PriorityQueue<>(\n+            (client, other) -> {\n+                final int clientTasksPerThread = tasksPerThread(client, taskLoadsByClient);\n+                final int otherTasksPerThread = tasksPerThread(other, taskLoadsByClient);\n+                if (clientTasksPerThread != otherTasksPerThread) {\n+                    return clientTasksPerThread - otherTasksPerThread;\n+                } else {\n+                    return client.compareTo(other);\n+                }\n+            });\n+\n+        queue.addAll(sortedClients);\n+        return queue;\n+    }\n+\n+    private int tasksPerThread(final ID client, final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        double numTasks = 0;\n+        for (final Map<ID, List<TaskId>> assignment : taskLoadsByClient) {\n+            numTasks += assignment.get(client).size();\n+        }\n+        return (int) Math.ceil(numTasks / clientsToNumberOfThreads.get(client));\n+    }\n+\n+    static class RankedClient<ID extends Comparable<? super ID>> implements Comparable<RankedClient<ID>> {\n+        private final ID clientId;\n+        private final long rank;\n+\n+        RankedClient(final ID clientId, final long rank) {\n+            this.clientId = clientId;\n+            this.rank = rank;\n+        }\n+\n+        ID clientId() {\n+            return clientId;\n+        }\n+\n+        long rank() {\n+            return rank;\n+        }\n+\n+        @Override\n+        public int compareTo(final RankedClient<ID> clientIdAndLag) {\n+            if (rank < clientIdAndLag.rank) {\n+                return -1;\n+            } else if (rank > clientIdAndLag.rank) {\n+                return 1;\n+            } else {\n+                return clientId.compareTo(clientIdAndLag.clientId);\n+            }\n+        }\n+\n+        @Override\n+        public boolean equals(final Object o) {\n+            if (this == o) {\n+                return true;\n+            } else if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            final RankedClient other = (RankedClient) o;\n+            return clientId == other.clientId() && rank == other.rank();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 447}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDM2MjMxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMjo1OToyNFrOF-DiLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwMToxODoyNVrOF-tnIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxMzkzNA==", "bodyText": "Here also. It looks like you used the IDE code generator to make these, but they don't seem to be correct. Perhaps there's a configuration wrong somewhere?\nHere's what mine produces:\n        @Override\n        public boolean equals(final Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            final Movement<?> movement = (Movement<?>) o;\n            return Objects.equals(task, movement.task) &&\n                Objects.equals(source, movement.source) &&\n                Objects.equals(destination, movement.destination);\n        }", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400613934", "createdAt": "2020-03-31T02:59:24Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The\n+     * previous assignment will be chosen and returned iff all of the following are true:\n+     *   1) it satisfies the state constraint, ie all tasks with caught up clients are assigned to one of those clients\n+     *   2) it satisfies the balance factor\n+     *   3) there are no unassigned tasks (eg due to a client that dropped out of the group)\n+     */\n+    private boolean shouldUsePreviousAssignment() {\n+        if (previousAssignmentIsValid()) {\n+            final int previousAssignmentBalanceFactor =\n+                computeBalanceFactor(clientStates.values(), statefulTasks, ClientState::prevActiveTasks);\n+            return previousAssignmentBalanceFactor <= configs.balanceFactor;\n+        } else {\n+            return false;\n+        }\n+    }\n+\n+    private Map<ID, List<TaskId>> initializeEmptyTaskAssignmentMap() {\n+        return sortedClients.stream().collect(Collectors.toMap(id -> id, id -> new ArrayList<>()));\n+    }\n+\n+    private void assignActiveTasksToClients(final Map<ID, List<TaskId>> activeTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignActiveTasks(activeTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignStandbyTasksToClients(final Map<ID, List<TaskId>> standbyTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignStandbyTasks(standbyTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignPreviousTasksToClientStates() {\n+        for (final ClientState clientState : clientStates.values()) {\n+            clientState.assignActiveTasks(clientState.prevActiveTasks());\n+            clientState.assignStandbyTasks(clientState.prevStandbyTasks());\n+        }\n+    }\n+\n+    private PriorityQueue<ID> getClientPriorityQueueByTaskLoad(final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        final PriorityQueue<ID> queue = new PriorityQueue<>(\n+            (client, other) -> {\n+                final int clientTasksPerThread = tasksPerThread(client, taskLoadsByClient);\n+                final int otherTasksPerThread = tasksPerThread(other, taskLoadsByClient);\n+                if (clientTasksPerThread != otherTasksPerThread) {\n+                    return clientTasksPerThread - otherTasksPerThread;\n+                } else {\n+                    return client.compareTo(other);\n+                }\n+            });\n+\n+        queue.addAll(sortedClients);\n+        return queue;\n+    }\n+\n+    private int tasksPerThread(final ID client, final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        double numTasks = 0;\n+        for (final Map<ID, List<TaskId>> assignment : taskLoadsByClient) {\n+            numTasks += assignment.get(client).size();\n+        }\n+        return (int) Math.ceil(numTasks / clientsToNumberOfThreads.get(client));\n+    }\n+\n+    static class RankedClient<ID extends Comparable<? super ID>> implements Comparable<RankedClient<ID>> {\n+        private final ID clientId;\n+        private final long rank;\n+\n+        RankedClient(final ID clientId, final long rank) {\n+            this.clientId = clientId;\n+            this.rank = rank;\n+        }\n+\n+        ID clientId() {\n+            return clientId;\n+        }\n+\n+        long rank() {\n+            return rank;\n+        }\n+\n+        @Override\n+        public int compareTo(final RankedClient<ID> clientIdAndLag) {\n+            if (rank < clientIdAndLag.rank) {\n+                return -1;\n+            } else if (rank > clientIdAndLag.rank) {\n+                return 1;\n+            } else {\n+                return clientId.compareTo(clientIdAndLag.clientId);\n+            }\n+        }\n+\n+        @Override\n+        public boolean equals(final Object o) {\n+            if (this == o) {\n+                return true;\n+            } else if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            final RankedClient other = (RankedClient) o;\n+            return clientId == other.clientId() && rank == other.rank();\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(rank, clientId);\n+        }\n+    }\n+\n+    static class Movement<ID> {\n+        final TaskId task;\n+        final ID source;\n+        final ID destination;\n+\n+        Movement(final TaskId task, final ID source, final ID destination) {\n+            this.task = task;\n+            this.source = source;\n+            this.destination = destination;\n+        }\n+\n+        @Override\n+        public boolean equals(final Object o) {\n+            if (this == o) {\n+                return true;\n+            } else if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            final Movement other = (Movement) o;\n+            return task == other.task && source == other.source && destination == other.destination;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 475}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk1MDIxOA==", "bodyText": "FYI: Mine produces the same as John's", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400950218", "createdAt": "2020-03-31T14:17:48Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The\n+     * previous assignment will be chosen and returned iff all of the following are true:\n+     *   1) it satisfies the state constraint, ie all tasks with caught up clients are assigned to one of those clients\n+     *   2) it satisfies the balance factor\n+     *   3) there are no unassigned tasks (eg due to a client that dropped out of the group)\n+     */\n+    private boolean shouldUsePreviousAssignment() {\n+        if (previousAssignmentIsValid()) {\n+            final int previousAssignmentBalanceFactor =\n+                computeBalanceFactor(clientStates.values(), statefulTasks, ClientState::prevActiveTasks);\n+            return previousAssignmentBalanceFactor <= configs.balanceFactor;\n+        } else {\n+            return false;\n+        }\n+    }\n+\n+    private Map<ID, List<TaskId>> initializeEmptyTaskAssignmentMap() {\n+        return sortedClients.stream().collect(Collectors.toMap(id -> id, id -> new ArrayList<>()));\n+    }\n+\n+    private void assignActiveTasksToClients(final Map<ID, List<TaskId>> activeTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignActiveTasks(activeTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignStandbyTasksToClients(final Map<ID, List<TaskId>> standbyTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignStandbyTasks(standbyTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignPreviousTasksToClientStates() {\n+        for (final ClientState clientState : clientStates.values()) {\n+            clientState.assignActiveTasks(clientState.prevActiveTasks());\n+            clientState.assignStandbyTasks(clientState.prevStandbyTasks());\n+        }\n+    }\n+\n+    private PriorityQueue<ID> getClientPriorityQueueByTaskLoad(final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        final PriorityQueue<ID> queue = new PriorityQueue<>(\n+            (client, other) -> {\n+                final int clientTasksPerThread = tasksPerThread(client, taskLoadsByClient);\n+                final int otherTasksPerThread = tasksPerThread(other, taskLoadsByClient);\n+                if (clientTasksPerThread != otherTasksPerThread) {\n+                    return clientTasksPerThread - otherTasksPerThread;\n+                } else {\n+                    return client.compareTo(other);\n+                }\n+            });\n+\n+        queue.addAll(sortedClients);\n+        return queue;\n+    }\n+\n+    private int tasksPerThread(final ID client, final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        double numTasks = 0;\n+        for (final Map<ID, List<TaskId>> assignment : taskLoadsByClient) {\n+            numTasks += assignment.get(client).size();\n+        }\n+        return (int) Math.ceil(numTasks / clientsToNumberOfThreads.get(client));\n+    }\n+\n+    static class RankedClient<ID extends Comparable<? super ID>> implements Comparable<RankedClient<ID>> {\n+        private final ID clientId;\n+        private final long rank;\n+\n+        RankedClient(final ID clientId, final long rank) {\n+            this.clientId = clientId;\n+            this.rank = rank;\n+        }\n+\n+        ID clientId() {\n+            return clientId;\n+        }\n+\n+        long rank() {\n+            return rank;\n+        }\n+\n+        @Override\n+        public int compareTo(final RankedClient<ID> clientIdAndLag) {\n+            if (rank < clientIdAndLag.rank) {\n+                return -1;\n+            } else if (rank > clientIdAndLag.rank) {\n+                return 1;\n+            } else {\n+                return clientId.compareTo(clientIdAndLag.clientId);\n+            }\n+        }\n+\n+        @Override\n+        public boolean equals(final Object o) {\n+            if (this == o) {\n+                return true;\n+            } else if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            final RankedClient other = (RankedClient) o;\n+            return clientId == other.clientId() && rank == other.rank();\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(rank, clientId);\n+        }\n+    }\n+\n+    static class Movement<ID> {\n+        final TaskId task;\n+        final ID source;\n+        final ID destination;\n+\n+        Movement(final TaskId task, final ID source, final ID destination) {\n+            this.task = task;\n+            this.source = source;\n+            this.destination = destination;\n+        }\n+\n+        @Override\n+        public boolean equals(final Object o) {\n+            if (this == o) {\n+                return true;\n+            } else if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            final Movement other = (Movement) o;\n+            return task == other.task && source == other.source && destination == other.destination;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxMzkzNA=="}, "originalCommit": null, "originalPosition": 475}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMwMzMzMQ==", "bodyText": "Actually I just copied over the implementation from some other class, and wasn't getting any warnings...I should probably do a thorough pass over all the warnings to see what else is missing, but this (and the raw types) is enabled now.\nThanks for catching", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401303331", "createdAt": "2020-04-01T01:18:25Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The\n+     * previous assignment will be chosen and returned iff all of the following are true:\n+     *   1) it satisfies the state constraint, ie all tasks with caught up clients are assigned to one of those clients\n+     *   2) it satisfies the balance factor\n+     *   3) there are no unassigned tasks (eg due to a client that dropped out of the group)\n+     */\n+    private boolean shouldUsePreviousAssignment() {\n+        if (previousAssignmentIsValid()) {\n+            final int previousAssignmentBalanceFactor =\n+                computeBalanceFactor(clientStates.values(), statefulTasks, ClientState::prevActiveTasks);\n+            return previousAssignmentBalanceFactor <= configs.balanceFactor;\n+        } else {\n+            return false;\n+        }\n+    }\n+\n+    private Map<ID, List<TaskId>> initializeEmptyTaskAssignmentMap() {\n+        return sortedClients.stream().collect(Collectors.toMap(id -> id, id -> new ArrayList<>()));\n+    }\n+\n+    private void assignActiveTasksToClients(final Map<ID, List<TaskId>> activeTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignActiveTasks(activeTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignStandbyTasksToClients(final Map<ID, List<TaskId>> standbyTasks) {\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID clientId = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            state.assignStandbyTasks(standbyTasks.get(clientId));\n+        }\n+    }\n+\n+    private void assignPreviousTasksToClientStates() {\n+        for (final ClientState clientState : clientStates.values()) {\n+            clientState.assignActiveTasks(clientState.prevActiveTasks());\n+            clientState.assignStandbyTasks(clientState.prevStandbyTasks());\n+        }\n+    }\n+\n+    private PriorityQueue<ID> getClientPriorityQueueByTaskLoad(final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        final PriorityQueue<ID> queue = new PriorityQueue<>(\n+            (client, other) -> {\n+                final int clientTasksPerThread = tasksPerThread(client, taskLoadsByClient);\n+                final int otherTasksPerThread = tasksPerThread(other, taskLoadsByClient);\n+                if (clientTasksPerThread != otherTasksPerThread) {\n+                    return clientTasksPerThread - otherTasksPerThread;\n+                } else {\n+                    return client.compareTo(other);\n+                }\n+            });\n+\n+        queue.addAll(sortedClients);\n+        return queue;\n+    }\n+\n+    private int tasksPerThread(final ID client, final List<Map<ID, List<TaskId>>> taskLoadsByClient) {\n+        double numTasks = 0;\n+        for (final Map<ID, List<TaskId>> assignment : taskLoadsByClient) {\n+            numTasks += assignment.get(client).size();\n+        }\n+        return (int) Math.ceil(numTasks / clientsToNumberOfThreads.get(client));\n+    }\n+\n+    static class RankedClient<ID extends Comparable<? super ID>> implements Comparable<RankedClient<ID>> {\n+        private final ID clientId;\n+        private final long rank;\n+\n+        RankedClient(final ID clientId, final long rank) {\n+            this.clientId = clientId;\n+            this.rank = rank;\n+        }\n+\n+        ID clientId() {\n+            return clientId;\n+        }\n+\n+        long rank() {\n+            return rank;\n+        }\n+\n+        @Override\n+        public int compareTo(final RankedClient<ID> clientIdAndLag) {\n+            if (rank < clientIdAndLag.rank) {\n+                return -1;\n+            } else if (rank > clientIdAndLag.rank) {\n+                return 1;\n+            } else {\n+                return clientId.compareTo(clientIdAndLag.clientId);\n+            }\n+        }\n+\n+        @Override\n+        public boolean equals(final Object o) {\n+            if (this == o) {\n+                return true;\n+            } else if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            final RankedClient other = (RankedClient) o;\n+            return clientId == other.clientId() && rank == other.rank();\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(rank, clientId);\n+        }\n+    }\n+\n+    static class Movement<ID> {\n+        final TaskId task;\n+        final ID source;\n+        final ID destination;\n+\n+        Movement(final TaskId task, final ID source, final ID destination) {\n+            this.task = task;\n+            this.source = source;\n+            this.destination = destination;\n+        }\n+\n+        @Override\n+        public boolean equals(final Object o) {\n+            if (this == o) {\n+                return true;\n+            } else if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            final Movement other = (Movement) o;\n+            return task == other.task && source == other.source && destination == other.destination;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxMzkzNA=="}, "originalCommit": null, "originalPosition": 475}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDM5MDE2OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMzoxNDo0OVrOF-Dyag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMTozOTowNVrOF_Tmrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxODA5MA==", "bodyText": "Should this class be parameterized to run all tests both with and without HA?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400618090", "createdAt": "2020-03-31T03:14:49Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -2011,7 +1883,7 @@ public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceIfEndOff\n         createMockTaskManager(allTasks, emptyTasks);\n         adminClient = EasyMock.createMock(AdminClient.class);\n         expect(adminClient.listOffsets(anyObject())).andThrow(new StreamsException(\"Should be handled\"));\n-        configureDefaultPartitionAssignor();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk2MDU0OA==", "bodyText": "See my comment above about making StreamPartitionAssignor independent of the assignor with a factory method.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400960548", "createdAt": "2020-03-31T14:30:48Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -2011,7 +1883,7 @@ public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceIfEndOff\n         createMockTaskManager(allTasks, emptyTasks);\n         adminClient = EasyMock.createMock(AdminClient.class);\n         expect(adminClient.listOffsets(anyObject())).andThrow(new StreamsException(\"Should be handled\"));\n-        configureDefaultPartitionAssignor();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxODA5MA=="}, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NzAxOQ==", "bodyText": "IIUC, it already is, since you can switch assignors via this config, but maybe I don't understand the implications of your suggestion. The intent of my question was: how many of these tests are specific to the sticky assignment strategy (and should be moved to a new test class specific to the sticky assignor), and how many are testing invariants about the StreamsPartitionAssignor, which should hold with both assignors.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401187019", "createdAt": "2020-03-31T20:15:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -2011,7 +1883,7 @@ public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceIfEndOff\n         createMockTaskManager(allTasks, emptyTasks);\n         adminClient = EasyMock.createMock(AdminClient.class);\n         expect(adminClient.listOffsets(anyObject())).andThrow(new StreamsException(\"Should be handled\"));\n-        configureDefaultPartitionAssignor();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxODA5MA=="}, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMxNDQ1Nw==", "bodyText": "If this answers your question, exactly 2 of the StreamsPartitionAssignor tests are incompatible with high availability being enabled/disabled. One of them is testing something specific to the \"enabled\" case, but the other is verifying the specific assignment unnecessarily, so we can just take that check out.\nIn sum, yes we can parameterize it. I did, and fixed the tests, but let me knows if you have any concerns about them", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401314457", "createdAt": "2020-04-01T02:03:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -2011,7 +1883,7 @@ public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceIfEndOff\n         createMockTaskManager(allTasks, emptyTasks);\n         adminClient = EasyMock.createMock(AdminClient.class);\n         expect(adminClient.listOffsets(anyObject())).andThrow(new StreamsException(\"Should be handled\"));\n-        configureDefaultPartitionAssignor();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxODA5MA=="}, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkyNTgwNg==", "bodyText": "I just looked at these changes before seeing this comment, and reached the same conclusion. Thanks for taking this on!", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401925806", "createdAt": "2020-04-01T21:39:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -2011,7 +1883,7 @@ public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceIfEndOff\n         createMockTaskManager(allTasks, emptyTasks);\n         adminClient = EasyMock.createMock(AdminClient.class);\n         expect(adminClient.listOffsets(anyObject())).andThrow(new StreamsException(\"Should be handled\"));\n-        configureDefaultPartitionAssignor();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYxODA5MA=="}, "originalCommit": null, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDQwNTU2OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMzoyMzo0N1rOF-D7fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMzoyMzo0N1rOF-D7fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYyMDQxMw==", "bodyText": "This can be a local variable (and should be, to avoid mistakes in the future)", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400620413", "createdAt": "2020-03-31T03:23:47Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDQxOTg1OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwMzozMjo0MFrOF-EEEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwMzo1ODo0NFrOF-wAgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYyMjYwOA==", "bodyText": "Ok, now, this variable name is just too far :)\nDo you mind removing that exclusion and just using ALL_CAPS style for constants?\n(and it seems like this should have been named CLIENT_1_ID or something?)", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400622608", "createdAt": "2020-03-31T03:32:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYyMzI5Mw==", "bodyText": "Oh, also, I think if you fix the == should be .equals comments above, you should be able to use UUIDs for the client ids, which would be good because that's what Streams is going to do, right?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400623293", "createdAt": "2020-03-31T03:35:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYyMjYwOA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk4ODM1Ng==", "bodyText": "I agree with @vvcephei.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400988356", "createdAt": "2020-03-31T15:05:46Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYyMjYwOA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM0MjU5Mg==", "bodyText": "Oh man, what happened here...I promise I'm not just playing a cruel joke on you  (looks like a case of Replace All biting me, again)\nIt's actually related to the UUID vs String question: initially I was using UUID but  realized that we need to be able to predict/enforce the order of the client ID for deterministic tests, or hitting specific edge cases (eg first client has very high capacity). This doesn't really work out well with UUID.randomUUID, so I just did a find and replace to switch from UUID to String, and here we are.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401342592", "createdAt": "2020-04-01T03:58:44Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYyMjYwOA=="}, "originalCommit": null, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDUzNzA0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwNDo0MzoyOFrOF-FHyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwMTozNTo1MlrOF-t4bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYzOTk0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void shouldAssignStandbyReplicasInAdditionToWarmupReplicas() {\n          \n          \n            \n                public void shouldReUseWarmupReplicasForStandbyReplicas() {\n          \n      \n    \n    \n  \n\nJust a name change suggestion. The behavior in the test is what I was expecting, but the method name led me to believe that we were basically doubling up on replicas (ie we'd get three copies of the replica: an active, a warmup, and a standby).", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400639944", "createdAt": "2020-03-31T04:43:28Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";\n+    private static final String String2 = \"client2\";\n+    private static final String String3 = \"client3\";\n+\n+    private ClientState client1;\n+    private ClientState client2;\n+    private ClientState client3;\n+\n+    private static final Set<TaskId> emptyTasks = emptySet();\n+\n+    private HighAvailabilityTaskAssignor<String> taskAssignor;\n+\n+    private void createTaskAssignor() {\n+        configs = new AssignmentConfigs(\n+            acceptableRecoveryLag,\n+            balanceFactor,\n+            maxWarmupReplicas,\n+            numStandbyReplicas,\n+            probingRebalanceInterval\n+        );\n+        taskAssignor = new HighAvailabilityTaskAssignor<>(\n+            clientStates,\n+            allTasks,\n+            statefulTasks,\n+            configs);\n+    }\n+\n+    @Test\n+    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(Task.LATEST_OFFSET);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, Task.LATEST_OFFSET),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankTaskWithUnknownOffsetSumBelowCaughtUpClientAndClientWithLargeLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(UNKNOWN_OFFSET_SUM);\n+        expect(client2.lagFor(task0_0)).andReturn(50L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String2, 0L),\n+            new RankedClient<>(String1, 1L),\n+            new RankedClient<>(String3, 500L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankAllClientsWithinAcceptableRecoveryLagWithRank0() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(100L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, 0L),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankNotCaughtUpClientsAccordingToLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(900L);\n+        expect(client2.lagFor(task0_0)).andReturn(800L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String3, 500L),\n+            new RankedClient<>(String2, 800L),\n+            new RankedClient<>(String1, 900L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldGetMovementsFromStateConstrainedToBalancedAssignment() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_2)),\n+            mkEntry(String2, asList(task0_1, task1_0)),\n+            mkEntry(String3, asList(task0_2, task1_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1)),\n+            mkEntry(String3, asList(task0_2, task1_2))\n+        );\n+        final Queue<Movement<String>> expectedMovements = new LinkedList<>();\n+        expectedMovements.add(new Movement<>(task1_2, String1, String3));\n+        expectedMovements.add(new Movement<>(task1_0, String2, String1));\n+        expectedMovements.add(new Movement<>(task1_1, String3, String2));\n+\n+        assertThat(getMovements(stateConstrainedAssignment, balancedAssignment), equalTo(expectedMovements));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalStateExceptionIfAssignmentsAreOfDifferentSize() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task0_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1))\n+        );\n+        assertThrows(IllegalStateException.class, () -> getMovements(stateConstrainedAssignment, balancedAssignment));\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0, task0_1);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedStandbyTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andStubReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        numStandbyReplicas = 1;\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfActiveTasksWasNotOnCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsValid() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskHasNoCaughtUpClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskIsCaughtUpOnClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        replay(client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnFalseIfTaskWasNotCaughtUpOnClientButCaughtUpClientsExist() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorAsDifferenceBetweenMostAndLeastLoadedClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        expect(client2.capacity()).andReturn(1);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        expect(client3.capacity()).andReturn(1);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(2));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorWithDifferentClientCapacities() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 4 tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(3));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorBasedOnStatefulTasksOnly() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+\n+        // 0_0 and 0_1 are stateless\n+        final Set<TaskId> statefulTasks = mkSet(task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 2 stateful tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 stateful task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 stateful task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorOfZeroWithOnlyOneClient() {\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        replay(client1);\n+        assertThat(computeBalanceFactor(singleton(client1), statefulTasks, ClientState::activeTasks), equalTo(0));\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbysForStatefulTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_1));\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0)));\n+        assertThat(client2.activeTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client1.standbyTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client2.standbyTasks(), equalTo(mkSet(task0_0)));\n+    }\n+\n+    @Test\n+    public void shouldNotAssignStandbysForStatelessTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = emptyTasks;\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTaskCount(), equalTo(1));\n+        assertThat(client2.activeTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1, client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignWarmupReplicasEvenIfNoStandbyReplicasConfigured() {\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+        \n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignMoreThanMaxWarmupReplicas() {\n+        maxWarmupReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbyReplicasInAdditionToWarmupReplicas() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 524}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMwNzc1OA==", "bodyText": "Does shouldNotAssignWarmupAndStandbyToTheSameClient make sense to you? I would interpret ReuseWarmupForStandby as meaning the warmup replicas will count as a standby (in terms of num.standby.replicas ), which is not the case.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401307758", "createdAt": "2020-04-01T01:35:52Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";\n+    private static final String String2 = \"client2\";\n+    private static final String String3 = \"client3\";\n+\n+    private ClientState client1;\n+    private ClientState client2;\n+    private ClientState client3;\n+\n+    private static final Set<TaskId> emptyTasks = emptySet();\n+\n+    private HighAvailabilityTaskAssignor<String> taskAssignor;\n+\n+    private void createTaskAssignor() {\n+        configs = new AssignmentConfigs(\n+            acceptableRecoveryLag,\n+            balanceFactor,\n+            maxWarmupReplicas,\n+            numStandbyReplicas,\n+            probingRebalanceInterval\n+        );\n+        taskAssignor = new HighAvailabilityTaskAssignor<>(\n+            clientStates,\n+            allTasks,\n+            statefulTasks,\n+            configs);\n+    }\n+\n+    @Test\n+    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(Task.LATEST_OFFSET);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, Task.LATEST_OFFSET),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankTaskWithUnknownOffsetSumBelowCaughtUpClientAndClientWithLargeLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(UNKNOWN_OFFSET_SUM);\n+        expect(client2.lagFor(task0_0)).andReturn(50L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String2, 0L),\n+            new RankedClient<>(String1, 1L),\n+            new RankedClient<>(String3, 500L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankAllClientsWithinAcceptableRecoveryLagWithRank0() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(100L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, 0L),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankNotCaughtUpClientsAccordingToLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(900L);\n+        expect(client2.lagFor(task0_0)).andReturn(800L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String3, 500L),\n+            new RankedClient<>(String2, 800L),\n+            new RankedClient<>(String1, 900L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldGetMovementsFromStateConstrainedToBalancedAssignment() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_2)),\n+            mkEntry(String2, asList(task0_1, task1_0)),\n+            mkEntry(String3, asList(task0_2, task1_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1)),\n+            mkEntry(String3, asList(task0_2, task1_2))\n+        );\n+        final Queue<Movement<String>> expectedMovements = new LinkedList<>();\n+        expectedMovements.add(new Movement<>(task1_2, String1, String3));\n+        expectedMovements.add(new Movement<>(task1_0, String2, String1));\n+        expectedMovements.add(new Movement<>(task1_1, String3, String2));\n+\n+        assertThat(getMovements(stateConstrainedAssignment, balancedAssignment), equalTo(expectedMovements));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalStateExceptionIfAssignmentsAreOfDifferentSize() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task0_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1))\n+        );\n+        assertThrows(IllegalStateException.class, () -> getMovements(stateConstrainedAssignment, balancedAssignment));\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0, task0_1);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedStandbyTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andStubReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        numStandbyReplicas = 1;\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfActiveTasksWasNotOnCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsValid() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskHasNoCaughtUpClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskIsCaughtUpOnClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        replay(client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnFalseIfTaskWasNotCaughtUpOnClientButCaughtUpClientsExist() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorAsDifferenceBetweenMostAndLeastLoadedClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        expect(client2.capacity()).andReturn(1);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        expect(client3.capacity()).andReturn(1);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(2));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorWithDifferentClientCapacities() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 4 tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(3));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorBasedOnStatefulTasksOnly() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+\n+        // 0_0 and 0_1 are stateless\n+        final Set<TaskId> statefulTasks = mkSet(task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 2 stateful tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 stateful task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 stateful task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorOfZeroWithOnlyOneClient() {\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        replay(client1);\n+        assertThat(computeBalanceFactor(singleton(client1), statefulTasks, ClientState::activeTasks), equalTo(0));\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbysForStatefulTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_1));\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0)));\n+        assertThat(client2.activeTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client1.standbyTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client2.standbyTasks(), equalTo(mkSet(task0_0)));\n+    }\n+\n+    @Test\n+    public void shouldNotAssignStandbysForStatelessTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = emptyTasks;\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTaskCount(), equalTo(1));\n+        assertThat(client2.activeTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1, client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignWarmupReplicasEvenIfNoStandbyReplicasConfigured() {\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+        \n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignMoreThanMaxWarmupReplicas() {\n+        maxWarmupReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbyReplicasInAdditionToWarmupReplicas() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDYzOTk0NA=="}, "originalCommit": null, "originalPosition": 524}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDUzODEyOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwNDo0Mzo1NVrOF-FIWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwNDo0Mzo1NVrOF-FIWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDY0MDA4OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void shouldReturnFalIfPreviousAssignmentIsReused() {\n          \n          \n            \n                public void shouldReturnFalseIfPreviousAssignmentIsReused() {", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400640088", "createdAt": "2020-03-31T04:43:55Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";\n+    private static final String String2 = \"client2\";\n+    private static final String String3 = \"client3\";\n+\n+    private ClientState client1;\n+    private ClientState client2;\n+    private ClientState client3;\n+\n+    private static final Set<TaskId> emptyTasks = emptySet();\n+\n+    private HighAvailabilityTaskAssignor<String> taskAssignor;\n+\n+    private void createTaskAssignor() {\n+        configs = new AssignmentConfigs(\n+            acceptableRecoveryLag,\n+            balanceFactor,\n+            maxWarmupReplicas,\n+            numStandbyReplicas,\n+            probingRebalanceInterval\n+        );\n+        taskAssignor = new HighAvailabilityTaskAssignor<>(\n+            clientStates,\n+            allTasks,\n+            statefulTasks,\n+            configs);\n+    }\n+\n+    @Test\n+    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(Task.LATEST_OFFSET);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, Task.LATEST_OFFSET),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankTaskWithUnknownOffsetSumBelowCaughtUpClientAndClientWithLargeLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(UNKNOWN_OFFSET_SUM);\n+        expect(client2.lagFor(task0_0)).andReturn(50L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String2, 0L),\n+            new RankedClient<>(String1, 1L),\n+            new RankedClient<>(String3, 500L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankAllClientsWithinAcceptableRecoveryLagWithRank0() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(100L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, 0L),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankNotCaughtUpClientsAccordingToLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(900L);\n+        expect(client2.lagFor(task0_0)).andReturn(800L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String3, 500L),\n+            new RankedClient<>(String2, 800L),\n+            new RankedClient<>(String1, 900L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldGetMovementsFromStateConstrainedToBalancedAssignment() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_2)),\n+            mkEntry(String2, asList(task0_1, task1_0)),\n+            mkEntry(String3, asList(task0_2, task1_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1)),\n+            mkEntry(String3, asList(task0_2, task1_2))\n+        );\n+        final Queue<Movement<String>> expectedMovements = new LinkedList<>();\n+        expectedMovements.add(new Movement<>(task1_2, String1, String3));\n+        expectedMovements.add(new Movement<>(task1_0, String2, String1));\n+        expectedMovements.add(new Movement<>(task1_1, String3, String2));\n+\n+        assertThat(getMovements(stateConstrainedAssignment, balancedAssignment), equalTo(expectedMovements));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalStateExceptionIfAssignmentsAreOfDifferentSize() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task0_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1))\n+        );\n+        assertThrows(IllegalStateException.class, () -> getMovements(stateConstrainedAssignment, balancedAssignment));\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0, task0_1);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedStandbyTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andStubReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        numStandbyReplicas = 1;\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfActiveTasksWasNotOnCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsValid() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskHasNoCaughtUpClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskIsCaughtUpOnClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        replay(client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnFalseIfTaskWasNotCaughtUpOnClientButCaughtUpClientsExist() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorAsDifferenceBetweenMostAndLeastLoadedClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        expect(client2.capacity()).andReturn(1);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        expect(client3.capacity()).andReturn(1);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(2));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorWithDifferentClientCapacities() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 4 tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(3));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorBasedOnStatefulTasksOnly() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+\n+        // 0_0 and 0_1 are stateless\n+        final Set<TaskId> statefulTasks = mkSet(task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 2 stateful tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 stateful task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 stateful task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorOfZeroWithOnlyOneClient() {\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        replay(client1);\n+        assertThat(computeBalanceFactor(singleton(client1), statefulTasks, ClientState::activeTasks), equalTo(0));\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbysForStatefulTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_1));\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0)));\n+        assertThat(client2.activeTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client1.standbyTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client2.standbyTasks(), equalTo(mkSet(task0_0)));\n+    }\n+\n+    @Test\n+    public void shouldNotAssignStandbysForStatelessTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = emptyTasks;\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTaskCount(), equalTo(1));\n+        assertThat(client2.activeTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1, client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignWarmupReplicasEvenIfNoStandbyReplicasConfigured() {\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+        \n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignMoreThanMaxWarmupReplicas() {\n+        maxWarmupReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbyReplicasInAdditionToWarmupReplicas() {\n+        numStandbyReplicas = 1;\n+        maxWarmupReplicas = 1;\n+\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.standbyTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignAnyStandbysWithInsufficientCapacity() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1));\n+\n+        clientStates = getClientStatesWithOneClient();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertHasNoStandbyTasks(client1);\n+    }\n+\n+    @Test\n+    public void shouldAssignActiveTasksToNotCaughtUpClientIfNoneExist() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithOneClient();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertHasNoStandbyTasks(client1);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignMoreThanMaxWarmupReplicasWithStandbys() {\n+        numStandbyReplicas = 1;\n+\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+        client3 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithThreeClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTaskCount(), equalTo(4));\n+        assertThat(client2.standbyTaskCount(), equalTo(3));\n+        assertThat(client3.standbyTaskCount(), equalTo(3));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2, client3);\n+    }\n+\n+    @Test\n+    public void shouldDistributeStatelessTasksToBalanceTotalActiveTaskLoad() {\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task1_2);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.activeTasks(), equalTo(mkSet(task1_0, task1_1, task1_2)));\n+    }\n+\n+    @Test\n+    public void shouldDistributeStatefulActiveTasksToAllClients() {\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task1_2, task1_3, task2_0); // 9 total\n+        statefulTasks = new HashSet<>(allTasks);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(allTasks).withCapacity(100);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(allTasks).withCapacity(50);\n+        client3 = getMockClientWithPreviousCaughtUpTasks(allTasks).withCapacity(1);\n+\n+        clientStates = getClientStatesWithThreeClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertFalse(client1.activeTasks().isEmpty());\n+        assertFalse(client2.activeTasks().isEmpty());\n+        assertFalse(client3.activeTasks().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldReturnFalIfPreviousAssignmentIsReused() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 628}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NDUzOTQxOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwNDo0NDo0OVrOF-FJGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwNDo0NDo0OVrOF-FJGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDY0MDI4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                MockClientState getMockClientWithPreviousCaughtUpTasks(final Set<TaskId> statefulActiveTasks) {\n          \n          \n            \n                private MockClientState getMockClientWithPreviousCaughtUpTasks(final Set<TaskId> statefulActiveTasks) {", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400640282", "createdAt": "2020-03-31T04:44:49Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";\n+    private static final String String2 = \"client2\";\n+    private static final String String3 = \"client3\";\n+\n+    private ClientState client1;\n+    private ClientState client2;\n+    private ClientState client3;\n+\n+    private static final Set<TaskId> emptyTasks = emptySet();\n+\n+    private HighAvailabilityTaskAssignor<String> taskAssignor;\n+\n+    private void createTaskAssignor() {\n+        configs = new AssignmentConfigs(\n+            acceptableRecoveryLag,\n+            balanceFactor,\n+            maxWarmupReplicas,\n+            numStandbyReplicas,\n+            probingRebalanceInterval\n+        );\n+        taskAssignor = new HighAvailabilityTaskAssignor<>(\n+            clientStates,\n+            allTasks,\n+            statefulTasks,\n+            configs);\n+    }\n+\n+    @Test\n+    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(Task.LATEST_OFFSET);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, Task.LATEST_OFFSET),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankTaskWithUnknownOffsetSumBelowCaughtUpClientAndClientWithLargeLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(UNKNOWN_OFFSET_SUM);\n+        expect(client2.lagFor(task0_0)).andReturn(50L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String2, 0L),\n+            new RankedClient<>(String1, 1L),\n+            new RankedClient<>(String3, 500L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankAllClientsWithinAcceptableRecoveryLagWithRank0() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(100L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, 0L),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankNotCaughtUpClientsAccordingToLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(900L);\n+        expect(client2.lagFor(task0_0)).andReturn(800L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String3, 500L),\n+            new RankedClient<>(String2, 800L),\n+            new RankedClient<>(String1, 900L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldGetMovementsFromStateConstrainedToBalancedAssignment() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_2)),\n+            mkEntry(String2, asList(task0_1, task1_0)),\n+            mkEntry(String3, asList(task0_2, task1_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1)),\n+            mkEntry(String3, asList(task0_2, task1_2))\n+        );\n+        final Queue<Movement<String>> expectedMovements = new LinkedList<>();\n+        expectedMovements.add(new Movement<>(task1_2, String1, String3));\n+        expectedMovements.add(new Movement<>(task1_0, String2, String1));\n+        expectedMovements.add(new Movement<>(task1_1, String3, String2));\n+\n+        assertThat(getMovements(stateConstrainedAssignment, balancedAssignment), equalTo(expectedMovements));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalStateExceptionIfAssignmentsAreOfDifferentSize() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task0_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1))\n+        );\n+        assertThrows(IllegalStateException.class, () -> getMovements(stateConstrainedAssignment, balancedAssignment));\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0, task0_1);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedStandbyTasks() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevActiveTasks()).andStubReturn(singleton(task0_0));\n+        expect(client1.prevStandbyTasks()).andReturn(emptyTasks);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        numStandbyReplicas = 1;\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsInvalidIfActiveTasksWasNotOnCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldDecidePreviousAssignmentIsValid() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.prevStandbyTasks()).andStubReturn(emptyTasks);\n+        expect(client2.prevStandbyTasks()).andStubReturn(emptyTasks);\n+\n+        expect(client1.prevActiveTasks()).andReturn(singleton(task0_0));\n+        expect(client2.prevActiveTasks()).andReturn(singleton(task0_1));\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        allTasks =  mkSet(task0_0, task0_1);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.previousAssignmentIsValid());\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskHasNoCaughtUpClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        replay(client1);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfTaskIsCaughtUpOnClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(0L);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = singletonMap(String1, client1);\n+        replay(client1);\n+        createTaskAssignor();\n+\n+        assertTrue(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldReturnFalseIfTaskWasNotCaughtUpOnClientButCaughtUpClientsExist() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(500L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+        allTasks =  mkSet(task0_0);\n+        statefulTasks =  mkSet(task0_0);\n+        clientStates = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+        createTaskAssignor();\n+\n+        assertFalse(taskAssignor.taskIsCaughtUpOnClient(task0_0, String1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorAsDifferenceBetweenMostAndLeastLoadedClients() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        expect(client2.capacity()).andReturn(1);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        expect(client3.capacity()).andReturn(1);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(2));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorWithDifferentClientCapacities() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 4 tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(3));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorBasedOnStatefulTasksOnly() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        final Set<ClientState> states = mkSet(client1, client2, client3);\n+\n+        // 0_0 and 0_1 are stateless\n+        final Set<TaskId> statefulTasks = mkSet(task0_2, task0_3, task1_0, task1_1, task2_0, task2_1, task2_3);\n+\n+        // client 1: 2 stateful tasks per thread\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+\n+        // client 2: 1 stateful task per thread\n+        expect(client2.capacity()).andReturn(2);\n+        expect(client2.activeTasks()).andReturn(mkSet(task1_0, task1_1));\n+\n+        // client 3: 1 stateful task per thread\n+        expect(client3.capacity()).andReturn(3);\n+        expect(client3.activeTasks()).andReturn(mkSet(task2_0, task2_1, task2_3));\n+\n+        replay(client1, client2, client3);\n+        assertThat(computeBalanceFactor(states, statefulTasks, ClientState::activeTasks), equalTo(1));\n+    }\n+\n+    @Test\n+    public void shouldComputeBalanceFactorOfZeroWithOnlyOneClient() {\n+        final Set<TaskId> statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.capacity()).andReturn(1);\n+        expect(client1.activeTasks()).andReturn(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        replay(client1);\n+        assertThat(computeBalanceFactor(singleton(client1), statefulTasks, ClientState::activeTasks), equalTo(0));\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbysForStatefulTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_1));\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0)));\n+        assertThat(client2.activeTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client1.standbyTasks(), equalTo(mkSet(task0_1)));\n+        assertThat(client2.standbyTasks(), equalTo(mkSet(task0_0)));\n+    }\n+\n+    @Test\n+    public void shouldNotAssignStandbysForStatelessTasks() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = emptyTasks;\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTaskCount(), equalTo(1));\n+        assertThat(client2.activeTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1, client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignWarmupReplicasEvenIfNoStandbyReplicasConfigured() {\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+        \n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignMoreThanMaxWarmupReplicas() {\n+        maxWarmupReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldAssignStandbyReplicasInAdditionToWarmupReplicas() {\n+        numStandbyReplicas = 1;\n+        maxWarmupReplicas = 1;\n+\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.standbyTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignAnyStandbysWithInsufficientCapacity() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1));\n+\n+        clientStates = getClientStatesWithOneClient();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertHasNoStandbyTasks(client1);\n+    }\n+\n+    @Test\n+    public void shouldAssignActiveTasksToNotCaughtUpClientIfNoneExist() {\n+        numStandbyReplicas = 1;\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithOneClient();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1)));\n+        assertHasNoStandbyTasks(client1);\n+    }\n+\n+    @Test\n+    public void shouldNotAssignMoreThanMaxWarmupReplicasWithStandbys() {\n+        numStandbyReplicas = 1;\n+\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+        client3 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithThreeClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTaskCount(), equalTo(4));\n+        assertThat(client2.standbyTaskCount(), equalTo(3));\n+        assertThat(client3.standbyTaskCount(), equalTo(3));\n+        assertHasNoStandbyTasks(client1);\n+        assertHasNoActiveTasks(client2, client3);\n+    }\n+\n+    @Test\n+    public void shouldDistributeStatelessTasksToBalanceTotalActiveTaskLoad() {\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task1_2);\n+        statefulTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+\n+        client1 = getMockClientWithPreviousCaughtUpTasks(mkSet(task0_0, task0_1, task0_2, task0_3));\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertThat(client1.activeTasks(), equalTo(mkSet(task0_0, task0_1, task0_2, task0_3)));\n+        assertThat(client2.activeTasks(), equalTo(mkSet(task1_0, task1_1, task1_2)));\n+    }\n+\n+    @Test\n+    public void shouldDistributeStatefulActiveTasksToAllClients() {\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3, task1_0, task1_1, task1_2, task1_3, task2_0); // 9 total\n+        statefulTasks = new HashSet<>(allTasks);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(allTasks).withCapacity(100);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(allTasks).withCapacity(50);\n+        client3 = getMockClientWithPreviousCaughtUpTasks(allTasks).withCapacity(1);\n+\n+        clientStates = getClientStatesWithThreeClients();\n+        createTaskAssignor();\n+        taskAssignor.assign();\n+\n+        assertFalse(client1.activeTasks().isEmpty());\n+        assertFalse(client2.activeTasks().isEmpty());\n+        assertFalse(client3.activeTasks().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldReturnFalIfPreviousAssignmentIsReused() {\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = new HashSet<>(allTasks);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(allTasks);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(allTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        assertFalse(taskAssignor.assign());\n+\n+        assertThat(client1.activeTasks(), equalTo(client1.prevActiveTasks()));\n+        assertThat(client2.activeTasks(), equalTo(client2.prevActiveTasks()));\n+    }\n+\n+    @Test\n+    public void shouldReturnFalseIfNoWarmupTasksAreAssigned() {\n+        allTasks = mkSet(task0_0, task0_1, task0_2, task0_3);\n+        statefulTasks = emptyTasks;\n+        client1 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        assertFalse(taskAssignor.assign());\n+        assertHasNoStandbyTasks(client1, client2);\n+    }\n+\n+    @Test\n+    public void shouldReturnTrueIfWarmupTasksAreAssigned() {\n+        allTasks = mkSet(task0_0, task0_1);\n+        statefulTasks = mkSet(task0_0, task0_1);\n+        client1 = getMockClientWithPreviousCaughtUpTasks(allTasks);\n+        client2 = getMockClientWithPreviousCaughtUpTasks(emptyTasks);\n+\n+        clientStates = getClientStatesWithTwoClients();\n+        createTaskAssignor();\n+        assertTrue(taskAssignor.assign());\n+        assertThat(client2.standbyTaskCount(), equalTo(1));\n+    }\n+\n+    private Map<String, ClientState> getClientStatesWithOneClient() {\n+        return singletonMap(String1, client1);\n+    }\n+\n+    private Map<String, ClientState> getClientStatesWithTwoClients() {\n+        return mkMap(mkEntry(String1, client1), mkEntry(String2, client2));\n+    }\n+\n+    private Map<String, ClientState> getClientStatesWithThreeClients() {\n+        return mkMap(mkEntry(String1, client1), mkEntry(String2, client2), mkEntry(String3, client3));\n+    }\n+\n+    private static void assertHasNoActiveTasks(final ClientState... clients) {\n+        for (final ClientState client : clients) {\n+            assertTrue(client.activeTasks().isEmpty());\n+        }\n+    }\n+\n+    private static void assertHasNoStandbyTasks(final ClientState... clients) {\n+        for (final ClientState client : clients) {\n+            assertTrue(client.standbyTasks().isEmpty());\n+        }\n+    }\n+\n+    MockClientState getMockClientWithPreviousCaughtUpTasks(final Set<TaskId> statefulActiveTasks) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 692}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NTg2OTE5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMTo1MDo0MlrOF-R8wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOToyNjoxMFrOF-k08w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg1MDExNA==", "bodyText": "req: Could you please specify a constructor that takes a flag to set whether the previous task assignment should be preserved? That would save us this ugly cast.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400850114", "createdAt": "2020-03-31T11:50:42Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE1OTQxMQ==", "bodyText": "Ack", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401159411", "createdAt": "2020-03-31T19:26:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg1MDExNA=="}, "originalCommit": null, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NTkxMjg1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjowMzoxMlrOF-SX1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOToyNDo1MFrOF-kyIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg1NzA0Ng==", "bodyText": "Q: Does our code styleguide not say we should in general write such calls that do not fit in one line as\ntaskAssignor = new HighAvailabilityTaskAssignor<>(\n    clientStates, \n    allTasks, \n    statefulTasks,\n    assignmentConfigs\n);\n\nIn this specific case, I would write all parameters in one line and make the line those 4 symbols longer than allowed.\nMaybe I am bit too picky about code style, but it makes code better readable if we use the same style for same situations. At the same time, I also think that sometimes it makes sense to break those rules for readability, but I cannot see how breaking the rule makes it more readable here. WDYT?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400857046", "createdAt": "2020-03-31T12:03:12Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAzOTM5NQ==", "bodyText": "Yes, you're correct, the code style says either to put all arguments on one line, or only one per line.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401039395", "createdAt": "2020-03-31T16:13:13Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg1NzA0Ng=="}, "originalCommit": null, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE1ODY5MQ==", "bodyText": "This was not intentional, sorry I just failed to catch it. IDEA has started doing this automatically recently...I don't want to blame the upgrade to Catalina, but...", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401158691", "createdAt": "2020-03-31T19:24:50Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg1NzA0Ng=="}, "originalCommit": null, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NTk5OTM3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjoyNDo1N1rOF-TLeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOTo0Mjo0NlrOF-lZFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg3MDI2NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n          \n          \n            \n                                             final Set<TaskId> allTasks,\n          \n          \n            \n                                             final Set<TaskId> statefulTasks,\n          \n          \n            \n                                             final AssignmentConfigs configs) {\n          \n          \n            \n                public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n          \n          \n            \n                                                    final Set<TaskId> allTasks,\n          \n          \n            \n                                                    final Set<TaskId> statefulTasks,\n          \n          \n            \n                                                    final AssignmentConfigs configs) {", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400870265", "createdAt": "2020-03-31T12:24:57Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2ODY2Mw==", "bodyText": "Ack, thanks", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401168663", "createdAt": "2020-03-31T19:42:46Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg3MDI2NQ=="}, "originalCommit": null, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NjA4NDU5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjo0NTo1NFrOF-T_HA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjo0NTo1NFrOF-T_HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg4MzQ4NA==", "bodyText": "prop: Could we pass into getMovements() the number of warm-up replicas and only compute as many movements as needed instead of computing all movements and then using just the first couple of movements.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400883484", "createdAt": "2020-03-31T12:45:54Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NjE5MDE2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMzoxMDozMlrOF-VAEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMzoxMDozMlrOF-VAEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkwMDExMw==", "bodyText": "prop: Could you rename the queue class and the variable to something like LeastLoadedClientsForStandbyTasks or ClientsForStandbyTaskSortedByLoad?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400900113", "createdAt": "2020-03-31T13:10:32Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NjIzNTkxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMzoyMDozOFrOF-VcOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMDoyMjoyMFrOF_RMIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkwNzMyMg==", "bodyText": "IMO, those comments are more confusing than helping. I would prefer to have methods with meaningful names to structure the code. For example,\n\nassignStatefulTasksAndWarmUpReplica() or computeAssignmentFor...\nassignStandByTasks() or computeAssignmentFor...\nassignStatelessTasks() or computeAssignmentFor...", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400907322", "createdAt": "2020-03-31T13:20:38Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg4NjI0Mw==", "bodyText": "Do you mind if this is saved for a followup PR? Specifically, when we move some of the static methods/classes to a separate file, and it's easier to get oriented in the HighAvailabilityTaskAssignor file", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401886243", "createdAt": "2020-04-01T20:22:20Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkwNzMyMg=="}, "originalCommit": null, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NjM4MjA1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMzo1MzoxM1rOF-W5jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOTo1MTozN1rOF-lsJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkzMTIxNQ==", "bodyText": "prop: Wouldn't this condition be equal to !movements.isEmpty()?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400931215", "createdAt": "2020-03-31T13:53:13Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE3MzU0MQ==", "bodyText": "Good point, yes it would", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401173541", "createdAt": "2020-03-31T19:51:37Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkzMTIxNQ=="}, "originalCommit": null, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NjQ3NDgxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNDoxMjo0NFrOF-X0UQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMTozNDo1NFrOF_TfKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NjI1Nw==", "bodyText": "Q: I might have missed the discussion. Why does an unknown offset result in 1 and not in Long.MAX_VALUE? Sorry if you have already answered this question elsewhere.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400946257", "createdAt": "2020-03-31T14:12:44Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NDU4MQ==", "bodyText": "It seems like it's worth including the explanation in the javadoc at the top of this method.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401184581", "createdAt": "2020-03-31T20:11:27Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NjI1Nw=="}, "originalCommit": null, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM0ODY4MQ==", "bodyText": "Added to the javadocs, but please lmk if the description is still too short to be clear", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401348681", "createdAt": "2020-04-01T04:25:16Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NjI1Nw=="}, "originalCommit": null, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkyMzg4Mg==", "bodyText": "It looks good to me.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401923882", "createdAt": "2020-04-01T21:34:54Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NjI1Nw=="}, "originalCommit": null, "originalPosition": 304}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NjU1NzYyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNDoyOTo0MlrOF-Yo1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMDowMzozMlrOF-mF-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk1OTcwMQ==", "bodyText": "prop: Shall we put this code into a factory method that takes the assignor configuration and returns the correct assignor. Due to the better encapsulation, we would be able to test this class independently from the assignor.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400959701", "createdAt": "2020-03-31T14:29:42Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n         }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4MDE1NQ==", "bodyText": "I have a proposal for refactoring the way we switch between assignors, but I'd like to table that discussion for now, and focus on getting this functionally complete so we can start to exercise it.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401180155", "createdAt": "2020-03-31T20:03:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,25 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(clientStates, allTasks, statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n+                ((StickyTaskAssignor) taskAssignor).preservePreviousTaskAssignment();\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs);\n         }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk1OTcwMQ=="}, "originalCommit": null, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4Njc1Nzc4OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNToxMDozMVrOF-am8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNToxMDozMVrOF-am8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk5MTk4Ng==", "bodyText": "req: Could you add a test for buildClientRankingsByTask() with an empty set of statefulTasks?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400991986", "createdAt": "2020-03-31T15:10:31Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";\n+    private static final String String2 = \"client2\";\n+    private static final String String3 = \"client3\";\n+\n+    private ClientState client1;\n+    private ClientState client2;\n+    private ClientState client3;\n+\n+    private static final Set<TaskId> emptyTasks = emptySet();\n+\n+    private HighAvailabilityTaskAssignor<String> taskAssignor;\n+\n+    private void createTaskAssignor() {\n+        configs = new AssignmentConfigs(\n+            acceptableRecoveryLag,\n+            balanceFactor,\n+            maxWarmupReplicas,\n+            numStandbyReplicas,\n+            probingRebalanceInterval\n+        );\n+        taskAssignor = new HighAvailabilityTaskAssignor<>(\n+            clientStates,\n+            allTasks,\n+            statefulTasks,\n+            configs);\n+    }\n+\n+    @Test\n+    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(Task.LATEST_OFFSET);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, Task.LATEST_OFFSET),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankTaskWithUnknownOffsetSumBelowCaughtUpClientAndClientWithLargeLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(UNKNOWN_OFFSET_SUM);\n+        expect(client2.lagFor(task0_0)).andReturn(50L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String2, 0L),\n+            new RankedClient<>(String1, 1L),\n+            new RankedClient<>(String3, 500L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankAllClientsWithinAcceptableRecoveryLagWithRank0() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(100L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, 0L),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankNotCaughtUpClientsAccordingToLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(900L);\n+        expect(client2.lagFor(task0_0)).andReturn(800L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String3, 500L),\n+            new RankedClient<>(String2, 800L),\n+            new RankedClient<>(String1, 900L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4Njc5NjMwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNToxODoyMlrOF-a-7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNToxODoyMlrOF-a-7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk5ODEyNQ==", "bodyText": "req: Could you please add a test that passes two empty assignments to getMovements().", "url": "https://github.com/apache/kafka/pull/8337#discussion_r400998125", "createdAt": "2020-03-31T15:18:22Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,732 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singleton;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.computeBalanceFactor;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.getMovements;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.replay;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.Movement;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.RankedClient;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+public class HighAvailabilityTaskAssignorTest {\n+    private long acceptableRecoveryLag = 100L;\n+    private int balanceFactor = 1;\n+    private int maxWarmupReplicas = 2;\n+    private int numStandbyReplicas = 0;\n+    private long probingRebalanceInterval = 60 * 1000L;\n+\n+    private AssignmentConfigs configs;\n+    private Map<String, ClientState> clientStates = new HashMap<>();\n+    private Set<TaskId> allTasks = new HashSet<>();\n+    private Set<TaskId> statefulTasks = new HashSet<>();\n+\n+    private static final TaskId task0_0 = new TaskId(0, 0);\n+    private static final TaskId task0_1 = new TaskId(0, 1);\n+    private static final TaskId task0_2 = new TaskId(0, 2);\n+    private static final TaskId task0_3 = new TaskId(0, 3);\n+    private static final TaskId task1_0 = new TaskId(1, 0);\n+    private static final TaskId task1_1 = new TaskId(1, 1);\n+    private static final TaskId task1_2 = new TaskId(1, 2);\n+    private static final TaskId task1_3 = new TaskId(1, 3);\n+    private static final TaskId task2_0 = new TaskId(2, 0);\n+    private static final TaskId task2_1 = new TaskId(2, 1);\n+    private static final TaskId task2_3 = new TaskId(2, 3);\n+\n+    private static final String String1 = \"client1\";\n+    private static final String String2 = \"client2\";\n+    private static final String String3 = \"client3\";\n+\n+    private ClientState client1;\n+    private ClientState client2;\n+    private ClientState client3;\n+\n+    private static final Set<TaskId> emptyTasks = emptySet();\n+\n+    private HighAvailabilityTaskAssignor<String> taskAssignor;\n+\n+    private void createTaskAssignor() {\n+        configs = new AssignmentConfigs(\n+            acceptableRecoveryLag,\n+            balanceFactor,\n+            maxWarmupReplicas,\n+            numStandbyReplicas,\n+            probingRebalanceInterval\n+        );\n+        taskAssignor = new HighAvailabilityTaskAssignor<>(\n+            clientStates,\n+            allTasks,\n+            statefulTasks,\n+            configs);\n+    }\n+\n+    @Test\n+    public void shouldRankPreviousClientAboveEquallyCaughtUpClient() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(Task.LATEST_OFFSET);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, Task.LATEST_OFFSET),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankTaskWithUnknownOffsetSumBelowCaughtUpClientAndClientWithLargeLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(UNKNOWN_OFFSET_SUM);\n+        expect(client2.lagFor(task0_0)).andReturn(50L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String2, 0L),\n+            new RankedClient<>(String1, 1L),\n+            new RankedClient<>(String3, 500L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        final SortedSet<RankedClient<String>> clientRanking = statefulTasksToRankedCandidates.get(task0_0);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(clientRanking, equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankAllClientsWithinAcceptableRecoveryLagWithRank0() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(100L);\n+        expect(client2.lagFor(task0_0)).andReturn(0L);\n+        replay(client1, client2);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String1, 0L),\n+            new RankedClient<>(String2, 0L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldRankNotCaughtUpClientsAccordingToLag() {\n+        client1 = EasyMock.createNiceMock(ClientState.class);\n+        client2 = EasyMock.createNiceMock(ClientState.class);\n+        client3 = EasyMock.createNiceMock(ClientState.class);\n+        expect(client1.lagFor(task0_0)).andReturn(900L);\n+        expect(client2.lagFor(task0_0)).andReturn(800L);\n+        expect(client3.lagFor(task0_0)).andReturn(500L);\n+        replay(client1, client2, client3);\n+\n+        final SortedSet<RankedClient<String>> expectedClientRanking = mkSortedSet(\n+            new RankedClient<>(String3, 500L),\n+            new RankedClient<>(String2, 800L),\n+            new RankedClient<>(String1, 900L)\n+        );\n+\n+        final Map<String, ClientState> states = mkMap(\n+            mkEntry(String1, client1),\n+            mkEntry(String2, client2),\n+            mkEntry(String3, client3)\n+        );\n+\n+        final Map<TaskId, SortedSet<RankedClient<String>>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(singleton(task0_0), states, acceptableRecoveryLag);\n+\n+        EasyMock.verify(client1, client2, client3);\n+        assertThat(statefulTasksToRankedCandidates.get(task0_0), equalTo(expectedClientRanking));\n+    }\n+\n+    @Test\n+    public void shouldGetMovementsFromStateConstrainedToBalancedAssignment() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_2)),\n+            mkEntry(String2, asList(task0_1, task1_0)),\n+            mkEntry(String3, asList(task0_2, task1_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1)),\n+            mkEntry(String3, asList(task0_2, task1_2))\n+        );\n+        final Queue<Movement<String>> expectedMovements = new LinkedList<>();\n+        expectedMovements.add(new Movement<>(task1_2, String1, String3));\n+        expectedMovements.add(new Movement<>(task1_0, String2, String1));\n+        expectedMovements.add(new Movement<>(task1_1, String3, String2));\n+\n+        assertThat(getMovements(stateConstrainedAssignment, balancedAssignment), equalTo(expectedMovements));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalStateExceptionIfAssignmentsAreOfDifferentSize() {\n+        final Map<String, List<TaskId>> stateConstrainedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task0_1))\n+        );\n+        final Map<String, List<TaskId>> balancedAssignment = mkMap(\n+            mkEntry(String1, asList(task0_0, task1_0)),\n+            mkEntry(String2, asList(task0_1, task1_1))\n+        );\n+        assertThrows(IllegalStateException.class, () -> getMovements(stateConstrainedAssignment, balancedAssignment));\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 249}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NjkyNzg5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNTo0NToyMlrOF-cRcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMDoxNzowOVrOF_RA4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAxOTI0OA==", "bodyText": "Q: Why do we only consider stateful tasks in computeBalanceFactor()?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401019248", "createdAt": "2020-03-31T15:45:22Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The\n+     * previous assignment will be chosen and returned iff all of the following are true:\n+     *   1) it satisfies the state constraint, ie all tasks with caught up clients are assigned to one of those clients\n+     *   2) it satisfies the balance factor\n+     *   3) there are no unassigned tasks (eg due to a client that dropped out of the group)\n+     */\n+    private boolean shouldUsePreviousAssignment() {\n+        if (previousAssignmentIsValid()) {\n+            final int previousAssignmentBalanceFactor =\n+                computeBalanceFactor(clientStates.values(), statefulTasks, ClientState::prevActiveTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 353}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg4MzM2Mw==", "bodyText": "It\u2019s a tighter and more useful measure. Stateless tasks are light, and we have no restrictions on their assignment. Therefore, if we have only stateless tasks, we should always produce a balanced assignment. If we have a mix, the stateful task balance factor is more meaningful: a client with 5 stateful tasks and no stateless tasks is obviously way more loaded than a client with no stateful tasks and 5 stateless tasks, but their overall task load is the same. And of course, if the stateful tasks are within the balance factor, then the total task count is necessarily balanced as well\nThe balanced and state constrained assignor only assign stateful tasks, thus the balance.factor inherently applies only to stateful tasks within that scope", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401883363", "createdAt": "2020-04-01T20:17:09Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {\n+        final Set<TaskId> unassignedActiveTasks = new HashSet<>(allTasks);\n+        final Map<TaskId, Integer> unassignedStandbyTasks =\n+            configs.numStandbyReplicas == 0 ?\n+                Collections.emptyMap() :\n+                new HashMap<>(statefulTasksToRankedCandidates.keySet().stream()\n+                                  .collect(Collectors.toMap(task -> task, task -> configs.numStandbyReplicas)));\n+\n+        for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+            final ID client = clientEntry.getKey();\n+            final ClientState state = clientEntry.getValue();\n+            final Set<TaskId> prevActiveTasks = state.prevActiveTasks();\n+\n+            // Verify that this client was caught-up on all stateful active tasks\n+            for (final TaskId activeTask : prevActiveTasks) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                    return false;\n+                }\n+            }\n+            unassignedActiveTasks.removeAll(prevActiveTasks);\n+\n+            if (!unassignedStandbyTasks.isEmpty()) {\n+                for (final TaskId task : state.prevStandbyTasks()) {\n+                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                    if (remainingStandbys != null) {\n+                        if (remainingStandbys == 1) {\n+                            unassignedStandbyTasks.remove(task);\n+                        } else {\n+                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n+    }\n+\n+    /**\n+     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n+     */\n+    boolean taskIsCaughtUpOnClient(final TaskId task, final ID client) {\n+        boolean hasNoCaughtUpClients = true;\n+        final SortedSet<RankedClient<ID>> rankedClients = statefulTasksToRankedCandidates.get(task);\n+        if (rankedClients == null) {\n+            return true;\n+        }\n+        for (final RankedClient rankedClient : rankedClients) {\n+            if (rankedClient.rank() <= 0L) {\n+                if (rankedClient.clientId().equals(client)) {\n+                    return true;\n+                } else {\n+                    hasNoCaughtUpClients = false;\n+                }\n+            }\n+\n+            // If we haven't found our client yet, it must not be caught-up\n+            if (rankedClient.rank() > 0L) {\n+                break;\n+            }\n+        }\n+        return hasNoCaughtUpClients;\n+    }\n+\n+    /**\n+     * Rankings are computed as follows, with lower being more caught up:\n+     *      Rank -1: active running task\n+     *      Rank 0: standby or restoring task whose overall lag is within the acceptableRecoveryLag bounds\n+     *      Rank 1: tasks whose lag is unknown, eg because it was not encoded in an older version subscription\n+     *      Rank 1+: all other tasks are ranked according to their actual total lag\n+     * @return Sorted set of all client candidates for each stateful task, ranked by their overall lag. Tasks are\n+     */\n+    static <ID extends Comparable<ID>> SortedMap<TaskId, SortedSet<RankedClient<ID>>> buildClientRankingsByTask(final Set<TaskId> statefulTasks,\n+                                                                                                                final Map<ID, ClientState> clientStates,\n+                                                                                                                final long acceptableRecoveryLag) {\n+        final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates = new TreeMap<>();\n+\n+        for (final TaskId task : statefulTasks) {\n+            final SortedSet<RankedClient<ID>> rankedClientCandidates = new TreeSet<>();\n+            statefulTasksToRankedCandidates.put(task, rankedClientCandidates);\n+\n+            for (final Map.Entry<ID, ClientState> clientEntry : clientStates.entrySet()) {\n+                final ID clientId = clientEntry.getKey();\n+                final long taskLag = clientEntry.getValue().lagFor(task);\n+                final long clientRank;\n+                if (taskLag == Task.LATEST_OFFSET) {\n+                    clientRank = Task.LATEST_OFFSET;\n+                } else if (taskLag == UNKNOWN_OFFSET_SUM) {\n+                    clientRank = 1L;\n+                } else if (taskLag <= acceptableRecoveryLag) {\n+                    clientRank = 0L;\n+                } else {\n+                    clientRank = taskLag;\n+                }\n+                rankedClientCandidates.add(new RankedClient<>(clientId, clientRank));\n+            }\n+        }\n+        log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n+\n+        return statefulTasksToRankedCandidates;\n+    }\n+\n+    /**\n+     * Compute the balance factor as the difference in stateful active task count per thread between the most and\n+     * least loaded clients\n+     */\n+    static int computeBalanceFactor(final Collection<ClientState> clientStates,\n+                                    final Set<TaskId> statefulTasks,\n+                                    final Function<ClientState, Set<TaskId>> activeTaskSet) {\n+        int minActiveStatefulTasksPerThreadCount = Integer.MAX_VALUE;\n+        int maxActiveStatefulTasksPerThreadCount = 0;\n+\n+        for (final ClientState state : clientStates) {\n+            final Set<TaskId> activeTasks = new HashSet<>(activeTaskSet.apply(state));\n+            activeTasks.retainAll(statefulTasks);\n+            final int taskPerThreadCount = activeTasks.size() / state.capacity();\n+            if (taskPerThreadCount < minActiveStatefulTasksPerThreadCount) {\n+                minActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+            if (taskPerThreadCount > maxActiveStatefulTasksPerThreadCount) {\n+                maxActiveStatefulTasksPerThreadCount = taskPerThreadCount;\n+            }\n+        }\n+\n+        return maxActiveStatefulTasksPerThreadCount - minActiveStatefulTasksPerThreadCount;\n+    }\n+\n+    /**\n+     * Determines whether to use the new proposed assignment or just return the group's previous assignment. The\n+     * previous assignment will be chosen and returned iff all of the following are true:\n+     *   1) it satisfies the state constraint, ie all tasks with caught up clients are assigned to one of those clients\n+     *   2) it satisfies the balance factor\n+     *   3) there are no unassigned tasks (eg due to a client that dropped out of the group)\n+     */\n+    private boolean shouldUsePreviousAssignment() {\n+        if (previousAssignmentIsValid()) {\n+            final int previousAssignmentBalanceFactor =\n+                computeBalanceFactor(clientStates.values(), statefulTasks, ClientState::prevActiveTasks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAxOTI0OA=="}, "originalCommit": null, "originalPosition": 353}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NzAyNjAyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNjowNjo0OFrOF-dPGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOTowNDoyNlrOF-kEaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAzNTAzMw==", "bodyText": "Q: There are a lot of methods that are package-private for testing. I would avoid to do that too much. If we feel we need to test sub components of a class then we might want to factor this components out to their own classes that we can test separately. For example, the validation of the previous assignment can be encapsulated into its own class. Does not need to be in this PR, though. WDYT?", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401035033", "createdAt": "2020-03-31T16:06:48Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTA0NjU0NQ==", "bodyText": "For my part, I think that several of the static inner classes of this assignor should be pulled out into top-level classes, which would have the side effect of pulling several of these tests that are unrelated to the HATA's contract itself. But I don't think it's high value right now, as I also want to refactor this module a bit before we finalize the feature.\nAs far as having visible-for-testing methods, I agree it's generally a failure of encapsulation, but in this particular case, we don't rely on the HATA's built-in public interface, but always access it via the TaskAssignor interface in the production code path, which makes package-visible methods in HATA much more palatable. It's also mitigating the risk that this method doesn't mutate the assignor, so I don't find this particular case too concerning.\nAltogether, if we had a package-visible static method on another object to do this, or a package-visible instance method on this object, it seems to be equally visible, so I guess it's no practical difference in this particular case. If you really want to encapsulate the method, the solution for both approaches is to put the HATA in its own package: o.a.k.s.p.i.assignment.ha.*, but it feels like diminishing returns.\nI do look forward to dropping support for Java 8 so that we can declare module public interfaces and not have to contend with compromises like this.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401046545", "createdAt": "2020-03-31T16:23:34Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAzNTAzMw=="}, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE0NDc2Mg==", "bodyText": "I agree with you that in this specific case, it might not be that concerning and that we can leave refactoring as a follow-up. However, I see the practice visible-for-testing a lot across the code base and I would like to keep it at the absolute minimum in new code.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401144762", "createdAt": "2020-03-31T19:00:39Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAzNTAzMw=="}, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE0Njk4NA==", "bodyText": "Oh, yeah, I'm 100% on board with you there. // visible for testing always reads to me like // setting a trap for the future:.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401146984", "createdAt": "2020-03-31T19:04:26Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,544 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                 final Set<TaskId> allTasks,\n+                                 final Set<TaskId> statefulTasks,\n+                                 final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+                new DefaultBalancedAssignor<ID>().assign(\n+                    sortedClients,\n+                    statefulTasks,\n+                    clientsToNumberOfThreads,\n+                    configs.balanceFactor);\n+\n+        final Queue<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment);\n+        for (int numWarmupReplicas = 0; numWarmupReplicas < configs.maxWarmupReplicas; ++numWarmupReplicas) {\n+            final Movement<ID> movement = movements.poll();\n+            if (movement == null) {\n+                break;\n+            }\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }\n+\n+        // ---------------- Standby Replica Tasks ---------------- //\n+\n+        final List<Map<ID, List<TaskId>>> allStatefulTaskAssignments = asList(\n+            statefulActiveTaskAssignment,\n+            warmupTaskAssignment,\n+            standbyTaskAssignment\n+        );\n+        final ClientValidatingPriorityQueue<ID> standbyTaskClientsQueue =\n+            new ClientValidatingPriorityQueue<>(\n+                configs.numStandbyReplicas,\n+                getClientPriorityQueueByTaskLoad(asList(warmupTaskAssignment, standbyTaskAssignment)),\n+                allStatefulTaskAssignments\n+            );\n+\n+        for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n+            final List<ID> clients = standbyTaskClientsQueue.poll(task);\n+            for (final ID client : clients) {\n+                standbyTaskAssignment.get(client).add(task);\n+            }\n+            final int numStandbysAssigned = clients.size();\n+            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+                log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n+                             \"There is not enough available capacity. You should \" +\n+                             \"increase the number of threads and/or application instances \" +\n+                             \"to maintain the requested number of standby replicas.\",\n+                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+            }\n+        }\n+\n+        // ---------------- Stateless Active Tasks ---------------- //\n+\n+        final PriorityQueue<ID> statelessActiveTaskClientsQueue =\n+            getClientPriorityQueueByTaskLoad(asList(statefulActiveTaskAssignment, statelessActiveTaskAssignment));\n+\n+        for (final TaskId task : statelessTasks) {\n+            final ID client = statelessActiveTaskClientsQueue.poll();\n+            statelessActiveTaskAssignment.get(client).add(task);\n+            statelessActiveTaskClientsQueue.offer(client);\n+        }\n+\n+        // ---------------- Assign Tasks To Clients ---------------- //\n+\n+        final boolean followupRebalanceRequired;\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            followupRebalanceRequired = false;\n+        } else {\n+            assignActiveTasksToClients(statefulActiveTaskAssignment);\n+            assignStandbyTasksToClients(warmupTaskAssignment);\n+            assignStandbyTasksToClients(standbyTaskAssignment);\n+            assignActiveTasksToClients(statelessActiveTaskAssignment);\n+\n+            followupRebalanceRequired = warmupTaskAssignment.entrySet().stream().anyMatch(tasks -> !tasks.getValue().isEmpty());\n+        }\n+        return followupRebalanceRequired;\n+    }\n+\n+    /**\n+     * Returns a list of the movements of tasks from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @param statefulActiveTaskAssignment the initial assignment, with source clients\n+     * @param balancedStatefulActiveTaskAssignment the final assignment, with destination clients\n+     */\n+    static <ID> Queue<Movement<ID>> getMovements(final Map<ID, List<TaskId>> statefulActiveTaskAssignment,\n+                                                 final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment) {\n+        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n+            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n+        }\n+\n+        final Map<TaskId, ID> taskToDestinationClient = new HashMap<>();\n+        for (final Map.Entry<ID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n+            final ID destination = clientEntry.getKey();\n+            for (final TaskId task : clientEntry.getValue()) {\n+                taskToDestinationClient.put(task, destination);\n+            }\n+        }\n+\n+        final Queue<Movement<ID>> movements = new LinkedList<>();\n+        for (final Map.Entry<ID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final ID source = sourceClientEntry.getKey();\n+\n+            for (final TaskId task : sourceClientEntry.getValue()) {\n+                final ID destination = taskToDestinationClient.get(task);\n+                if (destination == null) {\n+                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n+                                  \"balanced assignment.\", task, source);\n+                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n+                } else if (source != destination) {\n+                    movements.add(new Movement<>(task, source, destination));\n+                }\n+            }\n+        }\n+        return movements;\n+    }\n+\n+    /**\n+     * @return true iff all active tasks with caught-up client are assigned to one of them, and all tasks are assigned\n+     */\n+    boolean previousAssignmentIsValid() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAzNTAzMw=="}, "originalCommit": null, "originalPosition": 217}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4ODI2MTIxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMTo0Mjo1NlrOF-pUZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjo1OTo0NFrOF-rMmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTIzMjk5Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs, false);\n          \n          \n            \n                        taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs, !lagComputationSuccessful);\n          \n      \n    \n    \n  \n\nIt still seems like this is necessary to preserve the agreed on approach to ableegoldman#2 (comment)", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401232997", "createdAt": "2020-03-31T21:42:56Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,27 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(\n+                    clientStates,\n+                    allTasks,\n+                    statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI2Mzc2OA==", "bodyText": "Ok, I talked with Sophie offline, and I was mistaken about the impact of lagComputationSuccessful. I was thinking that StickyTaskAssignor couldn't produce a valid assignment without the lag computation, but this isn't true. So I'm resolving this conversation.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401263768", "createdAt": "2020-03-31T22:59:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -748,24 +702,27 @@ private void assignTasksToClients(final Set<String> allSourceTopics,\n         final boolean lagComputationSuccessful =\n             populateClientStatesMap(clientStates, clientMetadataMap, taskForPartition, changelogsByStatefulTask);\n \n-        // assign tasks to clients\n         final Set<TaskId> allTasks = partitionsForTask.keySet();\n-        final Set<TaskId> standbyTasks = changelogsByStatefulTask.keySet();\n-\n-        if (lagComputationSuccessful) {\n-            final Map<TaskId, SortedSet<RankedClient<UUID>>> statefulTasksToRankedCandidates =\n-                buildClientRankingsByTask(standbyTasks, clientStates, acceptableRecoveryLag());\n-            log.trace(\"Computed statefulTasksToRankedCandidates map as {}\", statefulTasksToRankedCandidates);\n-        }\n+        final Set<TaskId> statefulTasks = changelogsByStatefulTask.keySet();\n \n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, standbyTasks);\n-        if (!lagComputationSuccessful) {\n-            taskAssignor.preservePreviousTaskAssignment();\n+        final TaskAssignor<UUID> taskAssignor;\n+        if (highAvailabilityEnabled) {\n+            if (lagComputationSuccessful) {\n+                taskAssignor = new HighAvailabilityTaskAssignor<>(\n+                    clientStates,\n+                    allTasks,\n+                    statefulTasks,\n+                    assignmentConfigs);\n+            } else {\n+                taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n+            }\n+        } else {\n+            taskAssignor = new StickyTaskAssignor<>(clientStates, allTasks, statefulTasks, assignmentConfigs, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTIzMjk5Nw=="}, "originalCommit": null, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4OTM0NDUwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwNzowNjoyOVrOF-zYeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMToyNjoxMFrOF_TN8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM5Nzg4Mg==", "bodyText": "Q: IIUC, we do not check if the movement is for free. That is, if the destination is a caught-up client. If it were we would not need to assign a warm-up replica and could consider one more movement. I am also fine with post-poning that to a follow-up PR.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401397882", "createdAt": "2020-04-01T07:06:29Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,548 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                        final Set<TaskId> allTasks,\n+                                        final Set<TaskId> statefulTasks,\n+                                        final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            return false;\n+        }\n+\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+            new DefaultBalancedAssignor<ID>().assign(\n+                sortedClients,\n+                statefulTasks,\n+                clientsToNumberOfThreads,\n+                configs.balanceFactor);\n+\n+        final List<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment,\n+                configs.maxWarmupReplicas);\n+        for (final Movement<ID> movement : movements) {\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyNTMzNw==", "bodyText": "Oh, that's a really good point. We should totally do that, but I agree it would be easier to do it in a follow-on.", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401825337", "createdAt": "2020-04-01T18:33:41Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,548 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                        final Set<TaskId> allTasks,\n+                                        final Set<TaskId> statefulTasks,\n+                                        final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            return false;\n+        }\n+\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+            new DefaultBalancedAssignor<ID>().assign(\n+                sortedClients,\n+                statefulTasks,\n+                clientsToNumberOfThreads,\n+                configs.balanceFactor);\n+\n+        final List<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment,\n+                configs.maxWarmupReplicas);\n+        for (final Movement<ID> movement : movements) {\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM5Nzg4Mg=="}, "originalCommit": null, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkxOTQ3Mg==", "bodyText": "Sounds good", "url": "https://github.com/apache/kafka/pull/8337#discussion_r401919472", "createdAt": "2020-04-01T21:26:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -0,0 +1,548 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n+import java.util.SortedMap;\n+import java.util.SortedSet;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.Task;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class HighAvailabilityTaskAssignor<ID extends Comparable<ID>> implements TaskAssignor<ID> {\n+    private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n+\n+    private final Map<ID, ClientState> clientStates;\n+    private final Map<ID, Integer> clientsToNumberOfThreads;\n+    private final SortedSet<ID> sortedClients;\n+\n+    private final Set<TaskId> allTasks;\n+    private final SortedSet<TaskId> statefulTasks;\n+    private final SortedSet<TaskId> statelessTasks;\n+\n+    private final AssignmentConfigs configs;\n+\n+    private final SortedMap<TaskId, SortedSet<RankedClient<ID>>> statefulTasksToRankedCandidates;\n+\n+    public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates,\n+                                        final Set<TaskId> allTasks,\n+                                        final Set<TaskId> statefulTasks,\n+                                        final AssignmentConfigs configs) {\n+        this.configs = configs;\n+        this.clientStates = clientStates;\n+        this.allTasks = allTasks;\n+        this.statefulTasks = new TreeSet<>(statefulTasks);\n+\n+        statelessTasks = new TreeSet<>(allTasks);\n+        statelessTasks.removeAll(statefulTasks);\n+\n+        sortedClients = new TreeSet<>();\n+        clientsToNumberOfThreads = new HashMap<>();\n+        clientStates.forEach((client, state) -> {\n+            sortedClients.add(client);\n+            clientsToNumberOfThreads.put(client, state.capacity());\n+        });\n+\n+        statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n+    }\n+\n+    @Override\n+    public boolean assign() {\n+        if (shouldUsePreviousAssignment()) {\n+            assignPreviousTasksToClientStates();\n+            return false;\n+        }\n+\n+        final Map<ID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap();\n+        final Map<ID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap();\n+\n+        // ---------------- Stateful Active Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> statefulActiveTaskAssignment =\n+            new DefaultStateConstrainedBalancedAssignor<ID>().assign(\n+                statefulTasksToRankedCandidates,\n+                configs.balanceFactor,\n+                sortedClients,\n+                clientsToNumberOfThreads\n+            );\n+\n+        // ---------------- Warmup Replica Tasks ---------------- //\n+\n+        final Map<ID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+            new DefaultBalancedAssignor<ID>().assign(\n+                sortedClients,\n+                statefulTasks,\n+                clientsToNumberOfThreads,\n+                configs.balanceFactor);\n+\n+        final List<Movement<ID>> movements =\n+            getMovements(statefulActiveTaskAssignment, balancedStatefulActiveTaskAssignment,\n+                configs.maxWarmupReplicas);\n+        for (final Movement<ID> movement : movements) {\n+            warmupTaskAssignment.get(movement.destination).add(movement.task);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM5Nzg4Mg=="}, "originalCommit": null, "originalPosition": 118}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3072, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}