{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2MzkxMzQ2", "number": 8994, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMjozMDo0OVrOEMrR3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwNDo1ODowMlrOENeByA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzI3NDUyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMjozMDo0OVrOGu7ZDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMzowNjoxOFrOGvAGjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MDc1MA==", "bodyText": "I honestly couldn't figure out what is the default default default reset strategy... It seems (from the behavior of the test when we first start up) that if there's no strategy set, and no committed offset, then the client starts at the beginning, but the ClientConfig has the default policy as \"latest\"... What gives?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451860750", "createdAt": "2020-07-08T22:30:49Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -766,6 +769,24 @@ void runOnce() {\n         return records;\n     }\n \n+    private OffsetResetStrategy getResetStrategy(final TopicPartition partition) {\n+        if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.EARLIEST;\n+        } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.LATEST;\n+        } else {\n+            if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n+                return OffsetResetStrategy.EARLIEST;\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MjU4OA==", "bodyText": "Does Streams override the client default..?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451862588", "createdAt": "2020-07-08T22:36:19Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -766,6 +769,24 @@ void runOnce() {\n         return records;\n     }\n \n+    private OffsetResetStrategy getResetStrategy(final TopicPartition partition) {\n+        if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.EARLIEST;\n+        } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.LATEST;\n+        } else {\n+            if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n+                return OffsetResetStrategy.EARLIEST;\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MDc1MA=="}, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5MTgyMg==", "bodyText": "Does Streams override the client default..?\n\nYes. The client default is \"latest\" but we use \"earliest\" by default (cf https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L857). Of course, users can also change the default via StreamsConfig.\nNote that the consumer client can only apply a single strategy to all topics it subscribed to. Hence, if all topics use the same reset policy, we can rely on the consumer configures policy. However, if users specify different reset policies in their code via Consumed for individual topics, the consumer is re-configured to use \"none\" (cf. https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L362-L366) and we do a manual seekToBeginning/seekToEnd according to the user define strategy for the corresponding topic (cf https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L762-L764) because we need to make a per-topic decision that the consumer cannot make for us.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451891822", "createdAt": "2020-07-09T00:10:00Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -766,6 +769,24 @@ void runOnce() {\n         return records;\n     }\n \n+    private OffsetResetStrategy getResetStrategy(final TopicPartition partition) {\n+        if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.EARLIEST;\n+        } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.LATEST;\n+        } else {\n+            if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n+                return OffsetResetStrategy.EARLIEST;\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MDc1MA=="}, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNzkzNQ==", "bodyText": "Ah, thanks for the explanation, @mjsax !", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451937935", "createdAt": "2020-07-09T03:06:18Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -766,6 +769,24 @@ void runOnce() {\n         return records;\n     }\n \n+    private OffsetResetStrategy getResetStrategy(final TopicPartition partition) {\n+        if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.EARLIEST;\n+        } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n+            return OffsetResetStrategy.LATEST;\n+        } else {\n+            if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n+                return OffsetResetStrategy.EARLIEST;\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MDc1MA=="}, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzI3Njk4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMjozMTo1NlrOGu7amA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMzowNzowM1rOGvAHOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MTE0NA==", "bodyText": "This might be the worst thing I've ever proposed for AK... I can't figure out a better way to just \"reset\" the offset.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451861144", "createdAt": "2020-07-08T22:31:56Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +197,28 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            // Pause so we won't poll any more records for this task until it has been re-initialized\n+            // Note, closeDirty already clears the partitiongroup for the task.\n+            mainConsumer().pause(task.inputPartitions());\n+            final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(task.inputPartitions());\n+            for (final TopicPartition topicPartition : task.inputPartitions()) {\n+                final OffsetAndMetadata offsetAndMetadata = committed.get(topicPartition);\n+                if (offsetAndMetadata == null) {\n+                    final OffsetResetStrategy strategy = resetStrategy.apply(topicPartition);\n+                    switch (strategy) {\n+                        case EARLIEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        case LATEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        default:\n+                            throw new IllegalArgumentException(\"Unexpected reset strategy: \" + strategy);\n+                    }\n+                } else {\n+                    mainConsumer().seek(topicPartition, offsetAndMetadata);\n+                }\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTE3Ng==", "bodyText": "Why do you think is bad? That is just how the API works... Cf https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L769-L802 that does the same thing.\nWhat make we wonder, if we can share common code for both cases?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451895176", "createdAt": "2020-07-09T00:21:42Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +197,28 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            // Pause so we won't poll any more records for this task until it has been re-initialized\n+            // Note, closeDirty already clears the partitiongroup for the task.\n+            mainConsumer().pause(task.inputPartitions());\n+            final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(task.inputPartitions());\n+            for (final TopicPartition topicPartition : task.inputPartitions()) {\n+                final OffsetAndMetadata offsetAndMetadata = committed.get(topicPartition);\n+                if (offsetAndMetadata == null) {\n+                    final OffsetResetStrategy strategy = resetStrategy.apply(topicPartition);\n+                    switch (strategy) {\n+                        case EARLIEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        case LATEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        default:\n+                            throw new IllegalArgumentException(\"Unexpected reset strategy: \" + strategy);\n+                    }\n+                } else {\n+                    mainConsumer().seek(topicPartition, offsetAndMetadata);\n+                }\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MTE0NA=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzODEwNA==", "bodyText": "I think we can, too. I'll give it a shot, since it doesn't seem crazy to you :)", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451938104", "createdAt": "2020-07-09T03:07:03Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +197,28 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            // Pause so we won't poll any more records for this task until it has been re-initialized\n+            // Note, closeDirty already clears the partitiongroup for the task.\n+            mainConsumer().pause(task.inputPartitions());\n+            final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(task.inputPartitions());\n+            for (final TopicPartition topicPartition : task.inputPartitions()) {\n+                final OffsetAndMetadata offsetAndMetadata = committed.get(topicPartition);\n+                if (offsetAndMetadata == null) {\n+                    final OffsetResetStrategy strategy = resetStrategy.apply(topicPartition);\n+                    switch (strategy) {\n+                        case EARLIEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        case LATEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        default:\n+                            throw new IllegalArgumentException(\"Unexpected reset strategy: \" + strategy);\n+                    }\n+                } else {\n+                    mainConsumer().seek(topicPartition, offsetAndMetadata);\n+                }\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg2MTE0NA=="}, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzQyMzE5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMzo0MDoyMVrOGu8xhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMzowNzoxOVrOGvAHdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4MzM5Ng==", "bodyText": "Should this be seekToEnd ?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451883396", "createdAt": "2020-07-08T23:40:21Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +197,28 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            // Pause so we won't poll any more records for this task until it has been re-initialized\n+            // Note, closeDirty already clears the partitiongroup for the task.\n+            mainConsumer().pause(task.inputPartitions());\n+            final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(task.inputPartitions());\n+            for (final TopicPartition topicPartition : task.inputPartitions()) {\n+                final OffsetAndMetadata offsetAndMetadata = committed.get(topicPartition);\n+                if (offsetAndMetadata == null) {\n+                    final OffsetResetStrategy strategy = resetStrategy.apply(topicPartition);\n+                    switch (strategy) {\n+                        case EARLIEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        case LATEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzODE2Nw==", "bodyText": "ah... yes.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451938167", "createdAt": "2020-07-09T03:07:19Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +197,28 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            // Pause so we won't poll any more records for this task until it has been re-initialized\n+            // Note, closeDirty already clears the partitiongroup for the task.\n+            mainConsumer().pause(task.inputPartitions());\n+            final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(task.inputPartitions());\n+            for (final TopicPartition topicPartition : task.inputPartitions()) {\n+                final OffsetAndMetadata offsetAndMetadata = committed.get(topicPartition);\n+                if (offsetAndMetadata == null) {\n+                    final OffsetResetStrategy strategy = resetStrategy.apply(topicPartition);\n+                    switch (strategy) {\n+                        case EARLIEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));\n+                            break;\n+                        case LATEST:\n+                            mainConsumer().seekToBeginning(Collections.singleton(topicPartition));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4MzM5Ng=="}, "originalCommit": null, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzQzMDE1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMzo0Mzo1OFrOGu81oQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMTo0MTo0NFrOGwGZ2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ==", "bodyText": "What's up with this?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451884449", "createdAt": "2020-07-08T23:43:58Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzODg5MQ==", "bodyText": "I meant to call this out, too. In order to get the task re-initialized after a corruption recovery, we could set the state back to PartitionsAssigned, but I felt that would be confusing in the logs. Instead, I added an extra condition that we'll initialize tasks if there are any that need initialization, even though the thread may already be in running, which means we can have a self-transition from running to running.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451938891", "createdAt": "2020-07-09T03:10:36Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQxMDk2NQ==", "bodyText": "Where does this self-transition happen exactly? And could/should we detect this case and not call setState() for this case instead of allowing the transition?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452410965", "createdAt": "2020-07-09T18:29:26Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDM0Mw==", "bodyText": "\\cc @vvcephei", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452620343", "createdAt": "2020-07-10T04:50:02Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzNDU3MQ==", "bodyText": "Ah, sorry, I didn't see this follow-up question.\nIt's here: \n  \n    \n      kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java\n    \n    \n         Line 657\n      in\n      568dceb\n    \n    \n    \n    \n\n        \n          \n           setState(State.RUNNING); \n        \n    \n  \n\n\nI thought about it, but it seemed to be in violation of the whole idea of having a specified state machine. I.e., saying \"if not running, setState(running)\" is literally the same thing as saying that self-transitions are allowed. So why not just say that self-transitions are allowed (explicitly instead of implicitly)?\nAnyway, that's what I was thinking; no guarantee that it makes sense ;)", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453034571", "createdAt": "2020-07-10T19:19:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA3MDIxNA==", "bodyText": "I agree with the sentiment. However, our overall patterns is to disallowed self-transitions, ie, this pattern is applied throughout all other code. IMHO, I might be better to stick with one pattern. Thus, if we think we should allow self-transitions, we might want to do it for all states and update the code throughout accordingly. Begin in a \"mixed mode\" seems suboptimal.\nOf course this would be follow up work. And frankly I am not sure if it would buy us much at this point?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453070214", "createdAt": "2020-07-10T20:48:36Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA3MzMwNQ==", "bodyText": "FWIW, we actually do follow this pattern already, at least with respect to StreamThread state transitions. I'd rather be consistent with the other StreamThread state changes and inconsistent with other places in the code (eg the Task state changes), especially if we all agree this seems like the sensible approach to the fsm", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453073305", "createdAt": "2020-07-10T20:56:29Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4MjE5MQ==", "bodyText": "Thanks for chiming in. Seems I did not separate thread state and task state transition. Sorry for the confusion.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453082191", "createdAt": "2020-07-10T21:20:05Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4OTc1NQ==", "bodyText": "No worries, thanks for the review @mjsax . Now that I'm thinking about it, it does seem like Task is allowing self-transitions, rather than disallowing them, so we should probably add the self-transitions to the Task state machine. But maybe in a follow-up.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453089755", "createdAt": "2020-07-10T21:41:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -95,7 +96,7 @@\n      *          |      | Assigned (3)| <----+\n      *          |      +-----+-------+      |\n      *          |            |              |\n-     *          |            |              |\n+     *          |            |--------------+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4NDQ0OQ=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzQ2MDUxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMzo1OTozOFrOGu9HKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMzoxMToyNFrOGvALJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4ODkzOQ==", "bodyText": "Why do we need this?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451888939", "createdAt": "2020-07-08T23:59:38Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -137,7 +138,7 @@\n         STARTING(2, 3, 5),                // 1\n         PARTITIONS_REVOKED(2, 3, 5),      // 2\n         PARTITIONS_ASSIGNED(2, 3, 4, 5),  // 3\n-        RUNNING(2, 3, 5),                 // 4\n+        RUNNING(2, 3, 4, 5),              // 4", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzOTEwOA==", "bodyText": "Sorry, I meant to explain it, but forgot. I've explained it here: #8994 (comment)", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451939108", "createdAt": "2020-07-09T03:11:24Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -137,7 +138,7 @@\n         STARTING(2, 3, 5),                // 1\n         PARTITIONS_REVOKED(2, 3, 5),      // 2\n         PARTITIONS_ASSIGNED(2, 3, 4, 5),  // 3\n-        RUNNING(2, 3, 5),                 // 4\n+        RUNNING(2, 3, 4, 5),              // 4", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4ODkzOQ=="}, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzQ5NjQyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoxODoxOVrOGu9b7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMzoxMTo1MFrOGvALnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NDI1Mw==", "bodyText": "Wondering if we should reuse this method within StreamThread#resetInvalidOffsets?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451894253", "createdAt": "2020-07-09T00:18:19Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -766,6 +769,24 @@ void runOnce() {\n         return records;\n     }\n \n+    private OffsetResetStrategy getResetStrategy(final TopicPartition partition) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzOTIzMQ==", "bodyText": "Yeah, I think we should.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451939231", "createdAt": "2020-07-09T03:11:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -766,6 +769,24 @@ void runOnce() {\n         return records;\n     }\n \n+    private OffsetResetStrategy getResetStrategy(final TopicPartition partition) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NDI1Mw=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUwNTkwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyMzoyN1rOGu9hZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMzoxMjo1NlrOGvAMqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTY1Mg==", "bodyText": "I think the name of the method is not ideal. We don't set the strategy, but we set a function that can compute the strategy for a partitions. Needed to go forth and back on the PR to understand how it work, and assume the method name had its part in confusing me.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451895652", "createdAt": "2020-07-09T00:23:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -1140,4 +1166,12 @@ public static void executeAndMaybeSwallow(final boolean clean,\n             throw e; },\n             e -> log.debug(\"Ignoring error in unclean {}\", name));\n     }\n+\n+    boolean hasPreRunningTasks() {\n+        return tasks().values().stream().anyMatch(Task::preRunning);\n+    }\n+\n+    public void setResetStrategy(final Function<TopicPartition, OffsetResetStrategy> resetStrategy) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzOTQ5OQ==", "bodyText": "Sorry about that. I was only thinking \"set a field named 'reset strategy'\", but now I see that it sounds more like what you said. I'll rename it.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r451939499", "createdAt": "2020-07-09T03:12:56Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -1140,4 +1166,12 @@ public static void executeAndMaybeSwallow(final boolean clean,\n             throw e; },\n             e -> log.debug(\"Ignoring error in unclean {}\", name));\n     }\n+\n+    boolean hasPreRunningTasks() {\n+        return tasks().values().stream().anyMatch(Task::preRunning);\n+    }\n+\n+    public void setResetStrategy(final Function<TopicPartition, OffsetResetStrategy> resetStrategy) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTY1Mg=="}, "originalCommit": null, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQzMTg4OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1MjoyM1rOGvZq4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1MjoyM1rOGvZq4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1NjgzNA==", "bodyText": "Just a quality-of-life improvement.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452356834", "createdAt": "2020-07-09T16:52:23Z", "author": {"login": "vvcephei"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java", "diffHunk": "@@ -504,7 +504,7 @@ private void resetOffsetPosition(TopicPartition tp) {\n         if (strategy == OffsetResetStrategy.EARLIEST) {\n             offset = beginningOffsets.get(tp);\n             if (offset == null)\n-                throw new IllegalStateException(\"MockConsumer didn't have beginning offset specified, but tried to seek to beginning\");\n+                throw new IllegalStateException(\"MockConsumer didn't have beginning offset for \" + tp + \" specified, but tried to seek to beginning\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQzMjg3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1Mjo0NFrOGvZrnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1Mjo0NFrOGvZrnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1NzAyMA==", "bodyText": "Was deceptively named.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452357020", "createdAt": "2020-07-09T16:52:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -215,7 +215,7 @@ public StateStore getGlobalStore(final String name) {\n     }\n \n     // package-private for test only\n-    void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n+    void initializeStoreOffsetsFromCheckpoint(final boolean taskDirIsEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQzNTU3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1MzoyOVrOGvZtWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODo1NToxMVrOGvdzkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1NzQ2NA==", "bodyText": "Bugfix: we shouldn't call this task corrupted for not having a checkpoint of a non-persistent store.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452357464", "createdAt": "2020-07-09T16:53:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -232,10 +232,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                                   store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // with EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n+                        // With EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n                         // and hence we are uncertain that the current local state only contains committed data;\n                         // in that case we need to treat it as a task-corrupted exception\n-                        if (eosEnabled && !storeDirIsEmpty) {\n+\n+                        // Note, this is a little overzealous, since we aren't checking whether the store's specific\n+                        // directory is nonempty, only if there are any directories for any stores. So if there are\n+                        // two stores in a task, and one is correctly written and checkpointed, while the other is\n+                        // neither written nor checkpointed, we _could_ correctly load the first and recover the second\n+                        // but instead we'll consider the whole task corrupted and discard the first and recover both.\n+                        if (store.stateStore.persistent() && eosEnabled && !taskDirIsEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2NTE2Mg==", "bodyText": "I'm not sure how this came up now for me. I don't think it was related to any changes in this PR, but the SuppressionDurabilityIntegrationTest fails every time without it.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452365162", "createdAt": "2020-07-09T17:06:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -232,10 +232,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                                   store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // with EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n+                        // With EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n                         // and hence we are uncertain that the current local state only contains committed data;\n                         // in that case we need to treat it as a task-corrupted exception\n-                        if (eosEnabled && !storeDirIsEmpty) {\n+\n+                        // Note, this is a little overzealous, since we aren't checking whether the store's specific\n+                        // directory is nonempty, only if there are any directories for any stores. So if there are\n+                        // two stores in a task, and one is correctly written and checkpointed, while the other is\n+                        // neither written nor checkpointed, we _could_ correctly load the first and recover the second\n+                        // but instead we'll consider the whole task corrupted and discard the first and recover both.\n+                        if (store.stateStore.persistent() && eosEnabled && !taskDirIsEmpty) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1NzQ2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM4MTQ3NA==", "bodyText": "I like my fix better :P\nBut seriously, no need to block this PR on mine if it's suddenly causing tests to fail. Mysterious..", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452381474", "createdAt": "2020-07-09T17:35:18Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -232,10 +232,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                                   store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // with EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n+                        // With EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n                         // and hence we are uncertain that the current local state only contains committed data;\n                         // in that case we need to treat it as a task-corrupted exception\n-                        if (eosEnabled && !storeDirIsEmpty) {\n+\n+                        // Note, this is a little overzealous, since we aren't checking whether the store's specific\n+                        // directory is nonempty, only if there are any directories for any stores. So if there are\n+                        // two stores in a task, and one is correctly written and checkpointed, while the other is\n+                        // neither written nor checkpointed, we _could_ correctly load the first and recover the second\n+                        // but instead we'll consider the whole task corrupted and discard the first and recover both.\n+                        if (store.stateStore.persistent() && eosEnabled && !taskDirIsEmpty) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1NzQ2NA=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQyNDU5NQ==", "bodyText": "Oh, I put this comment on the wrong line:\n\nFYI, this is also fixed (better) in #8996\n\nIn other words, I agree, I like your fix better as well.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452424595", "createdAt": "2020-07-09T18:55:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -232,10 +232,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                                   store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // with EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n+                        // With EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n                         // and hence we are uncertain that the current local state only contains committed data;\n                         // in that case we need to treat it as a task-corrupted exception\n-                        if (eosEnabled && !storeDirIsEmpty) {\n+\n+                        // Note, this is a little overzealous, since we aren't checking whether the store's specific\n+                        // directory is nonempty, only if there are any directories for any stores. So if there are\n+                        // two stores in a task, and one is correctly written and checkpointed, while the other is\n+                        // neither written nor checkpointed, we _could_ correctly load the first and recover the second\n+                        // but instead we'll consider the whole task corrupted and discard the first and recover both.\n+                        if (store.stateStore.persistent() && eosEnabled && !taskDirIsEmpty) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1NzQ2NA=="}, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQzNjUyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1Mzo0OFrOGvZt-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1Mzo0OFrOGvZt-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1NzYyNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    log.info(\"Closing its state manager and all the registered state stores: {}\", stores);\n          \n          \n            \n                    log.debug(\"Closing its state manager and all the registered state stores: {}\", stores);", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452357627", "createdAt": "2020-07-09T16:53:48Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -462,7 +468,7 @@ public void flush() {\n      */\n     @Override\n     public void close() throws ProcessorStateException {\n-        log.debug(\"Closing its state manager and all the registered state stores: {}\", stores);\n+        log.info(\"Closing its state manager and all the registered state stores: {}\", stores);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQ0MTEyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1NTowNVrOGvZw7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1NTowNVrOGvZw7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1ODM4MQ==", "bodyText": "Bugfix: If a task has been revived, then it needs to be initialized and restored, even if the thread state is already RUNNING.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452358381", "createdAt": "2020-07-09T16:55:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -648,7 +650,7 @@ void runOnce() {\n \n         // only try to initialize the assigned tasks\n         // if the state is still in PARTITION_ASSIGNED after the poll call\n-        if (state == State.PARTITIONS_ASSIGNED) {\n+        if (state == State.PARTITIONS_ASSIGNED || taskManager.hasPreRunningTasks()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQ0NzE2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1Njo0NlrOGvZ04A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1Njo0NlrOGvZ04A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM1OTM5Mg==", "bodyText": "Refactored to share the new resetOffsets method.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452359392", "createdAt": "2020-07-09T16:56:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -768,9 +770,30 @@ void runOnce() {\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {\n         final Set<TopicPartition> partitions = e.partitions();\n+        final Set<TopicPartition> notReset = resetOffsets(partitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQ1Mjc0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1ODoxMlrOGvZ4XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNjo1ODoxMlrOGvZ4XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2MDI4NQ==", "bodyText": "Shouldn't try to reset the main consumer at all if this is only a standby.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452360285", "createdAt": "2020-07-09T16:58:12Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQ2MDkwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNzowMDoyNVrOGvZ9kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMTozOTowNlrOGwGWXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2MTYxOA==", "bodyText": "I don't fully understand how this could happen, but I saw it in several tests, that an active task isn't assigned the input TopicPartition for itself. Whether or not it can happen with a real consumer, or only in tests with the MockConsumer, doesn't really seem important. If the topic isn't assigned, we certainly don't need to reset it.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452361618", "createdAt": "2020-07-09T17:00:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    Utils.intersection(HashSet::new, currentAssignment, task.inputPartitions());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMTgxNA==", "bodyText": "I agree that it should never happen. Maybe it's a test setup issue? Should we be worries to mask a potential bug by writing the code like this? Or should we rather fail hard (if we assume it would indicate a bug) and make sure the update the tests?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452621814", "createdAt": "2020-07-10T04:56:32Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    Utils.intersection(HashSet::new, currentAssignment, task.inputPartitions());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2MTYxOA=="}, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAyNzEyNg==", "bodyText": "Could this happen due to some race condition with a deleted regex topic? I assume that's not happening in your tests, just wondering if this check actually is necessary or if it would only be masking a bug as Matthias said.\nI'm pretty sure we should always go through the process of updating the task's input partitions immediately after updating the consumer's assignment, so we would never find them to be out of sync.  But can the consumer assignment be updated outside of a rebalance, eg based on metadata refresh?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453027126", "createdAt": "2020-07-10T19:02:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    Utils.intersection(HashSet::new, currentAssignment, task.inputPartitions());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2MTYxOA=="}, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4ODg2MQ==", "bodyText": "Thanks all, it may well have just been a problem with the test setup, but @ableegoldman is making me nervous that there may be some edge cases where it's expected in production code.\nSince we would never have had a dependency on the task inputs actually being assigned here, and now we do, I'm concerned that if we try to be strict right now, we may be introducing an IllegalStateException. Normally, this wouldn't be a problem, but this PR is a last-minute hotfix before a release, so I'm feeling cautious.\nSince we seem to agree that the code is correct, although it may not be strict, I've added a warning log instead of increasing strictness.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453088861", "createdAt": "2020-07-10T21:39:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    Utils.intersection(HashSet::new, currentAssignment, task.inputPartitions());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2MTYxOA=="}, "originalCommit": null, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDQ2NzU0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNzowMjoyNlrOGvaB6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNzowMjoyNlrOGvaB6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2MjczMQ==", "bodyText": "This all amounts to checking that we really reset the consumer to the last committed position.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452362731", "createdAt": "2020-07-09T17:02:26Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -566,8 +567,20 @@ public void shouldReviveCorruptTasks() {\n         topologyBuilder.addSubscribedTopicsFromAssignment(anyObject(), anyString());\n         expectLastCall().anyTimes();\n \n+        expect(consumer.assignment()).andReturn(taskId00Partitions);\n+        consumer.pause(taskId00Partitions);\n+        expectLastCall();\n+        final OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(0L);\n+        expect(consumer.committed(taskId00Partitions)).andReturn(singletonMap(t1p0, offsetAndMetadata));\n+        consumer.seek(t1p0, offsetAndMetadata);\n+        expectLastCall();\n+        consumer.seekToBeginning(emptySet());\n+        expectLastCall();\n         replay(activeTaskCreator, topologyBuilder, consumer, changeLogReader);\n-\n+        taskManager.setPartitionResetter(tp -> {\n+            assertThat(tp, is(empty()));\n+            return emptySet();\n+        });", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDc0NDM1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyNDo0M1rOGvczzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToyNTowNVrOGvewBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwODI2OA==", "bodyText": "Sound like something we should fix. Can you file a ticket?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452408268", "createdAt": "2020-07-09T18:24:43Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -232,10 +232,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                                   store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // with EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n+                        // With EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n                         // and hence we are uncertain that the current local state only contains committed data;\n                         // in that case we need to treat it as a task-corrupted exception\n-                        if (eosEnabled && !storeDirIsEmpty) {\n+\n+                        // Note, this is a little overzealous, since we aren't checking whether the store's specific\n+                        // directory is nonempty, only if there are any directories for any stores. So if there are\n+                        // two stores in a task, and one is correctly written and checkpointed, while the other is\n+                        // neither written nor checkpointed, we _could_ correctly load the first and recover the second\n+                        // but instead we'll consider the whole task corrupted and discard the first and recover both.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQyMzkwMQ==", "bodyText": "IIRC, @guozhangwang tried to include this during the February refactor, but it's harder to get right than it sounds. Still, it would be very nice to have it, and I agree it would be good to file a ticket.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452423901", "createdAt": "2020-07-09T18:53:55Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -232,10 +232,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                                   store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // with EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n+                        // With EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n                         // and hence we are uncertain that the current local state only contains committed data;\n                         // in that case we need to treat it as a task-corrupted exception\n-                        if (eosEnabled && !storeDirIsEmpty) {\n+\n+                        // Note, this is a little overzealous, since we aren't checking whether the store's specific\n+                        // directory is nonempty, only if there are any directories for any stores. So if there are\n+                        // two stores in a task, and one is correctly written and checkpointed, while the other is\n+                        // neither written nor checkpointed, we _could_ correctly load the first and recover the second\n+                        // but instead we'll consider the whole task corrupted and discard the first and recover both.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwODI2OA=="}, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQ0MDA2OA==", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10260", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452440068", "createdAt": "2020-07-09T19:25:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -232,10 +232,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                                   store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // with EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n+                        // With EOS, if the previous run did not shutdown gracefully, we may lost the checkpoint file\n                         // and hence we are uncertain that the current local state only contains committed data;\n                         // in that case we need to treat it as a task-corrupted exception\n-                        if (eosEnabled && !storeDirIsEmpty) {\n+\n+                        // Note, this is a little overzealous, since we aren't checking whether the store's specific\n+                        // directory is nonempty, only if there are any directories for any stores. So if there are\n+                        // two stores in a task, and one is correctly written and checkpointed, while the other is\n+                        // neither written nor checkpointed, we _could_ correctly load the first and recover the second\n+                        // but instead we'll consider the whole task corrupted and discard the first and recover both.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwODI2OA=="}, "originalCommit": null, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMjEyMjY2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwNDo1MjoyMlrOGvpyaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxNjozMDowN1rOGv9ygA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ==", "bodyText": "Nit: As reset policy is set on a per topic basis, it's sufficient to list the topic names -- it does not add value if we list the partitions, because all assigned partitions would be affected anyway.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452620905", "createdAt": "2020-07-10T04:52:22Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -768,9 +770,30 @@ void runOnce() {\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {\n         final Set<TopicPartition> partitions = e.partitions();\n+        final Set<TopicPartition> notReset = resetOffsets(partitions);\n+        if (!notReset.isEmpty()) {\n+            final String notResetString =\n+                notReset.stream()\n+                        .map(tp -> \"topic \" + tp.topic() + \"(partition \" + tp.partition() + \")\")\n+                        .collect(Collectors.joining(\",\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjg4OTY0Mw==", "bodyText": "Interesting; good point. I just copied over the original message. Do you think I should drop the partition?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452889643", "createdAt": "2020-07-10T14:45:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -768,9 +770,30 @@ void runOnce() {\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {\n         final Set<TopicPartition> partitions = e.partitions();\n+        final Set<TopicPartition> notReset = resetOffsets(partitions);\n+        if (!notReset.isEmpty()) {\n+            final String notResetString =\n+                notReset.stream()\n+                        .map(tp -> \"topic \" + tp.topic() + \"(partition \" + tp.partition() + \")\")\n+                        .collect(Collectors.joining(\",\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ=="}, "originalCommit": {"oid": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjkzOTMyMw==", "bodyText": "I think it would be good. Seems the original message should not have contained the partition to begin with. Even if for the original one, it's less severe because we only consider a single topic-partition in the error message.\nFor this new case, we get multiple topic-partitions and the error message might become quite large? Also, if multiple task of the same sub-topology are assigned, the same topic name might appear multiple times for different partitions? Might be best to only add a Set or topic names to the error message?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452939323", "createdAt": "2020-07-10T16:12:06Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -768,9 +770,30 @@ void runOnce() {\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {\n         final Set<TopicPartition> partitions = e.partitions();\n+        final Set<TopicPartition> notReset = resetOffsets(partitions);\n+        if (!notReset.isEmpty()) {\n+            final String notResetString =\n+                notReset.stream()\n+                        .map(tp -> \"topic \" + tp.topic() + \"(partition \" + tp.partition() + \")\")\n+                        .collect(Collectors.joining(\",\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ=="}, "originalCommit": {"oid": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk0ODYwOA==", "bodyText": "Ok, I've done this.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452948608", "createdAt": "2020-07-10T16:30:07Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -768,9 +770,30 @@ void runOnce() {\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {\n         final Set<TopicPartition> partitions = e.partitions();\n+        final Set<TopicPartition> notReset = resetOffsets(partitions);\n+        if (!notReset.isEmpty()) {\n+            final String notResetString =\n+                notReset.stream()\n+                        .map(tp -> \"topic \" + tp.topic() + \"(partition \" + tp.partition() + \")\")\n+                        .collect(Collectors.joining(\",\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ=="}, "originalCommit": {"oid": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMjEzMDg3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwNDo1ODowM1rOGvp3UA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxNjozMToyMFrOGv907g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMjE2MA==", "bodyText": "I think we should fail for this case, because if user configures \"none\" they request that we fail if we loose track of the offset.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452622160", "createdAt": "2020-07-10T04:58:03Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    Utils.intersection(HashSet::new, currentAssignment, task.inputPartitions());\n+\n+                mainConsumer().pause(assignedToPauseAndReset);\n+                final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(assignedToPauseAndReset);\n+                for (final Map.Entry<TopicPartition, OffsetAndMetadata> committedEntry : committed.entrySet()) {\n+                    final OffsetAndMetadata offsetAndMetadata = committedEntry.getValue();\n+                    if (offsetAndMetadata != null) {\n+                        mainConsumer().seek(committedEntry.getKey(), offsetAndMetadata);\n+                        assignedToPauseAndReset.remove(committedEntry.getKey());\n+                    }\n+                }\n+                final Set<TopicPartition> remainder = resetter.apply(assignedToPauseAndReset);\n+                // If anything didn't have a configured policy, reset to beginning\n+                mainConsumer().seekToBeginning(remainder);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk0OTIzMA==", "bodyText": "Thanks. I was confused by the behaviour of the StreamThreadTest. I think there was something funny in the StreamThread factory, which just never surfaced before. I'll call it out for your review.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452949230", "createdAt": "2020-07-10T16:31:20Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    Utils.intersection(HashSet::new, currentAssignment, task.inputPartitions());\n+\n+                mainConsumer().pause(assignedToPauseAndReset);\n+                final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(assignedToPauseAndReset);\n+                for (final Map.Entry<TopicPartition, OffsetAndMetadata> committedEntry : committed.entrySet()) {\n+                    final OffsetAndMetadata offsetAndMetadata = committedEntry.getValue();\n+                    if (offsetAndMetadata != null) {\n+                        mainConsumer().seek(committedEntry.getKey(), offsetAndMetadata);\n+                        assignedToPauseAndReset.remove(committedEntry.getKey());\n+                    }\n+                }\n+                final Set<TopicPartition> remainder = resetter.apply(assignedToPauseAndReset);\n+                // If anything didn't have a configured policy, reset to beginning\n+                mainConsumer().seekToBeginning(remainder);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMjE2MA=="}, "originalCommit": {"oid": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNDIzMjQyOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxNjozNToyNFrOGv989g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMTozMjo0NlrOGwGODg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng==", "bodyText": "I had to change this to get the StreamThread test to actually use the configured ConsumerConfig.AUTO_OFFSET_RESET_CONFIG := earliest\nReading the conditional, it doesn't make any sense to me, but it's been in the codebase for a long time, so I'm doubting myself. It seems to say that we will only use the provided client configuration if there is an override, but it seems like it should have been \"if there is not an override\".\nRegardless, the \"originalReset\" is only used as a fallback after we apply the builder reset patterns, so I don't see why we should leave it null in any case.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452951286", "createdAt": "2020-07-10T16:35:24Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -359,11 +359,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n         consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);\n         final AtomicLong nextScheduledRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n         consumerConfigs.put(StreamsConfig.InternalConfig.NEXT_SCHEDULED_REBALANCE_MS, nextScheduledRebalanceMs);\n-        String originalReset = null;\n-        if (!builder.latestResetTopicsPattern().pattern().equals(\"\") || !builder.earliestResetTopicsPattern().pattern().equals(\"\")) {\n-            originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);\n-            consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n-        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ed4e27134758750c0fd24adbfba52010577327"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NDc0Mw==", "bodyText": "This makes my head hurt..", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452984743", "createdAt": "2020-07-10T17:44:53Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -359,11 +359,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n         consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);\n         final AtomicLong nextScheduledRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n         consumerConfigs.put(StreamsConfig.InternalConfig.NEXT_SCHEDULED_REBALANCE_MS, nextScheduledRebalanceMs);\n-        String originalReset = null;\n-        if (!builder.latestResetTopicsPattern().pattern().equals(\"\") || !builder.earliestResetTopicsPattern().pattern().equals(\"\")) {\n-            originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);\n-            consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n-        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}, "originalCommit": {"oid": "60ed4e27134758750c0fd24adbfba52010577327"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NTA0MA==", "bodyText": "If users specify an in-code overwrite via Consumed the user might do this only for some topics. For all other topics, the configures reset policy should be used. Hence, we just \"remember\" the default config in originalReset and set the consumer config to \"none\".\nIf there is no in-code overwrite, we don't need to remember the original strategy because all topics use the same strategy and thus we rely on the consumer anyway to do the reset for us.\nWith this change, we now always set the reset policy to \"none\". What is still possible of course, but it implies, we never rely on the consumer any longer to do any reset and we always to it manually. (This might be \"cleaner\" as it might be easier to reason about.) -- But the old code was correct, too.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452995040", "createdAt": "2020-07-10T18:06:12Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -359,11 +359,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n         consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);\n         final AtomicLong nextScheduledRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n         consumerConfigs.put(StreamsConfig.InternalConfig.NEXT_SCHEDULED_REBALANCE_MS, nextScheduledRebalanceMs);\n-        String originalReset = null;\n-        if (!builder.latestResetTopicsPattern().pattern().equals(\"\") || !builder.earliestResetTopicsPattern().pattern().equals(\"\")) {\n-            originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);\n-            consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n-        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}, "originalCommit": {"oid": "60ed4e27134758750c0fd24adbfba52010577327"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NTc3MQ==", "bodyText": "I guess the main reason for this old design was, that user see their config when ConsumerConfig is logged. -- Now, we would always log \"none\" independent what the users sets, and this might be confusing.\nI think we should update the docs accordingly and call it out.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452995771", "createdAt": "2020-07-10T18:07:50Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -359,11 +359,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n         consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);\n         final AtomicLong nextScheduledRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n         consumerConfigs.put(StreamsConfig.InternalConfig.NEXT_SCHEDULED_REBALANCE_MS, nextScheduledRebalanceMs);\n-        String originalReset = null;\n-        if (!builder.latestResetTopicsPattern().pattern().equals(\"\") || !builder.earliestResetTopicsPattern().pattern().equals(\"\")) {\n-            originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);\n-            consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n-        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}, "originalCommit": {"oid": "60ed4e27134758750c0fd24adbfba52010577327"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4NjczNA==", "bodyText": "Aha! Thanks, @mjsax. This makes sense. I hadn't considered that the consumer never throws InvalidOffsetException if there's a valid policy configured, so the prior logic only needs the fallback when there's an override.\nNow that we also use the resetting logic for our own \"manual\" reset when handling TaskCorruptedException, we do need to capture the Consumer config value, but we can still enforce that if there are any overrides in the topology, we set the consumer to \"none\". Just updated the code to do this.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453086734", "createdAt": "2020-07-10T21:32:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -359,11 +359,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n         consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);\n         final AtomicLong nextScheduledRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n         consumerConfigs.put(StreamsConfig.InternalConfig.NEXT_SCHEDULED_REBALANCE_MS, nextScheduledRebalanceMs);\n-        String originalReset = null;\n-        if (!builder.latestResetTopicsPattern().pattern().equals(\"\") || !builder.earliestResetTopicsPattern().pattern().equals(\"\")) {\n-            originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);\n-            consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n-        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}, "originalCommit": {"oid": "60ed4e27134758750c0fd24adbfba52010577327"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNDUxMjcxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxODowOToxOFrOGwAt1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMTozNDo1OFrOGwGRBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NjU2NA==", "bodyText": "This method is a one-liner now and is only called in a single place IIRC. Maybe better to remove the method and embed the call?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452996564", "createdAt": "2020-07-10T18:09:18Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -767,38 +766,65 @@ void runOnce() {\n     }\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568dceb5f61c71428432d23d947c5f9b29fb7bfb"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4NzQ5Mw==", "bodyText": "Good catch! I'll inline it.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453087493", "createdAt": "2020-07-10T21:34:58Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -767,38 +766,65 @@ void runOnce() {\n     }\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NjU2NA=="}, "originalCommit": {"oid": "568dceb5f61c71428432d23d947c5f9b29fb7bfb"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTIxMTAwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMjozMTozMVrOGwHYjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDozMzo1OVrOGwI9lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ==", "bodyText": "We we need this isEmpty check? What happens is we blindly pass an empty set into seekToBeginning? -- Similar for seekToEnd() below?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453105805", "createdAt": "2020-07-10T22:31:31Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -760,44 +763,62 @@ void runOnce() {\n         try {\n             records = mainConsumer.poll(pollTime);\n         } catch (final InvalidOffsetException e) {\n-            resetInvalidOffsets(e);\n+            resetOffsets(e.partitions(), e);\n         }\n \n         return records;\n     }\n \n-    private void resetInvalidOffsets(final InvalidOffsetException e) {\n-        final Set<TopicPartition> partitions = e.partitions();\n+    private void resetOffsets(final Set<TopicPartition> partitions, final Exception cause) {\n         final Set<String> loggedTopics = new HashSet<>();\n         final Set<TopicPartition> seekToBeginning = new HashSet<>();\n         final Set<TopicPartition> seekToEnd = new HashSet<>();\n+        final Set<TopicPartition> notReset = new HashSet<>();\n \n         for (final TopicPartition partition : partitions) {\n             if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToBeginning, \"Setting topic '{}' to consume from {} offset\", \"earliest\", loggedTopics);\n             } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToEnd, \"Setting topic '{}' to consume from {} offset\", \"latest\", loggedTopics);\n             } else {\n-                if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n-                    final String errorMessage = \"No valid committed offset found for input topic %s (partition %s) and no valid reset policy configured.\" +\n-                        \" You need to set configuration parameter \\\"auto.offset.reset\\\" or specify a topic specific reset \" +\n-                        \"policy via StreamsBuilder#stream(..., Consumed.with(Topology.AutoOffsetReset)) or StreamsBuilder#table(..., Consumed.with(Topology.AutoOffsetReset))\";\n-                    throw new StreamsException(String.format(errorMessage, partition.topic(), partition.partition()), e);\n-                }\n-\n-                if (originalReset.equals(\"earliest\")) {\n+                if (\"earliest\".equals(originalReset)) {\n                     addToResetList(partition, seekToBeginning, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"earliest\", loggedTopics);\n-                } else { // can only be \"latest\"\n+                } else if (\"latest\".equals(originalReset)) {\n                     addToResetList(partition, seekToEnd, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"latest\", loggedTopics);\n+                } else {\n+                    notReset.add(partition);\n                 }\n             }\n         }\n \n-        if (!seekToBeginning.isEmpty()) {\n-            mainConsumer.seekToBeginning(seekToBeginning);\n-        }\n-        if (!seekToEnd.isEmpty()) {\n-            mainConsumer.seekToEnd(seekToEnd);\n+        if (notReset.isEmpty()) {\n+            if (!seekToBeginning.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNzcwMA==", "bodyText": "Huh, I didn't wonder that before, but ... dear god. From the javadoc on KafkaConsumer:\n\nIf no partitions are provided, seek to the first offset for all of the currently assigned partitions.\n\nWhat a dangerous API!", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453107700", "createdAt": "2020-07-10T22:39:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -760,44 +763,62 @@ void runOnce() {\n         try {\n             records = mainConsumer.poll(pollTime);\n         } catch (final InvalidOffsetException e) {\n-            resetInvalidOffsets(e);\n+            resetOffsets(e.partitions(), e);\n         }\n \n         return records;\n     }\n \n-    private void resetInvalidOffsets(final InvalidOffsetException e) {\n-        final Set<TopicPartition> partitions = e.partitions();\n+    private void resetOffsets(final Set<TopicPartition> partitions, final Exception cause) {\n         final Set<String> loggedTopics = new HashSet<>();\n         final Set<TopicPartition> seekToBeginning = new HashSet<>();\n         final Set<TopicPartition> seekToEnd = new HashSet<>();\n+        final Set<TopicPartition> notReset = new HashSet<>();\n \n         for (final TopicPartition partition : partitions) {\n             if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToBeginning, \"Setting topic '{}' to consume from {} offset\", \"earliest\", loggedTopics);\n             } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToEnd, \"Setting topic '{}' to consume from {} offset\", \"latest\", loggedTopics);\n             } else {\n-                if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n-                    final String errorMessage = \"No valid committed offset found for input topic %s (partition %s) and no valid reset policy configured.\" +\n-                        \" You need to set configuration parameter \\\"auto.offset.reset\\\" or specify a topic specific reset \" +\n-                        \"policy via StreamsBuilder#stream(..., Consumed.with(Topology.AutoOffsetReset)) or StreamsBuilder#table(..., Consumed.with(Topology.AutoOffsetReset))\";\n-                    throw new StreamsException(String.format(errorMessage, partition.topic(), partition.partition()), e);\n-                }\n-\n-                if (originalReset.equals(\"earliest\")) {\n+                if (\"earliest\".equals(originalReset)) {\n                     addToResetList(partition, seekToBeginning, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"earliest\", loggedTopics);\n-                } else { // can only be \"latest\"\n+                } else if (\"latest\".equals(originalReset)) {\n                     addToResetList(partition, seekToEnd, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"latest\", loggedTopics);\n+                } else {\n+                    notReset.add(partition);\n                 }\n             }\n         }\n \n-        if (!seekToBeginning.isEmpty()) {\n-            mainConsumer.seekToBeginning(seekToBeginning);\n-        }\n-        if (!seekToEnd.isEmpty()) {\n-            mainConsumer.seekToEnd(seekToEnd);\n+        if (notReset.isEmpty()) {\n+            if (!seekToBeginning.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ=="}, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMTU1MQ==", "bodyText": "Just verified that we never call it unguarded by a non-empty check.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453121551", "createdAt": "2020-07-10T23:35:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -760,44 +763,62 @@ void runOnce() {\n         try {\n             records = mainConsumer.poll(pollTime);\n         } catch (final InvalidOffsetException e) {\n-            resetInvalidOffsets(e);\n+            resetOffsets(e.partitions(), e);\n         }\n \n         return records;\n     }\n \n-    private void resetInvalidOffsets(final InvalidOffsetException e) {\n-        final Set<TopicPartition> partitions = e.partitions();\n+    private void resetOffsets(final Set<TopicPartition> partitions, final Exception cause) {\n         final Set<String> loggedTopics = new HashSet<>();\n         final Set<TopicPartition> seekToBeginning = new HashSet<>();\n         final Set<TopicPartition> seekToEnd = new HashSet<>();\n+        final Set<TopicPartition> notReset = new HashSet<>();\n \n         for (final TopicPartition partition : partitions) {\n             if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToBeginning, \"Setting topic '{}' to consume from {} offset\", \"earliest\", loggedTopics);\n             } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToEnd, \"Setting topic '{}' to consume from {} offset\", \"latest\", loggedTopics);\n             } else {\n-                if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n-                    final String errorMessage = \"No valid committed offset found for input topic %s (partition %s) and no valid reset policy configured.\" +\n-                        \" You need to set configuration parameter \\\"auto.offset.reset\\\" or specify a topic specific reset \" +\n-                        \"policy via StreamsBuilder#stream(..., Consumed.with(Topology.AutoOffsetReset)) or StreamsBuilder#table(..., Consumed.with(Topology.AutoOffsetReset))\";\n-                    throw new StreamsException(String.format(errorMessage, partition.topic(), partition.partition()), e);\n-                }\n-\n-                if (originalReset.equals(\"earliest\")) {\n+                if (\"earliest\".equals(originalReset)) {\n                     addToResetList(partition, seekToBeginning, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"earliest\", loggedTopics);\n-                } else { // can only be \"latest\"\n+                } else if (\"latest\".equals(originalReset)) {\n                     addToResetList(partition, seekToEnd, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"latest\", loggedTopics);\n+                } else {\n+                    notReset.add(partition);\n                 }\n             }\n         }\n \n-        if (!seekToBeginning.isEmpty()) {\n-            mainConsumer.seekToBeginning(seekToBeginning);\n-        }\n-        if (!seekToEnd.isEmpty()) {\n-            mainConsumer.seekToEnd(seekToEnd);\n+        if (notReset.isEmpty()) {\n+            if (!seekToBeginning.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ=="}, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMTY3MQ==", "bodyText": "Awesome! Thanks for double checking!", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453131671", "createdAt": "2020-07-11T00:33:59Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -760,44 +763,62 @@ void runOnce() {\n         try {\n             records = mainConsumer.poll(pollTime);\n         } catch (final InvalidOffsetException e) {\n-            resetInvalidOffsets(e);\n+            resetOffsets(e.partitions(), e);\n         }\n \n         return records;\n     }\n \n-    private void resetInvalidOffsets(final InvalidOffsetException e) {\n-        final Set<TopicPartition> partitions = e.partitions();\n+    private void resetOffsets(final Set<TopicPartition> partitions, final Exception cause) {\n         final Set<String> loggedTopics = new HashSet<>();\n         final Set<TopicPartition> seekToBeginning = new HashSet<>();\n         final Set<TopicPartition> seekToEnd = new HashSet<>();\n+        final Set<TopicPartition> notReset = new HashSet<>();\n \n         for (final TopicPartition partition : partitions) {\n             if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToBeginning, \"Setting topic '{}' to consume from {} offset\", \"earliest\", loggedTopics);\n             } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToEnd, \"Setting topic '{}' to consume from {} offset\", \"latest\", loggedTopics);\n             } else {\n-                if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n-                    final String errorMessage = \"No valid committed offset found for input topic %s (partition %s) and no valid reset policy configured.\" +\n-                        \" You need to set configuration parameter \\\"auto.offset.reset\\\" or specify a topic specific reset \" +\n-                        \"policy via StreamsBuilder#stream(..., Consumed.with(Topology.AutoOffsetReset)) or StreamsBuilder#table(..., Consumed.with(Topology.AutoOffsetReset))\";\n-                    throw new StreamsException(String.format(errorMessage, partition.topic(), partition.partition()), e);\n-                }\n-\n-                if (originalReset.equals(\"earliest\")) {\n+                if (\"earliest\".equals(originalReset)) {\n                     addToResetList(partition, seekToBeginning, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"earliest\", loggedTopics);\n-                } else { // can only be \"latest\"\n+                } else if (\"latest\".equals(originalReset)) {\n                     addToResetList(partition, seekToEnd, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"latest\", loggedTopics);\n+                } else {\n+                    notReset.add(partition);\n                 }\n             }\n         }\n \n-        if (!seekToBeginning.isEmpty()) {\n-            mainConsumer.seekToBeginning(seekToBeginning);\n-        }\n-        if (!seekToEnd.isEmpty()) {\n-            mainConsumer.seekToEnd(seekToEnd);\n+        if (notReset.isEmpty()) {\n+            if (!seekToBeginning.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ=="}, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTIxNjM5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMjozNDozNVrOGwHbrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMjo0MDoyOFrOGwHhPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNjYwNg==", "bodyText": "For my own education: when does the actual resume() happen (and are we sure those partitions are resumed later?)", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453106606", "createdAt": "2020-07-10T22:34:35Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +195,34 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> taskInputPartitions = task.inputPartitions();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    intersection(HashSet::new, currentAssignment, taskInputPartitions);\n+                if (!assignedToPauseAndReset.equals(taskInputPartitions)) {\n+                    log.warn(\n+                        \"Expected the current consumer assignment {} to contain the input partitions {}. \" +\n+                            \"Will proceed to recover.\",\n+                        currentAssignment,\n+                        taskInputPartitions\n+                    );\n+                }\n+\n+                mainConsumer().pause(assignedToPauseAndReset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODAzMA==", "bodyText": "Good question!\nIt goes through the \"restoration\" phase, at the end of which, we resume the main consumer's partitions.\nIn TaskManager:\n        if (allRunning) {\n            // we can call resume multiple times since it is idempotent.\n            mainConsumer.resume(mainConsumer.assignment());\n        }", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453108030", "createdAt": "2020-07-10T22:40:28Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +195,34 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> taskInputPartitions = task.inputPartitions();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    intersection(HashSet::new, currentAssignment, taskInputPartitions);\n+                if (!assignedToPauseAndReset.equals(taskInputPartitions)) {\n+                    log.warn(\n+                        \"Expected the current consumer assignment {} to contain the input partitions {}. \" +\n+                            \"Will proceed to recover.\",\n+                        currentAssignment,\n+                        taskInputPartitions\n+                    );\n+                }\n+\n+                mainConsumer().pause(assignedToPauseAndReset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNjYwNg=="}, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTIzNzQyOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMjo0NzoxOFrOGwHn7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwNDo1NDowMVrOGwKbKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc0MQ==", "bodyText": "Do we need the sink?", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453109741", "createdAt": "2020-07-10T22:47:18Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "diffHunk": "@@ -1177,6 +1180,109 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerGotFencedInCommi\n         assertEquals(1, thread.activeTasks().size());\n     }\n \n+    @Test\n+    public void shouldReinitializeRevivedTasksInAnyState() {\n+        final StreamThread thread = createStreamThread(CLIENT_ID, new StreamsConfig(configProps(false)), false);\n+\n+        final String storeName = \"store\";\n+        final String storeChangelog = \"stream-thread-test-store-changelog\";\n+        final TopicPartition storeChangelogTopicPartition = new TopicPartition(storeChangelog, 1);\n+\n+        internalTopologyBuilder.addSource(null, \"name\", null, null, null, topic1);\n+        final AtomicBoolean shouldThrow = new AtomicBoolean(false);\n+        final AtomicBoolean processed = new AtomicBoolean(false);\n+        internalTopologyBuilder.addProcessor(\"proc\", new ProcessorSupplier<Object, Object>() {\n+            @Override\n+            public Processor<Object, Object> get() {\n+                return new Processor<Object, Object>() {\n+                    private ProcessorContext context;\n+\n+                    @Override\n+                    public void init(final ProcessorContext context) {\n+                        this.context = context;\n+                    }\n+\n+                    @Override\n+                    public void process(final Object key, final Object value) {\n+                        if (shouldThrow.get()) {\n+                            throw new TaskCorruptedException(singletonMap(task1, new HashSet<TopicPartition>(singleton(storeChangelogTopicPartition))));\n+                        } else {\n+                            processed.set(true);\n+                        }\n+                    }\n+\n+                    @Override\n+                    public void close() {\n+\n+                    }\n+                };\n+            }\n+        }, \"name\");\n+        internalTopologyBuilder.addStateStore(\n+            Stores.keyValueStoreBuilder(\n+                Stores.persistentKeyValueStore(storeName),\n+                Serdes.String(),\n+                Serdes.String()\n+            ),\n+            \"proc\"\n+        );\n+        internalTopologyBuilder.addSink(\"out\", \"output\", null, null, null, \"proc\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMTA5Mw==", "bodyText": "Uh, no. It was just copied with the rest of the test. Sorry about that.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453121093", "createdAt": "2020-07-10T23:33:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "diffHunk": "@@ -1177,6 +1180,109 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerGotFencedInCommi\n         assertEquals(1, thread.activeTasks().size());\n     }\n \n+    @Test\n+    public void shouldReinitializeRevivedTasksInAnyState() {\n+        final StreamThread thread = createStreamThread(CLIENT_ID, new StreamsConfig(configProps(false)), false);\n+\n+        final String storeName = \"store\";\n+        final String storeChangelog = \"stream-thread-test-store-changelog\";\n+        final TopicPartition storeChangelogTopicPartition = new TopicPartition(storeChangelog, 1);\n+\n+        internalTopologyBuilder.addSource(null, \"name\", null, null, null, topic1);\n+        final AtomicBoolean shouldThrow = new AtomicBoolean(false);\n+        final AtomicBoolean processed = new AtomicBoolean(false);\n+        internalTopologyBuilder.addProcessor(\"proc\", new ProcessorSupplier<Object, Object>() {\n+            @Override\n+            public Processor<Object, Object> get() {\n+                return new Processor<Object, Object>() {\n+                    private ProcessorContext context;\n+\n+                    @Override\n+                    public void init(final ProcessorContext context) {\n+                        this.context = context;\n+                    }\n+\n+                    @Override\n+                    public void process(final Object key, final Object value) {\n+                        if (shouldThrow.get()) {\n+                            throw new TaskCorruptedException(singletonMap(task1, new HashSet<TopicPartition>(singleton(storeChangelogTopicPartition))));\n+                        } else {\n+                            processed.set(true);\n+                        }\n+                    }\n+\n+                    @Override\n+                    public void close() {\n+\n+                    }\n+                };\n+            }\n+        }, \"name\");\n+        internalTopologyBuilder.addStateStore(\n+            Stores.keyValueStoreBuilder(\n+                Stores.persistentKeyValueStore(storeName),\n+                Serdes.String(),\n+                Serdes.String()\n+            ),\n+            \"proc\"\n+        );\n+        internalTopologyBuilder.addSink(\"out\", \"output\", null, null, null, \"proc\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc0MQ=="}, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE1NTYyNA==", "bodyText": "fixed this, since I needed more commits anyway.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453155624", "createdAt": "2020-07-11T04:54:01Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "diffHunk": "@@ -1177,6 +1180,109 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerGotFencedInCommi\n         assertEquals(1, thread.activeTasks().size());\n     }\n \n+    @Test\n+    public void shouldReinitializeRevivedTasksInAnyState() {\n+        final StreamThread thread = createStreamThread(CLIENT_ID, new StreamsConfig(configProps(false)), false);\n+\n+        final String storeName = \"store\";\n+        final String storeChangelog = \"stream-thread-test-store-changelog\";\n+        final TopicPartition storeChangelogTopicPartition = new TopicPartition(storeChangelog, 1);\n+\n+        internalTopologyBuilder.addSource(null, \"name\", null, null, null, topic1);\n+        final AtomicBoolean shouldThrow = new AtomicBoolean(false);\n+        final AtomicBoolean processed = new AtomicBoolean(false);\n+        internalTopologyBuilder.addProcessor(\"proc\", new ProcessorSupplier<Object, Object>() {\n+            @Override\n+            public Processor<Object, Object> get() {\n+                return new Processor<Object, Object>() {\n+                    private ProcessorContext context;\n+\n+                    @Override\n+                    public void init(final ProcessorContext context) {\n+                        this.context = context;\n+                    }\n+\n+                    @Override\n+                    public void process(final Object key, final Object value) {\n+                        if (shouldThrow.get()) {\n+                            throw new TaskCorruptedException(singletonMap(task1, new HashSet<TopicPartition>(singleton(storeChangelogTopicPartition))));\n+                        } else {\n+                            processed.set(true);\n+                        }\n+                    }\n+\n+                    @Override\n+                    public void close() {\n+\n+                    }\n+                };\n+            }\n+        }, \"name\");\n+        internalTopologyBuilder.addStateStore(\n+            Stores.keyValueStoreBuilder(\n+                Stores.persistentKeyValueStore(storeName),\n+                Serdes.String(),\n+                Serdes.String()\n+            ),\n+            \"proc\"\n+        );\n+        internalTopologyBuilder.addSink(\"out\", \"output\", null, null, null, \"proc\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc0MQ=="}, "originalCommit": {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTU4OTIwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwNDo1ODowMlrOGwKcbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxODozODo1N1rOGwOgew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE1NTk0OQ==", "bodyText": "The flaky test was related to us going into this block from other states. I finally got a clue when one of the tests failed on \"invalid transition from PARTITIONS_REVOKED to RUNNING\". I'm not sure how, exactly, but I think the shutdown test that failed on ConcurrentModificationException was also related, probably due to the test invoking the handleAssignment/Revocation/Lost methods from a different thread (which can normally never happen).\nAnyway, my prior code only intended to add the self transition, but failed to make sure we were actually in a self-transition. It's fixed now.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453155949", "createdAt": "2020-07-11T04:58:02Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -648,7 +651,9 @@ void runOnce() {\n \n         // only try to initialize the assigned tasks\n         // if the state is still in PARTITION_ASSIGNED after the poll call\n-        if (state == State.PARTITIONS_ASSIGNED) {\n+        if (state == State.PARTITIONS_ASSIGNED\n+            || state == State.RUNNING && taskManager.needsInitializationOrRestoration()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a8fbcc37a3f9881be49852b8a300b046a90a920"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIyMjUyMw==", "bodyText": "Thanks. The fix makes sense to me.", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453222523", "createdAt": "2020-07-11T18:38:57Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -648,7 +651,9 @@ void runOnce() {\n \n         // only try to initialize the assigned tasks\n         // if the state is still in PARTITION_ASSIGNED after the poll call\n-        if (state == State.PARTITIONS_ASSIGNED) {\n+        if (state == State.PARTITIONS_ASSIGNED\n+            || state == State.RUNNING && taskManager.needsInitializationOrRestoration()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE1NTk0OQ=="}, "originalCommit": {"oid": "7a8fbcc37a3f9881be49852b8a300b046a90a920"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2193, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}