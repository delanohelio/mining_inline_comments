{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM2NTI5NjEy", "number": 9732, "title": "KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel", "bodyText": "This patch contains the following improvements:\n\nSeparate inbound/outbound request flows so that we can open the door for concurrent inbound request handling\nRewrite KafkaNetworkChannel to use InterBrokerSendThread which fixes a number of bugs/shortcomings\nGet rid of a lot of boilerplate conversions in KafkaNetworkChannel\nImprove validation of inbound responses in KafkaRaftClient by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-12-11T05:45:43Z", "url": "https://github.com/apache/kafka/pull/9732", "merged": true, "mergeCommit": {"oid": "eb9fe411bbbfca1baf5e0900ee9cfb9147e5099e"}, "closed": true, "closedAt": "2020-12-22T02:15:16Z", "author": {"login": "hachikuji"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdlMUh1gFqTU1MDQzODg4MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdocLO4gBqjQxMzc1NjMzNDI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNDM4ODgx", "url": "https://github.com/apache/kafka/pull/9732#pullrequestreview-550438881", "createdAt": "2020-12-11T18:33:42Z", "commit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODozMzo0M1rOIEE_jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODozMzo0M1rOIEE_jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE0NzAyMw==", "bodyText": "It seems like there are many classes where direct access to data is not actually needed. How do you feel about having a public method in RequestUtils that exposes data for the raft layer instead? Something like:\npublic static ApiMessage requestData(AbstractRequest req)\npublic static ApiMessage responseData(AbstractResponse resp)\nOr is it not worth it?", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541147023", "createdAt": "2020-12-11T18:33:43Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnRequest.java", "diffHunk": "@@ -53,7 +53,7 @@ public AddOffsetsToTxnRequest(AddOffsetsToTxnRequestData data, short version) {\n     }\n \n     @Override\n-    protected AddOffsetsToTxnRequestData data() {\n+    public AddOffsetsToTxnRequestData data() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/6533921245e4f7d858dd186b77e4130703fa2de1", "committedDate": "2020-12-11T06:26:53Z", "message": "Fix unused import"}, "afterCommit": {"oid": "0a8358916e73fdf07a8c79a71956e7905a1f96f0", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/0a8358916e73fdf07a8c79a71956e7905a1f96f0", "committedDate": "2020-12-11T19:38:29Z", "message": "Add batch send api to `InterBrokerSendThread`"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0a8358916e73fdf07a8c79a71956e7905a1f96f0", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/0a8358916e73fdf07a8c79a71956e7905a1f96f0", "committedDate": "2020-12-11T19:38:29Z", "message": "Add batch send api to `InterBrokerSendThread`"}, "afterCommit": {"oid": "b5e476b1a144aa8582b2738260c8ea32c89f71e9", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/b5e476b1a144aa8582b2738260c8ea32c89f71e9", "committedDate": "2020-12-11T19:41:10Z", "message": "Add batch send api to `InterBrokerSendThread`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNDcwMzk3", "url": "https://github.com/apache/kafka/pull/9732#pullrequestreview-550470397", "createdAt": "2020-12-11T18:51:14Z", "commit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODo1MToxNFrOIEFlHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxOTo0NzozN1rOIEJHIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE1NjYzNg==", "bodyText": "Similar to above comment, either check for null or use remove()", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541156636", "createdAt": "2020-12-11T18:51:14Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -51,22 +53,30 @@ abstract class InterBrokerSendThread(name: String,\n     awaitShutdown()\n   }\n \n-  override def doWork(): Unit = {\n-    var now = time.milliseconds()\n+  def sendRequest(request: RequestAndCompletionHandler): Unit = {\n+    inboundQueue.offer(request)\n+    wakeup()\n+  }\n \n-    generateRequests().foreach { request =>\n+  private def drainInboundQueue(): Unit = {\n+    while (!inboundQueue.isEmpty) {\n+      val request = inboundQueue.poll()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE2MTQ0MQ==", "bodyText": "Just wondering, is there any reason why we might want a bounded queue here? I suspect not since we never expect too many requests to be enqueued at once.", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541161441", "createdAt": "2020-12-11T18:58:18Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -32,17 +33,18 @@ import scala.jdk.CollectionConverters._\n /**\n  *  Class for inter-broker send thread that utilize a non-blocking network client.\n  */\n-abstract class InterBrokerSendThread(name: String,\n-                                     networkClient: KafkaClient,\n-                                     time: Time,\n-                                     isInterruptible: Boolean = true)\n-  extends ShutdownableThread(name, isInterruptible) {\n-\n-  def generateRequests(): Iterable[RequestAndCompletionHandler]\n-  def requestTimeoutMs: Int\n+class InterBrokerSendThread(\n+  name: String,\n+  networkClient: KafkaClient,\n+  requestTimeoutMs: Int,\n+  time: Time,\n+  isInterruptible: Boolean = true\n+) extends ShutdownableThread(name, isInterruptible) {\n+\n+  private val inboundQueue = new ConcurrentLinkedQueue[RequestAndCompletionHandler]()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE2NTk2Mw==", "bodyText": "We should probably either check the result of offer or use BlockingQueue#add instead. Since we're using an unbounded queue and never expect this to fail, I would lean towards add", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541165963", "createdAt": "2020-12-11T19:02:23Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -51,22 +53,30 @@ abstract class InterBrokerSendThread(name: String,\n     awaitShutdown()\n   }\n \n-  override def doWork(): Unit = {\n-    var now = time.milliseconds()\n+  def sendRequest(request: RequestAndCompletionHandler): Unit = {\n+    inboundQueue.offer(request)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE3NzMzNw==", "bodyText": "Previously, all the requests we gathered from generateRequests would have the same timestamp which was also passed to the subsequent call to NetworkClient#ready and send (via sendRequests).\nWhat's the reason for recomputing the timestamp for each request we create?\nShould we get a newer timestamp for the call to NetworkClient?\nSeems a little weird to create requests at t1, t2, etc and then call NetworkClient.send with t0. I wonder if this would have weird throttling side effects.", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541177337", "createdAt": "2020-12-11T19:11:10Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -51,22 +53,30 @@ abstract class InterBrokerSendThread(name: String,\n     awaitShutdown()\n   }\n \n-  override def doWork(): Unit = {\n-    var now = time.milliseconds()\n+  def sendRequest(request: RequestAndCompletionHandler): Unit = {\n+    inboundQueue.offer(request)\n+    wakeup()\n+  }\n \n-    generateRequests().foreach { request =>\n+  private def drainInboundQueue(): Unit = {\n+    while (!inboundQueue.isEmpty) {\n+      val request = inboundQueue.poll()\n       val completionHandler = request.handler\n       unsentRequests.put(request.destination,\n         networkClient.newClientRequest(\n           request.destination.idString,\n           request.request,\n-          now,\n+          time.milliseconds(),\n           true,\n           requestTimeoutMs,\n           completionHandler))\n     }\n+  }\n \n+  override def doWork(): Unit = {\n     try {\n+      var now = time.milliseconds()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE4NjU0MA==", "bodyText": "Is this just for testing?", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541186540", "createdAt": "2020-12-11T19:18:29Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala", "diffHunk": "@@ -68,179 +51,93 @@ object KafkaNetworkChannel {\n     }\n   }\n \n-  private[raft] def responseData(response: AbstractResponse): ApiMessage = {\n-    response match {\n-      case voteResponse: VoteResponse => voteResponse.data\n-      case beginEpochResponse: BeginQuorumEpochResponse => beginEpochResponse.data\n-      case endEpochResponse: EndQuorumEpochResponse => endEpochResponse.data\n-      case fetchResponse: FetchResponse[_] => fetchResponse.data\n-      case _ => throw new IllegalArgumentException(s\"Unexpected type for response: $response\")\n-    }\n-  }\n-\n-  private[raft] def requestData(request: AbstractRequest): ApiMessage = {\n-    request match {\n-      case voteRequest: VoteRequest => voteRequest.data\n-      case beginEpochRequest: BeginQuorumEpochRequest => beginEpochRequest.data\n-      case endEpochRequest: EndQuorumEpochRequest => endEpochRequest.data\n-      case fetchRequest: FetchRequest => fetchRequest.data\n-      case _ => throw new IllegalArgumentException(s\"Unexpected type for request: $request\")\n-    }\n-  }\n-\n }\n \n-class KafkaNetworkChannel(time: Time,\n-                          client: KafkaClient,\n-                          clientId: String,\n-                          retryBackoffMs: Int,\n-                          requestTimeoutMs: Int) extends NetworkChannel with Logging {\n+class KafkaNetworkChannel(\n+  time: Time,\n+  client: KafkaClient,\n+  requestTimeoutMs: Int\n+) extends NetworkChannel with Logging {\n   import KafkaNetworkChannel._\n \n   type ResponseHandler = AbstractResponse => Unit\n \n   private val correlationIdCounter = new AtomicInteger(0)\n-  private val pendingInbound = mutable.Map.empty[Long, ResponseHandler]\n-  private val undelivered = new ArrayBlockingQueue[RaftMessage](10)\n-  private val pendingOutbound = new ArrayBlockingQueue[RaftRequest.Outbound](10)\n   private val endpoints = mutable.HashMap.empty[Int, Node]\n \n-  override def newCorrelationId(): Int = correlationIdCounter.getAndIncrement()\n-\n-  private def buildClientRequest(req: RaftRequest.Outbound): ClientRequest = {\n-    val destination = req.destinationId.toString\n-    val request = buildRequest(req.data)\n-    val correlationId = req.correlationId\n-    val createdTimeMs = req.createdTimeMs\n-    new ClientRequest(destination, request, correlationId, clientId, createdTimeMs, true,\n-      requestTimeoutMs, null)\n-  }\n-\n-  override def send(message: RaftMessage): Unit = {\n-    message match {\n-      case request: RaftRequest.Outbound =>\n-        if (!pendingOutbound.offer(request))\n-          throw new KafkaException(\"Pending outbound queue is full\")\n-\n-      case response: RaftResponse.Outbound =>\n-        pendingInbound.remove(response.correlationId).foreach { onResponseReceived: ResponseHandler =>\n-          onResponseReceived(buildResponse(response.data))\n-        }\n-      case _ =>\n-        throw new IllegalArgumentException(\"Unhandled message type \" + message)\n+  private val requestThread = new InterBrokerSendThread(\n+    name = \"raft-outbound-request-thread\",\n+    networkClient = client,\n+    requestTimeoutMs = requestTimeoutMs,\n+    time = time,\n+    isInterruptible = false\n+  )\n+\n+  override def send(request: RaftRequest.Outbound): Unit = {\n+    def completeFuture(message: ApiMessage): Unit = {\n+      val response = new RaftResponse.Inbound(\n+        request.correlationId,\n+        message,\n+        request.destinationId\n+      )\n+      request.completion.complete(response)\n     }\n-  }\n \n-  private def sendOutboundRequests(currentTimeMs: Long): Unit = {\n-    while (!pendingOutbound.isEmpty) {\n-      val request = pendingOutbound.peek()\n-      endpoints.get(request.destinationId) match {\n-        case Some(node) =>\n-          if (client.connectionFailed(node)) {\n-            pendingOutbound.poll()\n-            val apiKey = ApiKeys.forId(request.data.apiKey)\n-            val disconnectResponse = RaftUtil.errorResponse(apiKey, Errors.BROKER_NOT_AVAILABLE)\n-            val success = undelivered.offer(new RaftResponse.Inbound(\n-              request.correlationId, disconnectResponse, request.destinationId))\n-            if (!success) {\n-              throw new KafkaException(\"Undelivered queue is full\")\n-            }\n-\n-            // Make sure to reset the connection state\n-            client.ready(node, currentTimeMs)\n-          } else if (client.ready(node, currentTimeMs)) {\n-            pendingOutbound.poll()\n-            val clientRequest = buildClientRequest(request)\n-            client.send(clientRequest, currentTimeMs)\n-          } else {\n-            // We will retry this request on the next poll\n-            return\n-          }\n-\n-        case None =>\n-          pendingOutbound.poll()\n-          val apiKey = ApiKeys.forId(request.data.apiKey)\n-          val responseData = RaftUtil.errorResponse(apiKey, Errors.BROKER_NOT_AVAILABLE)\n-          val response = new RaftResponse.Inbound(request.correlationId, responseData, request.destinationId)\n-          if (!undelivered.offer(response))\n-            throw new KafkaException(\"Undelivered queue is full\")\n+    def onComplete(clientResponse: ClientResponse): Unit = {\n+      val response = if (clientResponse.authenticationException != null) {\n+        errorResponse(request.data, Errors.CLUSTER_AUTHORIZATION_FAILED)\n+      } else if (clientResponse.wasDisconnected()) {\n+        errorResponse(request.data, Errors.BROKER_NOT_AVAILABLE)\n+      } else {\n+        clientResponse.responseBody.data\n       }\n+      completeFuture(response)\n     }\n-  }\n-\n-  def getConnectionInfo(nodeId: Int): Node = {\n-    if (!endpoints.contains(nodeId))\n-      null\n-    else\n-      endpoints(nodeId)\n-  }\n-\n-  def allConnections(): Set[Node] = {\n-    endpoints.values.toSet\n-  }\n \n-  private def buildInboundRaftResponse(response: ClientResponse): RaftResponse.Inbound = {\n-    val header = response.requestHeader()\n-    val data = if (response.authenticationException != null) {\n-      RaftUtil.errorResponse(header.apiKey, Errors.CLUSTER_AUTHORIZATION_FAILED)\n-    } else if (response.wasDisconnected) {\n-      RaftUtil.errorResponse(header.apiKey, Errors.BROKER_NOT_AVAILABLE)\n-    } else {\n-      responseData(response.responseBody)\n-    }\n-    new RaftResponse.Inbound(header.correlationId, data, response.destination.toInt)\n-  }\n+    endpoints.get(request.destinationId) match {\n+      case Some(node) =>\n+        requestThread.sendRequest(RequestAndCompletionHandler(\n+          destination = node,\n+          request = buildRequest(request.data),\n+          handler = onComplete\n+        ))\n \n-  private def pollInboundResponses(timeoutMs: Long, inboundMessages: util.List[RaftMessage]): Unit = {\n-    val responses = client.poll(timeoutMs, time.milliseconds())\n-    for (response <- responses.asScala) {\n-      inboundMessages.add(buildInboundRaftResponse(response))\n+      case None =>\n+        completeFuture(errorResponse(request.data, Errors.BROKER_NOT_AVAILABLE))\n     }\n   }\n \n-  private def drainInboundRequests(inboundMessages: util.List[RaftMessage]): Unit = {\n-    undelivered.drainTo(inboundMessages)\n+  def pollOnce(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE5MjI5OQ==", "bodyText": "Not needed here, but I wonder if we should just make this return an Option[RequestAndCompletionHandler]", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541192299", "createdAt": "2020-12-11T19:24:35Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -164,13 +164,11 @@ class BrokerToControllerRequestThread(networkClient: KafkaClient,\n                                       listenerName: ListenerName,\n                                       time: Time,\n                                       threadName: String)\n-  extends InterBrokerSendThread(threadName, networkClient, time, isInterruptible = false) {\n+  extends InterBrokerSendThread(threadName, networkClient, config.controllerSocketTimeoutMs, time, isInterruptible = false) {\n \n   private var activeController: Option[Node] = None\n \n-  override def requestTimeoutMs: Int = config.controllerSocketTimeoutMs\n-\n-  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+  def generateRequests(): Iterable[RequestAndCompletionHandler] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6533921245e4f7d858dd186b77e4130703fa2de1"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTIxNDQ5Nw==", "bodyText": "Just a quick note: you had offer here before which has different behavior. It probably won't matter though, since the queue is unbounded.\n(I only noticed this because I had commented on the offer before your latest commit \ud83d\ude04 )", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541214497", "createdAt": "2020-12-11T19:47:37Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -51,22 +53,34 @@ abstract class InterBrokerSendThread(name: String,\n     awaitShutdown()\n   }\n \n-  override def doWork(): Unit = {\n-    var now = time.milliseconds()\n+  def sendRequest(request: RequestAndCompletionHandler): Unit = {\n+    sendRequests(Seq(request))\n+  }\n \n-    generateRequests().foreach { request =>\n+  def sendRequests(requests: Iterable[RequestAndCompletionHandler]): Unit = {\n+    inboundQueue.addAll(requests.asJavaCollection)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5e476b1a144aa8582b2738260c8ea32c89f71e9"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNTMxMzQ0", "url": "https://github.com/apache/kafka/pull/9732#pullrequestreview-550531344", "createdAt": "2020-12-11T19:50:16Z", "commit": {"oid": "b5e476b1a144aa8582b2738260c8ea32c89f71e9"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxOTo1MDoxNlrOIEJQ0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDowODo1MFrOIEKStw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTIxNjk3Nw==", "bodyText": "nit: indent misaligned", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541216977", "createdAt": "2020-12-11T19:50:16Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -135,7 +117,16 @@ class BrokerToControllerChannelManagerImpl(metadataCache: kafka.server.MetadataC\n       brokerToControllerListenerName, time, threadName)\n   }\n \n-  override def sendRequest(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+  /**\n+   * Send request to the controller.\n+   *\n+   * @param request         The request to be sent.\n+   * @param callback        Request completion callback.\n+   * @param retryDeadlineMs The retry deadline which will only be checked after receiving a response.\n+   *                        This means that in the worst case, the total timeout would be twice of\n+   *                        the configured timeout.\n+   */\n+  def sendRequest(request: AbstractRequest.Builder[_ <: AbstractRequest],\n                            callback: ControllerRequestCompletionHandler,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5e476b1a144aa8582b2738260c8ea32c89f71e9"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTIyNzg0OQ==", "bodyText": "nit: should add something like \"timeout was reached or #wakeup() was called\"", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541227849", "createdAt": "2020-12-11T20:02:20Z", "author": {"login": "mumrah"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftMessageQueue.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+/**\n+ * This class is used to serialize inbound requests or responses to outbound requests.\n+ * It basically just allows us to wrap a blocking queue so that we can have a mocked\n+ * implementation which does not depend on system time.\n+ *\n+ * See {@link org.apache.kafka.raft.internals.BlockingMessageQueue}.\n+ */\n+public interface RaftMessageQueue {\n+\n+    /**\n+     * Block for the arrival of a new message.\n+     *\n+     * @param timeoutMs timeout in milliseconds to wait for a new event\n+     * @return the event or null if the timeout was reached", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5e476b1a144aa8582b2738260c8ea32c89f71e9"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTIzMzg0Nw==", "bodyText": "Could you use a sentinel RaftMessage object here instead? Might simplify this class a bit. Not a big deal either way", "url": "https://github.com/apache/kafka/pull/9732#discussion_r541233847", "createdAt": "2020-12-11T20:08:50Z", "author": {"login": "mumrah"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BlockingMessageQueue.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.apache.kafka.raft.RaftMessage;\n+import org.apache.kafka.raft.RaftMessageQueue;\n+\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+public class BlockingMessageQueue implements RaftMessageQueue {\n+    private final BlockingQueue<RaftEvent> queue = new LinkedBlockingQueue<>();\n+    private final AtomicInteger size = new AtomicInteger(0);\n+\n+    @Override\n+    public RaftMessage poll(long timeoutMs) {\n+        try {\n+            RaftEvent event = queue.poll(timeoutMs, TimeUnit.MILLISECONDS);\n+            if (event instanceof MessageReceived) {\n+                size.decrementAndGet();\n+                return ((MessageReceived) event).message;\n+            } else {\n+                return null;\n+            }\n+        } catch (InterruptedException e) {\n+            throw new InterruptException(e);\n+        }\n+\n+    }\n+\n+    @Override\n+    public void offer(RaftMessage message) {\n+        queue.add(new MessageReceived(message));\n+        size.incrementAndGet();\n+    }\n+\n+    @Override\n+    public boolean isEmpty() {\n+        return size.get() == 0;\n+    }\n+\n+    @Override\n+    public void wakeup() {\n+        queue.add(Wakeup.INSTANCE);\n+    }\n+\n+    public interface RaftEvent {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5e476b1a144aa8582b2738260c8ea32c89f71e9"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNjg2NDc3", "url": "https://github.com/apache/kafka/pull/9732#pullrequestreview-552686477", "createdAt": "2020-12-15T17:16:23Z", "commit": {"oid": "19e4fbf75b95c591aabdc562d60ff4386a91db27"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoxNjoyNFrOIGWmzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzo0ODozOFrOIGYD7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzMjc0OQ==", "bodyText": "style nit/question: I think we have a mixture of argument indentation for Scala classes/methods. Do we have an established style convention for this? Normally I follow the opening paren when breaking out arguments into their own line (though I'm not sure that's correct)", "url": "https://github.com/apache/kafka/pull/9732#discussion_r543532749", "createdAt": "2020-12-15T17:16:24Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala", "diffHunk": "@@ -53,6 +54,41 @@ object KafkaNetworkChannel {\n \n }\n \n+private[raft] class RaftSendThread(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "19e4fbf75b95c591aabdc562d60ff4386a91db27"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzMzUzNw==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/9732#discussion_r543533537", "createdAt": "2020-12-15T17:17:23Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -149,29 +152,32 @@ case class BrokerToControllerQueueItem(request: AbstractRequest.Builder[_ <: Abs\n \n class BrokerToControllerRequestThread(networkClient: KafkaClient,\n                                       metadataUpdater: ManualMetadataUpdater,\n-                                      requestQueue: LinkedBlockingDeque[BrokerToControllerQueueItem],\n                                       metadataCache: kafka.server.MetadataCache,\n                                       config: KafkaConfig,\n                                       listenerName: ListenerName,\n                                       time: Time,\n                                       threadName: String)\n   extends InterBrokerSendThread(threadName, networkClient, config.controllerSocketTimeoutMs, time, isInterruptible = false) {\n \n+  private val requestQueue = new LinkedBlockingDeque[BrokerToControllerQueueItem]()\n   private var activeController: Option[Node] = None\n \n-  def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n-    val requestsToSend = new mutable.Queue[RequestAndCompletionHandler]\n-    val topRequest = requestQueue.poll()\n-    if (topRequest != null) {\n-      val request = RequestAndCompletionHandler(\n+  def enqueue(request: BrokerToControllerQueueItem): Unit = {\n+    requestQueue.add(request)\n+    if (activeController.isDefined) {\n+      wakeup()\n+    }\n+  }\n+\n+  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+    Option(requestQueue.poll()).map { queueItem =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "19e4fbf75b95c591aabdc562d60ff4386a91db27"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzNzc4MQ==", "bodyText": "Is it worth adding a size or isEmpty to UnsentRequests?", "url": "https://github.com/apache/kafka/pull/9732#discussion_r543537781", "createdAt": "2020-12-15T17:23:19Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -32,17 +32,19 @@ import scala.jdk.CollectionConverters._\n /**\n  *  Class for inter-broker send thread that utilize a non-blocking network client.\n  */\n-abstract class InterBrokerSendThread(name: String,\n-                                     networkClient: KafkaClient,\n-                                     time: Time,\n-                                     isInterruptible: Boolean = true)\n-  extends ShutdownableThread(name, isInterruptible) {\n+abstract class InterBrokerSendThread(\n+  name: String,\n+  networkClient: KafkaClient,\n+  requestTimeoutMs: Int,\n+  time: Time,\n+  isInterruptible: Boolean = true\n+) extends ShutdownableThread(name, isInterruptible) {\n \n-  def generateRequests(): Iterable[RequestAndCompletionHandler]\n-  def requestTimeoutMs: Int\n   private val unsentRequests = new UnsentRequests\n \n-  def hasUnsentRequests = unsentRequests.iterator().hasNext\n+  def generateRequests(): Iterable[RequestAndCompletionHandler]\n+\n+  def hasUnsentRequests: Boolean = unsentRequests.iterator().hasNext", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e36873c2ab9850cde03c0e2c6a1dea3f10293dc"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzOTk5MQ==", "bodyText": "nit: (related to style question elsewhere) if we want to change the style of these class definitions, can we do it as a separate PR? I always find it difficult when style changes are conflated with logical changes", "url": "https://github.com/apache/kafka/pull/9732#discussion_r543539991", "createdAt": "2020-12-15T17:26:24Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala", "diffHunk": "@@ -127,11 +127,14 @@ class TxnMarkerQueue(@volatile var destination: Node) {\n   def totalNumMarkers(txnTopicPartition: Int): Int = markersPerTxnTopicPartition.get(txnTopicPartition).fold(0)(_.size)\n }\n \n-class TransactionMarkerChannelManager(config: KafkaConfig,\n-                                      metadataCache: MetadataCache,\n-                                      networkClient: NetworkClient,\n-                                      txnStateManager: TransactionStateManager,\n-                                      time: Time) extends InterBrokerSendThread(\"TxnMarkerSenderThread-\" + config.brokerId, networkClient, time) with Logging with KafkaMetricsGroup {\n+class TransactionMarkerChannelManager(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e36873c2ab9850cde03c0e2c6a1dea3f10293dc"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU1NDYzMA==", "bodyText": "Hmm, this seems strange, though maybe I'm missing something.\nIf we get here, activeController is not defined. If we then pollOnce it looks like the request produced by generateRequests will get an exception since the activeController Option is empty.\nPreviously, the backoff method was just pausing the thread for some time.", "url": "https://github.com/apache/kafka/pull/9732#discussion_r543554630", "createdAt": "2020-12-15T17:46:01Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -221,7 +216,7 @@ class BrokerToControllerRequestThread(networkClient: KafkaClient,\n       } else {\n         // need to backoff to avoid tight loops\n         debug(\"No controller defined in metadata cache, retrying after backoff\")\n-        backoff()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e36873c2ab9850cde03c0e2c6a1dea3f10293dc"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU1NjM2OQ==", "bodyText": "nit: Maybe name this add so it aligns with the java.util.Queue method?", "url": "https://github.com/apache/kafka/pull/9732#discussion_r543556369", "createdAt": "2020-12-15T17:48:20Z", "author": {"login": "mumrah"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftMessageQueue.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+/**\n+ * This class is used to serialize inbound requests or responses to outbound requests.\n+ * It basically just allows us to wrap a blocking queue so that we can have a mocked\n+ * implementation which does not depend on system time.\n+ *\n+ * See {@link org.apache.kafka.raft.internals.BlockingMessageQueue}.\n+ */\n+public interface RaftMessageQueue {\n+\n+    /**\n+     * Block for the arrival of a new message.\n+     *\n+     * @param timeoutMs timeout in milliseconds to wait for a new event\n+     * @return the event or null if either the timeout was reached or there was\n+     *     a call to {@link #wakeup()} before any events became available\n+     */\n+    RaftMessage poll(long timeoutMs);\n+\n+    /**\n+     * Offer a new message to the queue.\n+     *\n+     * @param message the message to deliver\n+     * @throws IllegalStateException if the queue cannot accept the message\n+     */\n+    void offer(RaftMessage message);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e36873c2ab9850cde03c0e2c6a1dea3f10293dc"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU1NjU4OQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/9732#discussion_r543556589", "createdAt": "2020-12-15T17:48:38Z", "author": {"login": "mumrah"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BlockingMessageQueue.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.apache.kafka.common.protocol.ApiMessage;\n+import org.apache.kafka.raft.RaftMessage;\n+import org.apache.kafka.raft.RaftMessageQueue;\n+\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+public class BlockingMessageQueue implements RaftMessageQueue {\n+    private static final RaftMessage WAKEUP_MESSAGE = new RaftMessage() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e36873c2ab9850cde03c0e2c6a1dea3f10293dc"}, "originalPosition": 30}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ebb40be82b188781ae00333faed6dacf976dbfdc", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/ebb40be82b188781ae00333faed6dacf976dbfdc", "committedDate": "2020-12-15T22:20:18Z", "message": "Factor retry deadline out of `sendRequest`"}, "afterCommit": {"oid": "74cbbf86e4b8f1d35c1ae380d63841677db9d6ad", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/74cbbf86e4b8f1d35c1ae380d63841677db9d6ad", "committedDate": "2020-12-15T22:22:51Z", "message": "Factor retry deadline out of `sendRequest`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1Njg2MTYx", "url": "https://github.com/apache/kafka/pull/9732#pullrequestreview-555686161", "createdAt": "2020-12-18T17:40:43Z", "commit": {"oid": "35a8d951134fd3ee33b9c8c3ad29c4536ec19702"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0MDo0NFrOIIsYjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo1MzoyM1rOIIsyzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NjcwMw==", "bodyText": "I guess we're not sharing these channel managers between anything. In that case, moving them into the classes that need them seems fine.", "url": "https://github.com/apache/kafka/pull/9732#discussion_r545986703", "createdAt": "2020-12-18T17:40:44Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -34,25 +35,58 @@ import scala.collection.mutable\n import scala.collection.mutable.ListBuffer\n import scala.jdk.CollectionConverters._\n \n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n /**\n  * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n  * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n  */\n trait AlterIsrManager {\n-  def start(): Unit\n+  def start(): Unit = {}\n+\n+  def shutdown(): Unit = {}\n \n   def enqueue(alterIsrItem: AlterIsrItem): Boolean\n \n   def clearPending(topicPartition: TopicPartition): Unit\n }\n \n-case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+object AlterIsrManager {\n+  def apply(\n+    config: KafkaConfig,\n+    metadataCache: MetadataCache,\n+    scheduler: KafkaScheduler,\n+    time: Time,\n+    metrics: Metrics,\n+    threadNamePrefix: Option[String],\n+    brokerEpochSupplier: () => Long\n+  ): AlterIsrManager = {\n+    val channelManager = new BrokerToControllerChannelManager(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35a8d951134fd3ee33b9c8c3ad29c4536ec19702"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MzQyMw==", "bodyText": "Any reason to use Iterator here instead of queue methods (i.e., peek and remove). Is it to ensure a consistent view of the queue while we're going through it?", "url": "https://github.com/apache/kafka/pull/9732#discussion_r545993423", "createdAt": "2020-12-18T17:53:23Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -152,76 +151,89 @@ abstract class ControllerRequestCompletionHandler extends RequestCompletionHandl\n   def onTimeout(): Unit\n }\n \n-case class BrokerToControllerQueueItem(request: AbstractRequest.Builder[_ <: AbstractRequest],\n-                                       callback: ControllerRequestCompletionHandler,\n-                                       deadlineMs: Long)\n-\n-class BrokerToControllerRequestThread(networkClient: KafkaClient,\n-                                      metadataUpdater: ManualMetadataUpdater,\n-                                      requestQueue: LinkedBlockingDeque[BrokerToControllerQueueItem],\n-                                      metadataCache: kafka.server.MetadataCache,\n-                                      config: KafkaConfig,\n-                                      listenerName: ListenerName,\n-                                      time: Time,\n-                                      threadName: String)\n-  extends InterBrokerSendThread(threadName, networkClient, time, isInterruptible = false) {\n-\n+case class BrokerToControllerQueueItem(\n+  createdTimeMs: Long,\n+  request: AbstractRequest.Builder[_ <: AbstractRequest],\n+  callback: ControllerRequestCompletionHandler\n+)\n+\n+class BrokerToControllerRequestThread(\n+  networkClient: KafkaClient,\n+  metadataUpdater: ManualMetadataUpdater,\n+  metadataCache: kafka.server.MetadataCache,\n+  config: KafkaConfig,\n+  listenerName: ListenerName,\n+  time: Time,\n+  threadName: String,\n+  retryTimeoutMs: Long\n+) extends InterBrokerSendThread(threadName, networkClient, config.controllerSocketTimeoutMs, time, isInterruptible = false) {\n+\n+  private val requestQueue = new LinkedBlockingDeque[BrokerToControllerQueueItem]()\n   private var activeController: Option[Node] = None\n \n-  override def requestTimeoutMs: Int = config.controllerSocketTimeoutMs\n+  def enqueue(request: BrokerToControllerQueueItem): Unit = {\n+    requestQueue.add(request)\n+    if (activeController.isDefined) {\n+      wakeup()\n+    }\n+  }\n \n-  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n-    val requestsToSend = new mutable.Queue[RequestAndCompletionHandler]\n-    val topRequest = requestQueue.poll()\n-    if (topRequest != null) {\n-      val request = RequestAndCompletionHandler(\n-        activeController.get,\n-        topRequest.request,\n-        handleResponse(topRequest)\n-      )\n+  def queueSize: Int = {\n+    requestQueue.size\n+  }\n \n-      requestsToSend.enqueue(request)\n+  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+    val currentTimeMs = time.milliseconds()\n+    val requestIter = requestQueue.iterator()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35a8d951134fd3ee33b9c8c3ad29c4536ec19702"}, "originalPosition": 177}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ac6bb0650d4570ac052215297ab90a08bf8e7ea", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/9ac6bb0650d4570ac052215297ab90a08bf8e7ea", "committedDate": "2020-12-21T20:05:28Z", "message": "KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "065514e8e0caa9a5203a8364be11fde766b6ff0b", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/065514e8e0caa9a5203a8364be11fde766b6ff0b", "committedDate": "2020-12-21T20:05:28Z", "message": "Add batch send api to `InterBrokerSendThread`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6eb4ef252cb271b89563f78b4de64bb649094d02", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/6eb4ef252cb271b89563f78b4de64bb649094d02", "committedDate": "2020-12-21T20:07:48Z", "message": "Walk back addition of queue in `InterBrokerSendThread`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d0fd77f98411e1e28e113b04155c16ab42847879", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/d0fd77f98411e1e28e113b04155c16ab42847879", "committedDate": "2020-12-21T20:07:48Z", "message": "Use sentinel `RaftMessage` for wakeup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53cf918db43b1178751fed1eaae20378c89dd289", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/53cf918db43b1178751fed1eaae20378c89dd289", "committedDate": "2020-12-21T20:07:48Z", "message": "Fix timeout logic in `BrokerToControllerChannelManager`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2999fc7d174d24df9041ce86f1d599c2333a04b1", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/2999fc7d174d24df9041ce86f1d599c2333a04b1", "committedDate": "2020-12-21T20:16:28Z", "message": "Factor retry deadline out of `sendRequest`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67633d5b4a9b5aaa99e390e242f3139d27220d25", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/67633d5b4a9b5aaa99e390e242f3139d27220d25", "committedDate": "2020-12-21T20:16:28Z", "message": "We should use `disconnect` so that we get responses"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e113993296e77b12fe0fcbd17ec73ba4ddff52ba", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/e113993296e77b12fe0fcbd17ec73ba4ddff52ba", "committedDate": "2020-12-21T20:19:27Z", "message": "Fix startup/send bugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9574f3b1170653348a3d8f403d3b6a996ae91a3", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/e9574f3b1170653348a3d8f403d3b6a996ae91a3", "committedDate": "2020-12-21T20:19:27Z", "message": "Fix broken channel name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c31ba33f2ffb1144e0acbe975ddf45cc5d4065bc", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/c31ba33f2ffb1144e0acbe975ddf45cc5d4065bc", "committedDate": "2020-12-21T20:43:34Z", "message": "Remove start() call in ReplicaManager"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "35a8d951134fd3ee33b9c8c3ad29c4536ec19702", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/35a8d951134fd3ee33b9c8c3ad29c4536ec19702", "committedDate": "2020-12-16T19:39:00Z", "message": "Fix broken channel name"}, "afterCommit": {"oid": "c31ba33f2ffb1144e0acbe975ddf45cc5d4065bc", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/c31ba33f2ffb1144e0acbe975ddf45cc5d4065bc", "committedDate": "2020-12-21T20:43:34Z", "message": "Remove start() call in ReplicaManager"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2251, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}