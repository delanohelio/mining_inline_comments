{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzczNDk5NDU4", "number": 8088, "title": "KAFKA-9535: Update metadata upon retrying partitions for ListOffset", "bodyText": "Today if we attempt to list offsets with a fenced leader epoch, consumer will infinitely retry without updating the metadata.\nThe fix is to trigger the metadata update call whenever we see following retriable exceptions before a second attempt:\nNOT_LEADER_FOR_PARTITION\nREPLICA_NOT_AVAILABLE\nKAFKA_STORAGE_ERROR\nOFFSET_NOT_AVAILABLE\nLEADER_NOT_AVAILABLE\nFENCED_LEADER_EPOCH\nUNKNOWN_LEADER_EPOCH\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-02-11T06:51:28Z", "url": "https://github.com/apache/kafka/pull/8088", "merged": true, "mergeCommit": {"oid": "863b534f83aad50528828a30857e1ff56ac93f27"}, "closed": true, "closedAt": "2020-02-16T20:06:34Z", "author": {"login": "abbccdda"}, "timelineItems": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcDSPVrAFqTM1NjY3ODg2Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcEXMOegFqTM1OTI1MzE0Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2Njc4ODY2", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-356678866", "createdAt": "2020-02-11T14:02:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNDowMjo1M1rOFoKDcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNDowMjo1M1rOFoKDcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzY1MjA4Mw==", "bodyText": "Nit: would a switch be more reasonable than the huge if/else?", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377652083", "createdAt": "2020-02-11T14:02:53Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1018,15 +1018,16 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n                        error == Errors.REPLICA_NOT_AVAILABLE ||\n                        error == Errors.KAFKA_STORAGE_ERROR ||\n                        error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n-                        topicPartition, error);\n-                partitionsToRetry.add(topicPartition);\n-            } else if (error == Errors.FENCED_LEADER_EPOCH ||\n+                       error == Errors.LEADER_NOT_AVAILABLE ||\n                        error == Errors.UNKNOWN_LEADER_EPOCH) {\n                 log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n                         topicPartition, error);\n                 partitionsToRetry.add(topicPartition);\n+            } else if (error == Errors.FENCED_LEADER_EPOCH) {\n+                log.debug(\"Attempt to fetch offsets for partition {} failed due to fenced leader epoch, refresh \" +\n+                              \"the metadata and retrying.\", topicPartition);\n+                metadata.requestUpdate();\n+                partitionsToRetry.add(topicPartition);\n             } else if (error == Errors.UNKNOWN_TOPIC_OR_PARTITION) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2ODQ2MDk2", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-356846096", "createdAt": "2020-02-11T17:21:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzoyMTo0NVrOFoR9CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzoyMTo0NVrOFoR9CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4MTUxMg==", "bodyText": "There are other cases where we would need to refetch metadata. For example, LEADER_NOT_AVAILABLE. Another way we could handle this is to request an update any time we have partitions to retry.", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377781512", "createdAt": "2020-02-11T17:21:45Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1018,15 +1018,16 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n                        error == Errors.REPLICA_NOT_AVAILABLE ||\n                        error == Errors.KAFKA_STORAGE_ERROR ||\n                        error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n-                        topicPartition, error);\n-                partitionsToRetry.add(topicPartition);\n-            } else if (error == Errors.FENCED_LEADER_EPOCH ||\n+                       error == Errors.LEADER_NOT_AVAILABLE ||\n                        error == Errors.UNKNOWN_LEADER_EPOCH) {\n                 log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n                         topicPartition, error);\n                 partitionsToRetry.add(topicPartition);\n+            } else if (error == Errors.FENCED_LEADER_EPOCH) {\n+                log.debug(\"Attempt to fetch offsets for partition {} failed due to fenced leader epoch, refresh \" +\n+                              \"the metadata and retrying.\", topicPartition);\n+                metadata.requestUpdate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2ODgyMTYw", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-356882160", "createdAt": "2020-02-11T18:14:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxODoxNDo0OVrOFoTr8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxODoxNDo0OVrOFoTr8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgwOTkwNw==", "bodyText": "I have to follow @ijuma suggestion to convert the check into a case switch, as the checkstyle won't let me merge 7 boolean statements XD", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377809907", "createdAt": "2020-02-11T18:14:49Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -979,62 +979,65 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n             TopicPartition topicPartition = entry.getKey();\n             ListOffsetResponse.PartitionData partitionData = listOffsetResponse.responseData().get(topicPartition);\n             Errors error = partitionData.error;\n-            if (error == Errors.NONE) {\n-                if (partitionData.offsets != null) {\n-                    // Handle v0 response\n-                    long offset;\n-                    if (partitionData.offsets.size() > 1) {\n-                        future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n-                                partitionData.offsets.size()));\n-                        return;\n-                    } else if (partitionData.offsets.isEmpty()) {\n-                        offset = ListOffsetResponse.UNKNOWN_OFFSET;\n-                    } else {\n-                        offset = partitionData.offsets.get(0);\n-                    }\n-                    log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n+            switch (error) {\n+                case NONE:\n+                    if (partitionData.offsets != null) {\n+                        // Handle v0 response\n+                        long offset;\n+                        if (partitionData.offsets.size() > 1) {\n+                            future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n+                                                                       partitionData.offsets.size()));\n+                            return;\n+                        } else if (partitionData.offsets.isEmpty()) {\n+                            offset = ListOffsetResponse.UNKNOWN_OFFSET;\n+                        } else {\n+                            offset = partitionData.offsets.get(0);\n+                        }\n+                        log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n                             topicPartition, offset);\n-                    if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n-                        fetchedOffsets.put(topicPartition, offsetData);\n-                    }\n-                } else {\n-                    // Handle v1 and later response\n-                    log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n+                        if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n+                    } else {\n+                        // Handle v1 and later response\n+                        log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n                             topicPartition, partitionData.offset, partitionData.timestamp);\n-                    if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n+                        if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n                                 partitionData.leaderEpoch);\n-                        fetchedOffsets.put(topicPartition, offsetData);\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n                     }\n-                }\n-            } else if (error == Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT) {\n-                // The message format on the broker side is before 0.10.0, which means it does not\n-                // support timestamps. We treat this case the same as if we weren't able to find an\n-                // offset corresponding to the requested timestamp and leave it out of the result.\n-                log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n-                        \"is before 0.10.0\", topicPartition);\n-            } else if (error == Errors.NOT_LEADER_FOR_PARTITION ||\n-                       error == Errors.REPLICA_NOT_AVAILABLE ||\n-                       error == Errors.KAFKA_STORAGE_ERROR ||\n-                       error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n+                    break;\n+                case UNSUPPORTED_FOR_MESSAGE_FORMAT:\n+                    // The message format on the broker side is before 0.10.0, which means it does not\n+                    // support timestamps. We treat this case the same as if we weren't able to find an\n+                    // offset corresponding to the requested timestamp and leave it out of the result.\n+                    log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n+                                  \"is before 0.10.0\", topicPartition);\n+                    break;\n+                case NOT_LEADER_FOR_PARTITION:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 104}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2ODk5MDY1", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-356899065", "createdAt": "2020-02-11T18:40:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxODo0MDoxM1rOFoUg1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxODo0MDoxM1rOFoUg1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgyMzQ0Nw==", "bodyText": "This is the new test being added, other changes are just side cleanup.", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377823447", "createdAt": "2020-02-11T18:40:13Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2414,28 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2OTk0Njg0", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-356994684", "createdAt": "2020-02-11T21:07:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowNzowOVrOFoZGSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowNzowOVrOFoZGSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5ODU3MA==", "bodyText": "We can consolidate this with the logic below. Something like this:\nif (remainingToSearch.isEmpty()) {\n  return result;\n} else {\n  client.awaitMetadataUpdate(timer);\n}", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377898570", "createdAt": "2020-02-11T21:07:09Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -536,18 +536,18 @@ private ListOffsetResult fetchOffsetsByTimes(Map<TopicPartition, Long> timestamp\n             RequestFuture<ListOffsetResult> future = sendListOffsetsRequests(remainingToSearch, requireTimestamps);\n             client.poll(future, timer);\n \n-            if (!future.isDone())\n+            if (!future.isDone()) {\n                 break;\n-\n-            if (future.succeeded()) {\n+            } else if (future.succeeded()) {\n                 ListOffsetResult value = future.value();\n                 result.fetchedOffsets.putAll(value.fetchedOffsets);\n-                if (value.partitionsToRetry.isEmpty())\n-                    return result;\n-\n                 remainingToSearch.keySet().retainAll(value.partitionsToRetry);\n             } else if (!future.isRetriable()) {\n                 throw future.exception();\n+            }\n+\n+            if (remainingToSearch.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MDI3NzU3", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-357027757", "createdAt": "2020-02-11T21:59:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTo1OTowN1rOFoatgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTo1OTowN1rOFoatgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyNDk5NQ==", "bodyText": "Since we're refactoring from if/else to a switch/case as well as changing the logic, it would be useful to include the before/after of errors that cause retry in the PR description", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377924995", "createdAt": "2020-02-11T21:59:07Z", "author": {"login": "mumrah"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -979,62 +974,66 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n             TopicPartition topicPartition = entry.getKey();\n             ListOffsetResponse.PartitionData partitionData = listOffsetResponse.responseData().get(topicPartition);\n             Errors error = partitionData.error;\n-            if (error == Errors.NONE) {\n-                if (partitionData.offsets != null) {\n-                    // Handle v0 response\n-                    long offset;\n-                    if (partitionData.offsets.size() > 1) {\n-                        future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n-                                partitionData.offsets.size()));\n-                        return;\n-                    } else if (partitionData.offsets.isEmpty()) {\n-                        offset = ListOffsetResponse.UNKNOWN_OFFSET;\n-                    } else {\n-                        offset = partitionData.offsets.get(0);\n-                    }\n-                    log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n+            switch (error) {\n+                case NONE:\n+                    if (partitionData.offsets != null) {\n+                        // Handle v0 response\n+                        long offset;\n+                        if (partitionData.offsets.size() > 1) {\n+                            future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n+                                                                       partitionData.offsets.size()));\n+                            return;\n+                        } else if (partitionData.offsets.isEmpty()) {\n+                            offset = ListOffsetResponse.UNKNOWN_OFFSET;\n+                        } else {\n+                            offset = partitionData.offsets.get(0);\n+                        }\n+                        log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n                             topicPartition, offset);\n-                    if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n-                        fetchedOffsets.put(topicPartition, offsetData);\n-                    }\n-                } else {\n-                    // Handle v1 and later response\n-                    log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n+                        if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n+                    } else {\n+                        // Handle v1 and later response\n+                        log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n                             topicPartition, partitionData.offset, partitionData.timestamp);\n-                    if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n+                        if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n                                 partitionData.leaderEpoch);\n-                        fetchedOffsets.put(topicPartition, offsetData);\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n                     }\n-                }\n-            } else if (error == Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT) {\n-                // The message format on the broker side is before 0.10.0, which means it does not\n-                // support timestamps. We treat this case the same as if we weren't able to find an\n-                // offset corresponding to the requested timestamp and leave it out of the result.\n-                log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n-                        \"is before 0.10.0\", topicPartition);\n-            } else if (error == Errors.NOT_LEADER_FOR_PARTITION ||\n-                       error == Errors.REPLICA_NOT_AVAILABLE ||\n-                       error == Errors.KAFKA_STORAGE_ERROR ||\n-                       error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n-                        topicPartition, error);\n-                partitionsToRetry.add(topicPartition);\n-            } else if (error == Errors.FENCED_LEADER_EPOCH ||\n-                       error == Errors.UNKNOWN_LEADER_EPOCH) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n+                    break;\n+                case UNSUPPORTED_FOR_MESSAGE_FORMAT:\n+                    // The message format on the broker side is before 0.10.0, which means it does not\n+                    // support timestamps. We treat this case the same as if we weren't able to find an\n+                    // offset corresponding to the requested timestamp and leave it out of the result.\n+                    log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n+                                  \"is before 0.10.0\", topicPartition);\n+                    break;\n+                case NOT_LEADER_FOR_PARTITION:\n+                case REPLICA_NOT_AVAILABLE:\n+                case KAFKA_STORAGE_ERROR:\n+                case OFFSET_NOT_AVAILABLE:\n+                case LEADER_NOT_AVAILABLE:\n+                case FENCED_LEADER_EPOCH:\n+                case UNKNOWN_LEADER_EPOCH:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3NjU3NDM3", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-357657437", "createdAt": "2020-02-12T17:45:17Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzo0NToxN1rOFo4Xdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzo0NToxN1rOFo4Xdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQxMDg3MQ==", "bodyText": "This an awkward way to verify that the metadata gets updated. Nothing in the test actually involves the use of the new metadata. A more realistic scenario would be a leader change. Could we let the metadata update indicate a leader change and then use prepareResponseFrom to ensure that the retry goes to a particular node?", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378410871", "createdAt": "2020-02-12T17:45:17Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,34 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp0));\n+            client.updateMetadata(initialUpdateResponse);\n+            assertEquals(1, metadata.fetch().nodes().size());\n+\n+            Map<String, Integer> partitionNumByTopic = new HashMap<>();\n+            partitionNumByTopic.put(topicName, 1);\n+            final int updatedNodeSize = 3;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc84e6f2a996a743e74d4e904cda4c1b86c4915a", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/cc84e6f2a996a743e74d4e904cda4c1b86c4915a", "committedDate": "2020-02-12T21:39:01Z", "message": "update metadata upon FENCED_LEADER_EPOCH"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "32553454bb54d9db69030dcce992989ddcf2780c", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/32553454bb54d9db69030dcce992989ddcf2780c", "committedDate": "2020-02-12T21:39:01Z", "message": "blindly trigger metadata update for retry partitions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d731718803825bd8a65833594a26e3b49531ea68", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/d731718803825bd8a65833594a26e3b49531ea68", "committedDate": "2020-02-12T21:39:01Z", "message": "side cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "100bfd51ede28e59d3b675a7cf463d39e811f686", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/100bfd51ede28e59d3b675a7cf463d39e811f686", "committedDate": "2020-02-12T21:39:01Z", "message": "expand test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "616c245c474ec83acb72e2f5eb92e9e49174c91a", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/616c245c474ec83acb72e2f5eb92e9e49174c91a", "committedDate": "2020-02-12T21:39:01Z", "message": "consolidate"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "committedDate": "2020-02-12T21:39:01Z", "message": "add leader change"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "committedDate": "2020-02-12T21:39:01Z", "message": "add leader change"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3OTk5ODg2", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-357999886", "createdAt": "2020-02-13T07:26:18Z", "commit": {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwNzoyNjoxOFrOFpJU3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwNzozMjoyM1rOFpJdhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw==", "bodyText": "This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378688733", "createdAt": "2020-02-13T07:26:18Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODk1MQ==", "bodyText": "Not really sure this has value if the test case expects the leader change correctly.", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378688951", "createdAt": "2020-02-13T07:26:55Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, timestamp, 5L), newLeader);\n+            Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+                fetcher.offsetsForTimes(Collections.singletonMap(tp1, timestamp), time.timer(Integer.MAX_VALUE));\n+\n+            assertEquals(Collections.singletonMap(tp1, new OffsetAndTimestamp(5L, timestamp)), offsetAndTimestampMap);\n+            assertEquals(updatedNodeSize, metadata.fetch().nodes().size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4OTUzMg==", "bodyText": "Why construct this directly? Shouldn't we get it from the metadata update?", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378689532", "createdAt": "2020-02-13T07:28:47Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY5MDk0OA==", "bodyText": "Can we add an assertion like the following to ensure that both requests were sent?\n            assertFalse(client.hasPendingResponses());", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378690948", "createdAt": "2020-02-13T07:32:23Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, timestamp, 5L), newLeader);\n+            Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+                fetcher.offsetsForTimes(Collections.singletonMap(tp1, timestamp), time.timer(Integer.MAX_VALUE));\n+\n+            assertEquals(Collections.singletonMap(tp1, new OffsetAndTimestamp(5L, timestamp)), offsetAndTimestampMap);\n+            assertEquals(updatedNodeSize, metadata.fetch().nodes().size());\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "committedDate": "2020-02-13T17:20:53Z", "message": "improve test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "committedDate": "2020-02-13T17:20:53Z", "message": "improve test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/9fdf4b794c06e8788905bd79b72d3818306f68c9", "committedDate": "2020-02-13T22:44:26Z", "message": "request matcher"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/9fdf4b794c06e8788905bd79b72d3818306f68c9", "committedDate": "2020-02-13T22:44:26Z", "message": "request matcher"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NzI4MjIz", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-358728223", "createdAt": "2020-02-14T06:06:26Z", "commit": {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNjowNjoyNlrOFpsbEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNjoyNDozM1rOFpsp4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg==", "bodyText": "This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379263762", "createdAt": "2020-02-14T06:06:26Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,93 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        final int newLeaderEpoch = 3;\n+        MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3,\n+            singletonMap(topicName, Errors.NONE), singletonMap(topicName, 4), tp -> newLeaderEpoch);\n+        LogContext dummyContext = new LogContext();\n+        ConsumerMetadata dummyMetadata = new ConsumerMetadata(0, Long.MAX_VALUE, false, false,\n+            new SubscriptionState(dummyContext, OffsetResetStrategy.EARLIEST),\n+            dummyContext, new ClusterResourceListeners());\n+        dummyMetadata.updateWithCurrentRequestVersion(updatedMetadata, false, 0L);\n+\n+        Node newLeader = dummyMetadata.fetch().leaderFor(tp1);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            buildFetcher();\n+\n+            subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            final long fetchTimestamp = 10L;\n+            Map<TopicPartition, ListOffsetResponse.PartitionData> allPartitionData = new HashMap<>();\n+            allPartitionData.put(tp0, new ListOffsetResponse.PartitionData(\n+                Errors.NONE, fetchTimestamp, 4L, Optional.empty()));\n+            allPartitionData.put(tp1, new ListOffsetResponse.PartitionData(\n+                retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L, Optional.empty()));\n+\n+            client.prepareResponseFrom(body -> {\n+                boolean isListOffsetRequest = body instanceof ListOffsetRequest;\n+                if (isListOffsetRequest) {\n+                    ListOffsetRequest request = (ListOffsetRequest) body;\n+                    Map<TopicPartition, ListOffsetRequest.PartitionData> expectedTopicPartitions = new HashMap<>();\n+                    expectedTopicPartitions.put(tp0, new ListOffsetRequest.PartitionData(\n+                        fetchTimestamp, Optional.empty()));\n+                    expectedTopicPartitions.put(tp1, new ListOffsetRequest.PartitionData(\n+                        fetchTimestamp, Optional.empty()));\n+\n+                    return request.partitionTimestamps().equals(expectedTopicPartitions);\n+                } else {\n+                    return false;\n+                }\n+            }, new ListOffsetResponse(allPartitionData), originalLeader);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+\n+            // If the metadata wasn't updated before retrying, the fetcher would consult the original leader and hit a fatal exception.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2NzU1Mg==", "bodyText": "MetadataResponse allows us to get the Cluster directly, so we can do something simpler:\n        Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1);\n        Node newLeader = updatedMetadata.cluster().leaderFor(tp1);\n        assertNotEquals(oldLeader, newLeader);\n\nSince the metadata doesn't change, we can just do this check once.", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379267552", "createdAt": "2020-02-14T06:24:33Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,93 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        final int newLeaderEpoch = 3;\n+        MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3,\n+            singletonMap(topicName, Errors.NONE), singletonMap(topicName, 4), tp -> newLeaderEpoch);\n+        LogContext dummyContext = new LogContext();\n+        ConsumerMetadata dummyMetadata = new ConsumerMetadata(0, Long.MAX_VALUE, false, false,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9"}, "originalPosition": 66}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1500e239dc91b80a9dd653c321d40fda812a4a25", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/1500e239dc91b80a9dd653c321d40fda812a4a25", "committedDate": "2020-02-14T08:08:41Z", "message": "old/new leader put out"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78820ca0b3d6bde44409634dae6a78900838870f", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/78820ca0b3d6bde44409634dae6a78900838870f", "committedDate": "2020-02-14T18:32:16Z", "message": "not leader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5MjUzMTQz", "url": "https://github.com/apache/kafka/pull/8088#pullrequestreview-359253143", "createdAt": "2020-02-14T22:22:57Z", "commit": {"oid": "78820ca0b3d6bde44409634dae6a78900838870f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1848, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}