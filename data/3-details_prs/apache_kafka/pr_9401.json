{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAwMzYzNjA1", "number": 9401, "title": "KAFKA-9628 Replace Produce request/response with automated protocol", "bodyText": "issue: https://issues.apache.org/jira/browse/KAFKA-9628\nBenchmark\n\nloop 30 times\ncalculate average\n\nkafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput\n\n@cluster(num_nodes=5)\n@parametrize(acks=-1, topic=TOPIC_REP_THREE)\n\n\n+0.3144915325 %\n28.08766667 ->  28.1715625 (mb_per_sec)\n\n\n@cluster(num_nodes=5)\n@matrix(acks=[1], topic=[TOPIC_REP_THREE], message_size=[100000],compression_type=[\"none\"], security_protocol=['PLAINTEXT'])\n\n\n+4.220730323 %\n157.145 -> 163.7776667 (mb_per_sec)\n\n\n@cluster(num_nodes=7)\n@parametrize(acks=1, topic=TOPIC_REP_THREE, num_producers=3)\n\n\n+5.996241145%\n57.64166667 -> 61.098 (mb_per_sec)\n\n\n@cluster(num_nodes=5)\n@parametrize(acks=1, topic=TOPIC_REP_THREE)\n\n\n+0.3979572536%\n44.05833333 -> 44.23366667 (mb_per_sec)\n\n\n@cluster(num_nodes=5)\n@parametrize(acks=1, topic= TOPIC_REP_ONE)\n\n\n+2.228235226%\n69.23266667 -> 70.77533333 (mb_per_sec)\n\nJMH results\nIn short, most ops performance are regression since we have to convert data to protocol data. The cost is inevitable (like other request/response) before we use protocol data directly.\nJMH for ProduceRequest\n\nconstruction regression:\n\n281.474 -> 454.935 ns/op\n296.000 -> 1888.000 B/op\n\n\ntoErrorResponse regression:\n\n41.942 -> 107.528 ns/op\n1216.000 -> 1616.000 B/op\n\n\ntoStruct improvement:\n\n255.185 -> 90.728 ns/op\n864.000 -> 304.000 B/op\n\n\n\nBEFORE\nBenchmark                                                                        Mode  Cnt     Score    Error   Units\nProducerRequestBenchmark.constructorErrorResponse                                avgt   15    41.942 \u00b1  0.036   ns/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.alloc.rate                 avgt   15  6409.263 \u00b1  5.478  MB/sec\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.alloc.rate.norm            avgt   15   296.000 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Eden_Space        avgt   15  6416.420 \u00b1 76.071  MB/sec\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Eden_Space.norm   avgt   15   296.331 \u00b1  3.539    B/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Old_Gen           avgt   15     0.002 \u00b1  0.002  MB/sec\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Old_Gen.norm      avgt   15    \u2248 10\u207b\u2074             B/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.count                      avgt   15   698.000           counts\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.time                       avgt   15   378.000               ms\nProducerRequestBenchmark.constructorProduceRequest                               avgt   15   281.474 \u00b1  3.286   ns/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.alloc.rate                avgt   15  3923.868 \u00b1 46.303  MB/sec\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.alloc.rate.norm           avgt   15  1216.000 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Eden_Space       avgt   15  3923.375 \u00b1 59.568  MB/sec\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Eden_Space.norm  avgt   15  1215.844 \u00b1 11.184    B/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Old_Gen          avgt   15     0.004 \u00b1  0.001  MB/sec\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Old_Gen.norm     avgt   15     0.001 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.count                     avgt   15   515.000           counts\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.time                      avgt   15   279.000               ms\nProducerRequestBenchmark.constructorStruct                                       avgt   15   255.185 \u00b1  0.069   ns/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.alloc.rate                        avgt   15  3074.889 \u00b1  0.823  MB/sec\nProducerRequestBenchmark.constructorStruct:\u00b7gc.alloc.rate.norm                   avgt   15   864.000 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space               avgt   15  3077.737 \u00b1 31.537  MB/sec\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space.norm          avgt   15   864.800 \u00b1  8.823    B/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen                  avgt   15     0.003 \u00b1  0.001  MB/sec\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen.norm             avgt   15     0.001 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.count                             avgt   15   404.000           counts\nProducerRequestBenchmark.constructorStruct:\u00b7gc.time                              avgt   15   214.000               ms\n\nAFTER\nBenchmark                                                                        Mode  Cnt     Score    Error   Units\nProducerRequestBenchmark.constructorErrorResponse                                avgt   15   107.528 \u00b1  0.270   ns/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.alloc.rate                 avgt   15  4864.899 \u00b1 12.132  MB/sec\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.alloc.rate.norm            avgt   15   576.000 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Eden_Space        avgt   15  4868.023 \u00b1 61.943  MB/sec\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Eden_Space.norm   avgt   15   576.371 \u00b1  7.331    B/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Old_Gen           avgt   15     0.005 \u00b1  0.001  MB/sec\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.churn.G1_Old_Gen.norm      avgt   15     0.001 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.count                      avgt   15   639.000           counts\nProducerRequestBenchmark.constructorErrorResponse:\u00b7gc.time                       avgt   15   339.000               ms\nProducerRequestBenchmark.constructorProduceRequest                               avgt   15   454.935 \u00b1  0.332   ns/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.alloc.rate                avgt   15  3769.014 \u00b1  2.767  MB/sec\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.alloc.rate.norm           avgt   15  1888.000 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Eden_Space       avgt   15  3763.407 \u00b1 31.530  MB/sec\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Eden_Space.norm  avgt   15  1885.190 \u00b1 15.594    B/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Old_Gen          avgt   15     0.004 \u00b1  0.001  MB/sec\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.churn.G1_Old_Gen.norm     avgt   15     0.002 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.count                     avgt   15   494.000           counts\nProducerRequestBenchmark.constructorProduceRequest:\u00b7gc.time                      avgt   15   264.000               ms\nProducerRequestBenchmark.constructorStruct                                       avgt   15    90.728 \u00b1  0.695   ns/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.alloc.rate                        avgt   15  3043.140 \u00b1 23.246  MB/sec\nProducerRequestBenchmark.constructorStruct:\u00b7gc.alloc.rate.norm                   avgt   15   304.000 \u00b1  0.001    B/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space               avgt   15  3047.251 \u00b1 59.638  MB/sec\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space.norm          avgt   15   304.404 \u00b1  5.034    B/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen                  avgt   15     0.003 \u00b1  0.001  MB/sec\nProducerRequestBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen.norm             avgt   15    \u2248 10\u207b\u2074             B/op\nProducerRequestBenchmark.constructorStruct:\u00b7gc.count                             avgt   15   400.000           counts\nProducerRequestBenchmark.constructorStruct:\u00b7gc.time                              avgt   15   205.000               ms\n\nJMH for ProduceResponse\n\nconstruction regression:\n\n3.293 -> 303.226 ns/op\n24.000 -> 1848.000 B/op\n\n\ntoStruct improvement:\n\n825.889 -> 311.725 ns/op\n2208.000 -> 896.000 B/op\n\n\n\nBEFORE\nBenchmark                                                                          Mode  Cnt     Score    Error   Units\nProducerResponseBenchmark.constructorProduceResponse                               avgt   15     3.293 \u00b1  0.004   ns/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.alloc.rate                avgt   15  6619.731 \u00b1  9.075  MB/sec\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.alloc.rate.norm           avgt   15    24.000 \u00b1  0.001    B/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Eden_Space       avgt   15  6618.648 \u00b1  0.153  MB/sec\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Eden_Space.norm  avgt   15    23.996 \u00b1  0.033    B/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Old_Gen          avgt   15     0.003 \u00b1  0.002  MB/sec\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Old_Gen.norm     avgt   15    \u2248 10\u207b\u2075             B/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.count                     avgt   15   720.000           counts\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.time                      avgt   15   383.000               ms\nProducerResponseBenchmark.constructorStruct                                        avgt   15   825.889 \u00b1  0.638   ns/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.alloc.rate                         avgt   15  2428.000 \u00b1  1.899  MB/sec\nProducerResponseBenchmark.constructorStruct:\u00b7gc.alloc.rate.norm                    avgt   15  2208.000 \u00b1  0.001    B/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space                avgt   15  2430.196 \u00b1 55.894  MB/sec\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space.norm           avgt   15  2210.001 \u00b1 51.009    B/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen                   avgt   15     0.003 \u00b1  0.001  MB/sec\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen.norm              avgt   15     0.002 \u00b1  0.001    B/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.count                              avgt   15   319.000           counts\nProducerResponseBenchmark.constructorStruct:\u00b7gc.time                               avgt   15   166.000               ms\n\nAFTER\nBenchmark                                                                          Mode  Cnt     Score    Error   Units\nProducerResponseBenchmark.constructorProduceResponse                               avgt   15   303.226 \u00b1  0.517   ns/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.alloc.rate                avgt   15  5534.940 \u00b1  9.439  MB/sec\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.alloc.rate.norm           avgt   15  1848.000 \u00b1  0.001    B/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Eden_Space       avgt   15  5534.046 \u00b1 51.849  MB/sec\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Eden_Space.norm  avgt   15  1847.710 \u00b1 18.105    B/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Old_Gen          avgt   15     0.007 \u00b1  0.001  MB/sec\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.churn.G1_Old_Gen.norm     avgt   15     0.002 \u00b1  0.001    B/op\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.count                     avgt   15   602.000           counts\nProducerResponseBenchmark.constructorProduceResponse:\u00b7gc.time                      avgt   15   318.000               ms\nProducerResponseBenchmark.constructorStruct                                        avgt   15   311.725 \u00b1  3.132   ns/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.alloc.rate                         avgt   15  2610.602 \u00b1 25.964  MB/sec\nProducerResponseBenchmark.constructorStruct:\u00b7gc.alloc.rate.norm                    avgt   15   896.000 \u00b1  0.001    B/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space                avgt   15  2613.021 \u00b1 42.965  MB/sec\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Eden_Space.norm           avgt   15   896.824 \u00b1 11.331    B/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen                   avgt   15     0.003 \u00b1  0.001  MB/sec\nProducerResponseBenchmark.constructorStruct:\u00b7gc.churn.G1_Old_Gen.norm              avgt   15     0.001 \u00b1  0.001    B/op\nProducerResponseBenchmark.constructorStruct:\u00b7gc.count                              avgt   15   343.000           counts\nProducerResponseBenchmark.constructorStruct:\u00b7gc.time                               avgt   15   194.000               ms\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-10-09T05:58:00Z", "url": "https://github.com/apache/kafka/pull/9401", "merged": true, "mergeCommit": {"oid": "30bc21ca35b165f04c472b4ce794893843809ccc"}, "closed": true, "closedAt": "2020-11-18T21:44:22Z", "author": {"login": "chia7712"}, "timelineItems": {"totalCount": 37, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdXT2OVgFqTUxOTc1NzMwOQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABddyUfAABqjQwMTIxMDYwNDc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE5NzU3MzA5", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-519757309", "createdAt": "2020-10-29T14:58:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNDo1ODoxOVrOHqf-Bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNToyNDowMVrOHqhNJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMyNjAyMg==", "bodyText": "Would it make sense to move this to the builder where we are already doing a pass over the partitions?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514326022", "createdAt": "2020-10-29T14:58:19Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -210,65 +142,42 @@ public String toString() {\n         }\n     }\n \n+    /**\n+     * We have to copy acks, timeout, transactionalId and partitionSizes from data since data maybe reset to eliminate\n+     * the reference to ByteBuffer but those metadata are still useful.\n+     */\n     private final short acks;\n     private final int timeout;\n     private final String transactionalId;\n-\n-    private final Map<TopicPartition, Integer> partitionSizes;\n-\n+    // visible for testing\n+    final Map<TopicPartition, Integer> partitionSizes;\n+    private boolean hasTransactionalRecords = false;\n+    private boolean hasIdempotentRecords = false;\n     // This is set to null by `clearPartitionRecords` to prevent unnecessary memory retention when a produce request is\n     // put in the purgatory (due to client throttling, it can take a while before the response is sent).\n     // Care should be taken in methods that use this field.\n-    private volatile Map<TopicPartition, MemoryRecords> partitionRecords;\n-    private boolean hasTransactionalRecords = false;\n-    private boolean hasIdempotentRecords = false;\n-\n-    private ProduceRequest(short version, short acks, int timeout, Map<TopicPartition, MemoryRecords> partitionRecords, String transactionalId) {\n-        super(ApiKeys.PRODUCE, version);\n-        this.acks = acks;\n-        this.timeout = timeout;\n-\n-        this.transactionalId = transactionalId;\n-        this.partitionRecords = partitionRecords;\n-        this.partitionSizes = createPartitionSizes(partitionRecords);\n+    private volatile ProduceRequestData data;\n \n-        for (MemoryRecords records : partitionRecords.values()) {\n-            setFlags(records);\n-        }\n-    }\n-\n-    private static Map<TopicPartition, Integer> createPartitionSizes(Map<TopicPartition, MemoryRecords> partitionRecords) {\n-        Map<TopicPartition, Integer> result = new HashMap<>(partitionRecords.size());\n-        for (Map.Entry<TopicPartition, MemoryRecords> entry : partitionRecords.entrySet())\n-            result.put(entry.getKey(), entry.getValue().sizeInBytes());\n-        return result;\n-    }\n-\n-    public ProduceRequest(Struct struct, short version) {\n+    public ProduceRequest(ProduceRequestData produceRequestData, short version) {\n         super(ApiKeys.PRODUCE, version);\n-        partitionRecords = new HashMap<>();\n-        for (Object topicDataObj : struct.getArray(TOPIC_DATA_KEY_NAME)) {\n-            Struct topicData = (Struct) topicDataObj;\n-            String topic = topicData.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicData.getArray(PARTITION_DATA_KEY_NAME)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                int partition = partitionResponse.get(PARTITION_ID);\n-                MemoryRecords records = (MemoryRecords) partitionResponse.getRecords(RECORD_SET_KEY_NAME);\n-                setFlags(records);\n-                partitionRecords.put(new TopicPartition(topic, partition), records);\n-            }\n-        }\n-        partitionSizes = createPartitionSizes(partitionRecords);\n-        acks = struct.getShort(ACKS_KEY_NAME);\n-        timeout = struct.getInt(TIMEOUT_KEY_NAME);\n-        transactionalId = struct.getOrElse(NULLABLE_TRANSACTIONAL_ID, null);\n-    }\n-\n-    private void setFlags(MemoryRecords records) {\n-        Iterator<MutableRecordBatch> iterator = records.batches().iterator();\n-        MutableRecordBatch entry = iterator.next();\n-        hasIdempotentRecords = hasIdempotentRecords || entry.hasProducerId();\n-        hasTransactionalRecords = hasTransactionalRecords || entry.isTransactional();\n+        this.data = produceRequestData;\n+        this.data.topicData().forEach(topicProduceData -> topicProduceData.partitions()\n+            .forEach(partitionProduceData -> {\n+                MemoryRecords records = MemoryRecords.readableRecords(partitionProduceData.records());\n+                Iterator<MutableRecordBatch> iterator = records.batches().iterator();\n+                MutableRecordBatch entry = iterator.next();\n+                hasIdempotentRecords = hasIdempotentRecords || entry.hasProducerId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMzNTcxNg==", "bodyText": "Kind of a pity to lose this. Can we move it to the class documentation?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514335716", "createdAt": "2020-10-29T15:10:31Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -17,179 +17,48 @@\n package org.apache.kafka.common.requests;\n \n import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.message.ProduceResponseData;\n+import org.apache.kafka.common.protocol.ByteBufferAccessor;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n import org.apache.kafka.common.record.RecordBatch;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n+import java.util.AbstractMap;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-\n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT64;\n+import java.util.stream.Collectors;\n \n /**\n- * This wrapper supports both v0 and v1 of ProduceResponse.\n+ * This wrapper supports both v0 and v8 of ProduceResponse.\n  */\n public class ProduceResponse extends AbstractResponse {\n-\n-    private static final String RESPONSES_KEY_NAME = \"responses\";\n-\n-    // topic level field names\n-    private static final String PARTITION_RESPONSES_KEY_NAME = \"partition_responses\";\n-\n     public static final long INVALID_OFFSET = -1L;\n+    private final ProduceResponseData data;\n+    private final Map<TopicPartition, PartitionResponse> responses;\n \n-    /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MDE3OA==", "bodyText": "Hmm, not sure about making this ignorable. For transactional data, I think the broker would just fail if it cannot authorize the transactionalId.\nAlso, should we set the default to null?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514340178", "createdAt": "2020-10-29T15:16:09Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"ignorable\": true, \"entityType\": \"transactionalId\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MjI4NQ==", "bodyText": "nit: I kind of liked the original name to make the unit clear. We're probably not consistent on this convention though.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514342285", "createdAt": "2020-10-29T15:18:52Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"ignorable\": true, \"entityType\": \"transactionalId\",\n       \"about\": \"The transactional ID, or null if the producer is not transactional.\" },\n     { \"name\": \"Acks\", \"type\": \"int16\", \"versions\": \"0+\",\n       \"about\": \"The number of acknowledgments the producer requires the leader to have received before considering a request complete. Allowed values: 0 for no acknowledgments, 1 for only the leader and -1 for the full ISR.\" },\n-    { \"name\": \"TimeoutMs\", \"type\": \"int32\", \"versions\": \"0+\",\n+    { \"name\": \"Timeout\", \"type\": \"int32\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0NjI3OQ==", "bodyText": "Wondering if we may as well rewrite this using ProduceResponseData.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514346279", "createdAt": "2020-10-29T15:24:01Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "diffHunk": "@@ -218,6 +218,7 @@ private void checkSimpleRequestResponse(NetworkClient networkClient) {\n                 request.apiKey().responseHeaderVersion(PRODUCE.latestVersion()));\n         Struct resp = new Struct(PRODUCE.responseSchema(PRODUCE.latestVersion()));\n         resp.set(\"responses\", new Object[0]);\n+        resp.set(CommonFields.THROTTLE_TIME_MS, 100);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NDYwMjg1", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-525460285", "createdAt": "2020-11-06T20:06:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMDowNjowOVrOHu70XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMDoyODowM1rOHu8jRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3NjYwNQ==", "bodyText": "Is the plan to save this for a follow-up? It looks like it will be a bit of effort to trace down all the uses, but seems doable.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518976605", "createdAt": "2020-11-06T20:06:09Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "diffHunk": "@@ -560,13 +561,23 @@ private void handleProduceResponse(ClientResponse response, Map<TopicPartition,\n             log.trace(\"Received produce response from node {} with correlation id {}\", response.destination(), correlationId);\n             // if we have a response, parse it\n             if (response.hasResponse()) {\n+                // TODO: Sender should exercise PartitionProduceResponse rather than ProduceResponse.PartitionResponse", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3ODg2Mw==", "bodyText": "I wonder if we would get any benefit computing partitionSizes during this pass.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518978863", "createdAt": "2020-11-06T20:09:06Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -194,7 +107,27 @@ private ProduceRequest build(short version, boolean validate) {\n                     ProduceRequest.validateRecords(version, records);\n                 }\n             }\n-            return new ProduceRequest(version, acks, timeout, partitionRecords, transactionalId);\n+\n+            List<ProduceRequestData.TopicProduceData> tpd = partitionRecords", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDA2NA==", "bodyText": "Not required, but this would be easier to follow up if we had some helpers.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518984064", "createdAt": "2020-11-06T20:16:48Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -204,118 +75,78 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(new ProduceResponseData()\n+            .setResponses(responses.entrySet()\n+                .stream()\n+                .collect(Collectors.groupingBy(e -> e.getKey().topic()))\n+                .entrySet()\n+                .stream()\n+                .map(topicData -> new ProduceResponseData.TopicProduceResponse()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDM0OQ==", "bodyText": "Sounds good to refactor. Perhaps we can turn this TODO into a jira?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518984349", "createdAt": "2020-11-06T20:17:32Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -204,118 +75,78 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(new ProduceResponseData()\n+            .setResponses(responses.entrySet()\n+                .stream()\n+                .collect(Collectors.groupingBy(e -> e.getKey().topic()))\n+                .entrySet()\n+                .stream()\n+                .map(topicData -> new ProduceResponseData.TopicProduceResponse()\n+                    .setTopic(topicData.getKey())\n+                    .setPartitionResponses(topicData.getValue()\n+                        .stream()\n+                        .map(p -> new ProduceResponseData.PartitionProduceResponse()\n+                            .setPartition(p.getKey().partition())\n+                            .setBaseOffset(p.getValue().baseOffset)\n+                            .setLogStartOffset(p.getValue().logStartOffset)\n+                            .setLogAppendTime(p.getValue().logAppendTime)\n+                            .setErrorMessage(p.getValue().errorMessage)\n+                            .setErrorCode(p.getValue().error.code())\n+                            .setRecordErrors(p.getValue().recordErrors\n+                                .stream()\n+                                .map(e -> new ProduceResponseData.BatchIndexAndErrorMessage()\n+                                    .setBatchIndex(e.batchIndex)\n+                                    .setBatchIndexErrorMessage(e.message))\n+                                .collect(Collectors.toList())))\n+                        .collect(Collectors.toList())))\n+                .collect(Collectors.toList()))\n+            .setThrottleTimeMs(throttleTimeMs));\n     }\n \n     /**\n-     * Constructor from a {@link Struct}.\n+     * Visible for testing.\n      */\n-    public ProduceResponse(Struct struct) {\n-        responses = new HashMap<>();\n-        for (Object topicResponse : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicRespStruct = (Struct) topicResponse;\n-            String topic = topicRespStruct.get(TOPIC_NAME);\n-\n-            for (Object partResponse : topicRespStruct.getArray(PARTITION_RESPONSES_KEY_NAME)) {\n-                Struct partRespStruct = (Struct) partResponse;\n-                int partition = partRespStruct.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partRespStruct.get(ERROR_CODE));\n-                long offset = partRespStruct.getLong(BASE_OFFSET_KEY_NAME);\n-                long logAppendTime = partRespStruct.hasField(LOG_APPEND_TIME_KEY_NAME) ?\n-                        partRespStruct.getLong(LOG_APPEND_TIME_KEY_NAME) : RecordBatch.NO_TIMESTAMP;\n-                long logStartOffset = partRespStruct.getOrElse(LOG_START_OFFSET_FIELD, INVALID_OFFSET);\n-\n-                List<RecordError> recordErrors = Collections.emptyList();\n-                if (partRespStruct.hasField(RECORD_ERRORS_KEY_NAME)) {\n-                    Object[] recordErrorsArray = partRespStruct.getArray(RECORD_ERRORS_KEY_NAME);\n-                    if (recordErrorsArray.length > 0) {\n-                        recordErrors = new ArrayList<>(recordErrorsArray.length);\n-                        for (Object indexAndMessage : recordErrorsArray) {\n-                            Struct indexAndMessageStruct = (Struct) indexAndMessage;\n-                            recordErrors.add(new RecordError(\n-                                    indexAndMessageStruct.getInt(BATCH_INDEX_KEY_NAME),\n-                                    indexAndMessageStruct.get(BATCH_INDEX_ERROR_MESSAGE_FIELD)\n-                            ));\n-                        }\n-                    }\n-                }\n-\n-                String errorMessage = partRespStruct.getOrElse(ERROR_MESSAGE_FIELD, null);\n-                TopicPartition tp = new TopicPartition(topic, partition);\n-                responses.put(tp, new PartitionResponse(error, offset, logAppendTime, logStartOffset, recordErrors, errorMessage));\n-            }\n-        }\n-        this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);\n-    }\n-\n     @Override\n-    protected Struct toStruct(short version) {\n-        Struct struct = new Struct(ApiKeys.PRODUCE.responseSchema(version));\n-\n-        Map<String, Map<Integer, PartitionResponse>> responseByTopic = CollectionUtils.groupPartitionDataByTopic(responses);\n-        List<Struct> topicDatas = new ArrayList<>(responseByTopic.size());\n-        for (Map.Entry<String, Map<Integer, PartitionResponse>> entry : responseByTopic.entrySet()) {\n-            Struct topicData = struct.instance(RESPONSES_KEY_NAME);\n-            topicData.set(TOPIC_NAME, entry.getKey());\n-            List<Struct> partitionArray = new ArrayList<>();\n-            for (Map.Entry<Integer, PartitionResponse> partitionEntry : entry.getValue().entrySet()) {\n-                PartitionResponse part = partitionEntry.getValue();\n-                short errorCode = part.error.code();\n-                // If producer sends ProduceRequest V3 or earlier, the client library is not guaranteed to recognize the error code\n-                // for KafkaStorageException. In this case the client library will translate KafkaStorageException to\n-                // UnknownServerException which is not retriable. We can ensure that producer will update metadata and retry\n-                // by converting the KafkaStorageException to NotLeaderOrFollowerException in the response if ProduceRequest version <= 3\n-                if (errorCode == Errors.KAFKA_STORAGE_ERROR.code() && version <= 3)\n-                    errorCode = Errors.NOT_LEADER_OR_FOLLOWER.code();\n-                Struct partStruct = topicData.instance(PARTITION_RESPONSES_KEY_NAME)\n-                        .set(PARTITION_ID, partitionEntry.getKey())\n-                        .set(ERROR_CODE, errorCode)\n-                        .set(BASE_OFFSET_KEY_NAME, part.baseOffset);\n-                partStruct.setIfExists(LOG_APPEND_TIME_KEY_NAME, part.logAppendTime);\n-                partStruct.setIfExists(LOG_START_OFFSET_FIELD, part.logStartOffset);\n-\n-                if (partStruct.hasField(RECORD_ERRORS_KEY_NAME)) {\n-                    List<Struct> recordErrors = Collections.emptyList();\n-                    if (!part.recordErrors.isEmpty()) {\n-                        recordErrors = new ArrayList<>();\n-                        for (RecordError indexAndMessage : part.recordErrors) {\n-                            Struct indexAndMessageStruct = partStruct.instance(RECORD_ERRORS_KEY_NAME)\n-                                    .set(BATCH_INDEX_KEY_NAME, indexAndMessage.batchIndex)\n-                                    .set(BATCH_INDEX_ERROR_MESSAGE_FIELD, indexAndMessage.message);\n-                            recordErrors.add(indexAndMessageStruct);\n-                        }\n-                    }\n-                    partStruct.set(RECORD_ERRORS_KEY_NAME, recordErrors.toArray());\n-                }\n-\n-                partStruct.setIfExists(ERROR_MESSAGE_FIELD, part.errorMessage);\n-                partitionArray.add(partStruct);\n-            }\n-            topicData.set(PARTITION_RESPONSES_KEY_NAME, partitionArray.toArray());\n-            topicDatas.add(topicData);\n-        }\n-        struct.set(RESPONSES_KEY_NAME, topicDatas.toArray());\n-        struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);\n+    public Struct toStruct(short version) {\n+        return data.toStruct(version);\n+    }\n \n-        return struct;\n+    public ProduceResponseData data() {\n+        return this.data;\n     }\n \n+    /**\n+     * this method is used by testing only.\n+     * TODO: refactor the tests which are using this method and then remove this method from production code.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 342}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDg5MQ==", "bodyText": "The names do not get serialized, so I think we can make them whatever we want.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518984891", "createdAt": "2020-11-06T20:18:56Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"ignorable\": true, \"entityType\": \"transactionalId\",\n       \"about\": \"The transactional ID, or null if the producer is not transactional.\" },\n     { \"name\": \"Acks\", \"type\": \"int16\", \"versions\": \"0+\",\n       \"about\": \"The number of acknowledgments the producer requires the leader to have received before considering a request complete. Allowed values: 0 for no acknowledgments, 1 for only the leader and -1 for the full ISR.\" },\n-    { \"name\": \"TimeoutMs\", \"type\": \"int32\", \"versions\": \"0+\",\n+    { \"name\": \"Timeout\", \"type\": \"int32\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MjI4NQ=="}, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NzE4MA==", "bodyText": "This logic surprised me a little bit until I realized that we were trying to avoid redundant authorization calls. Might be worth adding a comment since I was almost ready to suggest moving this logic into the loop.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518987180", "createdAt": "2020-11-06T20:24:33Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -517,19 +517,23 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n       // Note that authorization to a transactionalId implies ProducerId authorization\n \n-    } else if (produceRequest.hasIdempotentRecords && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n+    } else if (RequestUtils.hasIdempotentRecords(produceRequest) && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n       sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)\n       return\n     }\n \n-    val produceRecords = produceRequest.partitionRecordsOrFail.asScala\n     val unauthorizedTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val nonExistingTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val invalidRequestResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val authorizedRequestInfo = mutable.Map[TopicPartition, MemoryRecords]()\n-    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRecords)(_._1.topic)\n-\n-    for ((topicPartition, memoryRecords) <- produceRecords) {\n+    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRequest.dataOrException().topicData().asScala)(_.topic())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NzU4Ng==", "bodyText": "That's a good question. I can't think of any great options. We ended up making FetchResponse generic to address a similar issue. I think the cast is reasonable for now. Can we move the TODO to a jira?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518987586", "createdAt": "2020-11-06T20:25:30Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -517,19 +517,23 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n       // Note that authorization to a transactionalId implies ProducerId authorization\n \n-    } else if (produceRequest.hasIdempotentRecords && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n+    } else if (RequestUtils.hasIdempotentRecords(produceRequest) && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n       sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)\n       return\n     }\n \n-    val produceRecords = produceRequest.partitionRecordsOrFail.asScala\n     val unauthorizedTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val nonExistingTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val invalidRequestResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val authorizedRequestInfo = mutable.Map[TopicPartition, MemoryRecords]()\n-    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRecords)(_._1.topic)\n-\n-    for ((topicPartition, memoryRecords) <- produceRecords) {\n+    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRequest.dataOrException().topicData().asScala)(_.topic())\n+\n+    produceRequest.dataOrException().topicData().forEach(topic => topic.data().forEach { partition =>\n+      val topicPartition = new TopicPartition(topic.topic(), partition.partition())\n+      // This caller assumes the type is MemoryRecords and that is true on current serialization\n+      // We cast the type to avoid causing big change to code base.\n+      // TODO: maybe we need to refactor code to avoid this casting", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4ODYxMg==", "bodyText": "nit: unnecessary parenthesis (a few of these around here)", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518988612", "createdAt": "2020-11-06T20:28:03Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -517,19 +517,23 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n       // Note that authorization to a transactionalId implies ProducerId authorization\n \n-    } else if (produceRequest.hasIdempotentRecords && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n+    } else if (RequestUtils.hasIdempotentRecords(produceRequest) && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n       sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)\n       return\n     }\n \n-    val produceRecords = produceRequest.partitionRecordsOrFail.asScala\n     val unauthorizedTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val nonExistingTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val invalidRequestResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val authorizedRequestInfo = mutable.Map[TopicPartition, MemoryRecords]()\n-    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRecords)(_._1.topic)\n-\n-    for ((topicPartition, memoryRecords) <- produceRecords) {\n+    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRequest.dataOrException().topicData().asScala)(_.topic())\n+\n+    produceRequest.dataOrException().topicData().forEach(topic => topic.data().forEach { partition =>\n+      val topicPartition = new TopicPartition(topic.topic(), partition.partition())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4ODI4MzE5", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-528828319", "createdAt": "2020-11-12T08:00:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODowMDowNlrOHxubWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNzoyMFrOHxvlLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwMjkzNg==", "bodyText": "I wonder if we could avoid all of this by requesting the Sender to create TopicProduceData directly. It seems that the Sender creates partitionRecords right before calling the builder: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java#L734. So we may be able to directly construct the expect data structure there.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521902936", "createdAt": "2020-11-12T08:00:06Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -194,7 +106,27 @@ private ProduceRequest build(short version, boolean validate) {\n                     ProduceRequest.validateRecords(version, records);\n                 }\n             }\n-            return new ProduceRequest(version, acks, timeout, partitionRecords, transactionalId);\n+\n+            List<ProduceRequestData.TopicProduceData> tpd = partitionRecords", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwOTU4Nw==", "bodyText": "It seems that we could create ProduceResponseData based on data. This avoids the cost of the group-by operation and the cost of constructing partitionSizes. That should bring the benchmark inline with what we had before. Would this work?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521909587", "createdAt": "2020-11-12T08:11:33Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -323,27 +222,30 @@ public String toString(boolean verbose) {\n     @Override\n     public ProduceResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n         /* In case the producer doesn't actually want any response */\n-        if (acks == 0)\n-            return null;\n-\n+        if (acks == 0) return null;\n         Errors error = Errors.forException(e);\n-        Map<TopicPartition, ProduceResponse.PartitionResponse> responseMap = new HashMap<>();\n-        ProduceResponse.PartitionResponse partitionResponse = new ProduceResponse.PartitionResponse(error);\n-\n-        for (TopicPartition tp : partitions())\n-            responseMap.put(tp, partitionResponse);\n-\n-        return new ProduceResponse(responseMap, throttleTimeMs);\n+        return new ProduceResponse(new ProduceResponseData()\n+            .setResponses(partitionSizes().keySet().stream().collect(Collectors.groupingBy(TopicPartition::topic)).entrySet()\n+                .stream()\n+                .map(entry -> new ProduceResponseData.TopicProduceResponse()\n+                    .setPartitionResponses(entry.getValue().stream().map(p -> new ProduceResponseData.PartitionProduceResponse()\n+                        .setIndex(p.partition())\n+                        .setRecordErrors(Collections.emptyList())\n+                        .setBaseOffset(INVALID_OFFSET)\n+                        .setLogAppendTimeMs(RecordBatch.NO_TIMESTAMP)\n+                        .setLogStartOffset(INVALID_OFFSET)\n+                        .setErrorMessage(e.getMessage())\n+                        .setErrorCode(error.code()))\n+                        .collect(Collectors.toList()))\n+                    .setName(entry.getKey()))\n+                .collect(Collectors.toList()))\n+            .setThrottleTimeMs(throttleTimeMs));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMTgzNw==", "bodyText": "As we care of performances here, I wonder if we should try not using the stream api here.\nAnother trick would be to  turn TopicProduceResponse in the ProduceResponse schema into a map by setting \"mapKey\": true for the topic name. This would allow to iterate over responses, get or create TopicProduceResponse for the topic, and add the PartitionProduceResponse into it.\nIt may be worth trying different implementation to compare their performances.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521921837", "createdAt": "2020-11-12T08:27:20Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -204,118 +75,79 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(new ProduceResponseData()\n+            .setResponses(responses.entrySet()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 214}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyODg5Mjg5", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-532889289", "createdAt": "2020-11-17T23:11:04Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMzoxMTowNVrOH1PRCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMzozMjo0MlrOH1Pyuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU4NjY5Nw==", "bodyText": "nit: since we have the jira for tracking, can we remove the TODO? A few more of these in the PR.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525586697", "createdAt": "2020-11-17T23:11:05Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "diffHunk": "@@ -560,13 +562,24 @@ private void handleProduceResponse(ClientResponse response, Map<TopicPartition,\n             log.trace(\"Received produce response from node {} with correlation id {}\", response.destination(), correlationId);\n             // if we have a response, parse it\n             if (response.hasResponse()) {\n+                // TODO: Sender should exercise PartitionProduceResponse rather than ProduceResponse.PartitionResponse\n+                // https://issues.apache.org/jira/browse/KAFKA-10696", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NTMyMw==", "bodyText": "I think this test might be overkill. We haven't done anything like this for the other converted APIs. It's a little similar to MessageTest.testRequestSchemas, which was useful verifying the generated schemas when the message generator was being written. Soon testRequestSchemas will be redundant, so I guess we have to decide if we just trust the generator and our compatibility system tests or if we want some other canonical representation. Thoughts?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525595323", "createdAt": "2020-11-17T23:32:42Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/ProduceResponseTest.java", "diffHunk": "@@ -100,16 +113,92 @@ public void produceResponseRecordErrorsTest() {\n             ProduceResponse response = new ProduceResponse(responseData);\n             Struct struct = response.toStruct(ver);\n             assertEquals(\"Should use schema version \" + ver, ApiKeys.PRODUCE.responseSchema(ver), struct.schema());\n-            ProduceResponse.PartitionResponse deserialized = new ProduceResponse(struct).responses().get(tp);\n+            ProduceResponse.PartitionResponse deserialized = new ProduceResponse(new ProduceResponseData(struct, ver)).responses().get(tp);\n             if (ver >= 8) {\n                 assertEquals(1, deserialized.recordErrors.size());\n                 assertEquals(3, deserialized.recordErrors.get(0).batchIndex);\n                 assertEquals(\"Record error\", deserialized.recordErrors.get(0).message);\n                 assertEquals(\"Produce failed\", deserialized.errorMessage);\n             } else {\n                 assertEquals(0, deserialized.recordErrors.size());\n-                assertEquals(null, deserialized.errorMessage);\n+                assertNull(deserialized.errorMessage);\n             }\n         }\n     }\n+\n+    /**\n+     * the schema in this test is from previous code and the automatic protocol should be compatible to previous schema.\n+     */\n+    @Test\n+    public void testCompatibility() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 71}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzMjg5MDQ4", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-533289048", "createdAt": "2020-11-18T10:11:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDoxMToxM1rOH1mXCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDo0NTowN1rOH1nuBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk2NTA2NQ==", "bodyText": "nit: As we got rid of the streaming api in this section, would it make sense to also remove this one?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525965065", "createdAt": "2020-11-18T10:11:13Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "diffHunk": "@@ -560,13 +562,24 @@ private void handleProduceResponse(ClientResponse response, Map<TopicPartition,\n             log.trace(\"Received produce response from node {} with correlation id {}\", response.destination(), correlationId);\n             // if we have a response, parse it\n             if (response.hasResponse()) {\n+                // Sender should exercise PartitionProduceResponse rather than ProduceResponse.PartitionResponse\n+                // https://issues.apache.org/jira/browse/KAFKA-10696\n                 ProduceResponse produceResponse = (ProduceResponse) response.responseBody();\n-                for (Map.Entry<TopicPartition, ProduceResponse.PartitionResponse> entry : produceResponse.responses().entrySet()) {\n-                    TopicPartition tp = entry.getKey();\n-                    ProduceResponse.PartitionResponse partResp = entry.getValue();\n+                produceResponse.data().responses().forEach(r -> r.partitionResponses().forEach(p -> {\n+                    TopicPartition tp = new TopicPartition(r.name(), p.index());\n+                    ProduceResponse.PartitionResponse partResp = new ProduceResponse.PartitionResponse(\n+                            Errors.forCode(p.errorCode()),\n+                            p.baseOffset(),\n+                            p.logAppendTimeMs(),\n+                            p.logStartOffset(),\n+                            p.recordErrors()\n+                                .stream()\n+                                .map(e -> new ProduceResponse.RecordError(e.batchIndex(), e.batchIndexErrorMessage()))\n+                                .collect(Collectors.toList()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3MTQ5Nw==", "bodyText": "nit: Should we remove this usage of the stream api here as well?", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525971497", "createdAt": "2020-11-18T10:20:53Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -203,119 +77,88 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param responses Produced data grouped by topic-partition\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n+    @Deprecated\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(toData(responses, throttleTimeMs));\n     }\n \n-    /**\n-     * Constructor from a {@link Struct}.\n-     */\n-    public ProduceResponse(Struct struct) {\n-        responses = new HashMap<>();\n-        for (Object topicResponse : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicRespStruct = (Struct) topicResponse;\n-            String topic = topicRespStruct.get(TOPIC_NAME);\n-\n-            for (Object partResponse : topicRespStruct.getArray(PARTITION_RESPONSES_KEY_NAME)) {\n-                Struct partRespStruct = (Struct) partResponse;\n-                int partition = partRespStruct.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partRespStruct.get(ERROR_CODE));\n-                long offset = partRespStruct.getLong(BASE_OFFSET_KEY_NAME);\n-                long logAppendTime = partRespStruct.hasField(LOG_APPEND_TIME_KEY_NAME) ?\n-                        partRespStruct.getLong(LOG_APPEND_TIME_KEY_NAME) : RecordBatch.NO_TIMESTAMP;\n-                long logStartOffset = partRespStruct.getOrElse(LOG_START_OFFSET_FIELD, INVALID_OFFSET);\n-\n-                List<RecordError> recordErrors = Collections.emptyList();\n-                if (partRespStruct.hasField(RECORD_ERRORS_KEY_NAME)) {\n-                    Object[] recordErrorsArray = partRespStruct.getArray(RECORD_ERRORS_KEY_NAME);\n-                    if (recordErrorsArray.length > 0) {\n-                        recordErrors = new ArrayList<>(recordErrorsArray.length);\n-                        for (Object indexAndMessage : recordErrorsArray) {\n-                            Struct indexAndMessageStruct = (Struct) indexAndMessage;\n-                            recordErrors.add(new RecordError(\n-                                    indexAndMessageStruct.getInt(BATCH_INDEX_KEY_NAME),\n-                                    indexAndMessageStruct.get(BATCH_INDEX_ERROR_MESSAGE_FIELD)\n-                            ));\n-                        }\n-                    }\n-                }\n+    @Override\n+    protected Send toSend(String destination, ResponseHeader header, short apiVersion) {\n+        return SendBuilder.buildResponseSend(destination, header, this.data, apiVersion);\n+    }\n \n-                String errorMessage = partRespStruct.getOrElse(ERROR_MESSAGE_FIELD, null);\n-                TopicPartition tp = new TopicPartition(topic, partition);\n-                responses.put(tp, new PartitionResponse(error, offset, logAppendTime, logStartOffset, recordErrors, errorMessage));\n+    private static ProduceResponseData toData(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n+        ProduceResponseData data = new ProduceResponseData().setThrottleTimeMs(throttleTimeMs);\n+        responses.forEach((tp, response) -> {\n+            ProduceResponseData.TopicProduceResponse tpr = data.responses().find(tp.topic());\n+            if (tpr == null) {\n+                tpr = new ProduceResponseData.TopicProduceResponse().setName(tp.topic());\n+                data.responses().add(tpr);\n             }\n-        }\n-        this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);\n+            tpr.partitionResponses()\n+                .add(new ProduceResponseData.PartitionProduceResponse()\n+                    .setIndex(tp.partition())\n+                    .setBaseOffset(response.baseOffset)\n+                    .setLogStartOffset(response.logStartOffset)\n+                    .setLogAppendTimeMs(response.logAppendTime)\n+                    .setErrorMessage(response.errorMessage)\n+                    .setErrorCode(response.error.code())\n+                    .setRecordErrors(response.recordErrors\n+                        .stream()\n+                        .map(e -> new ProduceResponseData.BatchIndexAndErrorMessage()\n+                            .setBatchIndex(e.batchIndex)\n+                            .setBatchIndexErrorMessage(e.message))\n+                        .collect(Collectors.toList())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 288}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3MzIxMw==", "bodyText": "nit: Add a new line.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525973213", "createdAt": "2020-11-18T10:23:32Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java", "diffHunk": "@@ -48,4 +53,41 @@ public static ByteBuffer serialize(Struct headerStruct, Struct bodyStruct) {\n         buffer.rewind();\n         return buffer;\n     }\n-}\n+\n+    // visible for testing\n+    public static boolean hasIdempotentRecords(ProduceRequest request) {\n+        return flags(request).getKey();\n+    }\n+\n+    // visible for testing\n+    public static boolean hasTransactionalRecords(ProduceRequest request) {\n+        return flags(request).getValue();\n+    }\n+\n+    /**\n+     * Get both hasIdempotentRecords flag and hasTransactionalRecords flag from produce request.\n+     * Noted that we find all flags at once to avoid duplicate loop and record batch construction.\n+     * @return first flag is \"hasIdempotentRecords\" and another is \"hasTransactionalRecords\"\n+     */\n+    public static AbstractMap.SimpleEntry<Boolean, Boolean> flags(ProduceRequest request) {\n+        boolean hasIdempotentRecords = false;\n+        boolean hasTransactionalRecords = false;\n+        for (ProduceRequestData.TopicProduceData tpd : request.dataOrException().topicData()) {\n+            for (ProduceRequestData.PartitionProduceData ppd : tpd.partitionData()) {\n+                BaseRecords records = ppd.records();\n+                if (records instanceof Records) {\n+                    Iterator<? extends RecordBatch> iterator = ((Records) records).batches().iterator();\n+                    if (iterator.hasNext()) {\n+                        RecordBatch batch = iterator.next();\n+                        hasIdempotentRecords = hasIdempotentRecords || batch.hasProducerId();\n+                        hasTransactionalRecords = hasTransactionalRecords || batch.isTransactional();\n+                    }\n+                }\n+                // return early\n+                if (hasIdempotentRecords && hasTransactionalRecords)\n+                    return new AbstractMap.SimpleEntry<>(true, true);\n+            }\n+        }\n+        return new AbstractMap.SimpleEntry<>(hasIdempotentRecords, hasTransactionalRecords);\n+    }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3NTg0OQ==", "bodyText": "nit: It seem that TransactionalId is null by default so we don't have to set it explicitly all the time. There are few cases in the this file and in others. I am not sure if this was intentional so I am also fine if you want to keep them.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525975849", "createdAt": "2020-11-18T10:27:21Z", "author": {"login": "dajac"}, "path": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "diffHunk": "@@ -154,8 +155,11 @@ public void testClose() {\n         client.poll(1, time.milliseconds());\n         assertTrue(\"The client should be ready\", client.isReady(node, time.milliseconds()));\n \n-        ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000,\n-                Collections.emptyMap());\n+        ProduceRequest.Builder builder = ProduceRequest.forCurrentMagic(new ProduceRequestData()\n+                .setTopicData(new ProduceRequestData.TopicProduceDataCollection())\n+                .setAcks((short) 1)\n+                .setTimeoutMs(1000)\n+                .setTransactionalId(null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3ODM5Nw==", "bodyText": "nit: It may be better to use PRODUCE.latestVersion() to stay inline with L204. There are few cases like this in the file.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525978397", "createdAt": "2020-11-18T10:31:18Z", "author": {"login": "dajac"}, "path": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "diffHunk": "@@ -198,8 +202,9 @@ private void checkSimpleRequestResponse(NetworkClient networkClient) {\n         ResponseHeader respHeader =\n             new ResponseHeader(request.correlationId(),\n                 request.apiKey().responseHeaderVersion(PRODUCE.latestVersion()));\n-        Struct resp = new Struct(PRODUCE.responseSchema(PRODUCE.latestVersion()));\n-        resp.set(\"responses\", new Object[0]);\n+        Struct resp = new ProduceResponseData()\n+                .setThrottleTimeMs(100)\n+                .toStruct(ProduceResponseData.HIGHEST_SUPPORTED_VERSION);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MTkyMA==", "bodyText": "nit: I am not sure if this is important or not but we were using -1 previously.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525981920", "createdAt": "2020-11-18T10:36:43Z", "author": {"login": "dajac"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/ProduceRequestTest.java", "diffHunk": "@@ -192,16 +258,21 @@ public void testMixedTransactionalData() {\n         final MemoryRecords txnRecords = MemoryRecords.withTransactionalRecords(CompressionType.NONE, producerId,\n                 producerEpoch, sequence, new SimpleRecord(\"bar\".getBytes()));\n \n-        final Map<TopicPartition, MemoryRecords> recordsByPartition = new LinkedHashMap<>();\n-        recordsByPartition.put(new TopicPartition(\"foo\", 0), txnRecords);\n-        recordsByPartition.put(new TopicPartition(\"foo\", 1), nonTxnRecords);\n-\n-        final ProduceRequest.Builder builder = ProduceRequest.Builder.forMagic(RecordVersion.current().value, (short) -1, 5000,\n-                recordsByPartition, transactionalId);\n+        ProduceRequest.Builder builder = ProduceRequest.forMagic(RecordBatch.CURRENT_MAGIC_VALUE,\n+                new ProduceRequestData()\n+                        .setTopicData(new ProduceRequestData.TopicProduceDataCollection(Arrays.asList(\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(0).setRecords(txnRecords))),\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(1).setRecords(nonTxnRecords))))\n+                                .iterator()))\n+                        .setAcks((short) 1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MjUxMA==", "bodyText": "We were setting transactionalId previously.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525982510", "createdAt": "2020-11-18T10:37:41Z", "author": {"login": "dajac"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/ProduceRequestTest.java", "diffHunk": "@@ -192,16 +258,21 @@ public void testMixedTransactionalData() {\n         final MemoryRecords txnRecords = MemoryRecords.withTransactionalRecords(CompressionType.NONE, producerId,\n                 producerEpoch, sequence, new SimpleRecord(\"bar\".getBytes()));\n \n-        final Map<TopicPartition, MemoryRecords> recordsByPartition = new LinkedHashMap<>();\n-        recordsByPartition.put(new TopicPartition(\"foo\", 0), txnRecords);\n-        recordsByPartition.put(new TopicPartition(\"foo\", 1), nonTxnRecords);\n-\n-        final ProduceRequest.Builder builder = ProduceRequest.Builder.forMagic(RecordVersion.current().value, (short) -1, 5000,\n-                recordsByPartition, transactionalId);\n+        ProduceRequest.Builder builder = ProduceRequest.forMagic(RecordBatch.CURRENT_MAGIC_VALUE,\n+                new ProduceRequestData()\n+                        .setTopicData(new ProduceRequestData.TopicProduceDataCollection(Arrays.asList(\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(0).setRecords(txnRecords))),\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(1).setRecords(nonTxnRecords))))\n+                                .iterator()))\n+                        .setAcks((short) 1)\n+                        .setTimeoutMs(5000)\n+                        .setTransactionalId(null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4NzMzNQ==", "bodyText": "I wonder if this one should be ignorable. It seems that we were ignoring it before when it was not present in the target version:\nstruct.setIfExists(NULLABLE_TRANSACTIONAL_ID, transactionalId)", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525987335", "createdAt": "2020-11-18T10:45:07Z", "author": {"login": "dajac"}, "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"default\": \"null\", \"entityType\": \"transactionalId\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzNDMzNDk2", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-533433496", "createdAt": "2020-11-18T13:15:26Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzNTY0NzQ3", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-533564747", "createdAt": "2020-11-18T15:30:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNTozMDowNlrOH1zZOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNTozMDowNlrOH1zZOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE3ODYxNg==", "bodyText": "We should keep this one.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526178616", "createdAt": "2020-11-18T15:30:06Z", "author": {"login": "dajac"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1730,7 +1768,6 @@ private DeleteTopicsResponse createDeleteTopicsResponse() {\n \n     private InitProducerIdRequest createInitPidRequest() {\n         InitProducerIdRequestData requestData = new InitProducerIdRequestData()\n-                .setTransactionalId(null)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzNTY2NTA2", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-533566506", "createdAt": "2020-11-18T15:31:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNTozMTo0N1rOH1zekA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNTozMTo0N1rOH1zekA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE3OTk4NA==", "bodyText": "We should keep this one as well.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526179984", "createdAt": "2020-11-18T15:31:47Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java", "diffHunk": "@@ -580,7 +580,6 @@ synchronized void bumpIdempotentEpochAndResetIdIfNeeded() {\n             if (currentState != State.INITIALIZING && !hasProducerId()) {\n                 transitionTo(State.INITIALIZING);\n                 InitProducerIdRequestData requestData = new InitProducerIdRequestData()\n-                        .setTransactionalId(null)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzNjcyODE4", "url": "https://github.com/apache/kafka/pull/9401#pullrequestreview-533672818", "createdAt": "2020-11-18T17:11:54Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNzoxMTo1NVrOH14X2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNzoxMTo1NVrOH14X2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI2MDE4NA==", "bodyText": "I don't think it should be ignorable. Transactional requests require this in order to authorize.", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526260184", "createdAt": "2020-11-18T17:11:55Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"default\": \"null\", \"ignorable\": true, \"entityType\": \"transactionalId\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f34e7c2e4df48316c18a7a956cb4616b22236a22", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/f34e7c2e4df48316c18a7a956cb4616b22236a22", "committedDate": "2020-11-18T17:17:47Z", "message": "KAFKA-9628 Replace Produce request with automated protocol"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f68102df76c76d1b3a5ec055def10a34e83ac128", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/f68102df76c76d1b3a5ec055def10a34e83ac128", "committedDate": "2020-11-18T17:17:47Z", "message": "Replace Produce response with automated protocol"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96e354aacdb37243a6d438fa70844171e49d8185", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/96e354aacdb37243a6d438fa70844171e49d8185", "committedDate": "2020-11-18T17:17:48Z", "message": "add compatibility test; benchmark; a bit refactor on server to use protocol data"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c8e4fc6781a0c14f63e8b668e550e9c1df1881b", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/0c8e4fc6781a0c14f63e8b668e550e9c1df1881b", "committedDate": "2020-11-18T17:17:48Z", "message": "optimize getErrorResponse; fix SchemaTestUtils"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15a33f7d28eddf5442e56802d41c99821db257da", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/15a33f7d28eddf5442e56802d41c99821db257da", "committedDate": "2020-11-18T17:17:48Z", "message": "fix produceRequestGetErrorResponseTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c7c4877642c9371e42f55470f5ded60ce385efb0", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/c7c4877642c9371e42f55470f5ded60ce385efb0", "committedDate": "2020-11-18T17:17:48Z", "message": "address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9f32e50ecb64f3ef98618d8b54eee4d6a55cf3b", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/a9f32e50ecb64f3ef98618d8b54eee4d6a55cf3b", "committedDate": "2020-11-18T17:17:48Z", "message": "add jira link and improve partitionSizes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4bf6bcd948db2845e4528fb4b6e160bf4b95dea", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/c4bf6bcd948db2845e4528fb4b6e160bf4b95dea", "committedDate": "2020-11-18T17:17:48Z", "message": "git rid of stream APIs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0b3259d0e292ea05a689c64450a617759ec592c", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/b0b3259d0e292ea05a689c64450a617759ec592c", "committedDate": "2020-11-18T17:17:48Z", "message": "use writable to replace struct"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46449be74ba8cb58970073760f907a2a40d49e86", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/46449be74ba8cb58970073760f907a2a40d49e86", "committedDate": "2020-11-18T17:17:48Z", "message": "imporve conversion; add deprecation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f55116a4e06d2cc06bb39dc9645a61a6346b1829", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/f55116a4e06d2cc06bb39dc9645a61a6346b1829", "committedDate": "2020-11-18T17:17:48Z", "message": "remove deprecation reference of ProduceRequest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc11f9012963ba11dd60189dbdbdffde31ec2478", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/bc11f9012963ba11dd60189dbdbdffde31ec2478", "committedDate": "2020-11-18T17:17:48Z", "message": "remove compatibility test and redundant TODO"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2ca27d3012eb7a5c0868bce295dd895176475de", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/a2ca27d3012eb7a5c0868bce295dd895176475de", "committedDate": "2020-11-18T17:17:48Z", "message": "address review comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34b5b179e2baa4eb7a1fc7aeec05eb549cf5e7f2", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/34b5b179e2baa4eb7a1fc7aeec05eb549cf5e7f2", "committedDate": "2020-11-18T17:17:48Z", "message": "fix failed tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "committedDate": "2020-11-18T18:12:36Z", "message": "ignorable=false"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "committedDate": "2020-11-18T18:12:36Z", "message": "ignorable=false"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 486, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}