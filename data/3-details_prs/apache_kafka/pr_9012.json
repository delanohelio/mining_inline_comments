{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4MDUwODc4", "number": 9012, "title": "KAFKA-10270: A broker to controller channel manager", "bodyText": "Co-authored-by: Viktor Somogyi viktorsomogyi@gmail.com\nCo-authored-by: Boyang Chen boyang@confluent.io\nAdd a broker to controller channel manager for use cases from KIP-590 and KIP-497. Kudos to Viktor, the code template is from #7716\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-07-13T05:53:20Z", "url": "https://github.com/apache/kafka/pull/9012", "merged": true, "mergeCommit": {"oid": "de2e6938c8648f02254a645a8fff9c2fa8364ef1"}, "closed": true, "closedAt": "2020-07-29T18:40:15Z", "author": {"login": "abbccdda"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc0kBDygBqjM1NDAzMzM5OTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc5tnUBAFqTQ1NzY5MDkyNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NDU5MTAx", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447459101", "createdAt": "2020-07-13T17:34:29Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNzozNDoyOVrOGwytJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNzo0MDoyMFrOGwy6Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgxNTU4OA==", "bodyText": "Since take is blocking, do we need the peek call above? Actually, does InterBrokerSendThread expect generateRequests to block like this? Since it returns an iterable, I would guess that it doesn't. Maybe we should do something like:\nval topRequest = requestQueue.poll()\nif (topRequest != null) {\n   // ...\n}", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453815588", "createdAt": "2020-07-13T17:34:29Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{BlockingQueue, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.common.{InterBrokerSendThread, RequestAndCompletionHandler}\n+import kafka.network.RequestChannel\n+import kafka.utils.Logging\n+import org.apache.kafka.clients._\n+import org.apache.kafka.common.requests.AbstractRequest\n+import org.apache.kafka.common.utils.{LogContext, Time}\n+import org.apache.kafka.common.Node\n+import org.apache.kafka.common.metrics.Metrics\n+import org.apache.kafka.common.network._\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.AbstractRequest.NoOpRequestBuilder\n+import org.apache.kafka.common.security.JaasContext\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * This class manages the connection between a broker and the controller. It runs a single\n+ * {@link BrokerToControllerRequestThread} which uses the broker's metadata cache as its own metadata to find\n+ * and connect to the controller. The channel is async and runs the network connection in the background.\n+ * The maximum number of in-flight requests are set to one to ensure orderly response from the controller, therefore\n+ * care must be taken to not block on outstanding requests for too long.\n+ */\n+class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache,\n+                                       time: Time,\n+                                       metrics: Metrics,\n+                                       config: KafkaConfig,\n+                                       threadNamePrefix: Option[String] = None) extends Logging {\n+  private val requestQueue = new LinkedBlockingQueue[BrokerToControllerQueueItem]\n+  private val logContext = new LogContext(s\"[broker-${config.brokerId}-to-controller] \")\n+  private val manualMetadataUpdater = new ManualMetadataUpdater()\n+  private val requestThread = newRequestThread\n+\n+  def start(): Unit = {\n+    requestThread.start()\n+  }\n+\n+  def shutdown(): Unit = {\n+    requestThread.shutdown()\n+    requestThread.awaitShutdown()\n+  }\n+\n+  private[server] def newRequestThread = {\n+    val brokerToControllerListenerName = config.controlPlaneListenerName.getOrElse(config.interBrokerListenerName)\n+    val brokerToControllerSecurityProtocol = config.controlPlaneSecurityProtocol.getOrElse(config.interBrokerSecurityProtocol)\n+\n+    val networkClient = {\n+      val channelBuilder = ChannelBuilders.clientChannelBuilder(\n+        brokerToControllerSecurityProtocol,\n+        JaasContext.Type.SERVER,\n+        config,\n+        brokerToControllerListenerName,\n+        config.saslMechanismInterBrokerProtocol,\n+        time,\n+        config.saslInterBrokerHandshakeRequestEnable,\n+        logContext\n+      )\n+      val selector = new Selector(\n+        NetworkReceive.UNLIMITED,\n+        Selector.NO_IDLE_TIMEOUT_MS,\n+        metrics,\n+        time,\n+        \"BrokerToControllerChannel\",\n+        Map(\"BrokerId\" -> config.brokerId.toString).asJava,\n+        false,\n+        channelBuilder,\n+        logContext\n+      )\n+      new NetworkClient(\n+        selector,\n+        manualMetadataUpdater,\n+        config.brokerId.toString,\n+        1,\n+        0,\n+        0,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        config.requestTimeoutMs,\n+        config.connectionSetupTimeoutMs,\n+        config.connectionSetupTimeoutMaxMs,\n+        ClientDnsLookup.DEFAULT,\n+        time,\n+        false,\n+        new ApiVersions,\n+        logContext\n+      )\n+    }\n+    val threadName = threadNamePrefix match {\n+      case None => s\"broker-${config.brokerId}-to-controller-send-thread\"\n+      case Some(name) => s\"$name:broker-${config.brokerId}-to-controller-send-thread\"\n+    }\n+\n+    new BrokerToControllerRequestThread(networkClient, manualMetadataUpdater, requestQueue, metadataCache, config,\n+      brokerToControllerListenerName, time, threadName)\n+  }\n+\n+  private[server] def sendRequest(request: RequestChannel.Request,\n+                                  callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(request.context.header.apiKey, request.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, \"\"))\n+  }\n+\n+  private[server] def forwardRequest(originalRequest: RequestChannel.Request,\n+                                     callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(originalRequest.context.header.apiKey, originalRequest.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, originalRequest.context.principal.getName))\n+  }\n+}\n+\n+case class BrokerToControllerQueueItem(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                                       callback: RequestCompletionHandler,\n+                                       initialPrincipalName: String)\n+\n+class BrokerToControllerRequestThread(networkClient: KafkaClient,\n+                                      metadataUpdater: ManualMetadataUpdater,\n+                                      requestQueue: BlockingQueue[BrokerToControllerQueueItem],\n+                                      metadataCache: kafka.server.MetadataCache,\n+                                      config: KafkaConfig,\n+                                      listenerName: ListenerName,\n+                                      time: Time,\n+                                      threadName: String)\n+  extends InterBrokerSendThread(threadName, networkClient, time, isInterruptible = false) {\n+\n+  private var activeController: Option[Node] = None\n+\n+  override def requestTimeoutMs: Int = config.controllerSocketTimeoutMs\n+\n+  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+    val requestsToSend = new mutable.Queue[RequestAndCompletionHandler]\n+    if (requestQueue.peek() != null) {\n+      val topRequest = requestQueue.take()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgxODk2Mg==", "bodyText": "Do we need anything in the broker to controller connection to be reconfigurable? Just wondering if we need a way to recreate this thread in cases where config changes", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453818962", "createdAt": "2020-07-13T17:40:20Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{BlockingQueue, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.common.{InterBrokerSendThread, RequestAndCompletionHandler}\n+import kafka.network.RequestChannel\n+import kafka.utils.Logging\n+import org.apache.kafka.clients._\n+import org.apache.kafka.common.requests.AbstractRequest\n+import org.apache.kafka.common.utils.{LogContext, Time}\n+import org.apache.kafka.common.Node\n+import org.apache.kafka.common.metrics.Metrics\n+import org.apache.kafka.common.network._\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.AbstractRequest.NoOpRequestBuilder\n+import org.apache.kafka.common.security.JaasContext\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * This class manages the connection between a broker and the controller. It runs a single\n+ * {@link BrokerToControllerRequestThread} which uses the broker's metadata cache as its own metadata to find\n+ * and connect to the controller. The channel is async and runs the network connection in the background.\n+ * The maximum number of in-flight requests are set to one to ensure orderly response from the controller, therefore\n+ * care must be taken to not block on outstanding requests for too long.\n+ */\n+class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache,\n+                                       time: Time,\n+                                       metrics: Metrics,\n+                                       config: KafkaConfig,\n+                                       threadNamePrefix: Option[String] = None) extends Logging {\n+  private val requestQueue = new LinkedBlockingQueue[BrokerToControllerQueueItem]\n+  private val logContext = new LogContext(s\"[broker-${config.brokerId}-to-controller] \")\n+  private val manualMetadataUpdater = new ManualMetadataUpdater()\n+  private val requestThread = newRequestThread\n+\n+  def start(): Unit = {\n+    requestThread.start()\n+  }\n+\n+  def shutdown(): Unit = {\n+    requestThread.shutdown()\n+    requestThread.awaitShutdown()\n+  }\n+\n+  private[server] def newRequestThread = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTMwODk0", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447530894", "createdAt": "2020-07-13T19:18:02Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToxODowMlrOGw2WDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToxODowMlrOGw2WDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3NTIxMg==", "bodyText": "it would be good to use the existing style of periods at the end to avoid a big change here", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453875212", "createdAt": "2020-07-13T19:18:02Z", "author": {"login": "cmccabe"}, "path": "clients/src/main/java/org/apache/kafka/clients/ClientRequest.java", "diffHunk": "@@ -85,11 +89,12 @@ public ApiKeys apiKey() {\n     public RequestHeader makeHeader(short version) {\n         short requestApiKey = requestBuilder.apiKey().id;\n         return new RequestHeader(\n-            new RequestHeaderData().\n-                setRequestApiKey(requestApiKey).\n-                setRequestApiVersion(version).\n-                setClientId(clientId).\n-                setCorrelationId(correlationId),\n+            new RequestHeaderData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTMxMDM0", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447531034", "createdAt": "2020-07-13T19:18:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToxODoxNVrOGw2WcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToxODoxNVrOGw2WcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3NTMxMw==", "bodyText": "seems like a whitespace error here", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453875313", "createdAt": "2020-07-13T19:18:15Z", "author": {"login": "cmccabe"}, "path": "clients/src/main/java/org/apache/kafka/clients/KafkaClient.java", "diffHunk": "@@ -186,16 +186,18 @@ ClientRequest newClientRequest(String nodeId, AbstractRequest.Builder<?> request\n      * @param createdTimeMs the time in milliseconds to use as the creation time of the request\n      * @param expectResponse true iff we expect a response\n      * @param requestTimeoutMs Upper bound time in milliseconds to await a response before disconnecting the socket and\n-     *                         cancelling the request. The request may get cancelled sooner if the socket disconnects\n-     *                         for any reason including if another pending request to the same node timed out first.\n+*                         cancelling the request. The request may get cancelled sooner if the socket disconnects", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTMxNDQ5", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447531449", "createdAt": "2020-07-13T19:18:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToxODo1OFrOGw2X4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToxODo1OFrOGw2X4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3NTY4Mw==", "bodyText": "It seems like we only want this to be set when forwarding a request to the controller, right?  So the JavaDoc should discuss the fact that this can be null (or whatever the \"not set\" value is)", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453875683", "createdAt": "2020-07-13T19:18:58Z", "author": {"login": "cmccabe"}, "path": "clients/src/main/java/org/apache/kafka/clients/KafkaClient.java", "diffHunk": "@@ -186,16 +186,18 @@ ClientRequest newClientRequest(String nodeId, AbstractRequest.Builder<?> request\n      * @param createdTimeMs the time in milliseconds to use as the creation time of the request\n      * @param expectResponse true iff we expect a response\n      * @param requestTimeoutMs Upper bound time in milliseconds to await a response before disconnecting the socket and\n-     *                         cancelling the request. The request may get cancelled sooner if the socket disconnects\n-     *                         for any reason including if another pending request to the same node timed out first.\n+*                         cancelling the request. The request may get cancelled sooner if the socket disconnects\n+*                         for any reason including if another pending request to the same node timed out first.\n      * @param callback the callback to invoke when we get a response\n+     * @param initialPrincipalName the initial client principal name, when building a forward request", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTMyNDgx", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447532481", "createdAt": "2020-07-13T19:20:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyMDoyN1rOGw2bWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyMDoyN1rOGw2bWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3NjU3MQ==", "bodyText": "I think it would be best to use null here as the \"not set\" value.  We never clearly articulated that the empty string is not valid for principals.", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453876571", "createdAt": "2020-07-13T19:20:27Z", "author": {"login": "cmccabe"}, "path": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java", "diffHunk": "@@ -1199,7 +1199,7 @@ public ClientRequest newClientRequest(String nodeId,\n                                           AbstractRequest.Builder<?> requestBuilder,\n                                           long createdTimeMs,\n                                           boolean expectResponse) {\n-        return newClientRequest(nodeId, requestBuilder, createdTimeMs, expectResponse, defaultRequestTimeoutMs, null);\n+        return newClientRequest(nodeId, requestBuilder, createdTimeMs, expectResponse, defaultRequestTimeoutMs, null, \"\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTMzNjU5", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447533659", "createdAt": "2020-07-13T19:22:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyMjoxMVrOGw2fEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyMjoxMVrOGw2fEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3NzUyMg==", "bodyText": "\"NoOp\" doesn't seem quite right here since the builder isn't an operator.  Maybe \"pass through\" or \"simple\" ?", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453877522", "createdAt": "2020-07-13T19:22:11Z", "author": {"login": "cmccabe"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java", "diffHunk": "@@ -75,6 +75,19 @@ public T build() {\n         public abstract T build(short version);\n     }\n \n+    public static class NoOpRequestBuilder extends Builder<AbstractRequest> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTM0MDIx", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447534021", "createdAt": "2020-07-13T19:22:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyMjo0NlrOGw2gXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyMjo0NlrOGw2gXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3Nzg1Mg==", "bodyText": "Let's add an accessor here rather than making data fields public.", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453877852", "createdAt": "2020-07-13T19:22:46Z", "author": {"login": "cmccabe"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsResponse.java", "diffHunk": "@@ -29,7 +29,7 @@\n \n public class AlterConfigsResponse extends AbstractResponse {\n \n-    private final AlterConfigsResponseData data;\n+    public final AlterConfigsResponseData data;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTM1NTY1", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447535565", "createdAt": "2020-07-13T19:25:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyNTowM1rOGw2lNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOToyNTowM1rOGw2lNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3OTA5Mg==", "bodyText": "Why not just make all versions of the field nullable by setting a nullVersions: \"2+\"?", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453879092", "createdAt": "2020-07-13T19:25:03Z", "author": {"login": "cmccabe"}, "path": "clients/src/main/resources/common/message/RequestHeader.json", "diffHunk": "@@ -37,6 +37,9 @@\n     // Since the client is sending the ApiVersionsRequest in order to discover what\n     // versions are supported, the client does not know the best version to use.\n     { \"name\": \"ClientId\", \"type\": \"string\", \"versions\": \"1+\", \"nullableVersions\": \"1+\", \"ignorable\": true,\n-      \"flexibleVersions\": \"none\", \"about\": \"The client ID string.\" }\n+      \"flexibleVersions\": \"none\", \"about\": \"The client ID string.\" },\n+    // Could not set default to null, because not all versions of this field are nullable.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTQwNjgx", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447540681", "createdAt": "2020-07-13T19:32:23Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOTozMjoyM1rOGw211g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOTozMjoyM1rOGw211g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg4MzM1MA==", "bodyText": "This seems like a confusing comment.  How about \"forward the request to the controller for handling\"?\nAlso, we usually try to avoid return in KafkaApis unless it's really necessary.  I guess it's a Scala style thing.", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453883350", "createdAt": "2020-07-13T19:32:23Z", "author": {"login": "cmccabe"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2425,34 +2427,68 @@ class KafkaApis(val requestChannel: RequestChannel,\n \n   def handleAlterConfigsRequest(request: RequestChannel.Request): Unit = {\n     val alterConfigsRequest = request.body[AlterConfigsRequest]\n-    val (authorizedResources, unauthorizedResources) = alterConfigsRequest.configs.asScala.toMap.partition { case (resource, _) =>\n+    val requestResources = alterConfigsRequest.configs.asScala.toMap\n+\n+    def sendResponseCallback(results: Map[ConfigResource, ApiError]): Unit = {\n+      def responseCallback(requestThrottleMs: Int): AlterConfigsResponse = {\n+        val data = new AlterConfigsResponseData()\n+          .setThrottleTimeMs(requestThrottleMs)\n+        results.foreach { case (resource, error) =>\n+          data.responses().add(new AlterConfigsResourceResponse()\n+            .setErrorCode(error.error.code)\n+            .setErrorMessage(error.message)\n+            .setResourceName(resource.name)\n+            .setResourceType(resource.`type`.id))\n+        }\n+        new AlterConfigsResponse(data)\n+      }\n+      sendResponseMaybeThrottle(request, responseCallback)\n+    }\n+\n+    if (!controller.isActive) {\n+      // This is the original forwarded request which needs the handling.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTQyMjI2", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-447542226", "createdAt": "2020-07-13T19:34:52Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOTozNDo1MlrOGw26qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOTozNDo1MlrOGw26qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg4NDU4Ng==", "bodyText": "How about \"ControllerUplinkManager\" ?", "url": "https://github.com/apache/kafka/pull/9012#discussion_r453884586", "createdAt": "2020-07-13T19:34:52Z", "author": {"login": "cmccabe"}, "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -168,6 +168,8 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n \n   var kafkaController: KafkaController = null\n \n+  var brokerToControllerChannelManager: BrokerToControllerChannelManager = null", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4MDg1Nzkw", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-448085790", "createdAt": "2020-07-14T13:04:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxMzowNDo1MFrOGxSvhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxMzoxNDo1NFrOGxTIOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM0MDQ4NQ==", "bodyText": "If other requests are queued up, I think this could mess up the ordering.\nIf we have requests[A, B, C, D] in the requestQueue, and A popped off is sent out, and the controller channel is disconnected, we will then have a queue like [B, C, D, A]. I think we need strict ordering for controller requests, so we would need to handle this differently.\nMaybe if we used a deque for the request queue, we could fix this by putting a failed item at the head of the queue. That, or the doWork call would need to hold onto a queue item until it had succeeded or failed irrevocably. But, I would guess we shouldn't block too long in the doWork call.", "url": "https://github.com/apache/kafka/pull/9012#discussion_r454340485", "createdAt": "2020-07-14T13:04:50Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{BlockingQueue, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.common.{InterBrokerSendThread, RequestAndCompletionHandler}\n+import kafka.network.RequestChannel\n+import kafka.utils.Logging\n+import org.apache.kafka.clients._\n+import org.apache.kafka.common.requests.AbstractRequest\n+import org.apache.kafka.common.utils.{LogContext, Time}\n+import org.apache.kafka.common.Node\n+import org.apache.kafka.common.metrics.Metrics\n+import org.apache.kafka.common.network._\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.AbstractRequest.NoOpRequestBuilder\n+import org.apache.kafka.common.security.JaasContext\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * This class manages the connection between a broker and the controller. It runs a single\n+ * {@link BrokerToControllerRequestThread} which uses the broker's metadata cache as its own metadata to find\n+ * and connect to the controller. The channel is async and runs the network connection in the background.\n+ * The maximum number of in-flight requests are set to one to ensure orderly response from the controller, therefore\n+ * care must be taken to not block on outstanding requests for too long.\n+ */\n+class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache,\n+                                       time: Time,\n+                                       metrics: Metrics,\n+                                       config: KafkaConfig,\n+                                       threadNamePrefix: Option[String] = None) extends Logging {\n+  private val requestQueue = new LinkedBlockingQueue[BrokerToControllerQueueItem]\n+  private val logContext = new LogContext(s\"[broker-${config.brokerId}-to-controller] \")\n+  private val manualMetadataUpdater = new ManualMetadataUpdater()\n+  private val requestThread = newRequestThread\n+\n+  def start(): Unit = {\n+    requestThread.start()\n+  }\n+\n+  def shutdown(): Unit = {\n+    requestThread.shutdown()\n+    requestThread.awaitShutdown()\n+  }\n+\n+  private[server] def newRequestThread = {\n+    val brokerToControllerListenerName = config.controlPlaneListenerName.getOrElse(config.interBrokerListenerName)\n+    val brokerToControllerSecurityProtocol = config.controlPlaneSecurityProtocol.getOrElse(config.interBrokerSecurityProtocol)\n+\n+    val networkClient = {\n+      val channelBuilder = ChannelBuilders.clientChannelBuilder(\n+        brokerToControllerSecurityProtocol,\n+        JaasContext.Type.SERVER,\n+        config,\n+        brokerToControllerListenerName,\n+        config.saslMechanismInterBrokerProtocol,\n+        time,\n+        config.saslInterBrokerHandshakeRequestEnable,\n+        logContext\n+      )\n+      val selector = new Selector(\n+        NetworkReceive.UNLIMITED,\n+        Selector.NO_IDLE_TIMEOUT_MS,\n+        metrics,\n+        time,\n+        \"BrokerToControllerChannel\",\n+        Map(\"BrokerId\" -> config.brokerId.toString).asJava,\n+        false,\n+        channelBuilder,\n+        logContext\n+      )\n+      new NetworkClient(\n+        selector,\n+        manualMetadataUpdater,\n+        config.brokerId.toString,\n+        1,\n+        0,\n+        0,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        config.requestTimeoutMs,\n+        config.connectionSetupTimeoutMs,\n+        config.connectionSetupTimeoutMaxMs,\n+        ClientDnsLookup.DEFAULT,\n+        time,\n+        false,\n+        new ApiVersions,\n+        logContext\n+      )\n+    }\n+    val threadName = threadNamePrefix match {\n+      case None => s\"broker-${config.brokerId}-to-controller-send-thread\"\n+      case Some(name) => s\"$name:broker-${config.brokerId}-to-controller-send-thread\"\n+    }\n+\n+    new BrokerToControllerRequestThread(networkClient, manualMetadataUpdater, requestQueue, metadataCache, config,\n+      brokerToControllerListenerName, time, threadName)\n+  }\n+\n+  private[server] def sendRequest(request: RequestChannel.Request,\n+                                  callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(request.context.header.apiKey, request.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, \"\"))\n+  }\n+\n+  private[server] def forwardRequest(originalRequest: RequestChannel.Request,\n+                                     callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(originalRequest.context.header.apiKey, originalRequest.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, originalRequest.context.principal.getName))\n+  }\n+}\n+\n+case class BrokerToControllerQueueItem(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                                       callback: RequestCompletionHandler,\n+                                       initialPrincipalName: String)\n+\n+class BrokerToControllerRequestThread(networkClient: KafkaClient,\n+                                      metadataUpdater: ManualMetadataUpdater,\n+                                      requestQueue: BlockingQueue[BrokerToControllerQueueItem],\n+                                      metadataCache: kafka.server.MetadataCache,\n+                                      config: KafkaConfig,\n+                                      listenerName: ListenerName,\n+                                      time: Time,\n+                                      threadName: String)\n+  extends InterBrokerSendThread(threadName, networkClient, time, isInterruptible = false) {\n+\n+  private var activeController: Option[Node] = None\n+\n+  override def requestTimeoutMs: Int = config.controllerSocketTimeoutMs\n+\n+  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+    val requestsToSend = new mutable.Queue[RequestAndCompletionHandler]\n+    if (requestQueue.peek() != null) {\n+      val topRequest = requestQueue.take()\n+\n+      val request = RequestAndCompletionHandler(\n+        activeController.get,\n+        topRequest.request,\n+        handleResponse(topRequest),\n+        topRequest.initialPrincipalName)\n+      requestsToSend.enqueue(request)\n+    }\n+    requestsToSend\n+  }\n+\n+  private[server] def handleResponse(request: BrokerToControllerQueueItem)(response: ClientResponse): Unit = {\n+    if (response.wasDisconnected()) {\n+      activeController = None\n+      requestQueue.put(request)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM0Mzg1Nw==", "bodyText": "What exceptions can actually be captured here? It looks like InterBrokerSendThread will kill the JVM if it encounters any Throwable by throwing a FatalExitError (which won't be caught as an Exception).\nIt looks like IBST will handle disconnects by setting the disconnected flag in the ClientResponse (which we're checking up at L165)", "url": "https://github.com/apache/kafka/pull/9012#discussion_r454343857", "createdAt": "2020-07-14T13:10:09Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{BlockingQueue, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.common.{InterBrokerSendThread, RequestAndCompletionHandler}\n+import kafka.network.RequestChannel\n+import kafka.utils.Logging\n+import org.apache.kafka.clients._\n+import org.apache.kafka.common.requests.AbstractRequest\n+import org.apache.kafka.common.utils.{LogContext, Time}\n+import org.apache.kafka.common.Node\n+import org.apache.kafka.common.metrics.Metrics\n+import org.apache.kafka.common.network._\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.AbstractRequest.NoOpRequestBuilder\n+import org.apache.kafka.common.security.JaasContext\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * This class manages the connection between a broker and the controller. It runs a single\n+ * {@link BrokerToControllerRequestThread} which uses the broker's metadata cache as its own metadata to find\n+ * and connect to the controller. The channel is async and runs the network connection in the background.\n+ * The maximum number of in-flight requests are set to one to ensure orderly response from the controller, therefore\n+ * care must be taken to not block on outstanding requests for too long.\n+ */\n+class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache,\n+                                       time: Time,\n+                                       metrics: Metrics,\n+                                       config: KafkaConfig,\n+                                       threadNamePrefix: Option[String] = None) extends Logging {\n+  private val requestQueue = new LinkedBlockingQueue[BrokerToControllerQueueItem]\n+  private val logContext = new LogContext(s\"[broker-${config.brokerId}-to-controller] \")\n+  private val manualMetadataUpdater = new ManualMetadataUpdater()\n+  private val requestThread = newRequestThread\n+\n+  def start(): Unit = {\n+    requestThread.start()\n+  }\n+\n+  def shutdown(): Unit = {\n+    requestThread.shutdown()\n+    requestThread.awaitShutdown()\n+  }\n+\n+  private[server] def newRequestThread = {\n+    val brokerToControllerListenerName = config.controlPlaneListenerName.getOrElse(config.interBrokerListenerName)\n+    val brokerToControllerSecurityProtocol = config.controlPlaneSecurityProtocol.getOrElse(config.interBrokerSecurityProtocol)\n+\n+    val networkClient = {\n+      val channelBuilder = ChannelBuilders.clientChannelBuilder(\n+        brokerToControllerSecurityProtocol,\n+        JaasContext.Type.SERVER,\n+        config,\n+        brokerToControllerListenerName,\n+        config.saslMechanismInterBrokerProtocol,\n+        time,\n+        config.saslInterBrokerHandshakeRequestEnable,\n+        logContext\n+      )\n+      val selector = new Selector(\n+        NetworkReceive.UNLIMITED,\n+        Selector.NO_IDLE_TIMEOUT_MS,\n+        metrics,\n+        time,\n+        \"BrokerToControllerChannel\",\n+        Map(\"BrokerId\" -> config.brokerId.toString).asJava,\n+        false,\n+        channelBuilder,\n+        logContext\n+      )\n+      new NetworkClient(\n+        selector,\n+        manualMetadataUpdater,\n+        config.brokerId.toString,\n+        1,\n+        0,\n+        0,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        config.requestTimeoutMs,\n+        config.connectionSetupTimeoutMs,\n+        config.connectionSetupTimeoutMaxMs,\n+        ClientDnsLookup.DEFAULT,\n+        time,\n+        false,\n+        new ApiVersions,\n+        logContext\n+      )\n+    }\n+    val threadName = threadNamePrefix match {\n+      case None => s\"broker-${config.brokerId}-to-controller-send-thread\"\n+      case Some(name) => s\"$name:broker-${config.brokerId}-to-controller-send-thread\"\n+    }\n+\n+    new BrokerToControllerRequestThread(networkClient, manualMetadataUpdater, requestQueue, metadataCache, config,\n+      brokerToControllerListenerName, time, threadName)\n+  }\n+\n+  private[server] def sendRequest(request: RequestChannel.Request,\n+                                  callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(request.context.header.apiKey, request.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, \"\"))\n+  }\n+\n+  private[server] def forwardRequest(originalRequest: RequestChannel.Request,\n+                                     callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(originalRequest.context.header.apiKey, originalRequest.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, originalRequest.context.principal.getName))\n+  }\n+}\n+\n+case class BrokerToControllerQueueItem(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                                       callback: RequestCompletionHandler,\n+                                       initialPrincipalName: String)\n+\n+class BrokerToControllerRequestThread(networkClient: KafkaClient,\n+                                      metadataUpdater: ManualMetadataUpdater,\n+                                      requestQueue: BlockingQueue[BrokerToControllerQueueItem],\n+                                      metadataCache: kafka.server.MetadataCache,\n+                                      config: KafkaConfig,\n+                                      listenerName: ListenerName,\n+                                      time: Time,\n+                                      threadName: String)\n+  extends InterBrokerSendThread(threadName, networkClient, time, isInterruptible = false) {\n+\n+  private var activeController: Option[Node] = None\n+\n+  override def requestTimeoutMs: Int = config.controllerSocketTimeoutMs\n+\n+  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+    val requestsToSend = new mutable.Queue[RequestAndCompletionHandler]\n+    if (requestQueue.peek() != null) {\n+      val topRequest = requestQueue.take()\n+\n+      val request = RequestAndCompletionHandler(\n+        activeController.get,\n+        topRequest.request,\n+        handleResponse(topRequest),\n+        topRequest.initialPrincipalName)\n+      requestsToSend.enqueue(request)\n+    }\n+    requestsToSend\n+  }\n+\n+  private[server] def handleResponse(request: BrokerToControllerQueueItem)(response: ClientResponse): Unit = {\n+    if (response.wasDisconnected()) {\n+      activeController = None\n+      requestQueue.put(request)\n+    } else if (response.responseBody().errorCounts().containsKey(Errors.NOT_CONTROLLER)) {\n+      // just close the controller connection and wait for metadata cache update in doWork\n+      networkClient.close(activeController.get.idString)\n+      activeController = None\n+      requestQueue.put(request)\n+    } else {\n+      request.callback.onComplete(response)\n+    }\n+  }\n+\n+  private[server] def backoff(): Unit = pause(100, TimeUnit.MILLISECONDS)\n+\n+  override def doWork(): Unit = {\n+    try {\n+      if (activeController.isDefined) {\n+        super.doWork()\n+      } else {\n+        debug(\"Controller isn't cached, looking for local metadata changes\")\n+        val controllerOpt = metadataCache.getControllerId.flatMap(metadataCache.getAliveBroker)\n+        if (controllerOpt.isDefined) {\n+          if (activeController.isEmpty || activeController.exists(_.id != controllerOpt.get.id))\n+            info(s\"Recorded new controller, from now on will use broker ${controllerOpt.get.id}\")\n+          activeController = Option(controllerOpt.get.node(listenerName))\n+          metadataUpdater.setNodes(metadataCache.getAliveBrokers.map(_.node(listenerName)).asJava)\n+        } else {\n+          // need to backoff to avoid tight loops\n+          debug(\"No controller defined in metadata cache, retrying after backoff\")\n+          backoff()\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM0NjgxMA==", "bodyText": "Maybe an exponential backoff instead of constant?", "url": "https://github.com/apache/kafka/pull/9012#discussion_r454346810", "createdAt": "2020-07-14T13:14:54Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{BlockingQueue, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.common.{InterBrokerSendThread, RequestAndCompletionHandler}\n+import kafka.network.RequestChannel\n+import kafka.utils.Logging\n+import org.apache.kafka.clients._\n+import org.apache.kafka.common.requests.AbstractRequest\n+import org.apache.kafka.common.utils.{LogContext, Time}\n+import org.apache.kafka.common.Node\n+import org.apache.kafka.common.metrics.Metrics\n+import org.apache.kafka.common.network._\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.AbstractRequest.NoOpRequestBuilder\n+import org.apache.kafka.common.security.JaasContext\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * This class manages the connection between a broker and the controller. It runs a single\n+ * {@link BrokerToControllerRequestThread} which uses the broker's metadata cache as its own metadata to find\n+ * and connect to the controller. The channel is async and runs the network connection in the background.\n+ * The maximum number of in-flight requests are set to one to ensure orderly response from the controller, therefore\n+ * care must be taken to not block on outstanding requests for too long.\n+ */\n+class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache,\n+                                       time: Time,\n+                                       metrics: Metrics,\n+                                       config: KafkaConfig,\n+                                       threadNamePrefix: Option[String] = None) extends Logging {\n+  private val requestQueue = new LinkedBlockingQueue[BrokerToControllerQueueItem]\n+  private val logContext = new LogContext(s\"[broker-${config.brokerId}-to-controller] \")\n+  private val manualMetadataUpdater = new ManualMetadataUpdater()\n+  private val requestThread = newRequestThread\n+\n+  def start(): Unit = {\n+    requestThread.start()\n+  }\n+\n+  def shutdown(): Unit = {\n+    requestThread.shutdown()\n+    requestThread.awaitShutdown()\n+  }\n+\n+  private[server] def newRequestThread = {\n+    val brokerToControllerListenerName = config.controlPlaneListenerName.getOrElse(config.interBrokerListenerName)\n+    val brokerToControllerSecurityProtocol = config.controlPlaneSecurityProtocol.getOrElse(config.interBrokerSecurityProtocol)\n+\n+    val networkClient = {\n+      val channelBuilder = ChannelBuilders.clientChannelBuilder(\n+        brokerToControllerSecurityProtocol,\n+        JaasContext.Type.SERVER,\n+        config,\n+        brokerToControllerListenerName,\n+        config.saslMechanismInterBrokerProtocol,\n+        time,\n+        config.saslInterBrokerHandshakeRequestEnable,\n+        logContext\n+      )\n+      val selector = new Selector(\n+        NetworkReceive.UNLIMITED,\n+        Selector.NO_IDLE_TIMEOUT_MS,\n+        metrics,\n+        time,\n+        \"BrokerToControllerChannel\",\n+        Map(\"BrokerId\" -> config.brokerId.toString).asJava,\n+        false,\n+        channelBuilder,\n+        logContext\n+      )\n+      new NetworkClient(\n+        selector,\n+        manualMetadataUpdater,\n+        config.brokerId.toString,\n+        1,\n+        0,\n+        0,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        config.requestTimeoutMs,\n+        config.connectionSetupTimeoutMs,\n+        config.connectionSetupTimeoutMaxMs,\n+        ClientDnsLookup.DEFAULT,\n+        time,\n+        false,\n+        new ApiVersions,\n+        logContext\n+      )\n+    }\n+    val threadName = threadNamePrefix match {\n+      case None => s\"broker-${config.brokerId}-to-controller-send-thread\"\n+      case Some(name) => s\"$name:broker-${config.brokerId}-to-controller-send-thread\"\n+    }\n+\n+    new BrokerToControllerRequestThread(networkClient, manualMetadataUpdater, requestQueue, metadataCache, config,\n+      brokerToControllerListenerName, time, threadName)\n+  }\n+\n+  private[server] def sendRequest(request: RequestChannel.Request,\n+                                  callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(request.context.header.apiKey, request.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, \"\"))\n+  }\n+\n+  private[server] def forwardRequest(originalRequest: RequestChannel.Request,\n+                                     callback: RequestCompletionHandler): Unit = {\n+    val requestBuilder = new NoOpRequestBuilder(originalRequest.context.header.apiKey, originalRequest.body[AbstractRequest])\n+    requestQueue.put(BrokerToControllerQueueItem(requestBuilder, callback, originalRequest.context.principal.getName))\n+  }\n+}\n+\n+case class BrokerToControllerQueueItem(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                                       callback: RequestCompletionHandler,\n+                                       initialPrincipalName: String)\n+\n+class BrokerToControllerRequestThread(networkClient: KafkaClient,\n+                                      metadataUpdater: ManualMetadataUpdater,\n+                                      requestQueue: BlockingQueue[BrokerToControllerQueueItem],\n+                                      metadataCache: kafka.server.MetadataCache,\n+                                      config: KafkaConfig,\n+                                      listenerName: ListenerName,\n+                                      time: Time,\n+                                      threadName: String)\n+  extends InterBrokerSendThread(threadName, networkClient, time, isInterruptible = false) {\n+\n+  private var activeController: Option[Node] = None\n+\n+  override def requestTimeoutMs: Int = config.controllerSocketTimeoutMs\n+\n+  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+    val requestsToSend = new mutable.Queue[RequestAndCompletionHandler]\n+    if (requestQueue.peek() != null) {\n+      val topRequest = requestQueue.take()\n+\n+      val request = RequestAndCompletionHandler(\n+        activeController.get,\n+        topRequest.request,\n+        handleResponse(topRequest),\n+        topRequest.initialPrincipalName)\n+      requestsToSend.enqueue(request)\n+    }\n+    requestsToSend\n+  }\n+\n+  private[server] def handleResponse(request: BrokerToControllerQueueItem)(response: ClientResponse): Unit = {\n+    if (response.wasDisconnected()) {\n+      activeController = None\n+      requestQueue.put(request)\n+    } else if (response.responseBody().errorCounts().containsKey(Errors.NOT_CONTROLLER)) {\n+      // just close the controller connection and wait for metadata cache update in doWork\n+      networkClient.close(activeController.get.idString)\n+      activeController = None\n+      requestQueue.put(request)\n+    } else {\n+      request.callback.onComplete(response)\n+    }\n+  }\n+\n+  private[server] def backoff(): Unit = pause(100, TimeUnit.MILLISECONDS)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 178}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxMjYwMTM2", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-451260136", "createdAt": "2020-07-20T02:43:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwMjo0Mzo1OVrOGz1DWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwMjo0Mzo1OVrOGz1DWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk5OTc3MQ==", "bodyText": "We have switched this default everywhere else, right?", "url": "https://github.com/apache/kafka/pull/9012#discussion_r456999771", "createdAt": "2020-07-20T02:43:59Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{BlockingQueue, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.common.{InterBrokerSendThread, RequestAndCompletionHandler}\n+import kafka.network.RequestChannel\n+import kafka.utils.Logging\n+import org.apache.kafka.clients._\n+import org.apache.kafka.common.requests.AbstractRequest\n+import org.apache.kafka.common.utils.{LogContext, Time}\n+import org.apache.kafka.common.Node\n+import org.apache.kafka.common.metrics.Metrics\n+import org.apache.kafka.common.network._\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.AbstractRequest.NoOpRequestBuilder\n+import org.apache.kafka.common.security.JaasContext\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * This class manages the connection between a broker and the controller. It runs a single\n+ * {@link BrokerToControllerRequestThread} which uses the broker's metadata cache as its own metadata to find\n+ * and connect to the controller. The channel is async and runs the network connection in the background.\n+ * The maximum number of in-flight requests are set to one to ensure orderly response from the controller, therefore\n+ * care must be taken to not block on outstanding requests for too long.\n+ */\n+class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache,\n+                                       time: Time,\n+                                       metrics: Metrics,\n+                                       config: KafkaConfig,\n+                                       threadNamePrefix: Option[String] = None) extends Logging {\n+  private val requestQueue = new LinkedBlockingQueue[BrokerToControllerQueueItem]\n+  private val logContext = new LogContext(s\"[broker-${config.brokerId}-to-controller] \")\n+  private val manualMetadataUpdater = new ManualMetadataUpdater()\n+  private val requestThread = newRequestThread\n+\n+  def start(): Unit = {\n+    requestThread.start()\n+  }\n+\n+  def shutdown(): Unit = {\n+    requestThread.shutdown()\n+    requestThread.awaitShutdown()\n+  }\n+\n+  private[server] def newRequestThread = {\n+    val brokerToControllerListenerName = config.controlPlaneListenerName.getOrElse(config.interBrokerListenerName)\n+    val brokerToControllerSecurityProtocol = config.controlPlaneSecurityProtocol.getOrElse(config.interBrokerSecurityProtocol)\n+\n+    val networkClient = {\n+      val channelBuilder = ChannelBuilders.clientChannelBuilder(\n+        brokerToControllerSecurityProtocol,\n+        JaasContext.Type.SERVER,\n+        config,\n+        brokerToControllerListenerName,\n+        config.saslMechanismInterBrokerProtocol,\n+        time,\n+        config.saslInterBrokerHandshakeRequestEnable,\n+        logContext\n+      )\n+      val selector = new Selector(\n+        NetworkReceive.UNLIMITED,\n+        Selector.NO_IDLE_TIMEOUT_MS,\n+        metrics,\n+        time,\n+        \"BrokerToControllerChannel\",\n+        Map(\"BrokerId\" -> config.brokerId.toString).asJava,\n+        false,\n+        channelBuilder,\n+        logContext\n+      )\n+      new NetworkClient(\n+        selector,\n+        manualMetadataUpdater,\n+        config.brokerId.toString,\n+        1,\n+        0,\n+        0,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        config.requestTimeoutMs,\n+        config.connectionSetupTimeoutMs,\n+        config.connectionSetupTimeoutMaxMs,\n+        ClientDnsLookup.DEFAULT,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "cfc3da339c68ab50bdf390439bef378b84939571", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/cfc3da339c68ab50bdf390439bef378b84939571", "committedDate": "2020-07-28T22:36:40Z", "message": "Add a broker to controller channel manager\n\nCo-authored-by: Viktor Somogyi <viktorsomogyi@gmail.com>\nCo-authored-by: Boyang Chen <boyang@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5178db595a0b2f62db826e7a3c511eb4d28b7d72", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/5178db595a0b2f62db826e7a3c511eb4d28b7d72", "committedDate": "2020-07-28T22:37:38Z", "message": "address Colin's comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "5178db595a0b2f62db826e7a3c511eb4d28b7d72", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/5178db595a0b2f62db826e7a3c511eb4d28b7d72", "committedDate": "2020-07-28T22:37:38Z", "message": "address Colin's comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ba296669a410df32b536b63804edf8369f1b96c", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/1ba296669a410df32b536b63804edf8369f1b96c", "committedDate": "2020-07-29T00:30:56Z", "message": "remove initial principal name"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3NTkxNDA4", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-457591408", "createdAt": "2020-07-29T14:40:39Z", "commit": {"oid": "1ba296669a410df32b536b63804edf8369f1b96c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0MDo0MFrOG47xKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0MDo0MFrOG47xKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1MjY4Mg==", "bodyText": "Even though it won't be hit (since we just have a single thread accessing this queue) there's still a race between peek and poll. I think we should just use poll and do the null test on that result.", "url": "https://github.com/apache/kafka/pull/9012#discussion_r462352682", "createdAt": "2020-07-29T14:40:40Z", "author": {"login": "mumrah"}, "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{LinkedBlockingDeque, TimeUnit}\n+\n+import kafka.common.{InterBrokerSendThread, RequestAndCompletionHandler}\n+import kafka.utils.Logging\n+import org.apache.kafka.clients._\n+import org.apache.kafka.common.requests.AbstractRequest\n+import org.apache.kafka.common.utils.{LogContext, Time}\n+import org.apache.kafka.common.Node\n+import org.apache.kafka.common.metrics.Metrics\n+import org.apache.kafka.common.network._\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.security.JaasContext\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * This class manages the connection between a broker and the controller. It runs a single\n+ * {@link BrokerToControllerRequestThread} which uses the broker's metadata cache as its own metadata to find\n+ * and connect to the controller. The channel is async and runs the network connection in the background.\n+ * The maximum number of in-flight requests are set to one to ensure orderly response from the controller, therefore\n+ * care must be taken to not block on outstanding requests for too long.\n+ */\n+class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache,\n+                                       time: Time,\n+                                       metrics: Metrics,\n+                                       config: KafkaConfig,\n+                                       threadNamePrefix: Option[String] = None) extends Logging {\n+  private val requestQueue = new LinkedBlockingDeque[BrokerToControllerQueueItem]\n+  private val logContext = new LogContext(s\"[broker-${config.brokerId}-to-controller] \")\n+  private val manualMetadataUpdater = new ManualMetadataUpdater()\n+  private val requestThread = newRequestThread\n+\n+  def start(): Unit = {\n+    requestThread.start()\n+  }\n+\n+  def shutdown(): Unit = {\n+    requestThread.shutdown()\n+    requestThread.awaitShutdown()\n+  }\n+\n+  private[server] def newRequestThread = {\n+    val brokerToControllerListenerName = config.controlPlaneListenerName.getOrElse(config.interBrokerListenerName)\n+    val brokerToControllerSecurityProtocol = config.controlPlaneSecurityProtocol.getOrElse(config.interBrokerSecurityProtocol)\n+\n+    val networkClient = {\n+      val channelBuilder = ChannelBuilders.clientChannelBuilder(\n+        brokerToControllerSecurityProtocol,\n+        JaasContext.Type.SERVER,\n+        config,\n+        brokerToControllerListenerName,\n+        config.saslMechanismInterBrokerProtocol,\n+        time,\n+        config.saslInterBrokerHandshakeRequestEnable,\n+        logContext\n+      )\n+      val selector = new Selector(\n+        NetworkReceive.UNLIMITED,\n+        Selector.NO_IDLE_TIMEOUT_MS,\n+        metrics,\n+        time,\n+        \"BrokerToControllerChannel\",\n+        Map(\"BrokerId\" -> config.brokerId.toString).asJava,\n+        false,\n+        channelBuilder,\n+        logContext\n+      )\n+      new NetworkClient(\n+        selector,\n+        manualMetadataUpdater,\n+        config.brokerId.toString,\n+        1,\n+        0,\n+        0,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        Selectable.USE_DEFAULT_BUFFER_SIZE,\n+        config.requestTimeoutMs,\n+        config.connectionSetupTimeoutMs,\n+        config.connectionSetupTimeoutMaxMs,\n+        ClientDnsLookup.USE_ALL_DNS_IPS,\n+        time,\n+        false,\n+        new ApiVersions,\n+        logContext\n+      )\n+    }\n+    val threadName = threadNamePrefix match {\n+      case None => s\"broker-${config.brokerId}-to-controller-send-thread\"\n+      case Some(name) => s\"$name:broker-${config.brokerId}-to-controller-send-thread\"\n+    }\n+\n+    new BrokerToControllerRequestThread(networkClient, manualMetadataUpdater, requestQueue, metadataCache, config,\n+      brokerToControllerListenerName, time, threadName)\n+  }\n+\n+  private[server] def sendRequest(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                                  callback: RequestCompletionHandler): Unit = {\n+    requestQueue.put(BrokerToControllerQueueItem(request, callback))\n+  }\n+}\n+\n+case class BrokerToControllerQueueItem(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                                       callback: RequestCompletionHandler)\n+\n+class BrokerToControllerRequestThread(networkClient: KafkaClient,\n+                                      metadataUpdater: ManualMetadataUpdater,\n+                                      requestQueue: LinkedBlockingDeque[BrokerToControllerQueueItem],\n+                                      metadataCache: kafka.server.MetadataCache,\n+                                      config: KafkaConfig,\n+                                      listenerName: ListenerName,\n+                                      time: Time,\n+                                      threadName: String)\n+  extends InterBrokerSendThread(threadName, networkClient, time, isInterruptible = false) {\n+\n+  private var activeController: Option[Node] = None\n+\n+  override def requestTimeoutMs: Int = config.controllerSocketTimeoutMs\n+\n+  override def generateRequests(): Iterable[RequestAndCompletionHandler] = {\n+    val requestsToSend = new mutable.Queue[RequestAndCompletionHandler]\n+    if (requestQueue.peek() != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1ba296669a410df32b536b63804edf8369f1b96c"}, "originalPosition": 141}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66a3d65dc84f0b98379fc5955201922ce4f01112", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/66a3d65dc84f0b98379fc5955201922ce4f01112", "committedDate": "2020-07-29T16:09:27Z", "message": "use pool only"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3NjkwOTI3", "url": "https://github.com/apache/kafka/pull/9012#pullrequestreview-457690927", "createdAt": "2020-07-29T16:28:26Z", "commit": {"oid": "66a3d65dc84f0b98379fc5955201922ce4f01112"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1269, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}