{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwNDk1MDIw", "number": 8697, "title": "KAFKA-9983: KIP-613, add INFO level e2e latency metrics", "bodyText": "Moved all metrics to processor-node-level, but the INOF level metrics are recorded only at the source and \"sink\" nodes (in quotations as this may be a non-sink terminal node)", "createdAt": "2020-05-20T04:15:32Z", "url": "https://github.com/apache/kafka/pull/8697", "merged": true, "mergeCommit": {"oid": "83c616f70694637627edb6b1215738c78b74a50d"}, "closed": true, "closedAt": "2020-05-27T20:55:30Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 29, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcjkshxgFqTQxNjUwODkxMw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcle4OtgFqTQxOTU2NTAxNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2NTA4OTEz", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-416508913", "createdAt": "2020-05-21T21:33:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQyMTozMzowM1rOGZDr8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQyMTozODowMFrOGZD1zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkyNzk4NQ==", "bodyText": "Assuming that a task might have a cache, is this correct, ie, has been fully processed by the task)?", "url": "https://github.com/apache/kafka/pull/8697#discussion_r428927985", "createdAt": "2020-05-21T21:33:03Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java", "diffHunk": "@@ -86,6 +87,14 @@ private TaskMetrics() {}\n     private static final String NUM_BUFFERED_RECORDS_DESCRIPTION = \"The count of buffered records that are polled \" +\n         \"from consumer and not yet processed for this active task\";\n \n+    private static final String RECORD_E2E_LATENCY = \"record-e2e-latency\";\n+    static final String RECORD_E2E_LATENCY_MAX_DESCRIPTION =\n+        \"The maximum end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+            + \"system time when it has been fully processed by the task\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkzMDI3Ng==", "bodyText": "We we increase this ts to 35? This would allow to test min in the last step better", "url": "https://github.com/apache/kafka/pull/8697#discussion_r428930276", "createdAt": "2020-05-21T21:37:28Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -420,6 +422,50 @@ public void shouldRecordProcessRatio() {\n         assertThat(metric.metricValue(), equalTo(1.0d));\n     }\n \n+    @Test\n+    public void shouldRecordE2ELatency() {\n+        time = new MockTime(0L, 0L, 0L);\n+        metrics = new Metrics(new MetricConfig().recordLevel(Sensor.RecordingLevel.DEBUG), time);\n+\n+        task = createStatelessTask(createConfig(false, \"0\"), StreamsConfig.METRICS_LATEST);\n+\n+        final KafkaMetric maxMetric = getMetric(\"record-e2e-latency\", \"%s-max\", task.id().toString(), StreamsConfig.METRICS_LATEST);\n+        final KafkaMetric minMetric = getMetric(\"record-e2e-latency\", \"%s-min\", task.id().toString(), StreamsConfig.METRICS_LATEST);\n+\n+        assertThat(maxMetric.metricValue(), equalTo(Double.NaN));\n+\n+        task.addRecords(partition1, asList(\n+            getConsumerRecord(partition1, 0L),\n+            getConsumerRecord(partition1, 10L),\n+            getConsumerRecord(partition1, 5L),\n+            getConsumerRecord(partition1, 20L)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkzMDUwOQ==", "bodyText": "Nice one!", "url": "https://github.com/apache/kafka/pull/8697#discussion_r428930509", "createdAt": "2020-05-21T21:38:00Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetricsTest.java", "diffHunk": "@@ -14,15 +14,11 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.kafka.streams.kstream.internals.metrics;\n+package org.apache.kafka.streams.processor.internals.metrics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2Nzk1NDkw", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-416795490", "createdAt": "2020-05-22T10:04:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxMDowNDoxOVrOGZRv1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxMDozOTo1NVrOGZSq8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1ODM1Nw==", "bodyText": "prop (super-nit): Could you call the method addMinAndMaxToSensor() since we have already one method that is called addAvgAndMinAndMaxToSensor()?", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429158357", "createdAt": "2020-05-22T10:04:19Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -642,6 +642,30 @@ public static void addAvgAndMaxToSensor(final Sensor sensor,\n         );\n     }\n \n+    public static void addMaxAndMinToSensor(final Sensor sensor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1ODkzMQ==", "bodyText": "Yes, very nice!", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429158931", "createdAt": "2020-05-22T10:05:38Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetricsTest.java", "diffHunk": "@@ -14,15 +14,11 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.kafka.streams.kstream.internals.metrics;\n+package org.apache.kafka.streams.processor.internals.metrics;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkzMDUwOQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1OTU5Mw==", "bodyText": "req: Please do not use the constant here. The point of this test is also to check the correctness of the description, i.e., the content of that constant.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429159593", "createdAt": "2020-05-22T10:07:13Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetricsTest.java", "diffHunk": "@@ -356,4 +354,27 @@ public void shouldGetDroppedRecordsSensorOrLateRecordDropSensor() {\n             shouldGetDroppedRecordsSensor();\n         }\n     }\n+\n+    @Test\n+    public void shouldGetRecordE2ELatencySensor() {\n+        final String operation = \"record-e2e-latency\";\n+        expect(streamsMetrics.taskLevelSensor(THREAD_ID, TASK_ID, operation, RecordingLevel.INFO))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.taskLevelTagMap(THREAD_ID, TASK_ID)).andReturn(tagMap);\n+        StreamsMetricsImpl.addMaxAndMinToSensor(\n+            expectedSensor,\n+            TASK_LEVEL_GROUP,\n+            tagMap,\n+            operation,\n+            RECORD_E2E_LATENCY_MAX_DESCRIPTION,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MDY1NQ==", "bodyText": "req: Please define this constant as private as all the others. I left a request in TaskMetricsTest which makes this request clearer.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429160655", "createdAt": "2020-05-22T10:09:45Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java", "diffHunk": "@@ -86,6 +87,14 @@ private TaskMetrics() {}\n     private static final String NUM_BUFFERED_RECORDS_DESCRIPTION = \"The count of buffered records that are polled \" +\n         \"from consumer and not yet processed for this active task\";\n \n+    private static final String RECORD_E2E_LATENCY = \"record-e2e-latency\";\n+    static final String RECORD_E2E_LATENCY_MAX_DESCRIPTION =", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MjE3Ng==", "bodyText": "See my comment above.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429162176", "createdAt": "2020-05-22T10:13:10Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java", "diffHunk": "@@ -86,6 +87,14 @@ private TaskMetrics() {}\n     private static final String NUM_BUFFERED_RECORDS_DESCRIPTION = \"The count of buffered records that are polled \" +\n         \"from consumer and not yet processed for this active task\";\n \n+    private static final String RECORD_E2E_LATENCY = \"record-e2e-latency\";\n+    static final String RECORD_E2E_LATENCY_MAX_DESCRIPTION =\n+        \"The maximum end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+            + \"system time when it has been fully processed by the task\";\n+    static final String RECORD_E2E_LATENCY_MIN_DESCRIPTION =", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3MzQ4OA==", "bodyText": "Q: Is there a specific reason to init the sensor here and not in SinkNode? You can init and store it there. That was one motivation to make *Metrics classes (e.g. TaskMetrics) static, so that you do not need any code in the processor context to get specific sensors. If there is not specific reason, you could get rid of the changes in the *Context* classes.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429173488", "createdAt": "2020-05-22T10:39:55Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -137,6 +138,7 @@ public StreamTask(final TaskId id,\n         }\n         processRatioSensor = TaskMetrics.activeProcessRatioSensor(threadId, taskId, streamsMetrics);\n         processLatencySensor = TaskMetrics.processLatencySensor(threadId, taskId, streamsMetrics);\n+        recordE2ELatencySensor = TaskMetrics.recordE2ELatencySensor(threadId, taskId, streamsMetrics);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MTkxNzc3", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417191777", "createdAt": "2020-05-22T21:27:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMToyNzozNVrOGZj_ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMToyNzozNVrOGZj_ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ1NzMxMA==", "bodyText": "Checkstyle won't allow you to have a letter and number next to each other, but P90 and E2E_LATENCY seem preferable to P_90 and E_2_E_LATENCY", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429457310", "createdAt": "2020-05-22T21:27:35Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -183,6 +183,8 @@\n               files=\"StreamsPartitionAssignor.java\"/>\n     <suppress checks=\"JavaNCSS\"\n               files=\"EosBetaUpgradeIntegrationTest.java\"/>\n+    <suppress checks=\"StaticVariableName\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MTk0Mzgz", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417194383", "createdAt": "2020-05-22T21:34:46Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMTozNDo0NlrOGZkQLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMTozNDo0NlrOGZkQLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ2MTU1MA==", "bodyText": "Trying to build confidence in the Percentiles implementation and gauge the accuracy with a more complicated test.\nI found it was accurate to within 5% maybe 2/3 or 3/4 of the time, but it seems reasonable to expect it to be accurate to within 10%", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429461550", "createdAt": "2020-05-22T21:34:46Z", "author": {"login": "ableegoldman"}, "path": "clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java", "diffHunk": "@@ -492,6 +493,52 @@ public void testPercentiles() {\n         assertEquals(75, (Double) p75.metricValue(), 1.0);\n     }\n \n+    @Test\n+    public void testPercentilesWithRandomNumbersAndLinearBucketing() {\n+        long seed = new Random().nextLong();\n+        int sizeInBytes = 1000 * 1000;   // 1MB\n+        long maximumValue = 1000 * 24 * 60 * 60 * 1000L; // if values are ms, max is 1000 days\n+\n+        try {\n+            Random prng = new Random(seed);\n+            int numberOfValues = 5000 + prng.nextInt(10_000);  // ranges is [5000, 15000]\n+\n+            Percentiles percs = new Percentiles(sizeInBytes,\n+                                                maximumValue,\n+                                                BucketSizing.LINEAR,\n+                                                new Percentile(metrics.metricName(\"test.p90\", \"grp1\"), 90),\n+                                                new Percentile(metrics.metricName(\"test.p99\", \"grp1\"), 99));\n+            MetricConfig config = new MetricConfig().eventWindow(50).samples(2);\n+            Sensor sensor = metrics.sensor(\"test\", config);\n+            sensor.add(percs);\n+            Metric p90 = this.metrics.metrics().get(metrics.metricName(\"test.p90\", \"grp1\"));\n+            Metric p99 = this.metrics.metrics().get(metrics.metricName(\"test.p99\", \"grp1\"));\n+\n+            final List<Long> values = new ArrayList<>(numberOfValues);\n+            // record two windows worth of sequential values\n+            for (int i = 0; i < numberOfValues; ++i) {\n+                long value = Math.abs(prng.nextLong()) % maximumValue;\n+                values.add(value);\n+                sensor.record(value);\n+            }\n+\n+            Collections.sort(values);\n+\n+            int p90Index = (int)Math.ceil(((double)(90 * numberOfValues)) / 100);\n+            int p99Index = (int)Math.ceil(((double)(99 * numberOfValues)) / 100);\n+\n+            double expectedP90 = values.get(p90Index - 1);\n+            double expectedP99 = values.get(p99Index - 1);\n+\n+            assertEquals(expectedP90, (Double) p90.metricValue(), expectedP90 / 10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjA0MDA4", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417204008", "createdAt": "2020-05-22T21:57:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMTo1NzowMVrOGZkv0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMTo1NzowMVrOGZkv0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ2OTY1MQ==", "bodyText": "Want to call attention to these...do they seem reasonable? The size is the bytes per each percentile sensor, so 2 per source or terminal node. The minimum has to be 0 for the linear bucketing (which I found significantly more accurate than constant bucketing in my tests).\nOn the other hand, the maximum is obviously not representative of the maximum difference between the current and record timestamp. If someone's processing historical data, it can exceed this. But I figure if you're processing historical data than the e2e latency isn't really going to be at all useful anyways, so we may as well set it to something reasonable", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429469651", "createdAt": "2020-05-22T21:57:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -149,6 +154,10 @@ public int hashCode() {\n     public static final String RATE_DESCRIPTION_PREFIX = \"The average number of \";\n     public static final String RATE_DESCRIPTION_SUFFIX = \" per second\";\n \n+    public static final int PERCENTILES_SIZE_IN_BYTES = 1000 * 1000;    // 1 MB\n+    public static double MAXIMUM_E2E_LATENCY = 100 * 24 * 60 * 60 * 1000d; // maximum latency is 1000 days", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "80f1194441cfafda8d122c71e4b3f6bf88292541", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/80f1194441cfafda8d122c71e4b3f6bf88292541", "committedDate": "2020-05-23T00:03:10Z", "message": "get terminal nodes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9471ef214ca6a6a413d91402e178fbc841943b7a", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/9471ef214ca6a6a413d91402e178fbc841943b7a", "committedDate": "2020-05-23T00:03:10Z", "message": "add percentiles test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a97427912cfbee9631194294d7a9d2e1b3cb8e2", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/0a97427912cfbee9631194294d7a9d2e1b3cb8e2", "committedDate": "2020-05-23T00:03:10Z", "message": "add tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e0b85c1c22cfe5bfb431b8cc2b20018cc3ab61e", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/8e0b85c1c22cfe5bfb431b8cc2b20018cc3ab61e", "committedDate": "2020-05-23T00:03:10Z", "message": "add test for process"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9e7114afc6a10868c7202d60f36064304fc820d", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/f9e7114afc6a10868c7202d60f36064304fc820d", "committedDate": "2020-05-23T00:03:10Z", "message": "move method to correct place"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/a31d4f97d0e11e3842633bbf08deccdf06cc9f97", "committedDate": "2020-05-23T00:03:10Z", "message": "change to double"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/a31d4f97d0e11e3842633bbf08deccdf06cc9f97", "committedDate": "2020-05-23T00:03:10Z", "message": "change to double"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjM2NjY2", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417236666", "createdAt": "2020-05-23T01:01:58Z", "commit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMTowMTo1OFrOGZmlvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMTowMTo1OFrOGZmlvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTgzOQ==", "bodyText": "Not sure about this... Why do we need/want to have a limit?\nNit: double space", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429499839", "createdAt": "2020-05-23T01:01:58Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -899,6 +920,28 @@ public boolean maybePunctuateSystemTime() {\n         return punctuated;\n     }\n \n+    void maybeRecordE2ELatency(final long recordTimestamp, final String nodeName) {\n+        maybeRecordE2ELatency(recordTimestamp, time.milliseconds(), nodeName);\n+    }\n+\n+    private void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n+        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n+        if (e2eLatencySensor == null) {\n+            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n+        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n+            final long e2eLatency = now - recordTimestamp;\n+            if (e2eLatency >  MAXIMUM_E2E_LATENCY) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjM2NzE5", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417236719", "createdAt": "2020-05-23T01:02:23Z", "commit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMTowMjoyM1rOGZml7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMTowMjoyM1rOGZml7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTg4Nw==", "bodyText": "For this case, should we record \"zero\" instead?", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429499887", "createdAt": "2020-05-23T01:02:23Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -899,6 +920,28 @@ public boolean maybePunctuateSystemTime() {\n         return punctuated;\n     }\n \n+    void maybeRecordE2ELatency(final long recordTimestamp, final String nodeName) {\n+        maybeRecordE2ELatency(recordTimestamp, time.milliseconds(), nodeName);\n+    }\n+\n+    private void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n+        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n+        if (e2eLatencySensor == null) {\n+            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n+        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n+            final long e2eLatency = now - recordTimestamp;\n+            if (e2eLatency >  MAXIMUM_E2E_LATENCY) {\n+                log.warn(\"Skipped recording e2e latency for node {} because {} is higher than maximum allowed latency {}\",\n+                         nodeName, e2eLatency, MAXIMUM_E2E_LATENCY);\n+            } else if (e2eLatency < MINIMUM_E2E_LATENCY) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjM2ODM5", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417236839", "createdAt": "2020-05-23T01:03:31Z", "commit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMTowMzozMVrOGZmmag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMTowMzozMVrOGZmmag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwMDAxMA==", "bodyText": "Not sure about this. Why do we need a maximum to begin with? And why pick 10 days? Rather arbitrary?", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429500010", "createdAt": "2020-05-23T01:03:31Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -149,6 +154,10 @@ public int hashCode() {\n     public static final String RATE_DESCRIPTION_PREFIX = \"The average number of \";\n     public static final String RATE_DESCRIPTION_SUFFIX = \" per second\";\n \n+    public static final int PERCENTILES_SIZE_IN_BYTES = 1000 * 1000;    // 1 MB\n+    public static long MAXIMUM_E2E_LATENCY = 10 * 24 * 60 * 60 * 1000L; // maximum latency is 10 days", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3NTYwMDg0", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417560084", "createdAt": "2020-05-25T08:50:44Z", "commit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwODo1MDo0NVrOGZ5zFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMjowNzo0MlrOGZ_Ceg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTgxNDU0OA==", "bodyText": "req:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    replay(StreamsMetricsImpl.class, streamsMetrics);\n          \n          \n            \n            \n          \n          \n            \n                    final Sensor sensor = ProcessorNodeMetrics.recordE2ELatencySensor(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID, RecordingLevel.INFO, streamsMetrics);\n          \n          \n            \n            \n          \n          \n            \n                    verify(StreamsMetricsImpl.class, streamsMetrics);\n          \n          \n            \n                    assertThat(sensor, is(expectedSensor));\n          \n          \n            \n                    verifySensor(() -> ProcessorNodeMetrics.recordE2ELatencySensor(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID, RecordingLevel.INFO, streamsMetrics));", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429814548", "createdAt": "2020-05-25T08:50:45Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetricsTest.java", "diffHunk": "@@ -260,6 +261,43 @@ public void shouldGetProcessAtSourceSensorOrForwardSensor() {\n         }\n     }\n \n+    @Test\n+    public void shouldGetRecordE2ELatencySensor() {\n+        final String operation = \"record-e2e-latency\";\n+        final String recordE2ELatencyMinDescription =\n+            \"The minimum end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+                + \"system time when it has been fully processed by the node\";\n+        final String recordE2ELatencyMaxDescription =\n+            \"The maximum end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+                + \"system time when it has been fully processed by the node\";\n+        final String recordE2ELatencyP99Description =\n+            \"The 99th percentile end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+                + \"system time when it has been fully processed by the node\";\n+        final String recordE2ELatencyP90Description =\n+            \"The 90th percentile end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+                + \"system time when it has been fully processed by the node\";\n+        expect(streamsMetrics.nodeLevelSensor(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID, operation, RecordingLevel.INFO))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.nodeLevelTagMap(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID)).andReturn(tagMap);\n+        StreamsMetricsImpl.addMinAndMaxAndP99AndP90ToSensor(\n+            expectedSensor,\n+            PROCESSOR_NODE_LEVEL_GROUP,\n+            tagMap,\n+            operation,\n+            recordE2ELatencyMinDescription,\n+            recordE2ELatencyMaxDescription,\n+            recordE2ELatencyP99Description,\n+            recordE2ELatencyP90Description\n+        );\n+\n+        replay(StreamsMetricsImpl.class, streamsMetrics);\n+\n+        final Sensor sensor = ProcessorNodeMetrics.recordE2ELatencySensor(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID, RecordingLevel.INFO, streamsMetrics);\n+\n+        verify(StreamsMetricsImpl.class, streamsMetrics);\n+        assertThat(sensor, is(expectedSensor));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTgyMTY3Nw==", "bodyText": "prop: For the sake of readability, could you extract this check to a method named isTerminalNode()? Even better would be to add a method named isTerminalNode() to ProcessorNode and use it here and in ProcessorTopology.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429821677", "createdAt": "2020-05-25T09:04:39Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -223,6 +223,9 @@ public StateStore getStateStore(final String name) {\n                                 final V value) {\n         setCurrentNode(child);\n         child.process(key, value);\n+        if (child.children().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTgyODgwNg==", "bodyText": "See my comment in ProcessorContextImpl.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429828806", "createdAt": "2020-05-25T09:18:43Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java", "diffHunk": "@@ -50,6 +51,13 @@ public ProcessorTopology(final List<ProcessorNode<?, ?>> processorNodes,\n         this.globalStateStores = Collections.unmodifiableList(globalStateStores);\n         this.storeToChangelogTopic = Collections.unmodifiableMap(storeToChangelogTopic);\n         this.repartitionTopics = Collections.unmodifiableSet(repartitionTopics);\n+\n+        this.terminalNodes = new HashSet<>();\n+        for (final ProcessorNode<?, ?> node : processorNodes) {\n+            if (node.children().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg4NDIzNQ==", "bodyText": "I understand that we need a maximum due to the way the percentiles are approximated. Since the e2e latency depends on user requirements, it would make sense to consider a config for the max latency. I see two reasons for such a config.\n\n\nWe always think about near-realtime use cases, but there could also be use cases that are allowed to provide a much higher latency but the latency should still be within a certain limit. For example, one were producers are not always online. Admittedly, 10 days is already quite high.\n\n\nOTOH, decreasing the max latency would also make the metric more accurate, AFAIU. That would also be a reason for a config that users can tweak.\n\n\nFor both cases, we could leave it like it is for now and see if there is really the need for such a config. WDYT?", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429884235", "createdAt": "2020-05-25T11:28:12Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -149,6 +154,10 @@ public int hashCode() {\n     public static final String RATE_DESCRIPTION_PREFIX = \"The average number of \";\n     public static final String RATE_DESCRIPTION_SUFFIX = \" per second\";\n \n+    public static final int PERCENTILES_SIZE_IN_BYTES = 1000 * 1000;    // 1 MB\n+    public static long MAXIMUM_E2E_LATENCY = 10 * 24 * 60 * 60 * 1000L; // maximum latency is 10 days", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwMDAxMA=="}, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg4NjA0Mg==", "bodyText": "@ableegoldman I agree with your thinking here. IMO, we should just log the warning for now. If we see that there is a demand for such a metric, we can add it later on.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429886042", "createdAt": "2020-05-25T11:32:17Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -899,6 +920,28 @@ public boolean maybePunctuateSystemTime() {\n         return punctuated;\n     }\n \n+    void maybeRecordE2ELatency(final long recordTimestamp, final String nodeName) {\n+        maybeRecordE2ELatency(recordTimestamp, time.milliseconds(), nodeName);\n+    }\n+\n+    private void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n+        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n+        if (e2eLatencySensor == null) {\n+            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n+        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n+            final long e2eLatency = now - recordTimestamp;\n+            if (e2eLatency >  MAXIMUM_E2E_LATENCY) {\n+                log.warn(\"Skipped recording e2e latency for node {} because {} is higher than maximum allowed latency {}\",\n+                         nodeName, e2eLatency, MAXIMUM_E2E_LATENCY);\n+            } else if (e2eLatency < MINIMUM_E2E_LATENCY) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTg4Nw=="}, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg5NjUzNQ==", "bodyText": "Q: Wouldn't it be better to count measurements beyond the maximum latency towards the highest bucket as the Percentiles metric does?\nAdmittedly, the measured value would be quite wrong in the case of a lot of measurements greater than the maximum latency. However, with the sizes of the buckets that increase linearly, the reported values would be quite wrong anyways due to the increased approximation error. Furthermore, I guess users would put an alert on substantially smaller values.\nOTOH, not counting measurements beyond the maximum latency would falsify a bit the metric because they would not count towards the remaining 1% or 10% (for p99 and p90, respectively). Additionally, the max metric would also be falsified by not counting those measurements.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429896535", "createdAt": "2020-05-25T11:58:23Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -899,6 +920,28 @@ public boolean maybePunctuateSystemTime() {\n         return punctuated;\n     }\n \n+    void maybeRecordE2ELatency(final long recordTimestamp, final String nodeName) {\n+        maybeRecordE2ELatency(recordTimestamp, time.milliseconds(), nodeName);\n+    }\n+\n+    private void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n+        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n+        if (e2eLatencySensor == null) {\n+            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n+        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n+            final long e2eLatency = now - recordTimestamp;\n+            if (e2eLatency >  MAXIMUM_E2E_LATENCY) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTgzOQ=="}, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwMDQxMA==", "bodyText": "prop: Take a look into StreamsTestUtils and see what you can re-use there to retrieve specific metrics.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429900410", "createdAt": "2020-05-25T12:07:42Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -547,6 +624,28 @@ private KafkaMetric getMetric(final String operation,\n         ));\n     }\n \n+    private KafkaMetric getProcessorMetric(final String operation,\n+                                           final String nameFormat,\n+                                           final String taskId,\n+                                           final String processorNodeId,\n+                                           final String builtInMetricsVersion) {\n+        final String descriptionIsNotVerified = \"\";\n+        return metrics.metrics().get(metrics.metricName(\n+            String.format(nameFormat, operation),\n+            \"stream-processor-node-metrics\",\n+            descriptionIsNotVerified,\n+            mkMap(\n+                mkEntry(\"task-id\", taskId),\n+                mkEntry(\"processor-node-id\", processorNodeId),\n+                mkEntry(\n+                    StreamsConfig.METRICS_LATEST.equals(builtInMetricsVersion) ? THREAD_ID_TAG\n+                        : THREAD_ID_TAG_0100_TO_24,\n+                    Thread.currentThread().getName()\n+                )\n+            )\n+        ));\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 124}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3NjcyNDQ5", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-417672449", "createdAt": "2020-05-25T12:18:45Z", "commit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMjoxODo0NVrOGZ_ThA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMjoxODo0NVrOGZ_ThA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwNDc3Mg==", "bodyText": "req: Please add unit tests in StreamsMetricsImplTest.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r429904772", "createdAt": "2020-05-25T12:18:45Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -642,6 +651,55 @@ public static void addAvgAndMaxToSensor(final Sensor sensor,\n         );\n     }\n \n+    public static void addMinAndMaxAndP99AndP90ToSensor(final Sensor sensor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDkxODMy", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-418491832", "createdAt": "2020-05-26T16:50:39Z", "commit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNjo1MDozOVrOGana9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNzoyMToyMVrOGaoh1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU2MjAzOA==", "bodyText": "Not sure if it really matters, but this is not a uniform distribution (because MAX_VALUE and MIN_VALUE are not integer multiples of 1000 days. If you wanted a uniform distribution, it looks like you can use the bounded nextInt and cast to long.\nAlso, FYI, Math.abs(Long.MIN_VALUE) == Long.MIN_VALUE (which is a negative number), due to overflow.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r430562038", "createdAt": "2020-05-26T16:50:39Z", "author": {"login": "vvcephei"}, "path": "clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java", "diffHunk": "@@ -492,6 +493,52 @@ public void testPercentiles() {\n         assertEquals(75, (Double) p75.metricValue(), 1.0);\n     }\n \n+    @Test\n+    public void testPercentilesWithRandomNumbersAndLinearBucketing() {\n+        long seed = new Random().nextLong();\n+        int sizeInBytes = 1000 * 1000;   // 1MB\n+        long maximumValue = 1000 * 24 * 60 * 60 * 1000L; // if values are ms, max is 1000 days\n+\n+        try {\n+            Random prng = new Random(seed);\n+            int numberOfValues = 5000 + prng.nextInt(10_000);  // ranges is [5000, 15000]\n+\n+            Percentiles percs = new Percentiles(sizeInBytes,\n+                                                maximumValue,\n+                                                BucketSizing.LINEAR,\n+                                                new Percentile(metrics.metricName(\"test.p90\", \"grp1\"), 90),\n+                                                new Percentile(metrics.metricName(\"test.p99\", \"grp1\"), 99));\n+            MetricConfig config = new MetricConfig().eventWindow(50).samples(2);\n+            Sensor sensor = metrics.sensor(\"test\", config);\n+            sensor.add(percs);\n+            Metric p90 = this.metrics.metrics().get(metrics.metricName(\"test.p90\", \"grp1\"));\n+            Metric p99 = this.metrics.metrics().get(metrics.metricName(\"test.p99\", \"grp1\"));\n+\n+            final List<Long> values = new ArrayList<>(numberOfValues);\n+            // record two windows worth of sequential values\n+            for (int i = 0; i < numberOfValues; ++i) {\n+                long value = Math.abs(prng.nextLong()) % maximumValue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU3MjQzNw==", "bodyText": "I'm fine with this as well, although I think it makes more sense either to pin to zero and warn or to just record the negative latency and warn. It feels like we're overthinking it. If the clocks are drifting a little and we report small negative numbers, the e2e latency is still low, which is still meaningful information. I really don't see a problem with just naively reporting it and not even bothering with a warning.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r430572437", "createdAt": "2020-05-26T17:07:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -899,6 +920,28 @@ public boolean maybePunctuateSystemTime() {\n         return punctuated;\n     }\n \n+    void maybeRecordE2ELatency(final long recordTimestamp, final String nodeName) {\n+        maybeRecordE2ELatency(recordTimestamp, time.milliseconds(), nodeName);\n+    }\n+\n+    private void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n+        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n+        if (e2eLatencySensor == null) {\n+            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n+        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n+            final long e2eLatency = now - recordTimestamp;\n+            if (e2eLatency >  MAXIMUM_E2E_LATENCY) {\n+                log.warn(\"Skipped recording e2e latency for node {} because {} is higher than maximum allowed latency {}\",\n+                         nodeName, e2eLatency, MAXIMUM_E2E_LATENCY);\n+            } else if (e2eLatency < MINIMUM_E2E_LATENCY) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTg4Nw=="}, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU3MzQ0NQ==", "bodyText": "Meta-review procedural question: In the future, can we try to avoid making the same comment in multiple places in the PR, since it leads to split discussions like this one?", "url": "https://github.com/apache/kafka/pull/8697#discussion_r430573445", "createdAt": "2020-05-26T17:09:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -899,6 +920,28 @@ public boolean maybePunctuateSystemTime() {\n         return punctuated;\n     }\n \n+    void maybeRecordE2ELatency(final long recordTimestamp, final String nodeName) {\n+        maybeRecordE2ELatency(recordTimestamp, time.milliseconds(), nodeName);\n+    }\n+\n+    private void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n+        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n+        if (e2eLatencySensor == null) {\n+            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n+        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n+            final long e2eLatency = now - recordTimestamp;\n+            if (e2eLatency >  MAXIMUM_E2E_LATENCY) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTgzOQ=="}, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU3NTgyMg==", "bodyText": "This necessity makes me think that our Percentiles metric algorithm needs to be improved. Admittedly, I haven't looked at the math, but it seems like it should be possible to be more adaptive.\nI'm in favor of not adding a config and just leaving it alone for now, so that we can take the option in the future to fix the problem by fixing the algorithm.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r430575822", "createdAt": "2020-05-26T17:13:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -149,6 +154,10 @@ public int hashCode() {\n     public static final String RATE_DESCRIPTION_PREFIX = \"The average number of \";\n     public static final String RATE_DESCRIPTION_SUFFIX = \" per second\";\n \n+    public static final int PERCENTILES_SIZE_IN_BYTES = 1000 * 1000;    // 1 MB\n+    public static long MAXIMUM_E2E_LATENCY = 10 * 24 * 60 * 60 * 1000L; // maximum latency is 10 days", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwMDAxMA=="}, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU3NzA5Mg==", "bodyText": "However, I do not think we should restrict the max value for other metrics than the percentiles one. E.g., there's no reason to restrict the value we record for the max and min metrics. You should be able to update the Percentiles implementation to apply the maximum bound in the metric record method. Otherwise, I'd recommend recording two sensors separately; one for the bounded metrics, and one for the unbounded ones.", "url": "https://github.com/apache/kafka/pull/8697#discussion_r430577092", "createdAt": "2020-05-26T17:15:58Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -149,6 +154,10 @@ public int hashCode() {\n     public static final String RATE_DESCRIPTION_PREFIX = \"The average number of \";\n     public static final String RATE_DESCRIPTION_SUFFIX = \" per second\";\n \n+    public static final int PERCENTILES_SIZE_IN_BYTES = 1000 * 1000;    // 1 MB\n+    public static long MAXIMUM_E2E_LATENCY = 10 * 24 * 60 * 60 * 1000L; // maximum latency is 10 days", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwMDAxMA=="}, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU4MDE4Mg==", "bodyText": "If I understand this right, we are recording sink latencies after processing, but source latencies before processing. This nicely avoids the problem with recording non-sink latencies after processing, but is it accurate?", "url": "https://github.com/apache/kafka/pull/8697#discussion_r430580182", "createdAt": "2020-05-26T17:21:21Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -586,6 +606,7 @@ public boolean process(final long wallClockTime) {\n             log.trace(\"Start processing one record [{}]\", record);\n \n             updateProcessorContext(record, currNode, wallClockTime);\n+            maybeRecordE2ELatency(record.timestamp, wallClockTime, currNode.name());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a31d4f97d0e11e3842633bbf08deccdf06cc9f97"}, "originalPosition": 59}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf6e95b45d0858572556f1bcbef7d93ffc5bdc08", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/bf6e95b45d0858572556f1bcbef7d93ffc5bdc08", "committedDate": "2020-05-26T22:41:17Z", "message": "review comments; pin values outside min/max to bounds and log warning"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "452170d0906175c73b7ea5a15697aa00906d2a01", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/452170d0906175c73b7ea5a15697aa00906d2a01", "committedDate": "2020-05-26T22:42:13Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ddf0e569ff0d33560ef70bbf07d9d4b7a7814e6e", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/ddf0e569ff0d33560ef70bbf07d9d4b7a7814e6e", "committedDate": "2020-05-26T22:49:17Z", "message": "fix last remaining bit of spotBugs nonsense"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18bddac4d8ca88fd4da5bcc5b799531ba66888e5", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/18bddac4d8ca88fd4da5bcc5b799531ba66888e5", "committedDate": "2020-05-26T22:55:47Z", "message": "add to MetricsIntegrationTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "faa3c30b4e4118961cd8d472361b5b0ae5886038", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/faa3c30b4e4118961cd8d472361b5b0ae5886038", "committedDate": "2020-05-26T23:12:24Z", "message": "fix ActiveTaskCreatorTest"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzQxNjE5", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-418741619", "createdAt": "2020-05-26T23:16:59Z", "commit": {"oid": "faa3c30b4e4118961cd8d472361b5b0ae5886038"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8223ea29b07d9754b28ef4b06e765c66f208863c", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/8223ea29b07d9754b28ef4b06e765c66f208863c", "committedDate": "2020-05-26T23:33:19Z", "message": "clients checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODQ1NzQ3", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-418845747", "createdAt": "2020-05-27T04:18:19Z", "commit": {"oid": "8223ea29b07d9754b28ef4b06e765c66f208863c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NTY1MDE1", "url": "https://github.com/apache/kafka/pull/8697#pullrequestreview-419565015", "createdAt": "2020-05-27T19:59:51Z", "commit": {"oid": "8223ea29b07d9754b28ef4b06e765c66f208863c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1108, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}