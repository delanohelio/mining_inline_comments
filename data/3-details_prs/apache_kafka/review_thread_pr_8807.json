{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI4MjgzNzY1", "number": 8807, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNlQxODoxODozMFrOEDLqDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNlQxODoxODozMFrOEDLqDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNzcyMTczOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNlQxODoxODozMFrOGgE_OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNlQxODoxODozMFrOGgE_OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI4OTMzNw==", "bodyText": "For ease of review, it's worth mentioning in the PR body that the only lines that changed here are 1373-1377 and 1233, with the response being captured in 1246.", "url": "https://github.com/apache/kafka/pull/8807#discussion_r436289337", "createdAt": "2020-06-06T18:18:30Z", "author": {"login": "lbradstreet"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1242,131 +1243,138 @@ class ReplicaManager(val config: KafkaConfig,\n             s\"epoch ${leaderAndIsrRequest.controllerEpoch}\")\n         }\n \n-      if (leaderAndIsrRequest.controllerEpoch < controllerEpoch) {\n-        stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from controller $controllerId with \" +\n-          s\"correlation id $correlationId since its controller epoch ${leaderAndIsrRequest.controllerEpoch} is old. \" +\n-          s\"Latest known controller epoch is $controllerEpoch\")\n-        leaderAndIsrRequest.getErrorResponse(0, Errors.STALE_CONTROLLER_EPOCH.exception)\n-      } else {\n-        val responseMap = new mutable.HashMap[TopicPartition, Errors]\n-        controllerEpoch = leaderAndIsrRequest.controllerEpoch\n-\n-        val partitionStates = new mutable.HashMap[Partition, LeaderAndIsrPartitionState]()\n-\n-        // First create the partition if it doesn't exist already\n-        requestPartitionStates.foreach { partitionState =>\n-          val topicPartition = new TopicPartition(partitionState.topicName, partitionState.partitionIndex)\n-          val partitionOpt = getPartition(topicPartition) match {\n-            case HostedPartition.Offline =>\n-              stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from \" +\n-                s\"controller $controllerId with correlation id $correlationId \" +\n-                s\"epoch $controllerEpoch for partition $topicPartition as the local replica for the \" +\n-                \"partition is in an offline log directory\")\n-              responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)\n-              None\n+      val response = {\n+        if (leaderAndIsrRequest.controllerEpoch < controllerEpoch) {\n+          stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from controller $controllerId with \" +\n+            s\"correlation id $correlationId since its controller epoch ${leaderAndIsrRequest.controllerEpoch} is old. \" +\n+            s\"Latest known controller epoch is $controllerEpoch\")\n+          leaderAndIsrRequest.getErrorResponse(0, Errors.STALE_CONTROLLER_EPOCH.exception)\n+        } else {\n+          val responseMap = new mutable.HashMap[TopicPartition, Errors]\n+          controllerEpoch = leaderAndIsrRequest.controllerEpoch\n \n-            case HostedPartition.Online(partition) =>\n-              Some(partition)\n+          val partitionStates = new mutable.HashMap[Partition, LeaderAndIsrPartitionState]()\n \n-            case HostedPartition.None =>\n-              val partition = Partition(topicPartition, time, this)\n-              allPartitions.putIfNotExists(topicPartition, HostedPartition.Online(partition))\n-              Some(partition)\n-          }\n+          // First create the partition if it doesn't exist already\n+          requestPartitionStates.foreach { partitionState =>\n+            val topicPartition = new TopicPartition(partitionState.topicName, partitionState.partitionIndex)\n+            val partitionOpt = getPartition(topicPartition) match {\n+              case HostedPartition.Offline =>\n+                stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from \" +\n+                  s\"controller $controllerId with correlation id $correlationId \" +\n+                  s\"epoch $controllerEpoch for partition $topicPartition as the local replica for the \" +\n+                  \"partition is in an offline log directory\")\n+                responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)\n+                None\n+\n+              case HostedPartition.Online(partition) =>\n+                Some(partition)\n+\n+              case HostedPartition.None =>\n+                val partition = Partition(topicPartition, time, this)\n+                allPartitions.putIfNotExists(topicPartition, HostedPartition.Online(partition))\n+                Some(partition)\n+            }\n \n-          // Next check partition's leader epoch\n-          partitionOpt.foreach { partition =>\n-            val currentLeaderEpoch = partition.getLeaderEpoch\n-            val requestLeaderEpoch = partitionState.leaderEpoch\n-            if (requestLeaderEpoch > currentLeaderEpoch) {\n-              // If the leader epoch is valid record the epoch of the controller that made the leadership decision.\n-              // This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path\n-              if (partitionState.replicas.contains(localBrokerId))\n-                partitionStates.put(partition, partitionState)\n-              else {\n-                stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from controller $controllerId with \" +\n-                  s\"correlation id $correlationId epoch $controllerEpoch for partition $topicPartition as itself is not \" +\n-                  s\"in assigned replica list ${partitionState.replicas.asScala.mkString(\",\")}\")\n-                responseMap.put(topicPartition, Errors.UNKNOWN_TOPIC_OR_PARTITION)\n+            // Next check partition's leader epoch\n+            partitionOpt.foreach { partition =>\n+              val currentLeaderEpoch = partition.getLeaderEpoch\n+              val requestLeaderEpoch = partitionState.leaderEpoch\n+              if (requestLeaderEpoch > currentLeaderEpoch) {\n+                // If the leader epoch is valid record the epoch of the controller that made the leadership decision.\n+                // This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path\n+                if (partitionState.replicas.contains(localBrokerId))\n+                  partitionStates.put(partition, partitionState)\n+                else {\n+                  stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from controller $controllerId with \" +\n+                    s\"correlation id $correlationId epoch $controllerEpoch for partition $topicPartition as itself is not \" +\n+                    s\"in assigned replica list ${partitionState.replicas.asScala.mkString(\",\")}\")\n+                  responseMap.put(topicPartition, Errors.UNKNOWN_TOPIC_OR_PARTITION)\n+                }\n+              } else if (requestLeaderEpoch < currentLeaderEpoch) {\n+                stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from \" +\n+                  s\"controller $controllerId with correlation id $correlationId \" +\n+                  s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n+                  s\"leader epoch $requestLeaderEpoch is smaller than the current \" +\n+                  s\"leader epoch $currentLeaderEpoch\")\n+                responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)\n+              } else {\n+                stateChangeLogger.info(s\"Ignoring LeaderAndIsr request from \" +\n+                  s\"controller $controllerId with correlation id $correlationId \" +\n+                  s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n+                  s\"leader epoch $requestLeaderEpoch matches the current leader epoch\")\n+                responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)\n               }\n-            } else if (requestLeaderEpoch < currentLeaderEpoch) {\n-              stateChangeLogger.warn(s\"Ignoring LeaderAndIsr request from \" +\n-                s\"controller $controllerId with correlation id $correlationId \" +\n-                s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n-                s\"leader epoch $requestLeaderEpoch is smaller than the current \" +\n-                s\"leader epoch $currentLeaderEpoch\")\n-              responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)\n-            } else {\n-              stateChangeLogger.info(s\"Ignoring LeaderAndIsr request from \" +\n-                s\"controller $controllerId with correlation id $correlationId \" +\n-                s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n-                s\"leader epoch $requestLeaderEpoch matches the current leader epoch\")\n-              responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)\n             }\n           }\n-        }\n \n-        val partitionsToBeLeader = partitionStates.filter { case (_, partitionState) =>\n-          partitionState.leader == localBrokerId\n-        }\n-        val partitionsToBeFollower = partitionStates.filter { case (k, _) => !partitionsToBeLeader.contains(k) }\n+          val partitionsToBeLeader = partitionStates.filter { case (_, partitionState) =>\n+            partitionState.leader == localBrokerId\n+          }\n+          val partitionsToBeFollower = partitionStates.filter { case (k, _) => !partitionsToBeLeader.contains(k) }\n \n-        val highWatermarkCheckpoints = new LazyOffsetCheckpoints(this.highWatermarkCheckpoints)\n-        val partitionsBecomeLeader = if (partitionsToBeLeader.nonEmpty)\n-          makeLeaders(controllerId, controllerEpoch, partitionsToBeLeader, correlationId, responseMap,\n-            highWatermarkCheckpoints)\n-        else\n-          Set.empty[Partition]\n-        val partitionsBecomeFollower = if (partitionsToBeFollower.nonEmpty)\n-          makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap,\n-            highWatermarkCheckpoints)\n-        else\n-          Set.empty[Partition]\n+          val highWatermarkCheckpoints = new LazyOffsetCheckpoints(this.highWatermarkCheckpoints)\n+          val partitionsBecomeLeader = if (partitionsToBeLeader.nonEmpty)\n+            makeLeaders(controllerId, controllerEpoch, partitionsToBeLeader, correlationId, responseMap,\n+              highWatermarkCheckpoints)\n+          else\n+            Set.empty[Partition]\n+          val partitionsBecomeFollower = if (partitionsToBeFollower.nonEmpty)\n+            makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap,\n+              highWatermarkCheckpoints)\n+          else\n+            Set.empty[Partition]\n \n-        /*\n+          /*\n          * KAFKA-8392\n          * For topic partitions of which the broker is no longer a leader, delete metrics related to\n          * those topics. Note that this means the broker stops being either a replica or a leader of\n          * partitions of said topics\n          */\n-        val leaderTopicSet = leaderPartitionsIterator.map(_.topic).toSet\n-        val followerTopicSet = partitionsBecomeFollower.map(_.topic).toSet\n-        followerTopicSet.diff(leaderTopicSet).foreach(brokerTopicStats.removeOldLeaderMetrics)\n+          val leaderTopicSet = leaderPartitionsIterator.map(_.topic).toSet\n+          val followerTopicSet = partitionsBecomeFollower.map(_.topic).toSet\n+          followerTopicSet.diff(leaderTopicSet).foreach(brokerTopicStats.removeOldLeaderMetrics)\n \n-        // remove metrics for brokers which are not followers of a topic\n-        leaderTopicSet.diff(followerTopicSet).foreach(brokerTopicStats.removeOldFollowerMetrics)\n+          // remove metrics for brokers which are not followers of a topic\n+          leaderTopicSet.diff(followerTopicSet).foreach(brokerTopicStats.removeOldFollowerMetrics)\n \n-        leaderAndIsrRequest.partitionStates.forEach { partitionState =>\n-          val topicPartition = new TopicPartition(partitionState.topicName, partitionState.partitionIndex)\n-          /*\n+          leaderAndIsrRequest.partitionStates.forEach { partitionState =>\n+            val topicPartition = new TopicPartition(partitionState.topicName, partitionState.partitionIndex)\n+            /*\n            * If there is offline log directory, a Partition object may have been created by getOrCreatePartition()\n            * before getOrCreateReplica() failed to create local replica due to KafkaStorageException.\n            * In this case ReplicaManager.allPartitions will map this topic-partition to an empty Partition object.\n            * we need to map this topic-partition to OfflinePartition instead.\n            */\n-          if (localLog(topicPartition).isEmpty)\n-            markPartitionOffline(topicPartition)\n-        }\n-\n-        // we initialize highwatermark thread after the first leaderisrrequest. This ensures that all the partitions\n-        // have been completely populated before starting the checkpointing there by avoiding weird race conditions\n-        startHighWatermarkCheckPointThread()\n-\n-        maybeAddLogDirFetchers(partitionStates.keySet, highWatermarkCheckpoints)\n+            if (localLog(topicPartition).isEmpty)\n+              markPartitionOffline(topicPartition)\n+          }\n \n-        replicaFetcherManager.shutdownIdleFetcherThreads()\n-        replicaAlterLogDirsManager.shutdownIdleFetcherThreads()\n-        onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)\n-        val responsePartitions = responseMap.iterator.map { case (tp, error) =>\n-          new LeaderAndIsrPartitionError()\n-            .setTopicName(tp.topic)\n-            .setPartitionIndex(tp.partition)\n-            .setErrorCode(error.code)\n-        }.toBuffer\n-        new LeaderAndIsrResponse(new LeaderAndIsrResponseData()\n-          .setErrorCode(Errors.NONE.code)\n-          .setPartitionErrors(responsePartitions.asJava))\n+          // we initialize highwatermark thread after the first leaderisrrequest. This ensures that all the partitions\n+          // have been completely populated before starting the checkpointing there by avoiding weird race conditions\n+          startHighWatermarkCheckPointThread()\n+\n+          maybeAddLogDirFetchers(partitionStates.keySet, highWatermarkCheckpoints)\n+\n+          replicaFetcherManager.shutdownIdleFetcherThreads()\n+          replicaAlterLogDirsManager.shutdownIdleFetcherThreads()\n+          onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)\n+          val responsePartitions = responseMap.iterator.map { case (tp, error) =>\n+            new LeaderAndIsrPartitionError()\n+              .setTopicName(tp.topic)\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(error.code)\n+          }.toBuffer\n+          new LeaderAndIsrResponse(new LeaderAndIsrResponseData()\n+            .setErrorCode(Errors.NONE.code)\n+            .setPartitionErrors(responsePartitions.asJava))\n+        }\n       }\n+      val endMs = time.milliseconds()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1a91a410bdcbef7ccacec2a95f53c1e5009c962"}, "originalPosition": 241}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2521, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}