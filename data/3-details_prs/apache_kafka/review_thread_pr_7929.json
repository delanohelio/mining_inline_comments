{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYxNTgxOTkz", "number": 7929, "reviewThreads": {"totalCount": 29, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNjoxNjowNVrODYuTsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozMDozN1rOErzCqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MjUxMTIzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNjoxNjowNVrOFevtqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNjoxNjowNVrOFevtqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzc4MzMzNg==", "bodyText": "This should be moved to loadLog() since it only needs to run at startup. Also, the set of offsets to delete should exclude the log end offset, since that is snapshotted out during shutdown.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r367783336", "createdAt": "2020-01-17T06:16:05Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -870,6 +870,10 @@ class Log(@volatile var dir: File,\n   }\n \n   private def loadProducerState(lastOffset: Long, reloadFromCleanShutdown: Boolean): Unit = lock synchronized {\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NjM3Njg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxOTo0MDozNlrOFiPEUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxOTo0MDozNlrOFiPEUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTQ0Mjc2OQ==", "bodyText": "nit: I'd suggest a more descriptive name like segmentBaseOffsets", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371442769", "createdAt": "2020-01-27T19:40:36Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NjM4MTgzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogSegment.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxOTo0MjowNVrOFiPHbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxOTo0MjowNVrOFiPHbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTQ0MzU2Nw==", "bodyText": "nit: braces are probably not needed", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371443567", "createdAt": "2020-01-27T19:42:05Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -622,7 +623,8 @@ class LogSegment private[log] (val log: FileRecords,\n       () => delete(log.deleteIfExists _, \"log\", log.file, logIfMissing = true),\n       () => delete(offsetIndex.deleteIfExists _, \"offset index\", lazyOffsetIndex.file, logIfMissing = true),\n       () => delete(timeIndex.deleteIfExists _, \"time index\", lazyTimeIndex.file, logIfMissing = true),\n-      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false)\n+      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false),\n+      () => delete(() => {Files.deleteIfExists(producerStateSnapshot.toPath)}, \"producer state snapshot\", producerStateSnapshot, logIfMissing = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NjgxMzY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QyMjoxMjoxMVrOFiTSFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMzoyMjoyMFrOFmKoHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUxMTgyOA==", "bodyText": "Just mentioning this as an observation. Unlike the index files, which have a lifecycle clearly tied to the segment, producer snapshots are a bit of a mix. They are created in Log when a segment is rolled, but deletion is the job of the segment. I'm partially wondering if it would be clearer to handle both operations in the same context.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371511828", "createdAt": "2020-01-27T22:12:11Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgzNDI1Ng==", "bodyText": "Hmm I might be misunderstanding @hachikuji but the current PR should delete producer state snapshot files along with segment deletion. I included this additional cleanup step during log opening because snapshot files are not atomically renamed as part of deletion like other files, which seems to allow for orphaned files if there is a crash during segment deletion. Are you suggesting that we handle snapshot file deletion like the deletion of index/segment files by adding the deleted suffix?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r374834256", "createdAt": "2020-02-04T18:07:27Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUxMTgyOA=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU2NDMxNg==", "bodyText": "What I was saying is more in line with Jun's comment below. It is more about managing the lifecycle of the snapshot consistently. Typically it's simpler if creation and deletion happen in the same context.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375564316", "createdAt": "2020-02-05T23:22:20Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUxMTgyOA=="}, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5Njg2NzcwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QyMjozMjoxOFrOFiTyjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QyMjozMjoxOFrOFiTyjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUyMDE0MA==", "bodyText": "Can we add a log message when we delete snapshot files?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371520140", "createdAt": "2020-01-27T22:32:18Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -759,6 +759,15 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(keepOffsets: Iterable[Long]): Unit = {\n+    val expected = keepOffsets.map(Log.producerSnapshotFile(logDir, _)).toSet ++ latestSnapshotFile\n+    val actual = listSnapshotFiles.toSet\n+    actual.diff(expected).foreach(file => Files.deleteIfExists(file.toPath))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5Njg3NzY5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QyMjozNjowOFrOFiT4tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwNzoxMToyM1rOFkC-bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUyMTcxOA==", "bodyText": "Was there a reason you chose to cleanup the orphaned files before loading producer state? I'm not sure it matters too much, but it does mean that we won't remove the snapshot from the log end offset until the following restart.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371521718", "createdAt": "2020-01-27T22:36:08Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)\n     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzM0MTgwNQ==", "bodyText": "I don't have a specific reason for this, but that's a good point that we won't remove the log end offset snapshot file until the next restart. I'll swap the ordering here, thanks!", "url": "https://github.com/apache/kafka/pull/7929#discussion_r373341805", "createdAt": "2020-01-31T07:11:23Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)\n     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUyMTcxOA=="}, "originalCommit": null, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxOTI3NzUxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwMToyMDowMFrOFlpBzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNzoxMjoyNFrOFmADIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzgzOQ==", "bodyText": "So, this means that we will be removing the producer snapshot corresponding to log end offset. One of the potential impact is that if the broker has an immediate hard failure after this point, the recovery will have to rebuild the producer snapshot for the active segment, which can be expensive. We probably want to think if there is any other impact.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375013839", "createdAt": "2020-02-05T01:20:00Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -311,6 +311,10 @@ class Log(@volatile var dir: File,\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val segmentBaseOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(segmentBaseOffsets)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTM2MjQzMw==", "bodyText": "I think cleanupOrphanSnapshotFiles always retains the last snapshot.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375362433", "createdAt": "2020-02-05T16:25:03Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -311,6 +311,10 @@ class Log(@volatile var dir: File,\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val segmentBaseOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(segmentBaseOffsets)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzgzOQ=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTM5MTAwOA==", "bodyText": "Thanks. Sounds good. We probably should change the comment above to make it clear that the last snapshot is always preserved?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375391008", "createdAt": "2020-02-05T17:12:24Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -311,6 +311,10 @@ class Log(@volatile var dir: File,\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val segmentBaseOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(segmentBaseOffsets)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzgzOQ=="}, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxOTI4OTI3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogSegment.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwMToyNjo0OVrOFlpI7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwMToyNjo0OVrOFlpI7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTY2Mg==", "bodyText": "Hmm, should we include fileSuffix in producerStateSnapshot too?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375015662", "createdAt": "2020-02-05T01:26:49Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -653,11 +655,13 @@ object LogSegment {\n   def open(dir: File, baseOffset: Long, config: LogConfig, time: Time, fileAlreadyExists: Boolean = false,\n            initFileSize: Int = 0, preallocate: Boolean = false, fileSuffix: String = \"\"): LogSegment = {\n     val maxIndexSize = config.maxIndexSize\n+    val producerStateSnapshot = Log.producerSnapshotFile(dir, baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxOTI5MzEyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogSegment.scala", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwMToyOToxOFrOFlpLPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNzo1MjoyMlrOFmBUIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNjI1NA==", "bodyText": "Hmm, it seems that we are missing the handling of producerStateSnapshot in a few methods like changeFileSuffixes(), updateDir(), and potentially flush().", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375016254", "createdAt": "2020-02-05T01:29:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -57,6 +57,7 @@ class LogSegment private[log] (val log: FileRecords,\n                                val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                                val lazyTimeIndex: LazyIndex[TimeIndex],\n                                val txnIndex: TransactionIndex,\n+                               val producerStateSnapshot: File,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTA1MDI5Ng==", "bodyText": "Hmm I think if we want to treat producer snapshot files like other index files, it seems we may need a different approach. In this PR I was relying on cleanupOrphanSnapshotFiles to avoid having to deal with changeFileSuffixes and updateDir.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375050296", "createdAt": "2020-02-05T04:07:57Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -57,6 +57,7 @@ class LogSegment private[log] (val log: FileRecords,\n                                val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                                val lazyTimeIndex: LazyIndex[TimeIndex],\n                                val txnIndex: TransactionIndex,\n+                               val producerStateSnapshot: File,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNjI1NA=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTA1ODI4Mg==", "bodyText": "Hmm, by putting producerStateSnapshot inside LogSegment, it seems to suggest that producerStateSnapshot should be managed as other files belonging to a segment?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375058282", "createdAt": "2020-02-05T04:52:07Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -57,6 +57,7 @@ class LogSegment private[log] (val log: FileRecords,\n                                val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                                val lazyTimeIndex: LazyIndex[TimeIndex],\n                                val txnIndex: TransactionIndex,\n+                               val producerStateSnapshot: File,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNjI1NA=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTA1OTEzNg==", "bodyText": "I would agree.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375059136", "createdAt": "2020-02-05T04:56:52Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -57,6 +57,7 @@ class LogSegment private[log] (val log: FileRecords,\n                                val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                                val lazyTimeIndex: LazyIndex[TimeIndex],\n                                val txnIndex: TransactionIndex,\n+                               val producerStateSnapshot: File,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNjI1NA=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTM3OTgxMg==", "bodyText": "That sounds good, I was just saying the approach taken in this PR will have to be rethought depending on what degree we want the LogSegment to manage producer state snapshots versus the Log/ProducerStateManager. I can think of two options which can be taken to improve the ownership here a bit.\n\n\nThe Log continues to manage producer state snapshots via the ProducerStateManager and the logic for deletion is lifted back into the log layer. This means wherever we try to delete segments with LogSegment.deleteIfExists we also delete the corresponding producer state snapshot via a call to the ProducerStateManager. I think this most closely resembles the current state in trunk.\n\n\nThe LogSegment and Log/ProducerStateManager share ownership of the producer state snapshot with only the LogSegment owning deletion and access going through the ProducerStateManager. This may allow us to get rid of the producer state snapshot specific truncation logic in ProducerStateManager. The question here is what to do about producer state snapshot files which don't match a corresponding segments base offset. This may not be too bad as I think in all cases they would just be associated with the active segment.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375379812", "createdAt": "2020-02-05T16:53:15Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -57,6 +57,7 @@ class LogSegment private[log] (val log: FileRecords,\n                                val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                                val lazyTimeIndex: LazyIndex[TimeIndex],\n                                val txnIndex: TransactionIndex,\n+                               val producerStateSnapshot: File,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNjI1NA=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQxMTc0Nw==", "bodyText": "I see. The tricky thing is that the last snapshot doesn't have a corresponding log segment and needs to be managed separately outside of the LogSegment. However, for snapshots that have an associated segment, managing the snapshot file as part of the log segment seems convenient.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375411747", "createdAt": "2020-02-05T17:52:22Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -57,6 +57,7 @@ class LogSegment private[log] (val log: FileRecords,\n                                val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                                val lazyTimeIndex: LazyIndex[TimeIndex],\n                                val txnIndex: TransactionIndex,\n+                               val producerStateSnapshot: File,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNjI1NA=="}, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyOTEzMzMxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo0MToxOFrOF1yVPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo0MToxOFrOF1yVPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0MzQ4NA==", "bodyText": "unused import CoreUtils", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391943484", "createdAt": "2020-03-12T22:41:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -23,7 +23,7 @@ import java.nio.file.{Files, StandardOpenOption}\n \n import kafka.log.Log.offsetFromFile\n import kafka.server.LogOffsetMetadata\n-import kafka.utils.{Logging, nonthreadsafe, threadsafe}\n+import kafka.utils.{CoreUtils, Logging, nonthreadsafe, threadsafe}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyOTE1OTQ0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo1NDoyOFrOF1ylNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNToyMDowOFrOF7MevA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NzU3Mg==", "bodyText": "Since this is called on broker restart, it seems that we no longer need to call deleteSnapshotFiles() in truncateAndReload(), truncateHead() and truncate() ?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391947572", "createdAt": "2020-03-12T22:54:28Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -753,6 +753,18 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(segmentBaseOffsets: Iterable[Long]): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYxNDc4MA==", "bodyText": "Thanks @junrao,\nI wanted to enumerate why it's safe to remove these now to make sure we're on the same page.\n\n\ndeleteSnapshotFiles can be deleted because snapshot file deletion is done outside of the ProducerStateManager now.\n\n\ntruncate can be deleted because we only ever fully truncate the producer state snapshots if we are also truncating the log, and as a result, the segment files.\n\n\ntruncateAndReload and truncateHead seems to be called when recovering a segment and when loading the log. Upon starting up after an unclean shutdown, we will call truncateAndReload. I'm a bit less certain that we can get rid of these methods, I will need to investigate further.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r397614780", "createdAt": "2020-03-25T05:20:08Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -753,6 +753,18 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(segmentBaseOffsets: Iterable[Long]): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NzU3Mg=="}, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyOTE2NDg3OnYy", "diffSide": "LEFT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo1NzoyNlrOF1yonw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo1NzoyNlrOF1yonw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0ODQ0Nw==", "bodyText": "We don't need to clear the producer snapshot here. However, it seems that we still need to do other things in truncateHead() like removeUnreplicatedTransactions()?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391948447", "createdAt": "2020-03-12T22:57:26Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1260,7 +1265,6 @@ class Log(@volatile var dir: File,\n           info(s\"Incrementing log start offset to $newLogStartOffset\")\n           updateLogStartOffset(newLogStartOffset)\n           leaderEpochCache.foreach(_.truncateFromStart(logStartOffset))\n-          producerStateManager.truncateHead(newLogStartOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyOTE2ODYzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogSegment.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo1OTozMVrOF1yrGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo1OTozMVrOF1yrGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0OTA4MQ==", "bodyText": "segments base offset => segment's base offset", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391949081", "createdAt": "2020-03-12T22:59:31Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -47,6 +47,7 @@ import scala.math._\n  * @param lazyOffsetIndex The offset index\n  * @param lazyTimeIndex The timestamp index\n  * @param txnIndex The transaction index\n+ * @param producerStateSnapshot The producer state snapshot file matching this segments base offset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyOTIwMTExOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogSegment.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMzoxNzoyOFrOF1y_qQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNToyNjo0N1rOF7MkzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk1NDM0NQ==", "bodyText": "For consistency, should we cover producerStateSnapshot in lastModified_= ?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391954345", "createdAt": "2020-03-12T23:17:28Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -622,7 +631,8 @@ class LogSegment private[log] (val log: FileRecords,\n       () => delete(log.deleteIfExists _, \"log\", log.file, logIfMissing = true),\n       () => delete(offsetIndex.deleteIfExists _, \"offset index\", lazyOffsetIndex.file, logIfMissing = true),\n       () => delete(timeIndex.deleteIfExists _, \"time index\", lazyTimeIndex.file, logIfMissing = true),\n-      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false)\n+      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false),\n+      () => delete(() =>  Files.deleteIfExists(producerStateSnapshot.toPath), \"producer state snapshot\", producerStateSnapshot, logIfMissing = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYxNjMzMg==", "bodyText": "Yes thanks!", "url": "https://github.com/apache/kafka/pull/7929#discussion_r397616332", "createdAt": "2020-03-25T05:26:47Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -622,7 +631,8 @@ class LogSegment private[log] (val log: FileRecords,\n       () => delete(log.deleteIfExists _, \"log\", log.file, logIfMissing = true),\n       () => delete(offsetIndex.deleteIfExists _, \"offset index\", lazyOffsetIndex.file, logIfMissing = true),\n       () => delete(timeIndex.deleteIfExists _, \"time index\", lazyTimeIndex.file, logIfMissing = true),\n-      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false)\n+      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false),\n+      () => delete(() =>  Files.deleteIfExists(producerStateSnapshot.toPath), \"producer state snapshot\", producerStateSnapshot, logIfMissing = false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk1NDM0NQ=="}, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyOTIxOTA1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMzoyODowMVrOF1zKfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxNjoxNDozMlrOF7i9EQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk1NzExOA==", "bodyText": "Just to be clear. You mentioned\n\"I choose to implement #2 as it was simpler, and instead of just deleting the highest producer state snapshot file without a corresponding segment file, I delete all producer state snapshot files without a corresponding segment file.\"\nBut, we are still keeping the snapshot file with the largest offset?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391957118", "createdAt": "2020-03-12T23:28:01Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -753,6 +753,18 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(segmentBaseOffsets: Iterable[Long]): Unit = {\n+    val expected = segmentBaseOffsets.map(Log.producerSnapshotFile(logDir, _)).toSet ++ latestSnapshotFile", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzk4Mjk5Mw==", "bodyText": "Yes this is a mistake, now that cleanupOrphanSnapshotFiles occurs after loading the producer state, we should also delete the latest snapshot file.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r397982993", "createdAt": "2020-03-25T16:14:32Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -753,6 +753,18 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(segmentBaseOffsets: Iterable[Long]): Unit = {\n+    val expected = segmentBaseOffsets.map(Log.producerSnapshotFile(logDir, _)).toSet ++ latestSnapshotFile", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk1NzExOA=="}, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMTA4NTEwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNToyMzo0NFrOGgjsnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNToyMzo0NFrOGgjsnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5MjQ3Nw==", "bodyText": "Nit: case and newline are unnecessary here.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r436792477", "createdAt": "2020-06-08T15:23:44Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2237,7 +2209,11 @@ class Log(@volatile private var _dir: File,\n     def deleteSegments(): Unit = {\n       info(s\"Deleting segments ${segments.mkString(\",\")}\")\n       maybeHandleIOException(s\"Error while deleting segments for $topicPartition in dir ${dir.getParent}\") {\n-        segments.foreach(_.deleteIfExists())\n+        segments.foreach {\n+          case segment =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NDY2NTIzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTozODowOFrOGkIQYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0NToxM1rOGlQb6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUzNzE4Nw==", "bodyText": "What's keepOffsets?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440537187", "createdAt": "2020-06-16T01:38:08Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -751,6 +751,25 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxOTc4NA==", "bodyText": "This should be segmentBaseOffsets, thanks!", "url": "https://github.com/apache/kafka/pull/7929#discussion_r441719784", "createdAt": "2020-06-17T17:45:13Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -751,6 +751,25 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUzNzE4Nw=="}, "originalCommit": null, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NDY3NTA3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTo0NDowMVrOGkIWkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTo0NDowMVrOGkIWkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUzODc3MA==", "bodyText": "It doesn't seem that we generate producer snapshot files for those new segments?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440538770", "createdAt": "2020-06-16T01:44:01Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2421,6 +2396,7 @@ class Log(@volatile private var _dir: File,\n         newSegments.foreach { splitSegment =>\n           splitSegment.close()\n           splitSegment.deleteIfExists()\n+          producerStateManager.deleteIfExists(splitSegment.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NDY3ODU1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTo0NTo1OFrOGkIYmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTo0NTo1OFrOGkIYmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUzOTI5MQ==", "bodyText": "Hmm, the cleaned segment has the same base offset as the first segment. So, we don't want to delete that snapshot file.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440539291", "createdAt": "2020-06-16T01:45:58Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -595,8 +595,10 @@ private[log] class Cleaner(val id: Int,\n       log.replaceSegments(List(cleaned), segments)\n     } catch {\n       case e: LogCleaningAbortedException =>\n-        try cleaned.deleteIfExists()\n-        catch {\n+        try {\n+          cleaned.deleteIfExists()\n+          log.producerStateManager.deleteIfExists(cleaned.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NDY4ODc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTo1MjoxN1rOGkIfBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTo1MjoxN1rOGkIfBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU0MDkzMw==", "bodyText": "Hmm, this can be a bit tricky. When we replace old segments with a new segment in LogCleaner, each of the old segment will be deleted. However, the first old segment has the same offset as the new segment. So, we don't want to just delete the producer snapshot corresponding to the first old segment.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440540933", "createdAt": "2020-06-16T01:52:17Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2237,7 +2209,10 @@ class Log(@volatile private var _dir: File,\n     def deleteSegments(): Unit = {\n       info(s\"Deleting segments ${segments.mkString(\",\")}\")\n       maybeHandleIOException(s\"Error while deleting segments for $topicPartition in dir ${dir.getParent}\") {\n-        segments.foreach(_.deleteIfExists())\n+        segments.foreach { segment =>\n+          segment.deleteIfExists()\n+          producerStateManager.deleteIfExists(segment.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTA5MjYyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozOTo1M1rOHeGfiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozOTo1M1rOHeGfiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNTcwNQ==", "bodyText": "ProducerStateManager.listSnapshotFiles() could just be listSnapshotFiles() ?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501325705", "createdAt": "2020-10-07T21:39:53Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -496,6 +491,53 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   // completed transactions whose markers are at offsets above the high watermark\n   private val unreplicatedTxns = new util.TreeMap[Long, TxnMetadata]\n \n+  /**\n+   * Load producer state snapshots by scanning the _logDir.\n+   */\n+  private def loadSnapshots(): ConcurrentSkipListMap[java.lang.Long, SnapshotFile] = {\n+    val tm = new ConcurrentSkipListMap[java.lang.Long, SnapshotFile]()\n+    for (f <- ProducerStateManager.listSnapshotFiles(_logDir)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTA5NTIyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0MDo1NVrOHeGhKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0MDo1NVrOHeGhKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNjEyMA==", "bodyText": "Incomplete sentence after \"but not to remove\".", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501326120", "createdAt": "2020-10-07T21:40:55Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -496,6 +491,53 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   // completed transactions whose markers are at offsets above the high watermark\n   private val unreplicatedTxns = new util.TreeMap[Long, TxnMetadata]\n \n+  /**\n+   * Load producer state snapshots by scanning the _logDir.\n+   */\n+  private def loadSnapshots(): ConcurrentSkipListMap[java.lang.Long, SnapshotFile] = {\n+    val tm = new ConcurrentSkipListMap[java.lang.Long, SnapshotFile]()\n+    for (f <- ProducerStateManager.listSnapshotFiles(_logDir)) {\n+      tm.put(f.offset, f)\n+    }\n+    tm\n+  }\n+\n+  /**\n+   * Scans the log directory, gathering all producer state snapshot files. Snapshot files which do not have an offset\n+   * corresponding to one of the provided offsets in segmentBaseOffsets will be removed, except in the case that there\n+   * is a snapshot file at a higher offset than any offset in segmentBaseOffsets.\n+   *\n+   * The goal here is to remove any snapshot files which do not have an associated segment file, but not to remove", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTEwODUzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0NTozMVrOHeGoww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxODo1NjowNFrOHesPeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyODA2Nw==", "bodyText": "Could we also make sure that the offset for latestStraySnapshot is > the largest offset in segmentBaseOffsets?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501328067", "createdAt": "2020-10-07T21:45:31Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -496,6 +491,53 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   // completed transactions whose markers are at offsets above the high watermark\n   private val unreplicatedTxns = new util.TreeMap[Long, TxnMetadata]\n \n+  /**\n+   * Load producer state snapshots by scanning the _logDir.\n+   */\n+  private def loadSnapshots(): ConcurrentSkipListMap[java.lang.Long, SnapshotFile] = {\n+    val tm = new ConcurrentSkipListMap[java.lang.Long, SnapshotFile]()\n+    for (f <- ProducerStateManager.listSnapshotFiles(_logDir)) {\n+      tm.put(f.offset, f)\n+    }\n+    tm\n+  }\n+\n+  /**\n+   * Scans the log directory, gathering all producer state snapshot files. Snapshot files which do not have an offset\n+   * corresponding to one of the provided offsets in segmentBaseOffsets will be removed, except in the case that there\n+   * is a snapshot file at a higher offset than any offset in segmentBaseOffsets.\n+   *\n+   * The goal here is to remove any snapshot files which do not have an associated segment file, but not to remove\n+   */\n+  private[log] def removeStraySnapshots(segmentBaseOffsets: Set[Long]): Unit = {\n+    var latestStraySnapshot: Option[SnapshotFile] = None\n+    val ss = loadSnapshots()\n+    for (snapshot <- ss.values().asScala) {\n+      val key = snapshot.offset\n+      latestStraySnapshot match {\n+        case Some(prev) =>\n+          if (!segmentBaseOffsets.contains(key)) {\n+            // this snapshot is now the largest stray snapshot.\n+            prev.deleteIfExists()\n+            ss.remove(prev.offset)\n+            latestStraySnapshot = Some(snapshot)\n+          }\n+        case None =>\n+          if (!segmentBaseOffsets.contains(key)) {\n+            latestStraySnapshot = Some(snapshot)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTk0NDE4NQ==", "bodyText": "We perform a check below which may cover this case. After setting the snapshots map, we look at the latest snapshot in the map. If the latest snapshot in the map is not equal to the latestStraySnapshot, we delete the latestStraySnapshot.\nI think this is a bit confusing though, so it might be better if instead we directly check that the latestStraySnapshot is larger than the largest offset in segmentBaseOffsets.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501944185", "createdAt": "2020-10-08T18:56:04Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -496,6 +491,53 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   // completed transactions whose markers are at offsets above the high watermark\n   private val unreplicatedTxns = new util.TreeMap[Long, TxnMetadata]\n \n+  /**\n+   * Load producer state snapshots by scanning the _logDir.\n+   */\n+  private def loadSnapshots(): ConcurrentSkipListMap[java.lang.Long, SnapshotFile] = {\n+    val tm = new ConcurrentSkipListMap[java.lang.Long, SnapshotFile]()\n+    for (f <- ProducerStateManager.listSnapshotFiles(_logDir)) {\n+      tm.put(f.offset, f)\n+    }\n+    tm\n+  }\n+\n+  /**\n+   * Scans the log directory, gathering all producer state snapshot files. Snapshot files which do not have an offset\n+   * corresponding to one of the provided offsets in segmentBaseOffsets will be removed, except in the case that there\n+   * is a snapshot file at a higher offset than any offset in segmentBaseOffsets.\n+   *\n+   * The goal here is to remove any snapshot files which do not have an associated segment file, but not to remove\n+   */\n+  private[log] def removeStraySnapshots(segmentBaseOffsets: Set[Long]): Unit = {\n+    var latestStraySnapshot: Option[SnapshotFile] = None\n+    val ss = loadSnapshots()\n+    for (snapshot <- ss.values().asScala) {\n+      val key = snapshot.offset\n+      latestStraySnapshot match {\n+        case Some(prev) =>\n+          if (!segmentBaseOffsets.contains(key)) {\n+            // this snapshot is now the largest stray snapshot.\n+            prev.deleteIfExists()\n+            ss.remove(prev.offset)\n+            latestStraySnapshot = Some(snapshot)\n+          }\n+        case None =>\n+          if (!segmentBaseOffsets.contains(key)) {\n+            latestStraySnapshot = Some(snapshot)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyODA2Nw=="}, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTEzODEyOnYy", "diffSide": "LEFT", "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo1NTo0M1rOHeG6WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxODo1MzoxNVrOHesI8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMzMjU2OQ==", "bodyText": "Hmm, why don't we need to delete snapshots before logStartOffset?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501332569", "createdAt": "2020-10-07T21:55:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -653,36 +697,44 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   def takeSnapshot(): Unit = {\n     // If not a new offset, then it is not worth taking another snapshot\n     if (lastMapOffset > lastSnapOffset) {\n-      val snapshotFile = Log.producerSnapshotFile(logDir, lastMapOffset)\n+      val snapshotFile = SnapshotFile(Log.producerSnapshotFile(_logDir, lastMapOffset))\n       info(s\"Writing producer snapshot at offset $lastMapOffset\")\n-      writeSnapshot(snapshotFile, producers)\n+      writeSnapshot(snapshotFile.file, producers)\n+      snapshots.put(snapshotFile.offset, snapshotFile)\n \n       // Update the last snap offset according to the serialized map\n       lastSnapOffset = lastMapOffset\n     }\n   }\n \n+  /**\n+   * Update the parentDir for this ProducerStateManager and all of the snapshot files which it manages.\n+   */\n+  def updateParentDir(parentDir: File): Unit ={\n+    _logDir = parentDir\n+    snapshots.forEach((_, s) => s.updateParentDir(parentDir))\n+  }\n+\n   /**\n    * Get the last offset (exclusive) of the latest snapshot file.\n    */\n-  def latestSnapshotOffset: Option[Long] = latestSnapshotFile.map(file => offsetFromFile(file))\n+  def latestSnapshotOffset: Option[Long] = latestSnapshotFile.map(_.offset)\n \n   /**\n    * Get the last offset (exclusive) of the oldest snapshot file.\n    */\n-  def oldestSnapshotOffset: Option[Long] = oldestSnapshotFile.map(file => offsetFromFile(file))\n+  def oldestSnapshotOffset: Option[Long] = oldestSnapshotFile.map(_.offset)\n \n   /**\n-   * When we remove the head of the log due to retention, we need to remove snapshots older than\n-   * the new log start offset.\n+   * Remove any unreplicated transactions lower than the provided logStartOffset and bring the lastMapOffset forward\n+   * if necessary.\n    */\n-  def truncateHead(logStartOffset: Long): Unit = {\n+  def onLogStartOffsetIncremented(logStartOffset: Long): Unit = {\n     removeUnreplicatedTransactions(logStartOffset)\n \n     if (lastMapOffset < logStartOffset)\n       lastMapOffset = logStartOffset\n \n-    deleteSnapshotsBefore(logStartOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTk0MjUxMg==", "bodyText": "The idea here is to clear un-replicated transactions and optionally advance the lastMapOffset and lastSnapOffset when the logStartOffset is advanced, but to leave the snapshot files around. The corresponding snapshot files should be removed during the retention pass as we cleanup the associated segment files.\nI was attempting to optimize incrementing the logStartOffset a bit so that we don't need to delete the snapshot files from the request handler thread when handling DELETE_RECORDS.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501942512", "createdAt": "2020-10-08T18:53:15Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -653,36 +697,44 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   def takeSnapshot(): Unit = {\n     // If not a new offset, then it is not worth taking another snapshot\n     if (lastMapOffset > lastSnapOffset) {\n-      val snapshotFile = Log.producerSnapshotFile(logDir, lastMapOffset)\n+      val snapshotFile = SnapshotFile(Log.producerSnapshotFile(_logDir, lastMapOffset))\n       info(s\"Writing producer snapshot at offset $lastMapOffset\")\n-      writeSnapshot(snapshotFile, producers)\n+      writeSnapshot(snapshotFile.file, producers)\n+      snapshots.put(snapshotFile.offset, snapshotFile)\n \n       // Update the last snap offset according to the serialized map\n       lastSnapOffset = lastMapOffset\n     }\n   }\n \n+  /**\n+   * Update the parentDir for this ProducerStateManager and all of the snapshot files which it manages.\n+   */\n+  def updateParentDir(parentDir: File): Unit ={\n+    _logDir = parentDir\n+    snapshots.forEach((_, s) => s.updateParentDir(parentDir))\n+  }\n+\n   /**\n    * Get the last offset (exclusive) of the latest snapshot file.\n    */\n-  def latestSnapshotOffset: Option[Long] = latestSnapshotFile.map(file => offsetFromFile(file))\n+  def latestSnapshotOffset: Option[Long] = latestSnapshotFile.map(_.offset)\n \n   /**\n    * Get the last offset (exclusive) of the oldest snapshot file.\n    */\n-  def oldestSnapshotOffset: Option[Long] = oldestSnapshotFile.map(file => offsetFromFile(file))\n+  def oldestSnapshotOffset: Option[Long] = oldestSnapshotFile.map(_.offset)\n \n   /**\n-   * When we remove the head of the log due to retention, we need to remove snapshots older than\n-   * the new log start offset.\n+   * Remove any unreplicated transactions lower than the provided logStartOffset and bring the lastMapOffset forward\n+   * if necessary.\n    */\n-  def truncateHead(logStartOffset: Long): Unit = {\n+  def onLogStartOffsetIncremented(logStartOffset: Long): Unit = {\n     removeUnreplicatedTransactions(logStartOffset)\n \n     if (lastMapOffset < logStartOffset)\n       lastMapOffset = logStartOffset\n \n-    deleteSnapshotsBefore(logStartOffset)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMzMjU2OQ=="}, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 214}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MjU2OTQwOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNjo0MjoxOVrOHenPvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMDowMTowM1rOHeuY_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2MjMzNQ==", "bodyText": "Hmm, why is orphanedSnapshotFile deleted since we keep the last stray snapshot?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501862335", "createdAt": "2020-10-08T16:42:19Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1226,6 +1225,104 @@ class LogTest {\n     assertEquals(retainedLastSeqOpt, reloadedLastSeqOpt)\n   }\n \n+  @Test\n+  def testRetentionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = 0, retentionMs = 1000 * 60, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+\n+    log.updateHighWatermark(log.logEndOffset)\n+\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+    // Sleep to breach the retention period\n+    mockTime.sleep(1000 * 60 + 1)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testLogStartOffsetMovementDeletesSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Increment the log start offset to exclude the first two segments.\n+    log.maybeIncrementLogStartOffset(log.logEndOffset - 1, ClientRecordDeletion)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testCompactionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, cleanupPolicy = LogConfig.Compact, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+    val cleaner = new Cleaner(id = 0,\n+      offsetMap = new FakeOffsetMap(Int.MaxValue),\n+      ioBufferSize = 64 * 1024,\n+      maxIoBufferSize = 64 * 1024,\n+      dupBufferLoadFactor = 0.75,\n+      throttler = new Throttler(Double.MaxValue, Long.MaxValue, false, time = mockTime),\n+      time = mockTime,\n+      checkDone = _ => {})\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"a\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"b\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"c\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(\"expected a snapshot file per segment base offset, except the first segment\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Clean segments, this should delete everything except the active segment since there only\n+    // exists the key \"a\".\n+    cleaner.clean(LogToClean(log.topicPartition, log, 0, log.logEndOffset))\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expected a snapshot file per segment base offset, excluding the first\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+  }\n+\n+  @Test\n+  def testLoadingLogCleansOrphanedProducerStateSnapshots(): Unit = {\n+    val orphanedSnapshotFile = Log.producerSnapshotFile(logDir, 42).toPath\n+    Files.createFile(orphanedSnapshotFile)\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    createLog(logDir, logConfig)\n+    assertEquals(\"expected orphaned producer state snapshot file to be cleaned up\", 0, ProducerStateManager.listSnapshotFiles(logDir).size)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTk1NzgwMA==", "bodyText": "It's being deleted because during producer state loading because we truncate producer state to match the bounds of the log, and the snapshot file written out at offset 42 is higher than the log end offset of the empty log. The test name is not very clear in this case though.\nI will fix the name and add another test which checks that we keep around the largest stray producer state snapshot file.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501957800", "createdAt": "2020-10-08T19:20:09Z", "author": {"login": "gardnervickers"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1226,6 +1225,104 @@ class LogTest {\n     assertEquals(retainedLastSeqOpt, reloadedLastSeqOpt)\n   }\n \n+  @Test\n+  def testRetentionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = 0, retentionMs = 1000 * 60, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+\n+    log.updateHighWatermark(log.logEndOffset)\n+\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+    // Sleep to breach the retention period\n+    mockTime.sleep(1000 * 60 + 1)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testLogStartOffsetMovementDeletesSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Increment the log start offset to exclude the first two segments.\n+    log.maybeIncrementLogStartOffset(log.logEndOffset - 1, ClientRecordDeletion)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testCompactionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, cleanupPolicy = LogConfig.Compact, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+    val cleaner = new Cleaner(id = 0,\n+      offsetMap = new FakeOffsetMap(Int.MaxValue),\n+      ioBufferSize = 64 * 1024,\n+      maxIoBufferSize = 64 * 1024,\n+      dupBufferLoadFactor = 0.75,\n+      throttler = new Throttler(Double.MaxValue, Long.MaxValue, false, time = mockTime),\n+      time = mockTime,\n+      checkDone = _ => {})\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"a\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"b\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"c\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(\"expected a snapshot file per segment base offset, except the first segment\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Clean segments, this should delete everything except the active segment since there only\n+    // exists the key \"a\".\n+    cleaner.clean(LogToClean(log.topicPartition, log, 0, log.logEndOffset))\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expected a snapshot file per segment base offset, excluding the first\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+  }\n+\n+  @Test\n+  def testLoadingLogCleansOrphanedProducerStateSnapshots(): Unit = {\n+    val orphanedSnapshotFile = Log.producerSnapshotFile(logDir, 42).toPath\n+    Files.createFile(orphanedSnapshotFile)\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    createLog(logDir, logConfig)\n+    assertEquals(\"expected orphaned producer state snapshot file to be cleaned up\", 0, ProducerStateManager.listSnapshotFiles(logDir).size)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2MjMzNQ=="}, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTk3OTM4OA==", "bodyText": "I added some extra context to the existing test and created a new test which verifies from the log that the largest stray snapshot which is within the logs end offset is retained testLoadingLogKeepsLargestStrayProducerStateSnapshot.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501979388", "createdAt": "2020-10-08T20:01:03Z", "author": {"login": "gardnervickers"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1226,6 +1225,104 @@ class LogTest {\n     assertEquals(retainedLastSeqOpt, reloadedLastSeqOpt)\n   }\n \n+  @Test\n+  def testRetentionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = 0, retentionMs = 1000 * 60, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+\n+    log.updateHighWatermark(log.logEndOffset)\n+\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+    // Sleep to breach the retention period\n+    mockTime.sleep(1000 * 60 + 1)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testLogStartOffsetMovementDeletesSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Increment the log start offset to exclude the first two segments.\n+    log.maybeIncrementLogStartOffset(log.logEndOffset - 1, ClientRecordDeletion)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testCompactionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, cleanupPolicy = LogConfig.Compact, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+    val cleaner = new Cleaner(id = 0,\n+      offsetMap = new FakeOffsetMap(Int.MaxValue),\n+      ioBufferSize = 64 * 1024,\n+      maxIoBufferSize = 64 * 1024,\n+      dupBufferLoadFactor = 0.75,\n+      throttler = new Throttler(Double.MaxValue, Long.MaxValue, false, time = mockTime),\n+      time = mockTime,\n+      checkDone = _ => {})\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"a\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"b\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"c\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(\"expected a snapshot file per segment base offset, except the first segment\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Clean segments, this should delete everything except the active segment since there only\n+    // exists the key \"a\".\n+    cleaner.clean(LogToClean(log.topicPartition, log, 0, log.logEndOffset))\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expected a snapshot file per segment base offset, excluding the first\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+  }\n+\n+  @Test\n+  def testLoadingLogCleansOrphanedProducerStateSnapshots(): Unit = {\n+    val orphanedSnapshotFile = Log.producerSnapshotFile(logDir, 42).toPath\n+    Files.createFile(orphanedSnapshotFile)\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    createLog(logDir, logConfig)\n+    assertEquals(\"expected orphaned producer state snapshot file to be cleaned up\", 0, ProducerStateManager.listSnapshotFiles(logDir).size)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2MjMzNQ=="}, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MjU3Mzk0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNjo0MzozN1rOHenSxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNjo0MzozN1rOHenSxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2MzExMQ==", "bodyText": "mockTime.sleep() calls scheduler.tick() already.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501863111", "createdAt": "2020-10-08T16:43:37Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1661,14 +1753,17 @@ class LogTest {\n     log.roll()\n \n     assertEquals(2, log.activeProducersWithLastSequence.size)\n-    assertEquals(2, ProducerStateManager.listSnapshotFiles(log.producerStateManager.logDir).size)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(log.dir).size)\n \n     log.updateHighWatermark(log.logEndOffset)\n     log.maybeIncrementLogStartOffset(2L, ClientRecordDeletion)\n+    log.deleteOldSegments() // force retention to kick in so that the snapshot files are cleaned up.\n+    mockTime.sleep(logConfig.fileDeleteDelayMs + 1000) // advance the clock so file deletion takes place\n+    mockTime.scheduler.tick()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MjU5MTM1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNjo0ODoxM1rOHendzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxOTo0NDozM1rOHet3HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2NTkzMg==", "bodyText": "Below, the base offset of the largest segment equals to and doesn't exceed the offset of the largest stray snapshot.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501865932", "createdAt": "2020-10-08T16:48:13Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala", "diffHunk": "@@ -834,6 +834,40 @@ class ProducerStateManagerTest {\n     assertEquals(None, stateManager.lastEntry(producerId).get.currentTxnFirstOffset)\n   }\n \n+  @Test\n+  def testRemoveStraySnapshotsKeepCleanShutdownSnapshot(): Unit = {\n+    // Test that when stray snapshots are removed, the largest stray snapshot is kept around. This covers the case where\n+    // the broker shutdown cleanly and emitted a snapshot file larger than the base offset of the active segment.\n+\n+    // Create 3 snapshot files at different offsets.\n+    Log.producerSnapshotFile(logDir, 42).createNewFile()\n+    Log.producerSnapshotFile(logDir, 5).createNewFile()\n+    Log.producerSnapshotFile(logDir, 2).createNewFile()\n+\n+    // claim that we only have one segment with a base offset of 5\n+    stateManager.removeStraySnapshots(Set(5))\n+\n+    // The snapshot file at offset 2 should be considered a stray, but the snapshot at 42 should be kept\n+    // around because it is the largest snapshot.\n+    assertEquals(Some(42), stateManager.latestSnapshotOffset)\n+    assertEquals(Some(5), stateManager.oldestSnapshotOffset)\n+    assertEquals(Seq(5, 42), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+  }\n+\n+  @Test\n+  def testRemoveAllStraySnapshots(): Unit = {\n+    // Test that when stray snapshots are removed, all stray snapshots are removed when the base offset of the largest\n+    // segment exceeds the offset of the largest stray snapshot.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTk3MDcxNg==", "bodyText": "Hmm, I think my comment here could be worded better. Offset 42 here is not a \"stray\", since we provide it along with the list of segmentBaseOffsets to removeStraySnapshots.\nI'll change up the wording on this, thanks!", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501970716", "createdAt": "2020-10-08T19:44:33Z", "author": {"login": "gardnervickers"}, "path": "core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala", "diffHunk": "@@ -834,6 +834,40 @@ class ProducerStateManagerTest {\n     assertEquals(None, stateManager.lastEntry(producerId).get.currentTxnFirstOffset)\n   }\n \n+  @Test\n+  def testRemoveStraySnapshotsKeepCleanShutdownSnapshot(): Unit = {\n+    // Test that when stray snapshots are removed, the largest stray snapshot is kept around. This covers the case where\n+    // the broker shutdown cleanly and emitted a snapshot file larger than the base offset of the active segment.\n+\n+    // Create 3 snapshot files at different offsets.\n+    Log.producerSnapshotFile(logDir, 42).createNewFile()\n+    Log.producerSnapshotFile(logDir, 5).createNewFile()\n+    Log.producerSnapshotFile(logDir, 2).createNewFile()\n+\n+    // claim that we only have one segment with a base offset of 5\n+    stateManager.removeStraySnapshots(Set(5))\n+\n+    // The snapshot file at offset 2 should be considered a stray, but the snapshot at 42 should be kept\n+    // around because it is the largest snapshot.\n+    assertEquals(Some(42), stateManager.latestSnapshotOffset)\n+    assertEquals(Some(5), stateManager.oldestSnapshotOffset)\n+    assertEquals(Seq(5, 42), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+  }\n+\n+  @Test\n+  def testRemoveAllStraySnapshots(): Unit = {\n+    // Test that when stray snapshots are removed, all stray snapshots are removed when the base offset of the largest\n+    // segment exceeds the offset of the largest stray snapshot.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2NTkzMg=="}, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MzYwNDU1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozMDozMFrOHexG2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMzozMzowMlrOHe0ceA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAyMzg5OQ==", "bodyText": "Since deleteSnapshotsBefore() still exist, could we keep using it?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r502023899", "createdAt": "2020-10-08T21:30:30Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -782,7 +782,7 @@ class LogTest {\n     }\n \n     // Retain snapshots for the last 2 segments\n-    ProducerStateManager.deleteSnapshotsBefore(logDir, segmentOffsets(segmentOffsets.size - 2))\n+    ProducerStateManager.listSnapshotFiles(logDir).filter(_.offset < segmentOffsets(segmentOffsets.size - 2)).foreach(_.deleteIfExists())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16f51f78f676c2dcaf2f75b0ec41d0847f277390"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA3ODU4NA==", "bodyText": "Yes, it should work if we switch these back to using deleteSnapshotsBefore. Thanks!", "url": "https://github.com/apache/kafka/pull/7929#discussion_r502078584", "createdAt": "2020-10-08T23:33:02Z", "author": {"login": "gardnervickers"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -782,7 +782,7 @@ class LogTest {\n     }\n \n     // Retain snapshots for the last 2 segments\n-    ProducerStateManager.deleteSnapshotsBefore(logDir, segmentOffsets(segmentOffsets.size - 2))\n+    ProducerStateManager.listSnapshotFiles(logDir).filter(_.offset < segmentOffsets(segmentOffsets.size - 2)).foreach(_.deleteIfExists())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAyMzg5OQ=="}, "originalCommit": {"oid": "16f51f78f676c2dcaf2f75b0ec41d0847f277390"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MzYwNDkxOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozMDozN1rOHexHFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozMDozN1rOHexHFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAyMzk1OA==", "bodyText": "Since deleteSnapshotsBefore() still exist, could we keep using it?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r502023958", "createdAt": "2020-10-08T21:30:37Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -794,16 +794,12 @@ class LogTest {\n \n     // Only delete snapshots before the base offset of the recovery point segment (post KAFKA-5829 behaviour) to\n     // avoid reading all segments\n-    ProducerStateManager.deleteSnapshotsBefore(logDir, offsetForRecoveryPointSegment)\n+    ProducerStateManager.listSnapshotFiles(logDir).filter(_.offset < offsetForRecoveryPointSegment).foreach(_.deleteIfExists())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16f51f78f676c2dcaf2f75b0ec41d0847f277390"}, "originalPosition": 32}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4329, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}