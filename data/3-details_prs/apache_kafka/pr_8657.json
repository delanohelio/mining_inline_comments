{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE2ODY0MzEx", "number": 8657, "title": "KAFKA-8334 Make sure the thread which tries to complete delayed reque\u2026", "bodyText": "The main changes of this PR are shown below.\n\nreplace tryLock by lock for DelayedOperation#maybeTryComplete\ncomplete the delayed requests without holding group lock\n\nBEFORE\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.num_producers=3.acks=1\nstatus: PASS\nrun time: 56.718 seconds\n{\"records_per_sec\": 621619.67445, \"mb_per_sec\": 59.28}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 16.067 seconds\n{\"records_per_sec\": 1565190.1706, \"mb_per_sec\": 149.2682}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 1 minute 2.486 seconds\n{\"records_per_sec\": 3165558.7211, \"mb_per_sec\": 301.8912}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 19.929 seconds\n{\"records_per_sec\": 1350621.2858, \"mb_per_sec\": 128.8053}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 1 minute 3.014 seconds\n{\"records_per_sec\": 3653635.3672, \"mb_per_sec\": 348.4378}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 58.852 seconds\n{\"records_per_sec\": 3252032.5203, \"mb_per_sec\": 310.138}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 59.315 seconds\n{\"records_per_sec\": 3825554.7054, \"mb_per_sec\": 364.8333}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 41.012 seconds\n{\"latency_99th_ms\": 6.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 16.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 44.975 seconds\n{\"latency_99th_ms\": 5.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 19.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_SSL.compression_type=none\nstatus: PASS\nrun time: 49.868 seconds\n{\"latency_99th_ms\": 5.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 15.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_SSL.compression_type=snappy\nstatus: PASS\nrun time: 48.454 seconds\n{\"latency_99th_ms\": 5.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 19.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 9.145 seconds\n{\"consumer\": {\"records_per_sec\": 610426.0774, \"mb_per_sec\": 58.2148}, \"producer\": {\"records_per_sec\": 620385.880017, \"mb_per_sec\": 59.16}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 1 minute 2.140 seconds\n{\"consumer\": {\"records_per_sec\": 1465845.793, \"mb_per_sec\": 139.7939}, \"producer\": {\"records_per_sec\": 1416831.963729, \"mb_per_sec\": 135.12}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 10.968 seconds\n{\"consumer\": {\"records_per_sec\": 599089.3841, \"mb_per_sec\": 57.1336}, \"producer\": {\"records_per_sec\": 626370.184779, \"mb_per_sec\": 59.74}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 58.237 seconds\n{\"consumer\": {\"records_per_sec\": 1298532.6581, \"mb_per_sec\": 123.8377}, \"producer\": {\"records_per_sec\": 1315443.304394, \"mb_per_sec\": 125.45}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 1 minute 0.201 seconds\n{\"consumer\": {\"records_per_sec\": 997705.2779, \"mb_per_sec\": 95.1486}, \"producer\": {\"records_per_sec\": 957212.596918, \"mb_per_sec\": 91.29}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 56.187 seconds\n{\"consumer\": {\"records_per_sec\": 1313025.2101, \"mb_per_sec\": 125.2198}, \"producer\": {\"records_per_sec\": 1363512.407963, \"mb_per_sec\": 130.03}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 57.195 seconds\n{\"latency_99th_ms\": 3.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 11.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 57.311 seconds\n{\"latency_99th_ms\": 3.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 8.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 57.756 seconds\n{\"latency_99th_ms\": 3.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 11.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 57.291 seconds\n{\"latency_99th_ms\": 3.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 8.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 48.981 seconds\n{\"latency_99th_ms\": 3.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 15.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 51.503 seconds\n{\"latency_99th_ms\": 3.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 9.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 8.161 seconds\n{\"0\": {\"records_per_sec\": 698421.567258, \"mb_per_sec\": 66.61}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 56.530 seconds\n{\"0\": {\"records_per_sec\": 1639881.928501, \"mb_per_sec\": 156.39}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 4.389 seconds\n{\"0\": {\"records_per_sec\": 720097.933319, \"mb_per_sec\": 68.67}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 59.589 seconds\n{\"0\": {\"records_per_sec\": 1621271.076524, \"mb_per_sec\": 154.62}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 56.165 seconds\n{\"0\": {\"records_per_sec\": 1152737.752161, \"mb_per_sec\": 109.93}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 54.846 seconds\n{\"0\": {\"records_per_sec\": 1646903.820817, \"mb_per_sec\": 157.06}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 59.692 seconds\n{\"records_per_sec\": 1794354.545455, \"mb_per_sec\": 17.11}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 58.774 seconds\n{\"records_per_sec\": 1973499.779444, \"mb_per_sec\": 18.82}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 57.450 seconds\n{\"records_per_sec\": 325613.051917, \"mb_per_sec\": 31.05}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 54.134 seconds\n{\"records_per_sec\": 734232.49453, \"mb_per_sec\": 70.02}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 54.106 seconds\n{\"records_per_sec\": 41259.452813, \"mb_per_sec\": 39.35}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 52.577 seconds\n{\"records_per_sec\": 51681.555641, \"mb_per_sec\": 49.29}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 57.747 seconds\n{\"records_per_sec\": 4320.991629, \"mb_per_sec\": 41.21}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 55.024 seconds\n{\"records_per_sec\": 4223.096287, \"mb_per_sec\": 40.27}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 53.355 seconds\n{\"records_per_sec\": 817.794028, \"mb_per_sec\": 77.99}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 53.097 seconds\n{\"records_per_sec\": 797.859691, \"mb_per_sec\": 76.09}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 57.602 seconds\n{\"records_per_sec\": 1779132.025451, \"mb_per_sec\": 16.97}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 59.207 seconds\n{\"records_per_sec\": 1935367.267484, \"mb_per_sec\": 18.46}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 58.127 seconds\n{\"records_per_sec\": 330911.489152, \"mb_per_sec\": 31.56}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 50.805 seconds\n{\"records_per_sec\": 615677.522936, \"mb_per_sec\": 58.72}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 51.221 seconds\n{\"records_per_sec\": 40378.158845, \"mb_per_sec\": 38.51}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 50.578 seconds\n{\"records_per_sec\": 51901.392111, \"mb_per_sec\": 49.5}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 53.369 seconds\n{\"records_per_sec\": 4363.13394, \"mb_per_sec\": 41.61}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 53.982 seconds\n{\"records_per_sec\": 4323.775773, \"mb_per_sec\": 41.23}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 54.736 seconds\n{\"records_per_sec\": 810.386473, \"mb_per_sec\": 77.28}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 54.867 seconds\n{\"records_per_sec\": 795.023697, \"mb_per_sec\": 75.82}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-one.acks=1\nstatus: PASS\nrun time: 48.440 seconds\n{\"records_per_sec\": 701608.468374, \"mb_per_sec\": 66.91}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.acks=-1\nstatus: PASS\nrun time: 55.268 seconds\n{\"records_per_sec\": 268274.435339, \"mb_per_sec\": 25.58}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.acks=1\nstatus: PASS\nrun time: 50.207 seconds\n{\"records_per_sec\": 467657.491289, \"mb_per_sec\": 44.6}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=10\nstatus: PASS\nrun time: 55.395 seconds\n{\"records_per_sec\": 2038543.742406, \"mb_per_sec\": 19.44}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=100\nstatus: PASS\nrun time: 52.835 seconds\n{\"records_per_sec\": 479520.185781, \"mb_per_sec\": 45.73}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=1000\nstatus: PASS\nrun time: 47.566 seconds\n{\"records_per_sec\": 50609.728507, \"mb_per_sec\": 48.27}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=10000\nstatus: PASS\nrun time: 49.949 seconds\n{\"records_per_sec\": 5941.124391, \"mb_per_sec\": 56.66}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=100000\nstatus: PASS\nrun time: 48.718 seconds\n{\"records_per_sec\": 1698.734177, \"mb_per_sec\": 162.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=10\nstatus: PASS\nrun time: 55.253 seconds\n{\"records_per_sec\": 1946594.923858, \"mb_per_sec\": 18.56}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=100\nstatus: PASS\nrun time: 50.712 seconds\n{\"records_per_sec\": 986894.852941, \"mb_per_sec\": 94.12}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=1000\nstatus: PASS\nrun time: 58.378 seconds\n{\"records_per_sec\": 112787.394958, \"mb_per_sec\": 107.56}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=10000\nstatus: PASS\nrun time: 50.972 seconds\n{\"records_per_sec\": 5747.751606, \"mb_per_sec\": 54.81}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=100000\nstatus: PASS\nrun time: 47.419 seconds\n{\"records_per_sec\": 1580.683157, \"mb_per_sec\": 150.75}\n--------------------------------------------------------------------------------\n\nAFTER\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.num_producers=3.acks=1\nstatus: PASS\nrun time: 56.262 seconds\n{\"records_per_sec\": 625731.99396, \"mb_per_sec\": 59.68}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 15.345 seconds\n{\"records_per_sec\": 1458151.0645, \"mb_per_sec\": 139.0601}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 1 minute 5.284 seconds\n{\"records_per_sec\": 3173595.6839, \"mb_per_sec\": 302.6577}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 16.265 seconds\n{\"records_per_sec\": 1477759.7163, \"mb_per_sec\": 140.9301}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 1 minute 4.992 seconds\n{\"records_per_sec\": 2992220.2274, \"mb_per_sec\": 285.3604}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 1 minute 2.562 seconds\n{\"records_per_sec\": 3987240.8293, \"mb_per_sec\": 380.2529}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 1 minute 1.068 seconds\n{\"records_per_sec\": 3531073.4463, \"mb_per_sec\": 336.7494}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 43.213 seconds\n{\"latency_99th_ms\": 6.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 19.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 44.302 seconds\n{\"latency_99th_ms\": 6.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 17.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_SSL.compression_type=none\nstatus: PASS\nrun time: 52.117 seconds\n{\"latency_99th_ms\": 6.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 17.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_SSL.compression_type=snappy\nstatus: PASS\nrun time: 48.599 seconds\n{\"latency_99th_ms\": 6.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 15.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 6.347 seconds\n{\"consumer\": {\"records_per_sec\": 610165.3548, \"mb_per_sec\": 58.1899}, \"producer\": {\"records_per_sec\": 645161.290323, \"mb_per_sec\": 61.53}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 58.196 seconds\n{\"consumer\": {\"records_per_sec\": 1365001.365, \"mb_per_sec\": 130.1767}, \"producer\": {\"records_per_sec\": 1315270.288044, \"mb_per_sec\": 125.43}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 8.056 seconds\n{\"consumer\": {\"records_per_sec\": 635364.3815, \"mb_per_sec\": 60.5931}, \"producer\": {\"records_per_sec\": 645369.474024, \"mb_per_sec\": 61.55}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 55.585 seconds\n{\"consumer\": {\"records_per_sec\": 1396453.0094, \"mb_per_sec\": 133.1761}, \"producer\": {\"records_per_sec\": 1345170.836696, \"mb_per_sec\": 128.29}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 57.844 seconds\n{\"consumer\": {\"records_per_sec\": 995024.8756, \"mb_per_sec\": 94.893}, \"producer\": {\"records_per_sec\": 934928.9454, \"mb_per_sec\": 89.16}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 57.728 seconds\n{\"consumer\": {\"records_per_sec\": 1442793.2477, \"mb_per_sec\": 137.5955}, \"producer\": {\"records_per_sec\": 1343002.954607, \"mb_per_sec\": 128.08}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 59.918 seconds\n{\"latency_99th_ms\": 4.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 10.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 58.414 seconds\n{\"latency_99th_ms\": 4.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 13.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 58.689 seconds\n{\"latency_99th_ms\": 4.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 10.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 57.322 seconds\n{\"latency_99th_ms\": 4.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 11.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 53.221 seconds\n{\"latency_99th_ms\": 4.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 12.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 53.012 seconds\n{\"latency_99th_ms\": 4.0, \"latency_50th_ms\": 0.0, \"latency_999th_ms\": 13.0}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 9.797 seconds\n{\"0\": {\"records_per_sec\": 712352.186921, \"mb_per_sec\": 67.94}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 58.567 seconds\n{\"0\": {\"records_per_sec\": 1586294.416244, \"mb_per_sec\": 151.28}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none\nstatus: PASS\nrun time: 1 minute 3.881 seconds\n{\"0\": {\"records_per_sec\": 730513.551026, \"mb_per_sec\": 69.67}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy\nstatus: PASS\nrun time: 58.038 seconds\n{\"0\": {\"records_per_sec\": 1624959.376016, \"mb_per_sec\": 154.97}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.security_protocol=PLAINTEXT.compression_type=none\nstatus: PASS\nrun time: 54.064 seconds\n{\"0\": {\"records_per_sec\": 1184834.123223, \"mb_per_sec\": 112.99}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.security_protocol=PLAINTEXT.compression_type=snappy\nstatus: PASS\nrun time: 53.514 seconds\n{\"0\": {\"records_per_sec\": 1647175.094713, \"mb_per_sec\": 157.09}}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 1 minute 0.244 seconds\n{\"records_per_sec\": 1814733.910222, \"mb_per_sec\": 17.31}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 59.460 seconds\n{\"records_per_sec\": 2014373.705538, \"mb_per_sec\": 19.21}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 58.051 seconds\n{\"records_per_sec\": 328240.890193, \"mb_per_sec\": 31.3}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 52.228 seconds\n{\"records_per_sec\": 747730.91922, \"mb_per_sec\": 71.31}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 53.001 seconds\n{\"records_per_sec\": 40475.572979, \"mb_per_sec\": 38.6}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 54.782 seconds\n{\"records_per_sec\": 70752.24038, \"mb_per_sec\": 67.47}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 57.875 seconds\n{\"records_per_sec\": 4461.768617, \"mb_per_sec\": 42.55}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 57.089 seconds\n{\"records_per_sec\": 4282.386726, \"mb_per_sec\": 40.84}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 54.228 seconds\n{\"records_per_sec\": 824.324324, \"mb_per_sec\": 78.61}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 51.462 seconds\n{\"records_per_sec\": 809.897405, \"mb_per_sec\": 77.24}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 59.836 seconds\n{\"records_per_sec\": 1812773.095624, \"mb_per_sec\": 17.29}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 56.087 seconds\n{\"records_per_sec\": 2029604.113111, \"mb_per_sec\": 19.36}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 55.743 seconds\n{\"records_per_sec\": 348707.976098, \"mb_per_sec\": 33.26}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 52.794 seconds\n{\"records_per_sec\": 974003.628447, \"mb_per_sec\": 92.89}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 55.364 seconds\n{\"records_per_sec\": 41284.835435, \"mb_per_sec\": 39.37}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 51.486 seconds\n{\"records_per_sec\": 57827.229642, \"mb_per_sec\": 55.15}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 55.384 seconds\n{\"records_per_sec\": 4502.180476, \"mb_per_sec\": 42.94}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 54.028 seconds\n{\"records_per_sec\": 4217.787555, \"mb_per_sec\": 40.22}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none\nstatus: PASS\nrun time: 56.079 seconds\n{\"records_per_sec\": 839.79975, \"mb_per_sec\": 80.09}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy\nstatus: PASS\nrun time: 54.970 seconds\n{\"records_per_sec\": 826.35468, \"mb_per_sec\": 78.81}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-one.acks=1\nstatus: PASS\nrun time: 46.430 seconds\n{\"records_per_sec\": 746068.371317, \"mb_per_sec\": 71.15}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.acks=-1\nstatus: PASS\nrun time: 53.541 seconds\n{\"records_per_sec\": 318277.685558, \"mb_per_sec\": 30.35}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.acks=1\nstatus: PASS\nrun time: 49.139 seconds\n{\"records_per_sec\": 487355.482934, \"mb_per_sec\": 46.48}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=10\nstatus: PASS\nrun time: 53.150 seconds\n{\"records_per_sec\": 2153686.136072, \"mb_per_sec\": 20.54}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=100\nstatus: PASS\nrun time: 51.156 seconds\n{\"records_per_sec\": 455438.411944, \"mb_per_sec\": 43.43}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=1000\nstatus: PASS\nrun time: 51.568 seconds\n{\"records_per_sec\": 52820.543093, \"mb_per_sec\": 50.37}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=10000\nstatus: PASS\nrun time: 46.992 seconds\n{\"records_per_sec\": 6253.960857, \"mb_per_sec\": 59.64}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=100000\nstatus: PASS\nrun time: 46.280 seconds\n{\"records_per_sec\": 1669.154229, \"mb_per_sec\": 159.18}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=10\nstatus: PASS\nrun time: 54.138 seconds\n{\"records_per_sec\": 1951122.546882, \"mb_per_sec\": 18.61}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=100\nstatus: PASS\nrun time: 47.680 seconds\n{\"records_per_sec\": 1021443.683409, \"mb_per_sec\": 97.41}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=1000\nstatus: PASS\nrun time: 47.886 seconds\n{\"records_per_sec\": 116104.67128, \"mb_per_sec\": 110.73}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=10000\nstatus: PASS\nrun time: 54.126 seconds\n{\"records_per_sec\": 5550.454921, \"mb_per_sec\": 52.93}\n--------------------------------------------------------------------------------\ntest_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=100000\nstatus: PASS\nrun time: 46.799 seconds\n{\"records_per_sec\": 1582.54717, \"mb_per_sec\": 150.92}\n--------------------------------------------------------------------------------\n\n\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-05-12T17:23:47Z", "url": "https://github.com/apache/kafka/pull/8657", "merged": true, "mergeCommit": {"oid": "c2273adc25b2bab0a3ac95bf7844fedf2860b40b"}, "closed": true, "closedAt": "2020-09-09T21:42:38Z", "author": {"login": "chia7712"}, "timelineItems": {"totalCount": 97, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcgnrY-gFqTQxMDI2MzU3OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHB5m6AFqTQ4NDU5NTk1MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMjYzNTc5", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-410263579", "createdAt": "2020-05-12T17:25:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyNToyMVrOGURNyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyNToyMVrOGURNyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwNjc2MA==", "bodyText": "It collects the \"key\" used to complete delayed requests. The completion is execute out of group lock.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423906760", "createdAt": "2020-05-12T17:25:21Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -377,6 +402,7 @@ class GroupCoordinator(val brokerId: Int,\n                           groupInstanceId: Option[String],\n                           groupAssignment: Map[String, Array[Byte]],\n                           responseCallback: SyncCallback): Unit = {\n+    val partitionsToComplete = mutable.Map[TopicPartition, Boolean]()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMjY0NzI3", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-410264727", "createdAt": "2020-05-12T17:26:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyNjo1M1rOGURRNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyNjo1M1rOGURRNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwNzYzOA==", "bodyText": "this is the main change of this PR (address #6915 (comment)).", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423907638", "createdAt": "2020-05-12T17:26:53Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,20 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\". There was a lot of cases that hold a lock and then try to\n+   * hold more locks to complete delayed requests. Unfortunately, that scenario causes deadlock and we had introduced\n+   * the \"tryLock\" to avoid deadlock. However, the \"tryLock\" causes another issue that thread_A holds a lock but it does\n+   * not complete the delayed requests and there are no threads can complete request as the lock is not free.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag, called completeDelayedRequests, to prevent the method from automatically completing delayed\n+   * request.\n    */\n   private[server] def maybeTryComplete(): Boolean = {\n-    var retry = false\n-    var done = false\n-    do {\n-      if (lock.tryLock()) {\n-        try {\n-          tryCompletePending.set(false)\n-          done = tryComplete()\n-        } finally {\n-          lock.unlock()\n-        }\n-        // While we were holding the lock, another thread may have invoked `maybeTryComplete` and set\n-        // `tryCompletePending`. In this case we should retry.\n-        retry = tryCompletePending.get()\n-      } else {\n-        // Another thread is holding the lock. If `tryCompletePending` is already set and this thread failed to\n-        // acquire the lock, then the thread that is holding the lock is guaranteed to see the flag and retry.\n-        // Otherwise, we should set the flag and retry on this thread since the thread holding the lock may have\n-        // released the lock and returned by the time the flag is set.\n-        retry = !tryCompletePending.getAndSet(true)\n-      }\n-    } while (!isCompleted && retry)\n-    done\n+    lock.lock()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMjY1MzA2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-410265306", "createdAt": "2020-05-12T17:27:34Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyNzozNFrOGURS9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyNzozNFrOGURS9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwODA4NA==", "bodyText": "new check for this PR. Make sure it does not hold group lock", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423908084", "createdAt": "2020-05-12T17:27:34Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala", "diffHunk": "@@ -3750,20 +3758,25 @@ class GroupCoordinatorTest {\n     val (responseFuture, responseCallback) = setupSyncGroupCallback\n \n     val capturedArgument: Capture[scala.collection.Map[TopicPartition, PartitionResponse] => Unit] = EasyMock.newCapture()\n-\n-    EasyMock.expect(replicaManager.appendRecords(EasyMock.anyLong(),\n-      EasyMock.anyShort(),\n+    EasyMock.expect(replicaManager.completeDelayedRequests(EasyMock.anyObject()))\n+      // No lock is held when completing delayed requests\n+      .andAnswer(() => assertFalse(groupCoordinator.groupManager.getGroup(groupId).get.lock.isLocked))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMjY1Nzgx", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-410265781", "createdAt": "2020-05-12T17:28:09Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyODowOVrOGURUdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyODowOVrOGURUdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwODQ2OA==", "bodyText": "This test case is for \"tryLock\" so I just remove it.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423908468", "createdAt": "2020-05-12T17:28:09Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/server/DelayedOperationTest.scala", "diffHunk": "@@ -192,43 +191,6 @@ class DelayedOperationTest {\n     assertEquals(Nil, cancelledOperations)\n   }\n \n-  /**\n-    * Verify that if there is lock contention between two threads attempting to complete,\n-    * completion is performed without any blocking in either thread.\n-    */\n-  @Test\n-  def testTryCompleteLockContention(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMjcxNzc0", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-410271774", "createdAt": "2020-05-12T17:35:42Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzozNTo0MlrOGURnHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzozNTo0MlrOGURnHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxMzI0Ng==", "bodyText": "This is another case of deadlock. DelayedJoin#tryComplete is in a group lock and it tries to complete other delayed joins which related to same __consumer_offsets partition.\nHence, this PR make it control the group lock manually in order to make sure it does not hold group lock when calling GroupCoordinator#onCompleteJoin", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423913246", "createdAt": "2020-05-12T17:35:42Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +33,15 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   */\n+  override def tryComplete(): Boolean = if (group.inLock(group.hasAllMembersJoined)) forceComplete() else false\n+  override def onExpiration(): Unit = coordinator.onExpireJoin()\n+  override def onComplete(): Unit = coordinator.onCompleteJoin(group)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0MDAwMjk3", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-414000297", "createdAt": "2020-05-18T23:19:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQyMzoxOTo1N1rOGXKkiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMDoyMDowNVrOGXLm_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MzYyNw==", "bodyText": "Currently we have a somewhat convoluted model where ReplicaManager creates delayed operations, but we depend on lower level components like Partition to be aware of them and complete them. This breaks encapsulation.\nNot something we should try to complete in this PR, but as an eventual goal, I think we can consider trying to factor delayed operations out of Partition so that they can be managed by ReplicaManager exclusively. If you assume that is the end state, then we could drop completeDelayedRequests and let ReplicaManager always be responsible for checking delayed operations after appending to the log.\nOther than ReplicaManager, the only caller of this method is GroupMetadataManager which uses it during offset expiration. I think the only reason we do this is because we didn't want to waste purgatory space. I don't think that's a good enough reason to go outside the normal flow. It would be simpler to follow the same path. Potentially we could make the callback an Option so that we still have a way to avoid polluting the purgatory.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r426943627", "createdAt": "2020-05-18T23:19:57Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -970,7 +970,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests It may requires a bunch of group locks when completing delayed requests so it may\n+   *                                produce deadlock if caller already holds a group lock. Hence, caller should pass\n+   *                                false to disable completion and then complete the delayed requests after releasing\n+   *                                held group lock\n+   */\n+  def appendRecordsToLeader(records: MemoryRecords,\n+                            origin: AppendOrigin,\n+                            requiredAcks: Int,\n+                            completeDelayedRequests: Boolean): LogAppendResult = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0NDY3OQ==", "bodyText": "Hmm.. Does the group purgatory suffer from the same deadlock potential? If we call checkAndComplete for a group \"foo,\" I don't think we would attempt completion for any other group.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r426944679", "createdAt": "2020-05-18T23:23:37Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +369,31 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions topic partition and leaderHWIncremented\n+   */\n+  private[this] def completeDelayedRequests(topicPartitions: Map[TopicPartition, Boolean]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) =>\n+        if (leaderHWIncremented) groupManager.replicaManager.completeDelayedRequests(tp)\n+        else groupManager.replicaManager.completeDelayedFetchRequests(tp)\n+    }\n+\n+  /**\n+   * complete the delayed join requests associated to input group keys.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param groupKeys group keys to complete\n+   */\n+  private[this] def completeDelayedJoinRequests(groupKeys: Set[GroupKey]): Unit =\n+    groupKeys.foreach(joinPurgatory.checkAndComplete)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2MDYzNw==", "bodyText": "For reference, here are links to two alternative approaches that I considered earlier this year:\n\nAsync completion: hachikuji@3bee4ac\nLock-safe offset cache: hachikuji@3705f33\n\nI think Jun was not satisfied with the first approach because it called for another thread pool. Its advantage though was a simpler and more intuitive API than what we have here. An idea which I never implemented was to let the request handlers also handle delayed operation completion so that we did not need another thread pool. Basically rather than calling the callback in DelayedProduce directly, we add a new operation to the request queues. Obviously this has its own tradeoffs.\nThe second commit tries to use lock-free data structures so that we do not need the lock when completing the callback. This was only a partial solution which handled offset commit appends, but not group metadata appends. I am not sure how to handle join group completion asynchronously, so I gave up on this idea.\nOnly posting in case it's useful to see how some of these alternatives might have looked. I'm ok with the approach here, but I do wish we could come up with a simpler API. One thought I had is whether we could make the need for external completion more explicit. For example, maybe appendRecords could return some kind of object which encapsulates purgatory completion.\nval completion = inLock(lock) {\n  replicaManager.appendRecords(...)\n}\ncompletion.run()\nJust a thought.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r426960637", "createdAt": "2020-05-19T00:20:05Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -769,20 +813,25 @@ class GroupCoordinator(val brokerId: Int,\n             // on heartbeat response to eventually notify the rebalance in progress signal to the consumer\n             val member = group.get(memberId)\n             completeAndScheduleNextHeartbeatExpiration(group, member)\n-            groupManager.storeOffsets(group, memberId, offsetMetadata, responseCallback)\n+            partitionsToComplete ++= groupManager.storeOffsets(\n+              group = group,\n+              consumerId = memberId,\n+              offsetMetadata = offsetMetadata,\n+              responseCallback = responseCallback,\n+              completeDelayedRequests = false)\n \n           case CompletingRebalance =>\n             // We should not receive a commit request if the group has not completed rebalance;\n             // but since the consumer's member.id and generation is valid, it means it has received\n             // the latest group generation information from the JoinResponse.\n             // So let's return a REBALANCE_IN_PROGRESS to let consumer handle it gracefully.\n             responseCallback(offsetMetadata.map { case (k, _) => k -> Errors.REBALANCE_IN_PROGRESS })\n-\n           case _ =>\n             throw new RuntimeException(s\"Logic error: unexpected group state ${group.currentState}\")\n         }\n       }\n     }\n+    completeDelayedRequests(partitionsToComplete)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 193}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2NTM1NjY1", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-416535665", "createdAt": "2020-05-21T22:28:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQyMjoyODo0MVrOGZFBCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMzoyNjowOFrOGazk3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk0OTc2OQ==", "bodyText": "Perhaps reword like the following?\nReturning a map of successfully appended topic partitions and a flag indicting whether the HWM has been incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete delayed requests for those returned partitions.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428949769", "createdAt": "2020-05-21T22:28:41Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -550,19 +584,22 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   * @return the topic partitions we succeed to append data. It is useful to caller which tries to complete delayed requests.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1MDM2MQ==", "bodyText": "Could we do localProduceResults.filter{ case (tp, logAppendResult) => ... } to avoid unnamed references?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428950361", "createdAt": "2020-05-21T22:30:37Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -593,6 +630,7 @@ class ReplicaManager(val config: KafkaConfig,\n         val produceResponseStatus = produceStatus.map { case (k, status) => k -> status.responseStatus }\n         responseCallback(produceResponseStatus)\n       }\n+      localProduceResults.filter(_._2.exception.isEmpty).map(e => e._1 -> e._2.leaderHWIncremented)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1NjA3OA==", "bodyText": "Could we add a comment for the return value?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428956078", "createdAt": "2020-05-21T22:48:37Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1103,7 +1153,7 @@ class GroupCoordinator(val brokerId: Int,\n     joinPurgatory.tryCompleteElseWatch(delayedRebalance, Seq(groupKey))\n   }\n \n-  private def removeMemberAndUpdateGroup(group: GroupMetadata, member: MemberMetadata, reason: String): Unit = {\n+  private def removeMemberAndUpdateGroup(group: GroupMetadata, member: MemberMetadata, reason: String): Option[GroupKey] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1ODUzMA==", "bodyText": "Could we add a comment for the return value?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428958530", "createdAt": "2020-05-21T22:56:39Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1113,33 +1163,27 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Option[GroupKey] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk3MjUzOA==", "bodyText": "This is bit tricky to untangle. It seems the original code holds the group lock for both the group.hasAllMembersJoined check and the call to forceComplete(). So, we probably want to keep doing that.\nI am thinking that we could do the following.\n\nChange GroupCoordinator.onCompleteJoin() so that (1) it checks group.hasAllMembersJoined inside the group lock and returns whether hasAllMembersJoined is true.\nIn DelayedJoin.tryComplete() , we do\n\n        if (GroupCoordinator.onCompleteJoin()) \n               forceComplete() \n}\n\nIn onComplete(), we do nothing.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428972538", "createdAt": "2020-05-21T23:47:16Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +33,15 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   */\n+  override def tryComplete(): Boolean = if (group.inLock(group.hasAllMembersJoined)) forceComplete() else false", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0ODU1Mw==", "bodyText": "I was trying to check if it's safe to do this. The intention for this is probably to avoid the deadlock between the group lock and the lock in DelayedOperation. None of the caller of joinPurgatory.checkAndComplete holds a group lock now. The only other caller that can first hold a group lock and then the lock in DelayedOperation is joinPurgatory.tryCompleteElseWatch(). However, that's not an issue since that's when the DelayedJoin operation is first added. So, this changes seems ok.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430748553", "createdAt": "2020-05-26T22:47:10Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +33,15 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MDAyMA==", "bodyText": "\"as the lock is not free\" : Do you mean \"when the lock is free\"?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430750020", "createdAt": "2020-05-26T22:51:20Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,20 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\". There was a lot of cases that hold a lock and then try to\n+   * hold more locks to complete delayed requests. Unfortunately, that scenario causes deadlock and we had introduced\n+   * the \"tryLock\" to avoid deadlock. However, the \"tryLock\" causes another issue that thread_A holds a lock but it does\n+   * not complete the delayed requests and there are no threads can complete request as the lock is not free.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MDcyMw==", "bodyText": "a flag => a flag in ReplicaManager.appendRecords().", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430750723", "createdAt": "2020-05-26T22:53:22Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,20 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\". There was a lot of cases that hold a lock and then try to\n+   * hold more locks to complete delayed requests. Unfortunately, that scenario causes deadlock and we had introduced\n+   * the \"tryLock\" to avoid deadlock. However, the \"tryLock\" causes another issue that thread_A holds a lock but it does\n+   * not complete the delayed requests and there are no threads can complete request as the lock is not free.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag, called completeDelayedRequests, to prevent the method from automatically completing delayed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NTY1Ng==", "bodyText": "Could we add a comment to explain the return value?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430755656", "createdAt": "2020-05-26T23:08:08Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -311,37 +312,40 @@ class GroupMetadataManager(brokerId: Int,\n \n           responseCallback(responseError)\n         }\n-        appendForGroup(group, groupMetadataRecords, putCacheCallback)\n-\n+        appendForGroup(group, groupMetadataRecords, putCacheCallback, completeDelayedRequests)\n       case None =>\n         responseCallback(Errors.NOT_COORDINATOR)\n-        None\n+        Map.empty\n     }\n   }\n \n   private def appendForGroup(group: GroupMetadata,\n                              records: Map[TopicPartition, MemoryRecords],\n-                             callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {\n+                             callback: Map[TopicPartition, PartitionResponse] => Unit,\n+                             completeDelayedRequests: Boolean): Map[TopicPartition, Boolean] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NjE4Mw==", "bodyText": "A map containing the topic partitions having new records and a flag indicating whether the HWM has been incremented.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430756183", "createdAt": "2020-05-26T23:09:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -311,37 +312,40 @@ class GroupMetadataManager(brokerId: Int,\n \n           responseCallback(responseError)\n         }\n-        appendForGroup(group, groupMetadataRecords, putCacheCallback)\n-\n+        appendForGroup(group, groupMetadataRecords, putCacheCallback, completeDelayedRequests)\n       case None =>\n         responseCallback(Errors.NOT_COORDINATOR)\n-        None\n+        Map.empty\n     }\n   }\n \n   private def appendForGroup(group: GroupMetadata,\n                              records: Map[TopicPartition, MemoryRecords],\n-                             callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {\n+                             callback: Map[TopicPartition, PartitionResponse] => Unit,\n+                             completeDelayedRequests: Boolean): Map[TopicPartition, Boolean] = {\n     // call replica manager to append the group message\n     replicaManager.appendRecords(\n       timeout = config.offsetCommitTimeoutMs.toLong,\n       requiredAcks = config.offsetCommitRequiredAcks,\n       internalTopicsAllowed = true,\n       origin = AppendOrigin.Coordinator,\n+      completeDelayedRequests = completeDelayedRequests,\n       entriesPerPartition = records,\n       delayedProduceLock = Some(group.lock),\n       responseCallback = callback)\n   }\n \n   /**\n    * Store offsets by appending it to the replicated log and then inserting to cache\n+   * @return the topic partitions having new records", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NzI4Nw==", "bodyText": "I agree that it's simpler to let the caller in ReplicaManager to complete the delayed requests. This way, we don't need to pass completeDelayedRequests in here.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430757287", "createdAt": "2020-05-26T23:13:06Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -970,7 +970,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests It may requires a bunch of group locks when completing delayed requests so it may\n+   *                                produce deadlock if caller already holds a group lock. Hence, caller should pass\n+   *                                false to disable completion and then complete the delayed requests after releasing\n+   *                                held group lock\n+   */\n+  def appendRecordsToLeader(records: MemoryRecords,\n+                            origin: AppendOrigin,\n+                            requiredAcks: Int,\n+                            completeDelayedRequests: Boolean): LogAppendResult = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MzYyNw=="}, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MTE4Mw==", "bodyText": "Could we add a comment to explain the return value?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430761183", "createdAt": "2020-05-26T23:26:08Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -241,7 +241,8 @@ class GroupMetadataManager(brokerId: Int,\n \n   def storeGroup(group: GroupMetadata,\n                  groupAssignment: Map[String, Array[Byte]],\n-                 responseCallback: Errors => Unit): Unit = {\n+                 responseCallback: Errors => Unit,\n+                 completeDelayedRequests: Boolean): Map[TopicPartition, Boolean] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5MzEwNTk3", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-419310597", "createdAt": "2020-05-27T14:58:54Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNDo1ODo1NVrOGbOvAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNDo1ODo1NVrOGbOvAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwNjE0Nw==", "bodyText": "make sure DelayedJoinTest does not cause deadlock", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431206147", "createdAt": "2020-05-27T14:58:55Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/DelayedJoinTest.scala", "diffHunk": "@@ -0,0 +1,73 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.coordinator.group\n+\n+import java.util.concurrent.{CountDownLatch, Executors, TimeUnit}\n+\n+import kafka.utils.MockTime\n+import org.easymock.EasyMock\n+import org.junit.{Assert, Test}\n+\n+class DelayedJoinTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5MzEyNzMy", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-419312732", "createdAt": "2020-05-27T15:00:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTowMDo0OFrOGbO4ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTowMDo0OFrOGbO4ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwODU0OQ==", "bodyText": "@junrao both methods are executed with lock", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431208549", "createdAt": "2020-05-27T15:00:48Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   *\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */\n+    if (group.hasAllMembersJoined) forceComplete()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5MzEzNDE5", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-419313419", "createdAt": "2020-05-27T15:01:31Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTowMTozMVrOGbO7mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTowMTozMVrOGbO7mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwOTM2OA==", "bodyText": "this is a workaround to deal with deadlock caused by taking multiples group locks", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431209368", "createdAt": "2020-05-27T15:01:31Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   *\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */\n+    if (group.hasAllMembersJoined) forceComplete()\n+    else false\n+  } finally completeDelayedRequests()\n+  override def onExpiration(): Unit = coordinator.onExpireJoin()\n+  override def onComplete(): Unit = try partitionsToComplete = coordinator.onCompleteJoin(group)\n+  finally completeDelayedRequests()\n+\n+  /**\n+   * try to complete delayed requests only if the caller does not hold the group lock.\n+   * This method is called by following cases:\n+   * 1) tryComplete -> hold lock -> onComplete -> release lock -> completeDelayedRequests\n+   * 2) onComplete -> completeDelayedRequests\n+   */\n+  private[group] def completeDelayedRequests(): Unit = if (!group.lock.isHeldByCurrentThread) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5MzE0MzM0", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-419314334", "createdAt": "2020-05-27T15:02:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTowMjoyOFrOGbPAAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTowMjoyOFrOGbPAAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxMDQ5Nw==", "bodyText": "this enum type is more readable than boolean", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431210497", "createdAt": "2020-05-27T15:02:28Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -65,13 +65,24 @@ import scala.compat.java8.OptionConverters._\n /*\n  * Result metadata of a log append operation on the log\n  */\n-case class LogAppendResult(info: LogAppendInfo, exception: Option[Throwable] = None) {\n+case class LogAppendResult(info: LogAppendInfo,\n+                           exception: Option[Throwable] = None,\n+                           leaderHWChange: LeaderHWChange = LeaderHWChange.None) {\n   def error: Errors = exception match {\n     case None => Errors.NONE\n     case Some(e) => Errors.forException(e)\n   }\n }\n \n+/**\n+ * a flag indicting whether the HWM has been changed.\n+ */\n+sealed trait LeaderHWChange", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5OTA3OTA1", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-429907905", "createdAt": "2020-06-12T17:12:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzoxMjowOFrOGjLnuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQyMzozNDo1MVrOGjgFOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MzczNw==", "bodyText": "It's cleaner to not pass in completeDelayedRequests here and let the caller (ReplicaManager.appendRecords()) check and complete purgatory instead.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439543737", "createdAt": "2020-06-12T17:12:08Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -967,7 +967,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests It may requires a bunch of group locks when completing delayed requests so it may", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0OTEzNg==", "bodyText": "Could we use map {case (tp, appendResult) => ...} here to avoid using unamed references?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439549136", "createdAt": "2020-06-12T17:23:34Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -603,6 +650,9 @@ class ReplicaManager(val config: KafkaConfig,\n         val produceResponseStatus = produceStatus.map { case (k, status) => k -> status.responseStatus }\n         responseCallback(produceResponseStatus)\n       }\n+      localProduceResults\n+        .filter { case (_, logAppendResult) => logAppendResult.exception.isEmpty}\n+        .map(e => e._1 -> e._2.leaderHWChange)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3Mjc0Nw==", "bodyText": "Another way that doesn't require checking lock.isHeldByCurrentThread is the following. But your approach seems simpler.\nOverride forceComplete() to\noverride def forceComplete() {\n    if (completed.compareAndSet(false, true)) {\n      // cancel the timeout timer\n      cancel()\n      partitionsToComplete  = coordinator.onCompleteJoin(group)\n      onComplete()\n      true\n    } else {\n      false\n    }\n}\n\nIn onComplete(), do nothing.\nIn tryComplete(), do\noverride def tryComplete() {\n  group.inLock {\n    if (group.hasAllMembersJoined) \n      isForceComplete = forceComplete()\n  }\n  completeDelayedRequests(partitionsToComplete)\n  isForceComplete\n}\n\nIn onExpiration(),\noverride def onExpiration() {\n  completeDelayedRequests(partitionsToComplete)\n}", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439872747", "createdAt": "2020-06-14T22:15:38Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   *\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */\n+    if (group.hasAllMembersJoined) forceComplete()\n+    else false\n+  } finally completeDelayedRequests()\n+  override def onExpiration(): Unit = coordinator.onExpireJoin()\n+  override def onComplete(): Unit = try partitionsToComplete = coordinator.onCompleteJoin(group)\n+  finally completeDelayedRequests()\n+\n+  /**\n+   * try to complete delayed requests only if the caller does not hold the group lock.\n+   * This method is called by following cases:\n+   * 1) tryComplete -> hold lock -> onComplete -> release lock -> completeDelayedRequests\n+   * 2) onComplete -> completeDelayedRequests\n+   */\n+  private[group] def completeDelayedRequests(): Unit = if (!group.lock.isHeldByCurrentThread) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwOTM2OA=="}, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3Mjg0Ng==", "bodyText": "expire ->  onComplete -> completeDelayedRequests", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439872846", "createdAt": "2020-06-14T22:16:58Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   *\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */\n+    if (group.hasAllMembersJoined) forceComplete()\n+    else false\n+  } finally completeDelayedRequests()\n+  override def onExpiration(): Unit = coordinator.onExpireJoin()\n+  override def onComplete(): Unit = try partitionsToComplete = coordinator.onCompleteJoin(group)\n+  finally completeDelayedRequests()\n+\n+  /**\n+   * try to complete delayed requests only if the caller does not hold the group lock.\n+   * This method is called by following cases:\n+   * 1) tryComplete -> hold lock -> onComplete -> release lock -> completeDelayedRequests\n+   * 2) onComplete -> completeDelayedRequests", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3MzE3NQ==", "bodyText": "DelyaedOperation.lockOpt defaults to None. So, we don't have to specify it explicitly.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439873175", "createdAt": "2020-06-14T22:21:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3MzQ0MQ==", "bodyText": "\"GroupCoordinator#onCompleteJoin() tries to complete delayed requests\" => since the completion of the delayed request for partitions returned from GroupCoordinator#onCompleteJoin() need to be done outside of the group lock.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439873441", "createdAt": "2020-06-14T22:25:23Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NTgwMQ==", "bodyText": "typo whihc", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439875801", "createdAt": "2020-06-14T22:54:00Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1118,33 +1170,38 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  /**\n+   * remove the pending member and then return the group key whihc is in PreparingRebalance,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NTg5Mg==", "bodyText": "The caller no longer passed in completeDelayedRequests.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439875892", "createdAt": "2020-06-14T22:55:07Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1118,33 +1170,38 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  /**\n+   * remove the pending member and then return the group key whihc is in PreparingRebalance,\n+   * @param group group\n+   * @param memberId member id\n+   * @return group key if it is in PreparingRebalance. Otherwise, None\n+   */\n+  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Option[GroupKey] = {\n     group.removePendingMember(memberId)\n \n-    if (group.is(PreparingRebalance)) {\n-      joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n-    }\n-  }\n-\n-  def tryCompleteJoin(group: GroupMetadata, forceComplete: () => Boolean) = {\n-    group.inLock {\n-      if (group.hasAllMembersJoined)\n-        forceComplete()\n-      else false\n-    }\n+    if (group.is(PreparingRebalance)) Some(GroupKey(group.groupId))\n+    else None\n   }\n \n   def onExpireJoin(): Unit = {\n     // TODO: add metrics for restabilize timeouts\n   }\n \n-  def onCompleteJoin(group: GroupMetadata): Unit = {\n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 288}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjIxNQ==", "bodyText": "All callers pass in completeDelayedRequests as false. Could we remove this param?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876215", "createdAt": "2020-06-14T22:59:03Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -311,37 +317,47 @@ class GroupMetadataManager(brokerId: Int,\n \n           responseCallback(responseError)\n         }\n-        appendForGroup(group, groupMetadataRecords, putCacheCallback)\n-\n+        appendForGroup(group, groupMetadataRecords, putCacheCallback, completeDelayedRequests)\n       case None =>\n         responseCallback(Errors.NOT_COORDINATOR)\n-        None\n+        Map.empty\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete\n+   *         delayed requests for those returned partitions.\n+   */\n   private def appendForGroup(group: GroupMetadata,\n                              records: Map[TopicPartition, MemoryRecords],\n-                             callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {\n+                             callback: Map[TopicPartition, PartitionResponse] => Unit,\n+                             completeDelayedRequests: Boolean): Map[TopicPartition, LeaderHWChange] = {\n     // call replica manager to append the group message\n     replicaManager.appendRecords(\n       timeout = config.offsetCommitTimeoutMs.toLong,\n       requiredAcks = config.offsetCommitRequiredAcks,\n       internalTopicsAllowed = true,\n       origin = AppendOrigin.Coordinator,\n+      completeDelayedRequests = completeDelayedRequests,\n       entriesPerPartition = records,\n       delayedProduceLock = Some(group.lock),\n       responseCallback = callback)\n   }\n \n   /**\n    * Store offsets by appending it to the replicated log and then inserting to cache\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete\n+   *         delayed requests for those returned partitions.\n    */\n   def storeOffsets(group: GroupMetadata,\n                    consumerId: String,\n                    offsetMetadata: immutable.Map[TopicPartition, OffsetAndMetadata],\n                    responseCallback: immutable.Map[TopicPartition, Errors] => Unit,\n+                   completeDelayedRequests: Boolean,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjM5Mg==", "bodyText": "There was => There were", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876392", "createdAt": "2020-06-14T23:01:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,24 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjQ5Mw==", "bodyText": "ReplicaManager.appendRecords()., => ReplicaManager.appendRecords(),", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876493", "createdAt": "2020-06-14T23:02:54Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,24 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag in ReplicaManager.appendRecords()., called completeDelayedRequests, to prevent the method", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjU1Nw==", "bodyText": "may requires => may require", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876557", "createdAt": "2020-06-14T23:04:08Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -967,7 +967,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests It may requires a bunch of group locks when completing delayed requests so it may", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ==", "bodyText": "Hmm, why do we need this logic now?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439878341", "createdAt": "2020-06-14T23:27:03Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -307,8 +307,14 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     override def runWithCallback(member: GroupMember, responseCallback: CompleteTxnCallback): Unit = {\n       val producerId = 1000L\n       val offsetsPartitions = (0 to numPartitions).map(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, _))\n-      groupCoordinator.groupManager.handleTxnCompletion(producerId,\n-        offsetsPartitions.map(_.partition).toSet, isCommit = random.nextBoolean)\n+      val isCommit = random.nextBoolean\n+      try groupCoordinator.groupManager.handleTxnCompletion(producerId,\n+        offsetsPartitions.map(_.partition).toSet, isCommit = isCommit)\n+      catch {\n+        case e: IllegalStateException if isCommit\n+          && e.getMessage.contains(\"though the offset commit record itself hasn't been appended to the log\")=>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODczNg==", "bodyText": "Hmm, why do we need to mock this since replicaManager.getMagic() is only called through replicaManager.handleWriteTxnMarkersRequest()?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439878736", "createdAt": "2020-06-14T23:31:46Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala", "diffHunk": "@@ -536,6 +537,11 @@ class GroupCoordinatorTest {\n     // Make sure the NewMemberTimeout is not still in effect, and the member is not kicked\n     assertEquals(1, group.size)\n \n+    // prepare the mock replica manager again since the delayed join is going to complete\n+    EasyMock.reset(replicaManager)\n+    EasyMock.expect(replicaManager.getMagic(EasyMock.anyObject())).andReturn(Some(RecordBatch.MAGIC_VALUE_V1)).anyTimes()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODk2OQ==", "bodyText": "Hmm, this should only be called with LeaderHWChange.LeaderHWIncremented, but the mock later returns LeaderHWChange.None? Ditto below.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439878969", "createdAt": "2020-06-14T23:34:51Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala", "diffHunk": "@@ -3921,22 +3934,26 @@ class GroupCoordinatorTest {\n     val (responseFuture, responseCallback) = setupCommitOffsetsCallback\n \n     val capturedArgument: Capture[scala.collection.Map[TopicPartition, PartitionResponse] => Unit] = EasyMock.newCapture()\n-\n-    EasyMock.expect(replicaManager.appendRecords(EasyMock.anyLong(),\n-      EasyMock.anyShort(),\n+    EasyMock.expect(replicaManager.completeDelayedRequests(EasyMock.anyObject()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 95}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMDc3MzYz", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-431077363", "createdAt": "2020-06-16T00:21:38Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxOTMzOTU2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-431933956", "createdAt": "2020-06-16T22:23:40Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMjoyMzo0MFrOGkvO_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMjo1NDozNlrOGkv4nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NTgwNg==", "bodyText": "typo rebalacne", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441175806", "createdAt": "2020-06-16T22:23:40Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1108,7 +1157,11 @@ class GroupCoordinator(val brokerId: Int,\n     joinPurgatory.tryCompleteElseWatch(delayedRebalance, Seq(groupKey))\n   }\n \n-  private def removeMemberAndUpdateGroup(group: GroupMetadata, member: MemberMetadata, reason: String): Unit = {\n+  /**\n+   * @return the group which is preparing to rebalacne. Callers should use this group key to complete delayed requests", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NzA1NQ==", "bodyText": "the delayed requests may be completed as much as possible =>  the delayed requests may be completed inside the call with the expectation that no conflicting locks are held by the caller", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441177055", "createdAt": "2020-06-16T22:27:14Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,27 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There were a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag \"completeDelayedRequests\" to ReplicaManager.appendRecords() (and related methods).\n+   *\n+   * 1) If \"completeDelayedRequests\" is true, the delayed requests may be completed as much as possible.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NzM1MQ==", "bodyText": "Callers can complete the delayed requests manually => Callers can complete the delayed requests after releasing any conflicting lock.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441177351", "createdAt": "2020-06-16T22:28:07Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,27 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There were a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag \"completeDelayedRequests\" to ReplicaManager.appendRecords() (and related methods).\n+   *\n+   * 1) If \"completeDelayedRequests\" is true, the delayed requests may be completed as much as possible.\n+   * 2) If \"completeDelayedRequests\" is false, the delayed requests are NOT completed and the watch key (for example,\n+   *    group key) are returned. Callers can complete the delayed requests manually.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3ODQxOA==", "bodyText": "\"If the caller no longer passed in completeDelayedRequests\" The caller still passes this in, just as false.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441178418", "createdAt": "2020-06-16T22:31:00Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -560,19 +603,23 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller no longer passed in completeDelayedRequests, the caller is expected to complete", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4MDY2Nw==", "bodyText": "\" If the caller no longer passed in completeDelayedRequests\" => There is no completeDelayedRequests passed in.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441180667", "createdAt": "2020-06-16T22:37:19Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1118,33 +1171,38 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  /**\n+   * remove the pending member and then return the group key which is in PreparingRebalance,\n+   * @param group group\n+   * @param memberId member id\n+   * @return group key if it is in PreparingRebalance. Otherwise, None\n+   */\n+  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Option[GroupKey] = {\n     group.removePendingMember(memberId)\n \n-    if (group.is(PreparingRebalance)) {\n-      joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n-    }\n-  }\n-\n-  def tryCompleteJoin(group: GroupMetadata, forceComplete: () => Boolean) = {\n-    group.inLock {\n-      if (group.hasAllMembersJoined)\n-        forceComplete()\n-      else false\n-    }\n+    if (group.is(PreparingRebalance)) Some(GroupKey(group.groupId))\n+    else None\n   }\n \n   def onExpireJoin(): Unit = {\n     // TODO: add metrics for restabilize timeouts\n   }\n \n-  def onCompleteJoin(group: GroupMetadata): Unit = {\n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller no longer passed in completeDelayedRequests, the caller is expected to complete", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4MjQ1MA==", "bodyText": "to to  => to", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441182450", "createdAt": "2020-06-16T22:42:33Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -239,9 +239,13 @@ class GroupMetadataManager(brokerId: Int,\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. The caller ought to to complete delayed requests for those returned partitions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4NjQ2Mw==", "bodyText": "Thanks for the great explanation. I understand the issue now. Essentially, this exposed a limitation of the existing test. The existing test happens to work because the producer callbacks are always completed in the same ReplicaManager.appendRecords() call under the group lock. However, this is not necessarily the general case.\nYour fix works, but may hide other real problems. I was thinking that another way to fix this is to change the test a bit. For example, we expect CompleteTxnOperation to happen after CommitTxnOffsetsOperation. So, instead of letting them run in parallel, we can change the test to make sure that CompleteTxnOperation only runs after CommitTxnOffsetsOperation completes successfully. JoinGroupOperation and SyncGroupOperation might need a similar consideration.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441186463", "createdAt": "2020-06-16T22:54:36Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -307,8 +307,14 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     override def runWithCallback(member: GroupMember, responseCallback: CompleteTxnCallback): Unit = {\n       val producerId = 1000L\n       val offsetsPartitions = (0 to numPartitions).map(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, _))\n-      groupCoordinator.groupManager.handleTxnCompletion(producerId,\n-        offsetsPartitions.map(_.partition).toSet, isCommit = random.nextBoolean)\n+      val isCommit = random.nextBoolean\n+      try groupCoordinator.groupManager.handleTxnCompletion(producerId,\n+        offsetsPartitions.map(_.partition).toSet, isCommit = isCommit)\n+      catch {\n+        case e: IllegalStateException if isCommit\n+          && e.getMessage.contains(\"though the offset commit record itself hasn't been appended to the log\")=>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ=="}, "originalCommit": null, "originalPosition": 11}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyNzkxNDU2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-432791456", "createdAt": "2020-06-17T21:31:23Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMTozMToyM1rOGlYI9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMjoxMDoxNFrOGlZHfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0NjAwNQ==", "bodyText": "if there is no completeDelayedRequests passed in => if completeDelayedRequests is false", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441846005", "createdAt": "2020-06-17T21:31:23Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -967,7 +967,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests true: the delayed requests may be completed inside the call with the expectation\n+   *                                that no conflicting locks are held by the caller. Otherwise, the caller is expected\n+   *                                to complete delayed requests for those returned partitions if there is no\n+   *                                completeDelayedRequests passed in.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0NjgxOQ==", "bodyText": "if there is no completeDelayedRequests passed in => if completeDelayedRequests is false", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441846819", "createdAt": "2020-06-17T21:33:16Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -560,19 +603,27 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * @param completeDelayedRequests true: the delayed requests may be completed inside the call with the expectation\n+   *                                that no conflicting locks are held by the caller. Otherwise, the caller is expected\n+   *                                to complete delayed requests for those returned partitions if there is no\n+   *                                completeDelayedRequests passed in.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg1OTExMA==", "bodyText": "I think the intention for the test is probably to use the same producerId since it tests more on transactional conflicts.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441859110", "createdAt": "2020-06-17T22:02:38Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -106,25 +97,56 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  def createGroupMembers(groupPrefix: String): Set[GroupMember] = {\n-    (0 until nGroups).flatMap { i =>\n-      new Group(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator, replicaManager).members\n-    }.toSet\n+  private var producerIdCount = 0L\n+  private val allGroupMembers = mutable.ArrayBuffer[GroupMember]()\n+\n+  def groupMembers(groupId: String, nMembers: Int, groupCoordinator: GroupCoordinator): Seq[GroupMember] = {\n+    val groupPartitionId = groupCoordinator.partitionFor(groupId)\n+    groupCoordinator.groupManager.addPartitionOwnership(groupPartitionId)\n+    val lock = new ReentrantLock()\n+    val producerId = producerIdCount\n+    producerIdCount += 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg1OTgyNA==", "bodyText": "Hmm, why don't we need the lock here since CommitTxnOffsetsOperation and CompleteTxnOperation could still run in parallel?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441859824", "createdAt": "2020-06-17T22:04:30Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -106,25 +97,56 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  def createGroupMembers(groupPrefix: String): Set[GroupMember] = {\n-    (0 until nGroups).flatMap { i =>\n-      new Group(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator, replicaManager).members\n-    }.toSet\n+  private var producerIdCount = 0L\n+  private val allGroupMembers = mutable.ArrayBuffer[GroupMember]()\n+\n+  def groupMembers(groupId: String, nMembers: Int, groupCoordinator: GroupCoordinator): Seq[GroupMember] = {\n+    val groupPartitionId = groupCoordinator.partitionFor(groupId)\n+    groupCoordinator.groupManager.addPartitionOwnership(groupPartitionId)\n+    val lock = new ReentrantLock()\n+    val producerId = producerIdCount\n+    producerIdCount += 1\n+    val members = (0 until nMembers).map(i => new GroupMember(groupId = groupId,\n+      groupPartitionId = groupPartitionId,\n+      leader = i == 0,\n+      producerId = producerId,\n+      txnLock = lock))\n+    allGroupMembers ++= members\n+    members\n   }\n \n+  def createGroupMembers(groupPrefix: String): Set[GroupMember] =\n+    (0 until nGroups).flatMap(i => groupMembers(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator)).toSet\n+\n   @Test\n   def testConcurrentGoodPathSequence(): Unit = {\n     verifyConcurrentOperations(createGroupMembers, allOperations)\n   }\n \n   @Test\n   def testConcurrentTxnGoodPathSequence(): Unit = {\n-    verifyConcurrentOperations(createGroupMembers, allOperationsWithTxn)\n+    verifyConcurrentOperations(createGroupMembers, Seq(\n+      new JoinGroupOperation,\n+      new SyncGroupOperation,\n+      new OffsetFetchOperation,\n+      new CommitTxnOffsetsOperation(needLock = false),\n+      new CompleteTxnOperation(needLock = false),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg2MjAxMw==", "bodyText": "Perhaps change the comment to sth like the following?\n\"Setting to true to make CompleteTxnOperation and CommitTxnOffsetsOperation complete atomically since they don't typically overlap. Otherwise CompleteTxnOperation may see a pending offsetAndMetadata without an appendedBatchOffset.\"", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441862013", "createdAt": "2020-06-17T22:10:14Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -278,37 +302,48 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  class CommitTxnOffsetsOperation extends CommitOffsetsOperation {\n+  /**\n+   * @param needLock true to make CompleteTxnOperation happen after CommitTxnOffsetsOperation. It presents error when\n+   *                 both CommitTxnOffsetsOperation and CompleteTxnOperation are executed on parallel.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 118}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNDIyOTkx", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-433422991", "createdAt": "2020-06-18T15:53:29Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo1MzozMFrOGl1t7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo1MzozMFrOGl1t7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMDYwNg==", "bodyText": "Since createGroupMembers() is called in multiple tests, it seems we will be accumulating allGroupMembers across tests. That seems unexpected?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r442330606", "createdAt": "2020-06-18T15:53:30Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -106,12 +107,29 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  def createGroupMembers(groupPrefix: String): Set[GroupMember] = {\n-    (0 until nGroups).flatMap { i =>\n-      new Group(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator, replicaManager).members\n-    }.toSet\n+  /**\n+   * make CompleteTxnOperation and CommitTxnOffsetsOperation complete atomically since they don't typically overlap.\n+   * Otherwise CompleteTxnOperation may see a pending offsetAndMetadata without an appendedBatchOffset.\n+   */\n+  private val txnLock = new ReentrantLock\n+  private val allGroupMembers = mutable.ArrayBuffer[GroupMember]()\n+\n+  def groupMembers(groupId: String, nMembers: Int, groupCoordinator: GroupCoordinator): Seq[GroupMember] = {\n+    val groupPartitionId = groupCoordinator.partitionFor(groupId)\n+    groupCoordinator.groupManager.addPartitionOwnership(groupPartitionId)\n+    val members = (0 until nMembers).map(i => new GroupMember(groupId = groupId,\n+      groupPartitionId = groupPartitionId,\n+      leader = i == 0,\n+      // same producerId to tests more on transactional conflicts.\n+      producerId = 1000,\n+      txnLock = txnLock))\n+    allGroupMembers ++= members", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNTY0NjA5", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-433564609", "createdAt": "2020-06-18T19:00:14Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzMzY2NTU2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-443366556", "createdAt": "2020-07-06T19:55:53Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2MTM1Nzg0", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-446135784", "createdAt": "2020-07-10T05:48:24Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwNTo0ODoyNFrOGvqnlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwNTo1MDozOFrOGvqp5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYzNDUxNg==", "bodyText": "It's weird to have a method that invokes a callback and returns a result. Do we need both? We have a number of other methods that do something similar. It would be good to reconsider that as it's difficult to reason about usage in such cases.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r452634516", "createdAt": "2020-07-10T05:48:24Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -239,9 +239,13 @@ class GroupMetadataManager(brokerId: Int,\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. The caller ought to complete delayed requests for those returned partitions.\n+   */\n   def storeGroup(group: GroupMetadata,\n                  groupAssignment: Map[String, Array[Byte]],\n-                 responseCallback: Errors => Unit): Unit = {\n+                 responseCallback: Errors => Unit): Map[TopicPartition, LeaderHWChange] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYzNTExMQ==", "bodyText": "The usual naming convention is to only capitalize the first letter, eg LeaderHwChange.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r452635111", "createdAt": "2020-07-10T05:50:38Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -65,13 +65,24 @@ import scala.compat.java8.OptionConverters._\n /*\n  * Result metadata of a log append operation on the log\n  */\n-case class LogAppendResult(info: LogAppendInfo, exception: Option[Throwable] = None) {\n+case class LogAppendResult(info: LogAppendInfo,\n+                           exception: Option[Throwable] = None,\n+                           leaderHWChange: LeaderHWChange = LeaderHWChange.None) {\n   def error: Errors = exception match {\n     case None => Errors.NONE\n     case Some(e) => Errors.forException(e)\n   }\n }\n \n+/**\n+ * a flag indicting whether the HWM has been changed.\n+ */\n+sealed trait LeaderHWChange\n+object LeaderHWChange {\n+  case object LeaderHWIncremented extends LeaderHWChange", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4NDYzMDMz", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-458463033", "createdAt": "2020-07-30T14:35:33Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDozNTozM1rOG5l54A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDozNzoxOVrOG5l_Ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0MzA0MA==", "bodyText": "I notice that we are including the $ here and in a few other places, we should not do that.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r463043040", "createdAt": "2020-07-30T14:35:33Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions a map contains the partition and a flag indicting whether the HWM has been changed\n+   */\n+  private[group] def completeDelayedRequests(topicPartitions: Map[TopicPartition, LeaderHwChange]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) => leaderHWIncremented match {\n+        case LeaderHwIncremented$ => groupManager.replicaManager.completeDelayedRequests(tp)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDM3MQ==", "bodyText": "I raised the point before that it's a bit unusual and unintuitive to have both a callback and a return value. Any thoughts on this?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r463044371", "createdAt": "2020-07-30T14:37:19Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -239,9 +239,13 @@ class GroupMetadataManager(brokerId: Int,\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. The caller ought to complete delayed requests for those returned partitions.\n+   */\n   def storeGroup(group: GroupMetadata,\n                  groupAssignment: Map[String, Array[Byte]],\n-                 responseCallback: Errors => Unit): Unit = {\n+                 responseCallback: Errors => Unit): Map[TopicPartition, LeaderHwChange] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwMjE4OTYz", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-460218963", "createdAt": "2020-08-03T17:39:30Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzozOTozMFrOG7CrHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMzozMzoxN1rOG7L-cA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2Mjk3Mg==", "bodyText": "Could we change the explanation to sth like the following?\nThis method may trigger the completeness check for delayed requests in a few purgatories. Occasionally, for serialization in the log, a caller may need to hold a lock while calling this method. To avoid deadlock, if the caller holds a conflicting lock while calling this method, the caller is expected to set completeDelayedRequests to false to avoid checking the delayed operations during this call. The caller will then explicitly complete those delayed operations based on the return value, without holding the conflicting lock.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464562972", "createdAt": "2020-08-03T17:39:30Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -554,19 +596,27 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * @param completeDelayedRequests true: the delayed requests may be completed inside the call with the expectation\n+   *                                that no conflicting locks are held by the caller. Otherwise, the caller is expected\n+   *                                to complete delayed requests for those returned partitions if completeDelayedRequests\n+   *                                is false", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzIxOQ==", "bodyText": "group lock => conflicting lock", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464567219", "createdAt": "2020-08-03T17:45:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -334,6 +345,37 @@ class ReplicaManager(val config: KafkaConfig,\n       brokerTopicStats.removeMetrics(topic)\n   }\n \n+  /**\n+   * try to complete delayed requests in following purgatories.\n+   * 1) delayedFetchPurgatory\n+   * 2) delayedProducePurgatory\n+   * 3) delayedDeleteRecordsPurgatory\n+   *\n+   * Noted that caller should NOT hold any group lock in order to avoid deadlock.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzI2Nw==", "bodyText": "group lock => conflicting lock", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464567267", "createdAt": "2020-08-03T17:45:24Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -334,6 +345,37 @@ class ReplicaManager(val config: KafkaConfig,\n       brokerTopicStats.removeMetrics(topic)\n   }\n \n+  /**\n+   * try to complete delayed requests in following purgatories.\n+   * 1) delayedFetchPurgatory\n+   * 2) delayedProducePurgatory\n+   * 3) delayedDeleteRecordsPurgatory\n+   *\n+   * Noted that caller should NOT hold any group lock in order to avoid deadlock.\n+   *\n+   * this method is visible for testing.\n+   */\n+  def completeDelayedRequests(topicPartition: TopicPartition): Unit = {\n+    val topicPartitionOperationKey = TopicPartitionOperationKey(topicPartition)\n+    delayedFetchPurgatory.checkAndComplete(topicPartitionOperationKey)\n+    delayedProducePurgatory.checkAndComplete(topicPartitionOperationKey)\n+    delayedDeleteRecordsPurgatory.checkAndComplete(topicPartitionOperationKey)\n+  }\n+\n+  /**\n+   * try to complete delayed requests in delayedFetchPurgatory.\n+   *\n+   * Noted that caller should NOT hold any group lock in order to avoid deadlock.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3NDgzNw==", "bodyText": "Perhaps add \"but completes the delayed requests without holding the group lock\".", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464574837", "createdAt": "2020-08-03T18:00:13Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHwChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since the completion of the delayed request for partitions returned from GroupCoordinator#onCompleteJoin()\n+   * need to be done outside of the group lock.\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MDM4OA==", "bodyText": "a lot of group lock => multiple group locks", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464580388", "createdAt": "2020-08-03T18:11:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MDg5MQ==", "bodyText": "\"this method may hold a lot of group lock\" : This is actually not true. Unlike producer/fetch purgatory, which is keyed on partition, joinPurgatory is keyed on the group. So, when we complete a key, only a single group's lock will be held.\nThe reason that we don't want the caller to hold a group lock is that DelayedJoin itself uses a lock other than the group lock for DelayedOperation.maybeTryComplete() and we want to avoid the deadlock between that lock and the group lock.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464580891", "createdAt": "2020-08-03T18:12:15Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions a map contains the partition and a flag indicting whether the HWM has been changed\n+   */\n+  private[group] def completeDelayedRequests(topicPartitions: Map[TopicPartition, LeaderHwChange]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) => leaderHWIncremented match {\n+        case LeaderHwIncremented => groupManager.replicaManager.completeDelayedRequests(tp)\n+        case _ => groupManager.replicaManager.completeDelayedFetchRequests(tp)\n+      }\n+    }\n+\n+  /**\n+   * complete the delayed join requests associated to input group keys.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5Nzg0MA==", "bodyText": "This could be completeDelayedJoinRequests(groupsToComplete) ?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464697840", "createdAt": "2020-08-03T22:36:59Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1240,26 +1298,30 @@ class GroupCoordinator(val brokerId: Int,\n   }\n \n   def onExpireHeartbeat(group: GroupMetadata, memberId: String, isPending: Boolean): Unit = {\n+    val groupsToComplete = scala.collection.mutable.Set[GroupKey]()\n     group.inLock {\n       if (group.is(Dead)) {\n         info(s\"Received notification of heartbeat expiration for member $memberId after group ${group.groupId} had already been unloaded or deleted.\")\n       } else if (isPending) {\n         info(s\"Pending member $memberId in group ${group.groupId} has been removed after session timeout expiration.\")\n-        removePendingMemberAndUpdateGroup(group, memberId)\n+        groupsToComplete ++= removePendingMemberAndUpdateGroup(group, memberId)\n       } else if (!group.has(memberId)) {\n         debug(s\"Member $memberId has already been removed from the group.\")\n       } else {\n         val member = group.get(memberId)\n         if (!member.hasSatisfiedHeartbeat) {\n           info(s\"Member ${member.memberId} in group ${group.groupId} has failed, removing it from the group\")\n-          removeMemberAndUpdateGroup(group, member, s\"removing member ${member.memberId} on heartbeat expiration\")\n+          groupsToComplete ++= removeMemberAndUpdateGroup(group, member, s\"removing member ${member.memberId} on heartbeat expiration\")\n         }\n       }\n     }\n+    groupsToComplete.foreach(joinPurgatory.checkAndComplete)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 329}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNTM3Ng==", "bodyText": "Perhaps we could add a comment on what this method is intended to test?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464715376", "createdAt": "2020-08-03T23:33:17Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/DelayedJoinTest.scala", "diffHunk": "@@ -0,0 +1,73 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.coordinator.group\n+\n+import java.util.concurrent.{CountDownLatch, Executors, TimeUnit}\n+\n+import kafka.utils.MockTime\n+import org.easymock.EasyMock\n+import org.junit.{Assert, Test}\n+\n+class DelayedJoinTest {\n+\n+  @Test\n+  def testCompleteDelayedRequests(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwOTc5Njky", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-460979692", "createdAt": "2020-08-04T16:31:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNjozMTo1MFrOG7oSxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNjozMTo1MFrOG7oSxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3OTMzNQ==", "bodyText": "Hmm, it seems that we are now introducing a new potential deadlock. The conflicting paths are the following.\npath 1\nhold group lock -> joinPurgatory.tryCompleteElseWatch(delayedJoin) -> watchForOperation (now delayedJoin visible through other threads) -> operation.maybeTryComplete() -> hold delayedJoin.lock\npath 2\ndelayedJoin.maybeTryComplete -> hold hold delayedJoin.lock -> tryComplete() -> hold group lock", "url": "https://github.com/apache/kafka/pull/8657#discussion_r465179335", "createdAt": "2020-08-04T16:31:50Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold multiple group locks so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions a map contains the partition and a flag indicting whether the HWM has been changed\n+   */\n+  private[group] def completeDelayedRequests(topicPartitions: Map[TopicPartition, LeaderHwChange]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) => leaderHWIncremented match {\n+        case LeaderHwIncremented => groupManager.replicaManager.completeDelayedRequests(tp)\n+        case _ => groupManager.replicaManager.completeDelayedFetchRequests(tp)\n+      }\n+    }\n+\n+  /**\n+   * complete the delayed join requests associated to input group keys.\n+   *\n+   * Noted that delayedJoin itself uses a lock other than the group lock for DelayedOperation.maybeTryComplete() and", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2OTU5MTMw", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-476959130", "createdAt": "2020-08-27T18:01:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxODowMTozNVrOHIbbKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxODowOTozM1rOHIbsBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5OTk3OQ==", "bodyText": "Unneeded new line.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478599979", "createdAt": "2020-08-27T18:01:35Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -585,6 +597,27 @@ class ReplicaManager(val config: KafkaConfig,\n                     result.info.logStartOffset, result.info.recordErrors.asJava, result.info.errorMessage)) // response status\n       }\n \n+      delayedActions.put {\n+        () =>\n+          localProduceResults.foreach {\n+            case (topicPartition, result) =>\n+              result.info.leaderHWIncremented.foreach {\n+                incremented =>\n+                  val requestKey = TopicPartitionOperationKey(topicPartition)\n+                  if (incremented) {\n+                    // some delayed operations may be unblocked after HW changed\n+                    delayedProducePurgatory.checkAndComplete(requestKey)\n+                    delayedFetchPurgatory.checkAndComplete(requestKey)\n+                    delayedDeleteRecordsPurgatory.checkAndComplete(requestKey)\n+                  } else {\n+                    // probably unblock some follower fetch requests since log end offset has been updated\n+                    delayedFetchPurgatory.checkAndComplete(requestKey)\n+                  }\n+              }\n+          }\n+      }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMDUwNw==", "bodyText": "We probably want to add a comment why this is needed.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478600507", "createdAt": "2020-08-27T18:02:36Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -36,8 +36,13 @@ private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n \n   override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n+  override def onExpiration() = {\n+    coordinator.onExpireJoin()\n+    tryToCompleteDelayedAction()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMzI0Mw==", "bodyText": "Hmm, why do we need to override this instead of using the one defined in DelayedJoin?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478603243", "createdAt": "2020-08-27T18:07:40Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -57,6 +62,8 @@ private[group] class InitialDelayedJoin(coordinator: GroupCoordinator,\n \n   override def tryComplete(): Boolean = false\n \n+  override def onExpiration(): Unit = tryToCompleteDelayedAction()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMzc3Ng==", "bodyText": "This probably should be included in the local time as before.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478603776", "createdAt": "2020-08-27T18:08:37Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -187,6 +187,8 @@ class KafkaApis(val requestChannel: RequestChannel,\n       // The local completion time may be set while processing the request. Only record it if it's unset.\n       if (request.apiLocalCompleteTimeNanos < 0)\n         request.apiLocalCompleteTimeNanos = time.nanoseconds\n+\n+      replicaManager.tryCompleteDelayedAction()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwNDI5Mg==", "bodyText": "Could we add the new param to the javadoc?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478604292", "createdAt": "2020-08-27T18:09:33Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -100,7 +100,8 @@ case class LogAppendInfo(var firstOffset: Option[Long],\n                          offsetsMonotonic: Boolean,\n                          lastOffsetOfFirstBatch: Long,\n                          recordErrors: Seq[RecordError] = List(),\n-                         errorMessage: String = null) {\n+                         errorMessage: String = null,\n+                         leaderHWIncremented: Option[Boolean] = None) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3ODI2MDQ1", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-477826045", "createdAt": "2020-08-28T15:31:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNTozMTowOFrOHJLA-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNTo0OTo0N1rOHJLpaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM3OTcwNQ==", "bodyText": "This can just be private.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479379705", "createdAt": "2020-08-28T15:31:08Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -36,8 +36,14 @@ private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n \n   override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  override def onExpiration(): Unit = {\n+    coordinator.onExpireJoin()\n+    // try to complete delayed actions introduced by coordinator.onCompleteJoin\n+    tryToCompleteDelayedAction()\n+  }\n+  override def onComplete(): Unit = coordinator.onCompleteJoin(group)\n+\n+  protected def tryToCompleteDelayedAction(): Unit = coordinator.groupManager.replicaManager.tryCompleteDelayedAction()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4MTk2Mw==", "bodyText": "KafkaApis.handle() => KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479381963", "createdAt": "2020-08-28T15:34:58Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -110,31 +109,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n    * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n    * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n    * the operation is actually completed.\n+   *\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() picks up and then execute an action when no lock is held.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4MjE2Nw==", "bodyText": "The above comment is outdated now.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479382167", "createdAt": "2020-08-28T15:35:19Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -110,31 +109,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n    * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n    * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n    * the operation is actually completed.\n+   *", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4Mjk2Mg==", "bodyText": "We probably should rename this to sth like safeTryComplete().", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479382962", "createdAt": "2020-08-28T15:36:41Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -110,31 +109,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n    * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n    * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n    * the operation is actually completed.\n+   *\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() picks up and then execute an action when no lock is held.\n    */\n-  private[server] def maybeTryComplete(): Boolean = {\n-    var retry = false\n-    var done = false\n-    do {\n-      if (lock.tryLock()) {\n-        try {\n-          tryCompletePending.set(false)\n-          done = tryComplete()\n-        } finally {\n-          lock.unlock()\n-        }\n-        // While we were holding the lock, another thread may have invoked `maybeTryComplete` and set\n-        // `tryCompletePending`. In this case we should retry.\n-        retry = tryCompletePending.get()\n-      } else {\n-        // Another thread is holding the lock. If `tryCompletePending` is already set and this thread failed to\n-        // acquire the lock, then the thread that is holding the lock is guaranteed to see the flag and retry.\n-        // Otherwise, we should set the flag and retry on this thread since the thread holding the lock may have\n-        // released the lock and returned by the time the flag is set.\n-        retry = !tryCompletePending.getAndSet(true)\n-      }\n-    } while (!isCompleted && retry)\n-    done\n-  }\n+  private[server] def maybeTryComplete(): Boolean = inLock(lock)(tryComplete())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4MzgwNw==", "bodyText": "The default value is None.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479383807", "createdAt": "2020-08-28T15:38:11Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +85,8 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWIncremented true if the high watermark is increased when appending record. Otherwise, false.\n+ *                            this field is updated after appending record so it has default value option.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4OTYxNw==", "bodyText": "at the end of KafkaApis.handle() => at the end of KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479389617", "createdAt": "2020-08-28T15:49:04Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -558,10 +558,27 @@ class ReplicaManager(val config: KafkaConfig,\n     localLog(topicPartition).map(_.parentDir)\n   }\n \n+  // visible for testing\n+  val delayedActions = new LinkedBlockingQueue[() => Unit]()\n+\n+  /**\n+   * try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests\n+   * are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM5MDA1Ng==", "bodyText": "in a queue => are stored in a queue", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479390056", "createdAt": "2020-08-28T15:49:47Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -558,10 +558,27 @@ class ReplicaManager(val config: KafkaConfig,\n     localLog(topicPartition).map(_.parentDir)\n   }\n \n+  // visible for testing\n+  val delayedActions = new LinkedBlockingQueue[() => Unit]()\n+\n+  /**\n+   * try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests\n+   * are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle(),\n+   * at which point, no conflicting locks will be held.\n+   */\n+  def tryCompleteDelayedAction(): Unit = {\n+    val action = delayedActions.poll()\n+    if (action != null) action()\n+  }\n+\n   /**\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * Noted that all pending delayed check operations in a queue. All callers to ReplicaManager.appendRecords() are", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3OTM5ODI3", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-477939827", "createdAt": "2020-08-28T18:24:30Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODoyNDozMFrOHJQYHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODoyNDozMFrOHJQYHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ2NzU0OA==", "bodyText": "The last sentence doesn't complete.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479467548", "createdAt": "2020-08-28T18:24:30Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,41 +99,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3OTQ5MzIx", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-477949321", "createdAt": "2020-08-28T18:41:05Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0MTowNVrOHJQ02A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0MTowNVrOHJQ02A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDkwNA==", "bodyText": "Can we not use a boolean here? false until it's been incremented and then true. Is there value in having the third state?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479474904", "createdAt": "2020-08-28T18:41:05Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -100,7 +102,8 @@ case class LogAppendInfo(var firstOffset: Option[Long],\n                          offsetsMonotonic: Boolean,\n                          lastOffsetOfFirstBatch: Long,\n                          recordErrors: Seq[RecordError] = List(),\n-                         errorMessage: String = null) {\n+                         errorMessage: String = null,\n+                         leaderHWIncremented: Option[Boolean] = None) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3OTUwMzQz", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-477950343", "createdAt": "2020-08-28T18:42:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0Mjo0N1rOHJQ34Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0Mjo0N1rOHJQ34Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NTY4MQ==", "bodyText": "Should we be guarding against exceptions here?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479475681", "createdAt": "2020-08-28T18:42:47Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -184,6 +184,7 @@ class KafkaApis(val requestChannel: RequestChannel,\n       case e: FatalExitError => throw e\n       case e: Throwable => handleError(request, e)\n     } finally {\n+      replicaManager.tryCompleteDelayedAction()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTM2MjA0", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478136204", "createdAt": "2020-08-29T15:40:22Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxNTo0MDoyMlrOHJcNSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxNzoyMDo0NlrOHJcxxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY2MTM4Ng==", "bodyText": "a action => an action", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479661386", "createdAt": "2020-08-29T15:40:22Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * picks up a action to complete.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY2OTM1NA==", "bodyText": "Even if we hit an exception in handleXXX(), it would still be useful to complete the actionQueue.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479669354", "createdAt": "2020-08-29T17:05:19Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -180,6 +181,11 @@ class KafkaApis(val requestChannel: RequestChannel,\n         case ApiKeys.DESCRIBE_CLIENT_QUOTAS => handleDescribeClientQuotasRequest(request)\n         case ApiKeys.ALTER_CLIENT_QUOTAS => handleAlterClientQuotasRequest(request)\n       }\n+\n+      // try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests\n+      // are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle() and the\n+      // expiration thread for certain delayed operations (e.g. DelayedJoin)\n+      actionQueue.tryCompleteAction()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDA1Ng==", "bodyText": "It seems that we have to distinguish 3 states here: (1) records not appended due to an error; (2) records appended successfully and HWM advanced; (3) records appended successfully and HWM not advanced. In case (1), no purgatory needs to be checked.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479670056", "createdAt": "2020-08-29T17:13:30Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -585,6 +591,23 @@ class ReplicaManager(val config: KafkaConfig,\n                     result.info.logStartOffset, result.info.recordErrors.asJava, result.info.errorMessage)) // response status\n       }\n \n+      actionQueue.add {\n+        () =>\n+          localProduceResults.foreach {\n+            case (topicPartition, result) =>\n+              val requestKey = TopicPartitionOperationKey(topicPartition)\n+              if (result.info.leaderHWIncremented) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDM1MQ==", "bodyText": "Is this used?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479670351", "createdAt": "2020-08-29T17:16:24Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * picks up a action to complete.\n+   */\n+  def tryCompleteAction(): Unit = {\n+    val action = queue.poll()\n+    if (action != null) action()\n+  }\n+\n+  /**\n+   * @return number of actions kept by this queue\n+   */\n+  def size: Int = queue.size()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDcyNw==", "bodyText": "Perhaps we could add a note at the top of DelayedOperation so that people are aware of the need to complete actions for new DelayedOperations in the future.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479670727", "createdAt": "2020-08-29T17:20:46Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,41 +99,22 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin) pick up and then\n+   * execute an action when no lock is held.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTQ4MTc2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478148176", "createdAt": "2020-08-29T18:51:18Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxODo1MToxOFrOHJdRKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxODo1MToxOFrOHJdRKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODc2MQ==", "bodyText": "What's the reasoning for taking just 1 item @junrao? Could this cause the queue to grow over time?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479678761", "createdAt": "2020-08-29T18:51:18Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -562,6 +564,10 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * Noted that all pending delayed check operations are stored in a queue. All callers to ReplicaManager.appendRecords()\n+   * are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTQ4MjE2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478148216", "createdAt": "2020-08-29T18:52:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxODo1MjowNFrOHJdRXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxODo1MjowNFrOHJdRXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODgxMg==", "bodyText": "Another approach would be for this queue to be per request thread instead of per server. That would simplify concurrency handling.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479678812", "createdAt": "2020-08-29T18:52:04Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -134,6 +134,8 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n   private val KAFKA_CLUSTER_ID: String = \"kafka.cluster.id\"\n   private val KAFKA_BROKER_ID: String = \"kafka.broker.id\"\n \n+  private val actionQueue = new ActionQueue", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTUwNjU2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478150656", "createdAt": "2020-08-29T19:37:54Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxOTozNzo1NFrOHJdhIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxOTo1NTo0M1rOHJdnAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4Mjg0OQ==", "bodyText": "Good question. It's based on the assumption that each KafkaApis.handle() call only calls ReplicaManager. appendRecords() once. Not sure if this is always true in the future. Perhaps a safer approach is to have Action.tryCompleteAction() get the current size of the queue and complete all those actions.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479682849", "createdAt": "2020-08-29T19:37:54Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -562,6 +564,10 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * Noted that all pending delayed check operations are stored in a queue. All callers to ReplicaManager.appendRecords()\n+   * are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODc2MQ=="}, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4MzIxMg==", "bodyText": "Yes, if actionQueue.tryCompleteAction() throws an exception, we can just catch it and log a warning in finally since the response has been sent by then.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479683212", "createdAt": "2020-08-29T19:42:24Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -180,6 +181,11 @@ class KafkaApis(val requestChannel: RequestChannel,\n         case ApiKeys.DESCRIBE_CLIENT_QUOTAS => handleDescribeClientQuotasRequest(request)\n         case ApiKeys.ALTER_CLIENT_QUOTAS => handleAlterClientQuotasRequest(request)\n       }\n+\n+      // try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests\n+      // are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle() and the\n+      // expiration thread for certain delayed operations (e.g. DelayedJoin)\n+      actionQueue.tryCompleteAction()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY2OTM1NA=="}, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4MzU0NA==", "bodyText": "I was thinking to add a comment so that if someone adds a future delayed operation that calls ReplicaManager.appendRecords() in onComplete() like DelayedJoin, he/she is aware that this operation's onExpiration() needs to call actionQueue.tryCompleteAction().", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479683544", "createdAt": "2020-08-29T19:45:56Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,41 +99,22 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin) pick up and then\n+   * execute an action when no lock is held.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDcyNw=="}, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NDE2Mw==", "bodyText": "Perhaps we can make this a bit clearer. Sth like the following.\nleaderHWIncremented has 3 possible values: (1) If records are not appended due to an error, the value will be None; (2) if records are appended successfully and HWM is advanced, the value is Some(true); (3) if records are appended successfully and HWM is not advanced, the value is Some(false).", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479684163", "createdAt": "2020-08-29T19:53:51Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +85,8 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWIncremented true if the high watermark is increased when appending record. Otherwise, false.\n+ *                            this field is updated after appending record so the default value is None.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NDM1Mw==", "bodyText": "Note that the action queue is not only called by requests threads, but also by the expiration thread for certain delayed operations.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479684353", "createdAt": "2020-08-29T19:55:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -134,6 +134,8 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n   private val KAFKA_CLUSTER_ID: String = \"kafka.cluster.id\"\n   private val KAFKA_BROKER_ID: String = \"kafka.broker.id\"\n \n+  private val actionQueue = new ActionQueue", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODgxMg=="}, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTU0NDg2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478154486", "createdAt": "2020-08-29T20:55:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMDo1NTo0NVrOHJd6pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMDo1NTo0NVrOHJd6pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTM4MQ==", "bodyText": "@junrao @ijuma please take a look at this method", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689381", "createdAt": "2020-08-29T20:55:45Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * picks up an action to complete.\n+   */\n+  def tryCompleteAction(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTU0NTQw", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478154540", "createdAt": "2020-08-29T20:56:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMDo1Njo1M1rOHJd68Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMDo1Njo1M1rOHJd68Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTQ1Nw==", "bodyText": "Main thing to decide is what to do in case of exception, do we stop processing or do we continue?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689457", "createdAt": "2020-08-29T20:56:53Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -37,7 +37,10 @@ class ActionQueue {\n    * picks up an action to complete.\n    */\n   def tryCompleteAction(): Unit = {\n-    val action = queue.poll()\n-    if (action != null) action()\n+    var action = queue.poll()\n+    while (action != null) {\n+      action()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTU0Njc2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478154676", "createdAt": "2020-08-29T20:59:38Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMDo1OTozOFrOHJd7tQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMDo1OTozOFrOHJd7tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY1Mw==", "bodyText": "Why are we using a blocking queue? It doesn't seem like we ever need the blocking functionality. Am I missing something?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689653", "createdAt": "2020-08-29T20:59:38Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTYzNTY2", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478163566", "createdAt": "2020-08-30T00:59:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQwMDo1OTozN1rOHJe8Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQwMTowMjo1NlrOHJe9Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwNjE4Mg==", "bodyText": "If we are unlucky, a single thread could be held up in this loop for a long time. Perhaps we could let each thread only complete the number of actions that it sees when entering tryCompleteActions().", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479706182", "createdAt": "2020-08-30T00:59:37Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * try to complete all delayed actions\n+   */\n+  def tryCompleteActions(): Unit = {\n+    var action = queue.poll()\n+    while (action != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwNjMwMw==", "bodyText": "Perhaps we could do the try/catch of each action here instead of KafkaApis. This way, we are guaranteed that all pending actions are processed in time.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479706303", "createdAt": "2020-08-30T01:01:16Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -37,7 +37,10 @@ class ActionQueue {\n    * picks up an action to complete.\n    */\n   def tryCompleteAction(): Unit = {\n-    val action = queue.poll()\n-    if (action != null) action()\n+    var action = queue.poll()\n+    while (action != null) {\n+      action()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTQ1Nw=="}, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwNjQwNw==", "bodyText": "Since we are draining more than 1 item now, this comment is no longer accurate.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479706407", "createdAt": "2020-08-30T01:02:56Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -562,6 +564,10 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * Noted that all pending delayed check operations are stored in a queue. All callers to ReplicaManager.appendRecords()\n+   * are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MjI1ODIz", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478225823", "createdAt": "2020-08-30T18:47:30Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQxODo0NzozMVrOHJk17A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQxODo1MDo1MFrOHJk3IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwMjg2MA==", "bodyText": "need => needs", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479802860", "createdAt": "2020-08-30T18:47:31Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +92,9 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWChange Incremental if the high watermark need to be increased after appending record.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwMjkxMg==", "bodyText": "is failed => failed", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479802912", "createdAt": "2020-08-30T18:48:05Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +92,9 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWChange Incremental if the high watermark need to be increased after appending record.\n+ *                       Same if high watermark is not changed. None is the default value and it means append is failed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwMzE2OA==", "bodyText": "Probably Increased is clearer than Incremental.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479803168", "createdAt": "2020-08-30T18:50:50Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -68,6 +68,13 @@ object LogAppendInfo {\n       offsetsMonotonic = false, -1L, recordErrors, errorMessage)\n }\n \n+sealed trait LeaderHWChange\n+object LeaderHWChange {\n+  case object Incremental extends LeaderHWChange", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4NzQyNjg0", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-478742684", "createdAt": "2020-08-31T16:00:54Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxNTM5MTYw", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-481539160", "createdAt": "2020-09-03T06:50:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QwNjo1MDo1MFrOHMYTIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QwNjo1MDo1MFrOHMYTIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg==", "bodyText": "@junrao This change avoids deadlock in TransactionCoordinatorConcurrencyTest.\nIf we update watchKeys before tryCompleteElseWatch, the other threads can take the same key to complete delayed request.  Hence the deadlock happens due to following conditions.\nthread_1  holds stateLock of TransactionStateManager to call appendRecords and it requires lock of delayed request to call tryCompleteElseWatch.\nthread_2 holds lock of delayed request to call onComplete (updateCacheCallback) and updateCacheCallback requires stateLock of TransactionStateManager.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r482743072", "createdAt": "2020-09-03T06:50:50Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -201,8 +201,8 @@ object AbstractCoordinatorConcurrencyTest {\n         }\n       }\n       val producerRequestKeys = entriesPerPartition.keys.map(TopicPartitionOperationKey(_)).toSeq\n-      watchKeys ++= producerRequestKeys\n       producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)\n+      watchKeys ++= producerRequestKeys", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzMTU3NDY0", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-483157464", "createdAt": "2020-09-06T16:38:01Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNjozODowMVrOHNqhhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzowNjoyOVrOHNqsEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDI0NQ==", "bodyText": "requiring => requires", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484090245", "createdAt": "2020-09-06T16:38:01Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDU2Mw==", "bodyText": "It seems that we don't need the if here?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484090563", "createdAt": "2020-09-06T16:41:37Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a\n+    // 6) thread_a requires lock of op to call safeTryComplete\n+    if (inLock(operation.lock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDY5NA==", "bodyText": "We should return if tryComplete() returns true.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484090694", "createdAt": "2020-09-06T16:43:11Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a\n+    // 6) thread_a requires lock of op to call safeTryComplete\n+    if (inLock(operation.lock) {\n+      var watchCreated = false\n+      watchKeys.foreach { key =>\n+        watchForOperation(key, operation)\n+        if (!watchCreated) {\n+          watchCreated = true\n+          estimatedTotalOperations.incrementAndGet()\n+        }\n       }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+      operation.tryComplete()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MTQ2Ng==", "bodyText": "This is an existing issue. I am not sure if calling tryComplete() without holding the operation's lock guarantees visibility to another thread. For example, thread 1 changes the state in the operation in tryComplete(). It then calls tryComplete() holding the operations's lock but doesn't change the state in the operation. thread 2 calls tryComplete() holding the operations's lock. Is thread 2 guaranteed to see the changes made by thread 1 since the update was made without crossing the memory boundary by subsequent readers?\nIf this is an issue, we could extend to lock to do the first tryComplete() check.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484091466", "createdAt": "2020-09-06T16:51:33Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjI1Mg==", "bodyText": "This approach is fine but leaks operation.lock beyond tests. Another way to package this is to add a new method in DelayedOperation like tryCompleteAndMaybeWatch(). If that's not very clean, we can keep the current approach.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484092252", "createdAt": "2020-09-06T16:59:20Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a\n+    // 6) thread_a requires lock of op to call safeTryComplete\n+    if (inLock(operation.lock) {\n+      var watchCreated = false\n+      watchKeys.foreach { key =>\n+        watchForOperation(key, operation)\n+        if (!watchCreated) {\n+          watchCreated = true\n+          estimatedTotalOperations.incrementAndGet()\n+        }\n       }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+      operation.tryComplete()\n+    }) return true", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjQ5Ng==", "bodyText": "Do we still need this change to avoid deadlocks?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484092496", "createdAt": "2020-09-06T17:02:00Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -201,8 +201,8 @@ object AbstractCoordinatorConcurrencyTest {\n         }\n       }\n       val producerRequestKeys = entriesPerPartition.keys.map(TopicPartitionOperationKey(_)).toSeq\n-      watchKeys ++= producerRequestKeys\n       producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)\n+      watchKeys ++= producerRequestKeys", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Mjk0NQ==", "bodyText": "With this change, DelayedOperations.checkAndCompleteFetch() is only used in tests. I am wondering if it can be removed. It's fine if we want to do this in a followup PR.\nUnrelated to this PR, DelayedOperations.checkAndCompleteProduce and DelayedOperations.checkAndCompleteDeleteRecords seem unused. We can probably remove them in a separate PR.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484092945", "createdAt": "2020-09-06T17:06:29Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1010,15 +1010,7 @@ class Partition(val topicPartition: TopicPartition,\n       }\n     }\n \n-    // some delayed operations may be unblocked after HW changed\n-    if (leaderHWIncremented)\n-      tryCompleteDelayedRequests()\n-    else {\n-      // probably unblock some follower fetch requests since log end offset has been updated\n-      delayedOperations.checkAndCompleteFetch()\n-    }\n-\n-    info\n+    info.copy(leaderHwChange = if (leaderHWIncremented) LeaderHwChange.Increased else LeaderHwChange.Same)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzNjYzNDk4", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-483663498", "createdAt": "2020-09-07T16:45:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjo0NTozNVrOHOEu4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNzowNDozNVrOHOFCAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxOTY1MQ==", "bodyText": "Since safeTryComplete() is no longer used in tryCompleteElseWatch(), the above comment is not completely relevant. Perhaps we could just explain what this method does \"Thread-safe variant of tryComplete().\"", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484519651", "createdAt": "2020-09-07T16:45:35Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,41 +102,22 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin) pick up and then\n+   * execute delayed actions when no lock is held.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMTYyOA==", "bodyText": "it requires lock_b => it tries to require lock_b", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484521628", "createdAt": "2020-09-07T16:52:44Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c holds lock of op (from watch list)\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op\n+    // 2) thread_a adds op to watch list\n+    // 3) thread_a calls op#tryComplete (and it requires lock_b)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMjgxNg==", "bodyText": "Perhaps we could change this comment to sth like the following.\nTo avoid the above scenario, we recommend DelayedOperationPurgatory.checkAndComplete() be called without holding any lock. Since DelayedOperationPurgatory.checkAndComplete() completes delayed operations asynchronously, holding a lock to make the call is often unnecessary.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484522816", "createdAt": "2020-09-07T16:57:32Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c holds lock of op (from watch list)\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op\n+    // 2) thread_a adds op to watch list\n+    // 3) thread_a calls op#tryComplete (and it requires lock_b)\n+    // 4) thread_b holds lock_b\n+    // 5) thread_b sees op from watch list\n+    // 6) thread_b needs lock of op\n+    // The above story produces a deadlock but it is not an issue in production yet since there is no reason for the", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMzI1Nw==", "bodyText": "There is a potential deadlock => There is a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete()", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484523257", "createdAt": "2020-09-07T16:59:24Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMzUzNA==", "bodyText": "thread_c holds lock of op => thread_c calls checkAndComplete () and holds lock of op", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484523534", "createdAt": "2020-09-07T17:00:21Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c holds lock of op (from watch list)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyNDU0NQ==", "bodyText": "Is this change still necessary now that we always call tryComplete() with lock in tryCompleteElseWatch?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484524545", "createdAt": "2020-09-07T17:04:35Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -201,8 +201,8 @@ object AbstractCoordinatorConcurrencyTest {\n         }\n       }\n       val producerRequestKeys = entriesPerPartition.keys.map(TopicPartitionOperationKey(_)).toSeq\n-      watchKeys ++= producerRequestKeys\n       producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)\n+      watchKeys ++= producerRequestKeys", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzNzQ1OTA1", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-483745905", "createdAt": "2020-09-08T01:37:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTozNzo1MVrOHOJ-iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMjowMTozMlrOHOKTMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTU3Ng==", "bodyText": "safeTryCompleteAndElse => safeTryCompleteOrElse ?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484605576", "createdAt": "2020-09-08T01:37:51Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,42 +102,24 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n-   *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * Thread-safe variant of tryComplete() and call extra function if first tryComplete returns false\n+   * @param f else function to be executed after first tryComplete returns false\n+   * @return result of tryComplete\n    */\n-  private[server] def maybeTryComplete(): Boolean = {\n-    var retry = false\n-    var done = false\n-    do {\n-      if (lock.tryLock()) {\n-        try {\n-          tryCompletePending.set(false)\n-          done = tryComplete()\n-        } finally {\n-          lock.unlock()\n-        }\n-        // While we were holding the lock, another thread may have invoked `maybeTryComplete` and set\n-        // `tryCompletePending`. In this case we should retry.\n-        retry = tryCompletePending.get()\n-      } else {\n-        // Another thread is holding the lock. If `tryCompletePending` is already set and this thread failed to\n-        // acquire the lock, then the thread that is holding the lock is guaranteed to see the flag and retry.\n-        // Otherwise, we should set the flag and retry on this thread since the thread holding the lock may have\n-        // released the lock and returned by the time the flag is set.\n-        retry = !tryCompletePending.getAndSet(true)\n-      }\n-    } while (!isCompleted && retry)\n-    done\n+  private[server] def safeTryCompleteAndElse(f: => Unit): Boolean = inLock(lock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNjgxMQ==", "bodyText": "The above comment is a bit out of context now. Perhaps we could change \"we do the check in the following way\" to \"we do the check in the following way through safeTryCompleteAndElse()\".", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484606811", "createdAt": "2020-09-08T01:43:41Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +212,33 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    //", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNzg4Mg==", "bodyText": "Do we still need to change the ordering now that we always call tryComplete() with lock in tryCompleteElseWatch?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484607882", "createdAt": "2020-09-08T01:48:33Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -202,9 +201,8 @@ object AbstractCoordinatorConcurrencyTest {\n         }\n       }\n       val producerRequestKeys = entriesPerPartition.keys.map(TopicPartitionOperationKey(_)).toSeq\n-      watchKeys ++= producerRequestKeys\n       producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)\n-      tryCompleteDelayedRequests()\n+      watchKeys ++= producerRequestKeys", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwOTA3Mg==", "bodyText": "checkAndComplete () => checkAndComplete()", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484609072", "createdAt": "2020-09-08T01:53:44Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +212,33 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete() in practice\n+    // if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete () and holds lock of op", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYxMDg2Ng==", "bodyText": "Perhaps change the above to the following?\nWe hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example, the following deadlock can happen if the lock is only held for the final tryComplete() check.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484610866", "createdAt": "2020-09-08T02:01:32Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +212,33 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete() in practice\n+    // if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzNzgyMTM1", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-483782135", "createdAt": "2020-09-08T04:08:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNDowODo1MVrOHOMA2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNDo0MDoxN1rOHOMb2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYzODkzOA==", "bodyText": "I think we still want to keep the rest of the paragraph starting from \"Call tryComplete().\".", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484638938", "createdAt": "2020-09-08T04:08:51Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYzOTIzNg==", "bodyText": "We hold => Through safeTryCompleteOrElse(), we hold", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484639236", "createdAt": "2020-09-08T04:10:06Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0MDE2OA==", "bodyText": "thread_b holds lock_b => thread_b holds lock_b and calls checkAndComplete()", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484640168", "createdAt": "2020-09-08T04:14:27Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is\n+    // to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example,\n+    // the following deadlock can happen if the lock is only held for the final tryComplete() check,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op\n+    // 2) thread_a adds op to watch list\n+    // 3) thread_a calls op#tryComplete (and it tries to require lock_b)\n+    // 4) thread_b holds lock_b", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0MDg2Mg==", "bodyText": "Noted that current approach can't prevent all deadlock. => Note that even with the current approach, deadlocks could still be introduced.", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484640862", "createdAt": "2020-09-08T04:17:41Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is\n+    // to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example,\n+    // the following deadlock can happen if the lock is only held for the final tryComplete() check,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0NDgwMg==", "bodyText": "Instead of introducing a global var, could we add a new param when constructing CommitTxnOffsetsOperation and CompleteTxnOperation?", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484644802", "createdAt": "2020-09-08T04:35:49Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -112,6 +113,13 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }.toSet\n   }\n \n+  /**\n+   * handleTxnCommitOffsets does not complete delayed requests now so it causes error if handleTxnCompletion is executed\n+   * before completing delayed request. In random mode, we use this global lock to prevent such error.\n+   */\n+  private var isRandomOrder = false", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0NTg0OQ==", "bodyText": "thread_a gets lock of op => thread_a calls tryCompleteElseWatch() and gets lock of op", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484645849", "createdAt": "2020-09-08T04:40:17Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is\n+    // to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example,\n+    // the following deadlock can happen if the lock is only held for the final tryComplete() check,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg0Mjc4NjQ3", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-484278647", "createdAt": "2020-09-08T15:52:56Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNTo1Mjo1NlrOHOjrFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNjowNDoyM1rOHOkJOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNjU4Mw==", "bodyText": "to call safeTryComplete => to call the final tryComplete()", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485026583", "createdAt": "2020-09-08T15:52:56Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -219,38 +203,38 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n   def tryCompleteElseWatch(operation: T, watchKeys: Seq[Any]): Boolean = {\n     assert(watchKeys.nonEmpty, \"The watch key list can't be empty\")\n \n-    // The cost of tryComplete() is typically proportional to the number of keys. Calling\n-    // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // The cost of tryComplete() is typically proportional to the number of keys. Calling tryComplete() for each key is\n+    // going to be expensive if there are many keys. Instead, we do the check in the following way through safeTryCompleteOrElse().\n+    // If the operation is not completed, we just add the operation to all keys. Then we call tryComplete() again. At\n+    // this time, if the operation is still not completed, we are guaranteed that it won't miss any future triggering\n+    // event since the operation is already on the watcher list for all keys.\n+    //\n+    // ==============[story about lock]==============\n+    // Through safeTryCompleteOrElse(), we hold the operation's lock while adding the operation to watch list and doing\n+    // the tryComplete() check. This is to avoid a potential deadlock between the callers to tryCompleteElseWatch() and\n+    // checkAndComplete(). For example, the following deadlock can happen if the lock is only held for the final tryComplete()\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNjc5OQ==", "bodyText": "tryCompleteElseWatch => tryCompleteElseWatch()", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485026799", "createdAt": "2020-09-08T15:53:16Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -219,38 +203,38 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n   def tryCompleteElseWatch(operation: T, watchKeys: Seq[Any]): Boolean = {\n     assert(watchKeys.nonEmpty, \"The watch key list can't be empty\")\n \n-    // The cost of tryComplete() is typically proportional to the number of keys. Calling\n-    // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // The cost of tryComplete() is typically proportional to the number of keys. Calling tryComplete() for each key is\n+    // going to be expensive if there are many keys. Instead, we do the check in the following way through safeTryCompleteOrElse().\n+    // If the operation is not completed, we just add the operation to all keys. Then we call tryComplete() again. At\n+    // this time, if the operation is still not completed, we are guaranteed that it won't miss any future triggering\n+    // event since the operation is already on the watcher list for all keys.\n+    //\n+    // ==============[story about lock]==============\n+    // Through safeTryCompleteOrElse(), we hold the operation's lock while adding the operation to watch list and doing\n+    // the tryComplete() check. This is to avoid a potential deadlock between the callers to tryCompleteElseWatch() and\n+    // checkAndComplete(). For example, the following deadlock can happen if the lock is only held for the final tryComplete()\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDI2Mg==", "bodyText": "causes error => causes an error", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485034262", "createdAt": "2020-09-08T16:04:21Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -119,12 +110,33 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n \n   @Test\n   def testConcurrentTxnGoodPathSequence(): Unit = {\n-    verifyConcurrentOperations(createGroupMembers, allOperationsWithTxn)\n+    verifyConcurrentOperations(createGroupMembers, Seq(\n+      new JoinGroupOperation,\n+      new SyncGroupOperation,\n+      new OffsetFetchOperation,\n+      new CommitTxnOffsetsOperation,\n+      new CompleteTxnOperation,\n+      new HeartbeatOperation,\n+      new LeaveGroupOperation\n+    ))\n   }\n \n   @Test\n   def testConcurrentRandomSequence(): Unit = {\n-    verifyConcurrentRandomSequences(createGroupMembers, allOperationsWithTxn)\n+    /**\n+     * handleTxnCommitOffsets does not complete delayed requests now so it causes error if handleTxnCompletion is executed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDI5Ng==", "bodyText": "such error => such an error", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485034296", "createdAt": "2020-09-08T16:04:23Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -119,12 +110,33 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n \n   @Test\n   def testConcurrentTxnGoodPathSequence(): Unit = {\n-    verifyConcurrentOperations(createGroupMembers, allOperationsWithTxn)\n+    verifyConcurrentOperations(createGroupMembers, Seq(\n+      new JoinGroupOperation,\n+      new SyncGroupOperation,\n+      new OffsetFetchOperation,\n+      new CommitTxnOffsetsOperation,\n+      new CompleteTxnOperation,\n+      new HeartbeatOperation,\n+      new LeaveGroupOperation\n+    ))\n   }\n \n   @Test\n   def testConcurrentRandomSequence(): Unit = {\n-    verifyConcurrentRandomSequences(createGroupMembers, allOperationsWithTxn)\n+    /**\n+     * handleTxnCommitOffsets does not complete delayed requests now so it causes error if handleTxnCompletion is executed\n+     * before completing delayed request. In random mode, we use this global lock to prevent such error.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c086bd2e8b9fecb10df5ab74837c5ef6c0ac29ef", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/c086bd2e8b9fecb10df5ab74837c5ef6c0ac29ef", "committedDate": "2020-09-08T16:10:57Z", "message": "KAFKA-8334 Make sure the thread which tries to complete delayed requests does NOT hold any group lock"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8f54add2b01625d3d990e44538c5e92c7c12d0e", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/c8f54add2b01625d3d990e44538c5e92c7c12d0e", "committedDate": "2020-09-08T16:10:57Z", "message": "address review comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfb9b4b6ca5633951fddc3ba708b5d0e8066c040", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/dfb9b4b6ca5633951fddc3ba708b5d0e8066c040", "committedDate": "2020-09-08T16:10:57Z", "message": "add more comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb7a9e53aa7d66585c71bb190197ba51e5406943", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/bb7a9e53aa7d66585c71bb190197ba51e5406943", "committedDate": "2020-09-08T16:10:57Z", "message": "address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e89824c711064acdf5e72954906c4090b7488eb", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/3e89824c711064acdf5e72954906c4090b7488eb", "committedDate": "2020-09-08T16:10:57Z", "message": "address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c981c56cb189f45741da7911ff2a77b9cfca97a", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/9c981c56cb189f45741da7911ff2a77b9cfca97a", "committedDate": "2020-09-08T16:10:57Z", "message": "revert leaderHWIncremented to option"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eba6df82ea1480c91107c9fee45c290a53462855", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/eba6df82ea1480c91107c9fee45c290a53462855", "committedDate": "2020-09-08T16:10:57Z", "message": "introduce leaderHWChange and consume all delayed actions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a9e49fc8a2213cb57a782f4a15bae352a4c7d43", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/4a9e49fc8a2213cb57a782f4a15bae352a4c7d43", "committedDate": "2020-09-08T16:10:57Z", "message": "revise comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c42f46426f4cdb2ed441d7bf99fbc887861dff93", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/c42f46426f4cdb2ed441d7bf99fbc887861dff93", "committedDate": "2020-09-08T16:10:57Z", "message": "revert action queue in per server"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1a6044b54fa50541955761c3e83e8247eb70026", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/e1a6044b54fa50541955761c3e83e8247eb70026", "committedDate": "2020-09-08T16:10:57Z", "message": "Incremental -> Increased; LeaderHWChange -> LeaderHwChange; and other grammar fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5da2fab1f02ebf3965a6385a20cf078ba692e541", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/5da2fab1f02ebf3965a6385a20cf078ba692e541", "committedDate": "2020-09-08T16:10:57Z", "message": "fix deadlock in TransactionCoordinatorConcurrencyTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e66db09b6a8fb2f93fd097fbbd327cab98b5107", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/4e66db09b6a8fb2f93fd097fbbd327cab98b5107", "committedDate": "2020-09-08T16:10:57Z", "message": "fix potential deadlock in tryCompleteElseWatch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3cd7add61a91949611c39ceb7d37ada41614cf8", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/b3cd7add61a91949611c39ceb7d37ada41614cf8", "committedDate": "2020-09-08T16:10:57Z", "message": "a bit tweak"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50032c16eda4c5845dea94b7076a4b6f432d9550", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/50032c16eda4c5845dea94b7076a4b6f432d9550", "committedDate": "2020-09-08T16:10:57Z", "message": "remove unused methods from DelayedOperations; add safeTryCompleteAndElse to DelayedOperation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28b68545183b13f39861ee45812042f5464777b2", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/28b68545183b13f39861ee45812042f5464777b2", "committedDate": "2020-09-08T16:10:57Z", "message": "rename safeTryCompleteOrElse to safeTryCompleteAndElse; revise comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9a49f046f96fe082de7bddb539dd68589e8853db", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/9a49f046f96fe082de7bddb539dd68589e8853db", "committedDate": "2020-09-08T16:10:57Z", "message": "remove global variable from GroupCoordinatorConcurrencyTest; revise comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "committedDate": "2020-09-08T16:13:09Z", "message": "tweak comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "committedDate": "2020-09-08T16:13:09Z", "message": "tweak comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg0NTk1OTUw", "url": "https://github.com/apache/kafka/pull/8657#pullrequestreview-484595950", "createdAt": "2020-09-09T01:27:32Z", "commit": {"oid": "fbd46565aa5e03f4fd9c857a184b7c2371ca5932"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 968, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}