{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExMDk1OTMw", "number": 8588, "title": "KAFKA-6145: KIP-441: Improve assignment balance", "bodyText": "Add validation that task assignment is always balanced after convergence.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-04-30T03:07:33Z", "url": "https://github.com/apache/kafka/pull/8588", "merged": true, "mergeCommit": {"oid": "d62f6ebdfe38adf894187e76546eedf13ee98432"}, "closed": true, "closedAt": "2020-05-14T13:32:08Z", "author": {"login": "vvcephei"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcczDedgFqTQwMzg1MTE0MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABchDhqaAH2gAyNDExMDk1OTMwOjljZjgzMTY0NDRjMjA1ZWEwNGY2ZmEwODc0NjE5NDUxZjViOTRkOTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzODUxMTQw", "url": "https://github.com/apache/kafka/pull/8588#pullrequestreview-403851140", "createdAt": "2020-04-30T20:24:54Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMDoyNDo1NFrOGO5CVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMDoyNDo1NFrOGO5CVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI2NzczMg==", "bodyText": "The HATA originally tried to balance each type of task individually (ie stateful active, standby, stateless active) and IIRC you made a convincing argument against doing that during the review and for balancing only the total task load. What's the rationale for enforcing this now? Or did I misremember and/or misinterpret your earlier point", "url": "https://github.com/apache/kafka/pull/8588#discussion_r418267732", "createdAt": "2020-04-30T20:24:54Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -358,6 +369,57 @@ private static void runRandomizedScenario(final long seed) {\n         }\n     }\n \n+    private static void verifyBalancedAssignment(final Harness harness, final int balanceFactor) {\n+        final int activeDiff;\n+        final int activeStatefulDiff;\n+        final double activeStatelessPerThreadDiff;\n+        final int assignedDiff;\n+        final int standbyDiff;\n+\n+        {\n+            final int maxActive = harness.clientStates.values().stream().map(ClientState::activeTaskCount).max(comparingInt(i -> i)).orElse(0);\n+            final int minActive = harness.clientStates.values().stream().map(ClientState::activeTaskCount).min(comparingInt(i -> i)).orElse(0);\n+            activeDiff = maxActive - minActive;\n+        }\n+        {\n+            final int maxActiveStateful = harness.clientStates.values().stream().map(s -> diff(TreeSet::new, s.activeTasks(), harness.statelessTasks).size()).max(comparingInt(i -> i)).orElse(0);\n+            final int minActiveStateful = harness.clientStates.values().stream().map(s -> diff(TreeSet::new, s.activeTasks(), harness.statelessTasks).size()).min(comparingInt(i -> i)).orElse(0);\n+            activeStatefulDiff = maxActiveStateful - minActiveStateful;\n+        }\n+        {\n+            final double maxActiveStatefulPerThread = harness.clientStates.values().stream().map(s1 -> 1.0 * intersection(TreeSet::new, s1.activeTasks(), harness.statelessTasks).size() / s1.capacity()).max(comparingDouble(i -> i)).orElse(0.0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzOTQ1OTM3", "url": "https://github.com/apache/kafka/pull/8588#pullrequestreview-403945937", "createdAt": "2020-04-30T23:33:34Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMzozMzozNVrOGO92Aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMzozMzozNVrOGO92Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM0NjQ5OQ==", "bodyText": "Ok, this gives me an idea of where you're coming from w.r.t client-level balance. I was thinking that we should scale the entire task load with the thread capacity, but that only makes sense when considering some resources. Mainly (or only?) cpu, which I suppose it unlikely to be the bottleneck or resource constraint in a stateful application. Of course, it would still be for stateless tasks. So I guess I do see that we might want to balance stateless tasks at a thread level, and anything stateful at the client-level where IO is more likely to be the constraint.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r418346499", "createdAt": "2020-04-30T23:33:35Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -358,6 +369,57 @@ private static void runRandomizedScenario(final long seed) {\n         }\n     }\n \n+    private static void verifyBalancedAssignment(final Harness harness, final int balanceFactor) {\n+        final int activeDiff;\n+        final int activeStatefulDiff;\n+        final double activeStatelessPerThreadDiff;\n+        final int assignedDiff;\n+        final int standbyDiff;\n+\n+        {\n+            final int maxActive = harness.clientStates.values().stream().map(ClientState::activeTaskCount).max(comparingInt(i -> i)).orElse(0);\n+            final int minActive = harness.clientStates.values().stream().map(ClientState::activeTaskCount).min(comparingInt(i -> i)).orElse(0);\n+            activeDiff = maxActive - minActive;\n+        }\n+        {\n+            final int maxActiveStateful = harness.clientStates.values().stream().map(s -> diff(TreeSet::new, s.activeTasks(), harness.statelessTasks).size()).max(comparingInt(i -> i)).orElse(0);\n+            final int minActiveStateful = harness.clientStates.values().stream().map(s -> diff(TreeSet::new, s.activeTasks(), harness.statelessTasks).size()).min(comparingInt(i -> i)).orElse(0);\n+            activeStatefulDiff = maxActiveStateful - minActiveStateful;\n+        }\n+        {\n+            final double maxActiveStatefulPerThread = harness.clientStates.values().stream().map(s1 -> 1.0 * intersection(TreeSet::new, s1.activeTasks(), harness.statelessTasks).size() / s1.capacity()).max(comparingDouble(i -> i)).orElse(0.0);\n+            final double minActiveStatefulPerThread = harness.clientStates.values().stream().map(s -> 1.0 * intersection(TreeSet::new, s.activeTasks(), harness.statelessTasks).size() / s.capacity()).min(comparingDouble(i -> i)).orElse(0.0);\n+            activeStatelessPerThreadDiff = maxActiveStatefulPerThread - minActiveStatefulPerThread;\n+        }\n+        {\n+            final int maxAssigned = harness.clientStates.values().stream().map(ClientState::assignedTaskCount).max(comparingInt(i -> i)).orElse(0);\n+            final int minAssigned = harness.clientStates.values().stream().map(ClientState::assignedTaskCount).min(comparingInt(i -> i)).orElse(0);\n+            assignedDiff = maxAssigned - minAssigned;\n+        }\n+        {\n+            final int maxStandby = harness.clientStates.values().stream().map(ClientState::standbyTaskCount).max(comparingInt(i -> i)).orElse(0);\n+            final int minStandby = harness.clientStates.values().stream().map(ClientState::standbyTaskCount).min(comparingInt(i -> i)).orElse(0);\n+            standbyDiff = maxStandby - minStandby;\n+        }\n+\n+        final Map<String, ? extends Number> results = new TreeMap<>(mkMap(\n+            mkEntry(\"activeDiff\", activeDiff),\n+            mkEntry(\"activeStatefulDiff\", activeStatefulDiff),\n+            mkEntry(\"activeStatelessPerThreadDiff\", activeStatelessPerThreadDiff),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 104}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "084dbfa84f86811f44cc9632e01cd5ca3d787b55", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/084dbfa84f86811f44cc9632e01cd5ca3d787b55", "committedDate": "2020-05-08T03:35:52Z", "message": "KAFKA-6145: KIP-441: Balanced active, stateful, and task assignment\n\n* Add verification to ensure assignments are balanced in each way\n* Tweak the assignment algorithm to produced assignments that are\n  balanced in each way"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7773df0f529c89164c43c67ec12a3ce4a44109a7", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/7773df0f529c89164c43c67ec12a3ce4a44109a7", "committedDate": "2020-05-08T04:53:12Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7bf19a2eaeb804b91d9ca201340a961f87937606", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/7bf19a2eaeb804b91d9ca201340a961f87937606", "committedDate": "2020-05-08T17:56:32Z", "message": "final tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/5201493c4cb1ada01fe75f59b3db65539144794e", "committedDate": "2020-05-08T20:36:55Z", "message": "remove rebase error from trunk"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01b2831ef17091761da2ced2c8bacb015df555ac", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/01b2831ef17091761da2ced2c8bacb015df555ac", "committedDate": "2020-05-08T21:14:31Z", "message": "tweak balance algorithm"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d70cca8c190b552e7fe7b81afe16731a17ebd034", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/d70cca8c190b552e7fe7b81afe16731a17ebd034", "committedDate": "2020-05-08T21:15:32Z", "message": "fix exception wording"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/a29a8a078cd03f746b5aa4285d6d874c306dbdca", "committedDate": "2020-05-08T21:23:37Z", "message": "better variable name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cbd08807fb74217207502606cfb86325515077f5", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/cbd08807fb74217207502606cfb86325515077f5", "committedDate": "2020-05-08T21:47:55Z", "message": "log message tweak"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4NTEyNTg3", "url": "https://github.com/apache/kafka/pull/8588#pullrequestreview-408512587", "createdAt": "2020-05-08T20:40:05Z", "commit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "state": "COMMENTED", "comments": {"totalCount": 47, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMDo0MDowNVrOGSy-CQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzoxOTozMVrOGS2QXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2MjYzMw==", "bodyText": "I wound up wanting the other two as well, so I went ahead and added them. Added tests as well.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422362633", "createdAt": "2020-05-08T20:40:05Z", "author": {"login": "vvcephei"}, "path": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java", "diffHunk": "@@ -1155,4 +1155,21 @@ private static byte checkRange(final byte i) {\n         }\n         return result;\n     }\n+\n+    @SafeVarargs\n+    public static <E> Set<E> intersection(final Supplier<Set<E>> constructor, final Set<E> first, final Set<E>... set) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Mjg4Mg==", "bodyText": "It's just nice to be able to print them in tests and debugging.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422362882", "createdAt": "2020-05-08T20:40:36Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -364,5 +364,15 @@ private AssignmentConfigs(final StreamsConfig configs) {\n             this.numStandbyReplicas = numStandbyReplicas;\n             this.probingRebalanceIntervalMs = probingRebalanceIntervalMs;\n         }\n+\n+        @Override\n+        public String toString() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2MzQwMA==", "bodyText": "We added this before because we were thinking this algorithm would be pluggable, but now that we made TaskAssignor the pluggable component, we might as well get rid of this interface.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422363400", "createdAt": "2020-05-08T20:41:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/BalancedAssignor.java", "diffHunk": "@@ -1,30 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-import java.util.UUID;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-import org.apache.kafka.streams.processor.TaskId;\n-\n-public interface BalancedAssignor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NDc4MQ==", "bodyText": "I dropped these collections, but maintained the getter method, which now just computes the union of the active and standby tasks. The reason was some inconsistencies that crept in when I was refactoring the balancing algorithm and messed up the code to maintain both collections consistently. I fixed that bug, but it doesn't seem worth the risk to have to maintain this collection correctly going forward.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422364781", "createdAt": "2020-05-08T20:44:48Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -23,27 +23,30 @@\n import org.slf4j.LoggerFactory;\n \n import java.util.Collection;\n-import java.util.HashMap;\n+import java.util.Comparator;\n import java.util.HashSet;\n import java.util.Map;\n import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n import java.util.UUID;\n \n import static java.util.Collections.emptyMap;\n import static java.util.Collections.unmodifiableMap;\n import static java.util.Collections.unmodifiableSet;\n+import static java.util.Comparator.comparing;\n import static org.apache.kafka.common.utils.Utils.union;\n import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n \n public class ClientState {\n     private static final Logger LOG = LoggerFactory.getLogger(ClientState.class);\n+    public static final Comparator<TopicPartition> TOPIC_PARTITION_COMPARATOR = comparing(TopicPartition::topic).thenComparing(TopicPartition::partition);\n \n     private final Set<TaskId> activeTasks;\n     private final Set<TaskId> standbyTasks;\n-    private final Set<TaskId> assignedTasks;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NTczOQ==", "bodyText": "I made all these collections sorted just to help readability while debugging. What do you think about keeping them sorted going forward (for the same reason)?", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422365739", "createdAt": "2020-05-08T20:46:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -56,34 +59,28 @@ public ClientState() {\n     }\n \n     ClientState(final int capacity) {\n-        this(new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashMap<>(),\n-             new HashMap<>(),\n-             new HashMap<>(),\n-             capacity);\n+        activeTasks = new TreeSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NjM4Mg==", "bodyText": "Unfortunately, TreeMap doesn't have a constructor that takes both a collection to copy and a comparator.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422366382", "createdAt": "2020-05-08T20:48:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Njg1Nw==", "bodyText": "In addition to writing tests to verify that assignments are valid at the cluster level, it seemed wise to at least make sure that we can't create an invalid assignment for a single node.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422366857", "createdAt": "2020-05-08T20:49:16Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Nzc2NQ==", "bodyText": "We previously did our \"task shuffling\" on secondary collections, like a map of client id to list of tasks. But after unifying the balancing logic for both active and standby tasks, it became more sensible just to shuffle tasks directly in the assignment, which necessitates these \"unassign\" methods.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422367765", "createdAt": "2020-05-08T20:51:14Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2ODQxMA==", "bodyText": "I was briefly mutating these returned collections to implement \"task shuffling\" before I realized what a terrible decision that is. To prevent similar poor choices (mostly by me) in the future, I've made all these return unmodifiable views.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422368410", "createdAt": "2020-05-08T20:52:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {\n+        if (!standbyTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign standby task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        standbyTasks.remove(task);\n     }\n \n-    void assignStandbyTasks(final Collection<TaskId> tasks) {\n-        standbyTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    public void assignActiveTasks(final Collection<TaskId> tasks) {\n+        activeTasks.addAll(tasks);\n     }\n \n     public Set<TaskId> activeTasks() {\n-        return activeTasks;\n+        return unmodifiableSet(activeTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM3ODkyOQ==", "bodyText": "This was handy for eyeballing balance issues.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422378929", "createdAt": "2020-05-08T21:15:36Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -338,35 +337,37 @@ private void initializeRemainingPrevTasksFromTaskOffsetSums() {\n \n     private void addPreviousActiveTask(final TaskId task) {\n         prevActiveTasks.add(task);\n-        prevAssignedTasks.add(task);\n     }\n \n     private void addPreviousStandbyTask(final TaskId task) {\n         prevStandbyTasks.add(task);\n-        prevAssignedTasks.add(task);\n+    }\n+\n+    private void assertNotAssigned(final TaskId task) {\n+        if (standbyTasks.contains(task) || activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to assign task \" + task + \", but it is already assigned: \" + this);\n+        }\n     }\n \n     @Override\n     public String toString() {\n         return \"[activeTasks: (\" + activeTasks +\n             \") standbyTasks: (\" + standbyTasks +\n-            \") assignedTasks: (\" + assignedTasks +\n             \") prevActiveTasks: (\" + prevActiveTasks +\n             \") prevStandbyTasks: (\" + prevStandbyTasks +\n-            \") prevAssignedTasks: (\" + prevAssignedTasks +\n             \") prevOwnedPartitionsByConsumerId: (\" + ownedPartitions.keySet() +\n             \") changelogOffsetTotalsByTask: (\" + taskOffsetSums.entrySet() +\n             \") capacity: \" + capacity +\n+            \" assigned: \" + (activeTasks.size() + standbyTasks.size()) +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70cca8c190b552e7fe7b81afe16731a17ebd034"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM3OTE4Mg==", "bodyText": "These didn't really provide much value on top of the active/standby tasks to be worth the noise.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422379182", "createdAt": "2020-05-08T21:16:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -338,35 +337,37 @@ private void initializeRemainingPrevTasksFromTaskOffsetSums() {\n \n     private void addPreviousActiveTask(final TaskId task) {\n         prevActiveTasks.add(task);\n-        prevAssignedTasks.add(task);\n     }\n \n     private void addPreviousStandbyTask(final TaskId task) {\n         prevStandbyTasks.add(task);\n-        prevAssignedTasks.add(task);\n+    }\n+\n+    private void assertNotAssigned(final TaskId task) {\n+        if (standbyTasks.contains(task) || activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to assign task \" + task + \", but it is already assigned: \" + this);\n+        }\n     }\n \n     @Override\n     public String toString() {\n         return \"[activeTasks: (\" + activeTasks +\n             \") standbyTasks: (\" + standbyTasks +\n-            \") assignedTasks: (\" + assignedTasks +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70cca8c190b552e7fe7b81afe16731a17ebd034"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM3OTgxNg==", "bodyText": "I generalized this data structure a little, and the name didn't really make sense anymore, so I went for a more \"abstract\" name that hopefully wraps up what the thing is like.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422379816", "createdAt": "2020-05-08T21:17:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70cca8c190b552e7fe7b81afe16731a17ebd034"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4MTQ4Mg==", "bodyText": "I moved the \"weight\" computation to the caller (was the taskLoad stuff below), so we can use this data structure for more stuff. As a consequence, we don't need the clientStates map in here anymore.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422381482", "createdAt": "2020-05-08T21:21:28Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70cca8c190b552e7fe7b81afe16731a17ebd034"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4MjMxNw==", "bodyText": "I discovered by the way that you can compose Comparators!", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422382317", "createdAt": "2020-05-08T21:23:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDA3Ng==", "bodyText": "I added a \"last mile\" constraint so that we can use the same priority queue when computing task movements, but first try to find a caught-up standby to move to and then fall back to any caught-up client. I thought about capturing this with the weight function itself, but it depends on which task you're interested in, so this worked better.\nI also dropped the \"poll N clients\" method. Now, if we need 4 clients, we'll poll 4 times. This actually results in better balancing characteristics when assigning standbys because the relative weights of the clients changes while you're assigning standbys to them, so it's better to poll/assign/offer one at a time.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422384076", "createdAt": "2020-05-08T21:28:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));\n     }\n \n     /**\n      * @return the next least loaded client that satisfies the given criteria, or null if none do\n      */\n-    UUID poll(final TaskId task) {\n-        final List<UUID> validClient = poll(task, 1);\n-        return validClient.isEmpty() ? null : validClient.get(0);\n-    }\n-\n-    /**\n-     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n-     */\n-    List<UUID> poll(final TaskId task, final int numClients) {\n-        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+    UUID poll(final TaskId task, final Function<UUID, Boolean> extraConstraint) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDQ5Nw==", "bodyText": "Just a little mild refactoring here.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422384497", "createdAt": "2020-05-08T21:29:07Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));\n     }\n \n     /**\n      * @return the next least loaded client that satisfies the given criteria, or null if none do\n      */\n-    UUID poll(final TaskId task) {\n-        final List<UUID> validClient = poll(task, 1);\n-        return validClient.isEmpty() ? null : validClient.get(0);\n-    }\n-\n-    /**\n-     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n-     */\n-    List<UUID> poll(final TaskId task, final int numClients) {\n-        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+    UUID poll(final TaskId task, final Function<UUID, Boolean> extraConstraint) {\n         final Set<UUID> invalidPolledClients = new HashSet<>();\n-        while (nextLeastLoadedValidClients.size() < numClients) {\n-            UUID candidateClient;\n-            while (true) {\n-                candidateClient = pollNextClient();\n-                if (candidateClient == null) {\n-                    offerAll(invalidPolledClients);\n-                    return nextLeastLoadedValidClients;\n-                }\n-\n-                if (validClientCriteria.apply(candidateClient, task)) {\n-                    nextLeastLoadedValidClients.add(candidateClient);\n-                    break;\n-                } else {\n-                    invalidPolledClients.add(candidateClient);\n-                }\n+        while (!clientsByTaskLoad.isEmpty()) {\n+            final UUID candidateClient = pollNextClient();\n+            if (constraint.apply(candidateClient, task) && extraConstraint.apply(candidateClient)) {\n+                // then we found the lightest, valid client\n+                offerAll(invalidPolledClients);\n+                return candidateClient;\n+            } else {\n+                // remember this client and try again later\n+                invalidPolledClients.add(candidateClient);\n             }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDg4Ng==", "bodyText": "Kept this as a convenience method when you don't care to specify a last-mile constraint.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422384886", "createdAt": "2020-05-08T21:30:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));\n     }\n \n     /**\n      * @return the next least loaded client that satisfies the given criteria, or null if none do\n      */\n-    UUID poll(final TaskId task) {\n-        final List<UUID> validClient = poll(task, 1);\n-        return validClient.isEmpty() ? null : validClient.get(0);\n-    }\n-\n-    /**\n-     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n-     */\n-    List<UUID> poll(final TaskId task, final int numClients) {\n-        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+    UUID poll(final TaskId task, final Function<UUID, Boolean> extraConstraint) {\n         final Set<UUID> invalidPolledClients = new HashSet<>();\n-        while (nextLeastLoadedValidClients.size() < numClients) {\n-            UUID candidateClient;\n-            while (true) {\n-                candidateClient = pollNextClient();\n-                if (candidateClient == null) {\n-                    offerAll(invalidPolledClients);\n-                    return nextLeastLoadedValidClients;\n-                }\n-\n-                if (validClientCriteria.apply(candidateClient, task)) {\n-                    nextLeastLoadedValidClients.add(candidateClient);\n-                    break;\n-                } else {\n-                    invalidPolledClients.add(candidateClient);\n-                }\n+        while (!clientsByTaskLoad.isEmpty()) {\n+            final UUID candidateClient = pollNextClient();\n+            if (constraint.apply(candidateClient, task) && extraConstraint.apply(candidateClient)) {\n+                // then we found the lightest, valid client\n+                offerAll(invalidPolledClients);\n+                return candidateClient;\n+            } else {\n+                // remember this client and try again later\n+                invalidPolledClients.add(candidateClient);\n             }\n         }\n+        // we tried all the clients, and none met the constraint (or there are no clients)\n         offerAll(invalidPolledClients);\n-        return nextLeastLoadedValidClients;\n+        return null;\n+    }\n+\n+    /**\n+     * @return the next least loaded client that satisfies the given criteria, or null if none do\n+     */\n+    UUID poll(final TaskId task) {\n+        return poll(task, client -> true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NTg2Mg==", "bodyText": "This was a lurking bug. We should never call this method if there's nothing in the queue, but we actually were in some cases before. This should have been an exception, since you're not supposed to pass null to Set#remove, but it turns out that we were using a HashSet, which just happens to actually accept a null to remove (and just ignore it).\nAnyway, Queue#remove should be used if you expect the queue to contain an element (it guarantees no null return but throws a NoSuchElementException if it's empty).", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422385862", "createdAt": "2020-05-08T21:32:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -105,7 +86,7 @@ void offer(final UUID client) {\n     }\n \n     private UUID pollNextClient() {\n-        final UUID client = clientsByTaskLoad.poll();\n+        final UUID client = clientsByTaskLoad.remove();\n         uniqueClients.remove(client);\n         return client;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NjQ2NQ==", "bodyText": "I just inlined this into the HighAvailabilityTaskAssignor. After refactorings, we'd wound up in a weird place where standby and stateless assignment logic was in HATA, but active assignment logic was here. Plus, I needed the balance method for standby assignment as well, so putting them all in one place allows for reuse.\nYou'll note that I moved all the tests for this class into the HATATest. Conveniently, if you request no standbys and no warmups, and also only provide stateful tasks, you effectively only exercise the active stateful task assignment logic, so we can still make the same assertions.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422386465", "createdAt": "2020-05-08T21:34:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignor.java", "diffHunk": "@@ -1,86 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-import java.util.UUID;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-import org.apache.kafka.streams.processor.TaskId;\n-\n-public class DefaultBalancedAssignor implements BalancedAssignor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NzEyNA==", "bodyText": "It was subtly misnamed.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422387124", "createdAt": "2020-05-08T21:36:15Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "diffHunk": "@@ -41,9 +41,9 @@ public FallbackPriorTaskAssignor() {\n     @Override\n     public boolean assign(final Map<UUID, ClientState> clients,\n                           final Set<TaskId> allTaskIds,\n-                          final Set<TaskId> standbyTaskIds,\n+                          final Set<TaskId> statefulTaskIds,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4ODA1MQ==", "bodyText": "These were all fields before because we used to partially set things up in the constructor. Now that we have to use the 0-arg constructor, and we pass everything in the method call, it's better to just use local fields. The actual reason I changed it was that it became confusing to figure out what references what, which is now clear because everything is passed explicitly.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422388051", "createdAt": "2020-05-08T21:38:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4ODQ1Mw==", "bodyText": "This used to do the active assignment AND the movements, but now it only does the active assignment (and balance it).", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422388453", "createdAt": "2020-05-08T21:39:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4ODYzNQ==", "bodyText": "Then, we do the standby assignment (and balance it).\nThat last bit was the thing that set off most of these other refactors. We had problems with the ultimate assignment balance because we can't guarantee to provide a balanced total assignment (including active and standby) analytically, due to unfortunate numerical relationships between the number of instances, number of standbys, and number of partitions of each task.\nSo, it's better to just apply the same optimization algorithm to shuffle tasks around until they're balanced.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422388635", "createdAt": "2020-05-08T21:40:12Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MDk1NQ==", "bodyText": "Finally, we compute movements. Since we're now starting from a nicely balanced active and standby assignment, the warmup assignment is pretty straightforward, and limited in the amount of skew it can introduce (by the max warmups config).\nIt would have been especially tricky to balance the standbys after the warmup assignments, since we couldn't know which ones needed to stay put and which could move.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422390955", "createdAt": "2020-05-08T21:46:21Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MTgzMA==", "bodyText": "The algorithm is the same as before, although I refactored the initial loop a little to avoid the break.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422391830", "createdAt": "2020-05-08T21:48:48Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MjIzNA==", "bodyText": "This is where we repeatedly poll one, rather than polling a bunch at once.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422392234", "createdAt": "2020-05-08T21:49:55Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MjYwNQ==", "bodyText": "This is the key fix for the balance problem. We actually balance the standby tasks after we assign them.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422392605", "createdAt": "2020-05-08T21:51:02Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MzEzMg==", "bodyText": "I had to make a few additions to the balance algorithm to adapt it to standbys. With actives, we know the tasks are unique, so we can just move them anywhere, but with standbys, there are more constraints, so now we loop over the tasks and only make legal movements. It nets out the same for active tasks.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422393132", "createdAt": "2020-05-08T21:52:38Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NDUwMg==", "bodyText": "This is a secondary fix. Once I started measuring the balance we're producing, a few other issues surfaced. so now, rather than just considering whether the source has more load than the destination, we also check to make sure that doing a movement would actually improve the skew, which it doesn't always.\nOne example that popped up was with two nodes both with two threads and five tasks overall. If the first had three, it would give one to the second, which would then give it right back, etc., for an infinite loop. Now, it terminates because we detect that the overall balance is the same whether the first node gives a task to the second or keeps it.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422394502", "createdAt": "2020-05-08T21:56:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();\n+        final double proposedAssignedTasksPerStreamThreadAtSource =\n+            (sourceClientState.assignedTasks().size() - 1.0) / sourceClientState.capacity();\n+        final double proposedSkew = proposedAssignedTasksPerStreamThreadAtSource - proposedAssignedTasksPerStreamThreadAtDestination;\n+\n+        if (proposedSkew < 0) {\n+            // then the move would only create an imbalance in the other direction.\n+            return false;\n+        }\n+        // we should only move a task if doing so would actually improve the skew.\n+        return proposedSkew < skew;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NDk4Nw==", "bodyText": "This was yet another minor balance problem that came up. Stateless tasks should only consider the active task load when computing their balance.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422394987", "createdAt": "2020-05-08T21:58:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();\n+        final double proposedAssignedTasksPerStreamThreadAtSource =\n+            (sourceClientState.assignedTasks().size() - 1.0) / sourceClientState.capacity();\n+        final double proposedSkew = proposedAssignedTasksPerStreamThreadAtSource - proposedAssignedTasksPerStreamThreadAtDestination;\n+\n+        if (proposedSkew < 0) {\n+            // then the move would only create an imbalance in the other direction.\n+            return false;\n+        }\n+        // we should only move a task if doing so would actually improve the skew.\n+        return proposedSkew < skew;\n+    }\n+\n+    private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState> clientStates,\n+                                                   final Iterable<TaskId> statelessTasks) {\n+        final ConstrainedPrioritySet statelessActiveTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> true,\n+            client -> {\n+                final ClientState clientState = clientStates.get(client);\n+                final double activeTaskLoad = 1.0 * clientState.activeTasks().size() / clientState.capacity();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NTM3MA==", "bodyText": "Needed these for method references below.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422395370", "createdAt": "2020-05-08T21:59:21Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -40,6 +46,14 @@ private TaskMovement(final TaskId task, final UUID destination, final SortedSet<\n         }\n     }\n \n+    private TaskId task() {\n+        return task;\n+    }\n+\n+    private int numCaughtUpClients() {\n+        return caughtUpClients.size();\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NTU5MQ==", "bodyText": "Here are the method references, and the nifty comparator composition thingy.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422395591", "createdAt": "2020-05-08T22:00:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n                                        final Map<UUID, ClientState> clientStates,\n-                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n                                        final int maxWarmupReplicas) {\n-        boolean warmupReplicasAssigned = false;\n+        final BiFunction<UUID, TaskId, Boolean> caughtUpPredicate =\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n \n-        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        final ConstrainedPrioritySet clientsByTaskLoad = new ConstrainedPrioritySet(\n+            caughtUpPredicate,\n+            client -> clientStates.get(client).taskLoad()\n         );\n \n-        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n-            (movement, other) -> {\n-                final int numCaughtUpClients = movement.caughtUpClients.size();\n-                final int otherNumCaughtUpClients = other.caughtUpClients.size();\n-                if (numCaughtUpClients != otherNumCaughtUpClients) {\n-                    return Integer.compare(numCaughtUpClients, otherNumCaughtUpClients);\n-                } else {\n-                    return movement.task.compareTo(other.task);\n-                }\n-            }\n+        final Queue<TaskMovement> taskMovements = new PriorityQueue<>(\n+            Comparator.comparing(TaskMovement::numCaughtUpClients).thenComparing(TaskMovement::task)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NjA0Nw==", "bodyText": "This logic has changed a little because now we're dealing with an assignment that has active and stateful tasks in it. But the basic algorithm is the same. Hopefully, the code comments clarify everything.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422396047", "createdAt": "2020-05-08T22:01:23Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNTQyNg==", "bodyText": "Here's where we try to re-use existing standbys first. If there's not a good candidate, we just settle for any client below.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422405426", "createdAt": "2020-05-08T22:32:59Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n                                        final Map<UUID, ClientState> clientStates,\n-                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n                                        final int maxWarmupReplicas) {\n-        boolean warmupReplicasAssigned = false;\n+        final BiFunction<UUID, TaskId, Boolean> caughtUpPredicate =\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n \n-        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        final ConstrainedPrioritySet clientsByTaskLoad = new ConstrainedPrioritySet(\n+            caughtUpPredicate,\n+            client -> clientStates.get(client).taskLoad()\n         );\n \n-        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n-            (movement, other) -> {\n-                final int numCaughtUpClients = movement.caughtUpClients.size();\n-                final int otherNumCaughtUpClients = other.caughtUpClients.size();\n-                if (numCaughtUpClients != otherNumCaughtUpClients) {\n-                    return Integer.compare(numCaughtUpClients, otherNumCaughtUpClients);\n-                } else {\n-                    return movement.task.compareTo(other.task);\n-                }\n-            }\n+        final Queue<TaskMovement> taskMovements = new PriorityQueue<>(\n+            Comparator.comparing(TaskMovement::numCaughtUpClients).thenComparing(TaskMovement::task)\n         );\n \n-        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID client = assignmentEntry.getKey();\n-            final ClientState state = clientStates.get(client);\n-            for (final TaskId task : assignmentEntry.getValue()) {\n-                if (taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n-                    state.assignActive(task);\n-                } else {\n-                    final TaskMovement taskMovement = new TaskMovement(task, client, tasksToCaughtUpClients.get(task));\n-                    taskMovements.add(taskMovement);\n+        for (final Map.Entry<UUID, ClientState> clientStateEntry : clientStates.entrySet()) {\n+            final UUID client = clientStateEntry.getKey();\n+            final ClientState state = clientStateEntry.getValue();\n+            for (final TaskId task : state.activeTasks()) {\n+                // if the desired client is not caught up, and there is another client that _is_ caught up, then\n+                // we schedule a movement, so we can move the active task to the caught-up client. We'll try to\n+                // assign a warm-up to the desired client so that we can move it later on.\n+                if (!taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n+                    taskMovements.add(new TaskMovement(task, client, tasksToCaughtUpClients.get(task)));\n                 }\n             }\n             clientsByTaskLoad.offer(client);\n         }\n \n+        final boolean movementsNeeded = !taskMovements.isEmpty();\n+\n         final AtomicInteger remainingWarmupReplicas = new AtomicInteger(maxWarmupReplicas);\n         for (final TaskMovement movement : taskMovements) {\n-            final UUID sourceClient = clientsByTaskLoad.poll(movement.task);\n-            if (sourceClient == null) {\n-                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");\n-            }\n-\n-            final ClientState sourceClientState = clientStates.get(sourceClient);\n-            sourceClientState.assignActive(movement.task);\n-            clientsByTaskLoad.offer(sourceClient);\n+            final UUID standbySourceClient = clientsByTaskLoad.poll(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNjE3NQ==", "bodyText": "I'm not sure where this came from. I'll fix it.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422406175", "createdAt": "2020-05-08T22:35:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java", "diffHunk": "@@ -632,7 +632,7 @@ public void shouldAllowConcurrentAccesses() throws Exception {\n         ProducerRunnable producerRunnable = new ProducerRunnable(streamThree, inputValues, 1);\n         producerRunnable.run();\n \n-        producerRunnable = new ProducerRunnable(streamConcurrent, inputValues, numIterations - 1);\n+        producerRunnable = new ProducerRunnable(streamConcurrent, inputValues, numIterations);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNjUxNg==", "bodyText": "I moved these validations from the TaskAssignorConvergence test so that we can also use them to make assertions about the correctness of our assignor implementations without having to expect specific assignments everywhere.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422406516", "createdAt": "2020-05-08T22:37:03Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMDMzNw==", "bodyText": "After wrestling a bit with the balance definitions I proposed earlier, I think it makes sense to define three kinds of balance:\n\noverall stateful tasks per thread (including both active and standby)\noverall active task per thread (including both stateful active and stateless)\ntask-parallelism (so that the number of partitions of the same task is as distributed as possible over the instances)\n\nWe can generally require both stateful and active task balance, but task parallelism may get sacrificed in favor of the other two at times. It's beyond the scope of this assignor to try and optimize these tradeoffs, so we just don't worry too much about task-parallel balance when it's overconstrained.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422410337", "createdAt": "2020-05-08T22:52:23Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        assertValidAssignment(\n+            numStandbyReplicas,\n+            0,\n+            statefulTasks,\n+            statelessTasks,\n+            assignedStates,\n+            failureContext\n+        );\n+    }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final int maxWarmupReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        final Map<TaskId, Set<UUID>> assignments = new TreeMap<>();\n+        for (final TaskId taskId : statefulTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final TaskId taskId : statelessTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final Map.Entry<UUID, ClientState> entry : assignedStates.entrySet()) {\n+            validateAndAddActiveAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+            validateAndAddStandbyAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+        }\n+\n+        final AtomicInteger remainingWarmups = new AtomicInteger(maxWarmupReplicas);\n+\n+        final TreeMap<TaskId, Set<UUID>> misassigned =\n+            assignments\n+                .entrySet()\n+                .stream()\n+                .filter(entry -> {\n+                    final int expectedActives = 1;\n+                    final boolean isStateless = statelessTasks.contains(entry.getKey());\n+                    final int expectedStandbys = isStateless ? 0 : numStandbyReplicas;\n+                    // We'll never assign even the expected number of standbys if they don't actually fit in the cluster\n+                    final int expectedAssignments = Math.min(\n+                        assignedStates.size(),\n+                        expectedActives + expectedStandbys\n+                    );\n+                    final int actualAssignments = entry.getValue().size();\n+                    if (actualAssignments == expectedAssignments) {\n+                        return false; // not misassigned\n+                    } else {\n+                        if (actualAssignments == expectedAssignments + 1 && remainingWarmups.get() > 0) {\n+                            remainingWarmups.getAndDecrement();\n+                            return false; // it's a warmup, so it's fine\n+                        } else {\n+                            return true; // misassigned\n+                        }\n+                    }\n+                })\n+                .collect(entriesToMap(TreeMap::new));\n+\n+        if (!misassigned.isEmpty()) {\n+            assertThat(\n+                new StringBuilder().append(\"Found some over- or under-assigned tasks in the final assignment with \")\n+                                   .append(numStandbyReplicas)\n+                                   .append(\" and max warmups \")\n+                                   .append(maxWarmupReplicas)\n+                                   .append(\" standby replicas, stateful tasks:\")\n+                                   .append(statefulTasks)\n+                                   .append(\", and stateless tasks:\")\n+                                   .append(statelessTasks)\n+                                   .append(failureContext)\n+                                   .toString(),\n+                misassigned,\n+                is(emptyMap()));\n+        }\n+    }\n+\n+    private static void validateAndAddStandbyAssignments(final Set<TaskId> statefulTasks,\n+                                                         final Set<TaskId> statelessTasks,\n+                                                         final StringBuilder failureContext,\n+                                                         final Map<TaskId, Set<UUID>> assignments,\n+                                                         final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId standbyTask : entry.getValue().standbyTasks()) {\n+            if (statelessTasks.contains(standbyTask)) {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found a standby task for stateless task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            } else if (assignments.containsKey(standbyTask)) {\n+                assignments.get(standbyTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra standby task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    private static void validateAndAddActiveAssignments(final Set<TaskId> statefulTasks,\n+                                                        final Set<TaskId> statelessTasks,\n+                                                        final StringBuilder failureContext,\n+                                                        final Map<TaskId, Set<UUID>> assignments,\n+                                                        final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId activeTask : entry.getValue().activeTasks()) {\n+            if (assignments.containsKey(activeTask)) {\n+                assignments.get(activeTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra active task \")\n+                                       .append(activeTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(\" and stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    static void assertBalancedStatefulAssignment(final Set<TaskId> allStatefulTasks,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMDc5NQ==", "bodyText": "I added a few matchers so that we can get more informative errors when our assertions fail.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422410795", "createdAt": "2020-05-08T22:54:16Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        assertValidAssignment(\n+            numStandbyReplicas,\n+            0,\n+            statefulTasks,\n+            statelessTasks,\n+            assignedStates,\n+            failureContext\n+        );\n+    }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final int maxWarmupReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        final Map<TaskId, Set<UUID>> assignments = new TreeMap<>();\n+        for (final TaskId taskId : statefulTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final TaskId taskId : statelessTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final Map.Entry<UUID, ClientState> entry : assignedStates.entrySet()) {\n+            validateAndAddActiveAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+            validateAndAddStandbyAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+        }\n+\n+        final AtomicInteger remainingWarmups = new AtomicInteger(maxWarmupReplicas);\n+\n+        final TreeMap<TaskId, Set<UUID>> misassigned =\n+            assignments\n+                .entrySet()\n+                .stream()\n+                .filter(entry -> {\n+                    final int expectedActives = 1;\n+                    final boolean isStateless = statelessTasks.contains(entry.getKey());\n+                    final int expectedStandbys = isStateless ? 0 : numStandbyReplicas;\n+                    // We'll never assign even the expected number of standbys if they don't actually fit in the cluster\n+                    final int expectedAssignments = Math.min(\n+                        assignedStates.size(),\n+                        expectedActives + expectedStandbys\n+                    );\n+                    final int actualAssignments = entry.getValue().size();\n+                    if (actualAssignments == expectedAssignments) {\n+                        return false; // not misassigned\n+                    } else {\n+                        if (actualAssignments == expectedAssignments + 1 && remainingWarmups.get() > 0) {\n+                            remainingWarmups.getAndDecrement();\n+                            return false; // it's a warmup, so it's fine\n+                        } else {\n+                            return true; // misassigned\n+                        }\n+                    }\n+                })\n+                .collect(entriesToMap(TreeMap::new));\n+\n+        if (!misassigned.isEmpty()) {\n+            assertThat(\n+                new StringBuilder().append(\"Found some over- or under-assigned tasks in the final assignment with \")\n+                                   .append(numStandbyReplicas)\n+                                   .append(\" and max warmups \")\n+                                   .append(maxWarmupReplicas)\n+                                   .append(\" standby replicas, stateful tasks:\")\n+                                   .append(statefulTasks)\n+                                   .append(\", and stateless tasks:\")\n+                                   .append(statelessTasks)\n+                                   .append(failureContext)\n+                                   .toString(),\n+                misassigned,\n+                is(emptyMap()));\n+        }\n+    }\n+\n+    private static void validateAndAddStandbyAssignments(final Set<TaskId> statefulTasks,\n+                                                         final Set<TaskId> statelessTasks,\n+                                                         final StringBuilder failureContext,\n+                                                         final Map<TaskId, Set<UUID>> assignments,\n+                                                         final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId standbyTask : entry.getValue().standbyTasks()) {\n+            if (statelessTasks.contains(standbyTask)) {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found a standby task for stateless task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            } else if (assignments.containsKey(standbyTask)) {\n+                assignments.get(standbyTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra standby task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    private static void validateAndAddActiveAssignments(final Set<TaskId> statefulTasks,\n+                                                        final Set<TaskId> statelessTasks,\n+                                                        final StringBuilder failureContext,\n+                                                        final Map<TaskId, Set<UUID>> assignments,\n+                                                        final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId activeTask : entry.getValue().activeTasks()) {\n+            if (assignments.containsKey(activeTask)) {\n+                assignments.get(activeTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra active task \")\n+                                       .append(activeTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(\" and stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    static void assertBalancedStatefulAssignment(final Set<TaskId> allStatefulTasks,\n+                                                 final Map<UUID, ClientState> clientStates,\n+                                                 final StringBuilder failureContext) {\n+        double maxStateful = Double.MIN_VALUE;\n+        double minStateful = Double.MAX_VALUE;\n+        for (final ClientState clientState : clientStates.values()) {\n+            final Set<TaskId> statefulTasks =\n+                intersection(HashSet::new, clientState.assignedTasks(), allStatefulTasks);\n+            final double statefulTaskLoad = 1.0 * statefulTasks.size() / clientState.capacity();\n+            maxStateful = Math.max(maxStateful, statefulTaskLoad);\n+            minStateful = Math.min(minStateful, statefulTaskLoad);\n+        }\n+        final double statefulDiff = maxStateful - minStateful;\n+\n+        if (statefulDiff > 1.0) {\n+            final StringBuilder builder = new StringBuilder()\n+                .append(\"detected a stateful assignment balance factor violation: \")\n+                .append(statefulDiff)\n+                .append(\">\")\n+                .append(1.0)\n+                .append(\" in: \");\n+            appendClientStates(builder, clientStates);\n+            fail(builder.append(failureContext).toString());\n+        }\n+    }\n+\n+    static void assertBalancedActiveAssignment(final Map<UUID, ClientState> clientStates,\n+                                               final StringBuilder failureContext) {\n+        double maxActive = Double.MIN_VALUE;\n+        double minActive = Double.MAX_VALUE;\n+        for (final ClientState clientState : clientStates.values()) {\n+            final double activeTaskLoad = 1.0 * clientState.activeTaskCount() / clientState.capacity();\n+            maxActive = Math.max(maxActive, activeTaskLoad);\n+            minActive = Math.min(minActive, activeTaskLoad);\n+        }\n+        final double activeDiff = maxActive - minActive;\n+        if (activeDiff > 1.0) {\n+            final StringBuilder builder = new StringBuilder()\n+                .append(\"detected an active assignment balance factor violation: \")\n+                .append(activeDiff)\n+                .append(\">\")\n+                .append(1.0)\n+                .append(\" in: \");\n+            appendClientStates(builder, clientStates);\n+            fail(builder.append(failureContext).toString());\n+        }\n+    }\n+\n+    static void assertBalancedTasks(final Map<UUID, ClientState> clientStates) {\n+        final TaskSkewReport taskSkewReport = analyzeTaskAssignmentBalance(clientStates);\n+        if (taskSkewReport.totalSkewedTasks() > 0) {\n+            fail(\"Expected a balanced task assignment, but was: \" + taskSkewReport);\n+        }\n+    }\n+\n+    static TaskSkewReport analyzeTaskAssignmentBalance(final Map<UUID, ClientState> clientStates) {\n+        final Function<Integer, Map<UUID, AtomicInteger>> initialClientCounts =\n+            i -> clientStates.keySet().stream().collect(Collectors.toMap(c -> c, c -> new AtomicInteger(0)));\n+\n+        final Map<Integer, Map<UUID, AtomicInteger>> subtopologyToClientsWithPartition = new TreeMap<>();\n+        for (final Map.Entry<UUID, ClientState> entry : clientStates.entrySet()) {\n+            final UUID client = entry.getKey();\n+            final ClientState clientState = entry.getValue();\n+            for (final TaskId task : clientState.activeTasks()) {\n+                final int subtopology = task.topicGroupId;\n+                subtopologyToClientsWithPartition\n+                    .computeIfAbsent(subtopology, initialClientCounts)\n+                    .get(client)\n+                    .incrementAndGet();\n+            }\n+        }\n+\n+        int maxTaskSkew = 0;\n+        final Set<Integer> skewedSubtopologies = new TreeSet<>();\n+\n+        for (final Map.Entry<Integer, Map<UUID, AtomicInteger>> entry : subtopologyToClientsWithPartition.entrySet()) {\n+            final Map<UUID, AtomicInteger> clientsWithPartition = entry.getValue();\n+            int max = Integer.MIN_VALUE;\n+            int min = Integer.MAX_VALUE;\n+            for (final AtomicInteger count : clientsWithPartition.values()) {\n+                max = Math.max(max, count.get());\n+                min = Math.min(min, count.get());\n+            }\n+            final int taskSkew = max - min;\n+            maxTaskSkew = Math.max(maxTaskSkew, taskSkew);\n+            if (taskSkew > 1) {\n+                skewedSubtopologies.add(entry.getKey());\n+            }\n+        }\n+\n+        return new TaskSkewReport(maxTaskSkew, skewedSubtopologies, subtopologyToClientsWithPartition);\n+    }\n+\n+    static Matcher<ClientState> hasActiveTasks(final int taskCount) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 287}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMTM0NA==", "bodyText": "This is the renamed test from the ValidClientPriorityQueue. The assertions are basically the same, but now we don't need to construct a full ClientState map, we can just craft a \"weight function\" for each assertion.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422411344", "createdAt": "2020-05-08T22:56:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySetTest.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Test;\n+\n+import java.util.UUID;\n+import java.util.function.BiFunction;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singleton;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+public class ConstrainedPrioritySetTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMTUwMQ==", "bodyText": "These tests all migrated into the HATATest.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422411501", "createdAt": "2020-05-08T22:57:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignorTest.java", "diffHunk": "@@ -1,295 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n-\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-import java.util.TreeSet;\n-\n-import static org.apache.kafka.common.utils.Utils.mkEntry;\n-import static org.apache.kafka.common.utils.Utils.mkMap;\n-import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.is;\n-\n-public class DefaultBalancedAssignorTest {\n-    private static final SortedSet<UUID> TWO_CLIENTS = new TreeSet<>(Arrays.asList(UUID_1, UUID_2));\n-    private static final SortedSet<UUID> THREE_CLIENTS = new TreeSet<>(Arrays.asList(UUID_1, UUID_2, UUID_3));\n-\n-    @Test\n-    public void shouldAssignTasksEvenlyOverClientsWhereNumberOfClientsIntegralDivisorOfNumberOfTasks() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMTk3Mg==", "bodyText": "And here they are! I adjusted the assertions to generally be less sensitive to the specific assignment, since my changes to the balance function had some trivial effects on the assignment, IIRC.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422411972", "createdAt": "2020-05-08T22:59:18Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -73,6 +77,164 @@\n         /*probingRebalanceIntervalMs*/ 60 * 1000L\n     );\n \n+    @Test\n+    public void shouldAssignActiveStatefulTasksEvenlyOverClientsWhereNumberOfClientsIntegralDivisorOfNumberOfTasks() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMjIxNw==", "bodyText": "Since the computeBalanceFactor method was only used by these tests, I removed both it and the tests.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422412217", "createdAt": "2020-05-08T23:00:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -125,89 +296,10 @@ public void shouldComputeNewAssignmentIfActiveTasksWasNotOnCaughtUpClient() {\n         // we'll warm up task 0_0 on client1 because it's first in sorted order,\n         // although this isn't an optimal convergence\n         assertThat(probingRebalanceNeeded, is(true));\n-    }\n-\n-    @Test\n-    public void shouldComputeBalanceFactorAsDifferenceBetweenMostAndLeastLoadedClients() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMzA0OA==", "bodyText": "I was able to reproduce some more edge cases by not creating exactly 4 partitions per subtopology. Now, we randomly generate the number of partitions for each subtopology. There's a supplier instead of a PRNG because for the specific scenarios, we still want a fixed partition scheme.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422413048", "createdAt": "2020-05-08T23:04:08Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -45,36 +46,29 @@\n \n         private static Harness initializeCluster(final int numStatelessTasks,\n                                                  final int numStatefulTasks,\n-                                                 final int numNodes) {\n+                                                 final int numNodes,\n+                                                 final Supplier<Integer> partitionCountSupplier) {\n             int subtopology = 0;\n             final Set<TaskId> statelessTasks = new TreeSet<>();\n-            {\n-                int partition = 0;\n-                for (int i = 0; i < numStatelessTasks; i++) {\n-                    statelessTasks.add(new TaskId(subtopology, partition));\n-                    if (partition == 4) {\n-                        subtopology++;\n-                        partition = 0;\n-                    } else {\n-                        partition++;\n-                    }\n+            int remainingStatelessTasks = numStatelessTasks;\n+            while (remainingStatelessTasks > 0) {\n+                final int partitions = Math.min(remainingStatelessTasks, partitionCountSupplier.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMzg3Ng==", "bodyText": "Not sure what I was thinking before. What I wanted was for all the key numbers to be coprime. Of course, the easiest way to do this is to only use prime numbers, but you might notice that 15 is not prime ;) Now it's fixed.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422413876", "createdAt": "2020-05-08T23:07:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -236,16 +233,17 @@ public void staticAssignmentShouldConvergeWithTheFirstAssignment() {\n                                                                 0,\n                                                                 1000L);\n \n-        final Harness harness = Harness.initializeCluster(1, 1, 1);\n+        final Harness harness = Harness.initializeCluster(1, 1, 1, () -> 1);\n \n         testForConvergence(harness, configs, 1);\n         verifyValidAssignment(0, harness);\n+        verifyBalancedAssignment(harness);\n     }\n \n     @Test\n     public void assignmentShouldConvergeAfterAddingNode() {\n-        final int numStatelessTasks = 15;\n-        final int numStatefulTasks = 13;\n+        final int numStatelessTasks = 7;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNTkyOA==", "bodyText": "The interface of assignTaskMovements has changed a little, since it no longer gets the assignments in a map and then applies them to the clientStates. Now, it just gets pre-assigned clientStates and mutates it. I think I've mostly faithfully reproduced the tests, but there were a few that I couldn't quite follow what they were previously doing.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422415928", "createdAt": "2020-05-08T23:17:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovementTest.java", "diffHunk": "@@ -35,262 +44,161 @@\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.getClientStatesMap;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.hasProperty;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.assertFalse;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.SortedSet;\n-import java.util.UUID;\n-import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.is;\n \n public class TaskMovementTest {\n-    private final ClientState client1 = new ClientState(1);\n-    private final ClientState client2 = new ClientState(1);\n-    private final ClientState client3 = new ClientState(1);\n-\n-    private final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n-\n-    private final Map<UUID, List<TaskId>> emptyWarmupAssignment = mkMap(\n-        mkEntry(UUID_1, EMPTY_TASK_LIST),\n-        mkEntry(UUID_2, EMPTY_TASK_LIST),\n-        mkEntry(UUID_3, EMPTY_TASK_LIST)\n-    );\n-\n     @Test\n     public void shouldAssignTasksToClientsAndReturnFalseWhenAllClientsCaughtUp() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n         final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n-\n         final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n         for (final TaskId task : allTasks) {\n             tasksToCaughtUpClients.put(task, mkSortedSet(UUID_1, UUID_2, UUID_3));\n         }\n-        \n-        assertFalse(\n+\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n+\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjI2Mg==", "bodyText": "I couldn't follow exactly what this test was doing, but it seemed like other tests already verify both balance and constrainedness, so I deleted this one. Not sure if that's the right call.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422416262", "createdAt": "2020-05-08T23:18:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovementTest.java", "diffHunk": "@@ -35,262 +44,161 @@\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.getClientStatesMap;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.hasProperty;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.assertFalse;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.SortedSet;\n-import java.util.UUID;\n-import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.is;\n \n public class TaskMovementTest {\n-    private final ClientState client1 = new ClientState(1);\n-    private final ClientState client2 = new ClientState(1);\n-    private final ClientState client3 = new ClientState(1);\n-\n-    private final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n-\n-    private final Map<UUID, List<TaskId>> emptyWarmupAssignment = mkMap(\n-        mkEntry(UUID_1, EMPTY_TASK_LIST),\n-        mkEntry(UUID_2, EMPTY_TASK_LIST),\n-        mkEntry(UUID_3, EMPTY_TASK_LIST)\n-    );\n-\n     @Test\n     public void shouldAssignTasksToClientsAndReturnFalseWhenAllClientsCaughtUp() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n         final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n-\n         final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n         for (final TaskId task : allTasks) {\n             tasksToCaughtUpClients.put(task, mkSortedSet(UUID_1, UUID_2, UUID_3));\n         }\n-        \n-        assertFalse(\n+\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n+\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)\n         );\n-\n-        verifyClientStateAssignments(balancedAssignment, emptyWarmupAssignment);\n     }\n \n     @Test\n     public void shouldAssignAllTasksToClientsAndReturnFalseIfNoClientsAreCaughtUp() {\n-        final int maxWarmupReplicas = 2;\n-        final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n+        final int maxWarmupReplicas = Integer.MAX_VALUE;\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n \n-        assertFalse(\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 emptyMap(),\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)\n         );\n-        verifyClientStateAssignments(balancedAssignment, emptyWarmupAssignment);\n     }\n \n     @Test\n     public void shouldMoveTasksToCaughtUpClientsAndAssignWarmupReplicasInTheirPlace() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n-        final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2);\n+        final ClientState client1 = getClientStateWithActiveAssignment(singletonList(TASK_0_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(singletonList(TASK_0_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(singletonList(TASK_0_2));\n+        final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, singletonList(TASK_0_0)),\n-            mkEntry(UUID_2, singletonList(TASK_0_1)),\n-            mkEntry(UUID_3, singletonList(TASK_0_2))\n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = mkMap(\n+            mkEntry(TASK_0_0, mkSortedSet(UUID_1)),\n+            mkEntry(TASK_0_1, mkSortedSet(UUID_3)),\n+            mkEntry(TASK_0_2, mkSortedSet(UUID_2))\n         );\n \n-        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n-        tasksToCaughtUpClients.put(TASK_0_0, mkSortedSet(UUID_1));\n-        tasksToCaughtUpClients.put(TASK_0_1, mkSortedSet(UUID_3));\n-        tasksToCaughtUpClients.put(TASK_0_2, mkSortedSet(UUID_2));\n-\n-        final Map<UUID, List<TaskId>> expectedActiveTaskAssignment = mkMap(\n-            mkEntry(UUID_1, singletonList(TASK_0_0)),\n-            mkEntry(UUID_2, singletonList(TASK_0_2)),\n-            mkEntry(UUID_3, singletonList(TASK_0_1))\n-        );\n-\n-        final Map<UUID, List<TaskId>> expectedWarmupTaskAssignment = mkMap(\n-            mkEntry(UUID_1, EMPTY_TASK_LIST),\n-            mkEntry(UUID_2, singletonList(TASK_0_1)),\n-            mkEntry(UUID_3, singletonList(TASK_0_2))\n-        );\n-\n-        assertTrue(\n+        assertThat(\n+            \"should have assigned movements\",\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n                 clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n-        );\n-        verifyClientStateAssignments(expectedActiveTaskAssignment, expectedWarmupTaskAssignment);\n-    }\n-\n-    @Test\n-    public void shouldProduceBalancedAndStateConstrainedAssignment() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjQ3Nw==", "bodyText": "This test is now the ConstrainedPrioritySetTest.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422416477", "createdAt": "2020-05-08T23:19:31Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueueTest.java", "diffHunk": "@@ -1,126 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-\n-import static java.util.Arrays.asList;\n-import static java.util.Collections.singletonList;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.getClientStatesMap;\n-import static org.hamcrest.CoreMatchers.equalTo;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertNull;\n-\n-import java.util.Map;\n-import java.util.UUID;\n-import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n-\n-public class ValidClientsByTaskLoadQueueTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5Nzk1MDQx", "url": "https://github.com/apache/kafka/pull/8588#pullrequestreview-409795041", "createdAt": "2020-05-12T08:04:32Z", "commit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "state": "COMMENTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwODowNDozM1rOGT61AA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMToyOTozNlrOGUCN-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzOTk2OA==", "bodyText": "req: Please add a test for the case where no other set is passed in, since it is not clear from the signature how the method should behave when only one set is passed in.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423539968", "createdAt": "2020-05-12T08:04:33Z", "author": {"login": "cadonna"}, "path": "clients/src/test/java/org/apache/kafka/common/utils/UtilsTest.java", "diffHunk": "@@ -597,4 +599,24 @@ public void testUnion() {\n         assertThat(union, is(mkSet(\"a\", \"b\", \"c\", \"d\", \"e\")));\n         assertThat(union.getClass(), equalTo(TreeSet.class));\n     }\n+\n+    @Test\n+    public void testIntersection() {\n+        final Set<String> oneSet = mkSet(\"a\", \"b\", \"c\");\n+        final Set<String> anotherSet = mkSet(\"c\", \"d\", \"e\");\n+        final Set<String> intersection = intersection(TreeSet::new, oneSet, anotherSet);\n+\n+        assertThat(intersection, is(mkSet(\"c\")));\n+        assertThat(intersection.getClass(), equalTo(TreeSet.class));\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU0NDc5NQ==", "bodyText": "prop: Wouldn't it be better to use the config names as labels instead of the names of the member fields?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return \"AssignmentConfigs{\" +\n          \n          \n            \n                            \"\\n  acceptableRecoveryLag=\" + acceptableRecoveryLag +\n          \n          \n            \n                            \"\\n  maxWarmupReplicas=\" + maxWarmupReplicas +\n          \n          \n            \n                            \"\\n  numStandbyReplicas=\" + numStandbyReplicas +\n          \n          \n            \n                            \"\\n  probingRebalanceIntervalMs=\" + probingRebalanceIntervalMs +\n          \n          \n            \n                            \"\\n}\";\n          \n          \n            \n                        return \"AssignmentConfigs{\" +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG + \" = \" + acceptableRecoveryLag +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.MAX_WARMUP_REPLICAS_CONFIG + \" = \" + maxWarmupReplicas +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG + \" = \" + numStandbyReplicas +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.PROBING_REBALANCE_INTERVAL_MS_CONFIG + \" = \" + probingRebalanceIntervalMs +\n          \n          \n            \n                            \"\\n}\";", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423544795", "createdAt": "2020-05-12T08:12:32Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -364,5 +364,15 @@ private AssignmentConfigs(final StreamsConfig configs) {\n             this.numStandbyReplicas = numStandbyReplicas;\n             this.probingRebalanceIntervalMs = probingRebalanceIntervalMs;\n         }\n+\n+        @Override\n+        public String toString() {\n+            return \"AssignmentConfigs{\" +\n+                \"\\n  acceptableRecoveryLag=\" + acceptableRecoveryLag +\n+                \"\\n  maxWarmupReplicas=\" + maxWarmupReplicas +\n+                \"\\n  numStandbyReplicas=\" + numStandbyReplicas +\n+                \"\\n  probingRebalanceIntervalMs=\" + probingRebalanceIntervalMs +\n+                \"\\n}\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU0ODA5NA==", "bodyText": "Fine with me.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423548094", "createdAt": "2020-05-12T08:17:43Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -56,34 +59,28 @@ public ClientState() {\n     }\n \n     ClientState(final int capacity) {\n-        this(new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashMap<>(),\n-             new HashMap<>(),\n-             new HashMap<>(),\n-             capacity);\n+        activeTasks = new TreeSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NTczOQ=="}, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3MDM0Ng==", "bodyText": "super-nit: I would just call it unassignActive.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423570346", "createdAt": "2020-05-12T08:51:28Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Nzc2NQ=="}, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3MTAyMg==", "bodyText": "see above about naming and testing", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423571022", "createdAt": "2020-05-12T08:52:27Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3NTA3NQ==", "bodyText": "req: Please add unit test to verify IllegalStateException.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423575075", "createdAt": "2020-05-12T08:58:34Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Njg1Nw=="}, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3NTE1Mg==", "bodyText": "req: Please add unit test to verify at least IllegalStateException.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423575152", "createdAt": "2020-05-12T08:58:42Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Nzc2NQ=="}, "originalCommit": {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3NTI1MQ==", "bodyText": "req: Please add unit test to verify at least IllegalStateException.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423575251", "createdAt": "2020-05-12T08:58:51Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU4NDk3MA==", "bodyText": "prop: Since we assert that active and stand-by tasks are disjunct, we could avoid to compute the union here and just sum up the sizes of activeTasks and standbyTasks.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423584970", "createdAt": "2020-05-12T09:14:03Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {\n+        if (!standbyTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign standby task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        standbyTasks.remove(task);\n     }\n \n-    void assignStandbyTasks(final Collection<TaskId> tasks) {\n-        standbyTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    public void assignActiveTasks(final Collection<TaskId> tasks) {\n+        activeTasks.addAll(tasks);\n     }\n \n     public Set<TaskId> activeTasks() {\n-        return activeTasks;\n+        return unmodifiableSet(activeTasks);\n     }\n \n     public Set<TaskId> standbyTasks() {\n-        return standbyTasks;\n+        return unmodifiableSet(standbyTasks);\n     }\n \n     Set<TaskId> prevActiveTasks() {\n-        return prevActiveTasks;\n+        return unmodifiableSet(prevActiveTasks);\n     }\n \n     Set<TaskId> prevStandbyTasks() {\n-        return prevStandbyTasks;\n+        return unmodifiableSet(prevStandbyTasks);\n     }\n \n     public Map<TopicPartition, String> ownedPartitions() {\n-        return ownedPartitions;\n+        return unmodifiableMap(ownedPartitions);\n     }\n \n-    @SuppressWarnings(\"WeakerAccess\")\n     public int assignedTaskCount() {\n-        return assignedTasks.size();\n+        return assignedTasks().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU4NTkwMw==", "bodyText": "req: I think here it would be better to call assignedTaskCount() instead of assignedTasks().size().", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423585903", "createdAt": "2020-05-12T09:15:29Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -254,11 +254,10 @@ long lagFor(final TaskId task) {\n \n     public void removeFromAssignment(final TaskId task) {\n         activeTasks.remove(task);\n-        assignedTasks.remove(task);\n     }\n \n     boolean reachedCapacity() {\n-        return assignedTasks.size() >= capacity;\n+        return assignedTasks().size() >= capacity;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5MDY1OQ==", "bodyText": "Is this still true? I see assignedTasks() used in HighAvailabilityTaskAssignor.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423590659", "createdAt": "2020-05-12T09:22:56Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -338,35 +337,37 @@ private void initializeRemainingPrevTasksFromTaskOffsetSums() {\n \n     private void addPreviousActiveTask(final TaskId task) {\n         prevActiveTasks.add(task);\n-        prevAssignedTasks.add(task);\n     }\n \n     private void addPreviousStandbyTask(final TaskId task) {\n         prevStandbyTasks.add(task);\n-        prevAssignedTasks.add(task);\n+    }\n+\n+    private void assertNotAssigned(final TaskId task) {\n+        if (standbyTasks.contains(task) || activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to assign task \" + task + \", but it is already assigned: \" + this);\n+        }\n     }\n \n     @Override\n     public String toString() {\n         return \"[activeTasks: (\" + activeTasks +\n             \") standbyTasks: (\" + standbyTasks +\n-            \") assignedTasks: (\" + assignedTasks +\n             \") prevActiveTasks: (\" + prevActiveTasks +\n             \") prevStandbyTasks: (\" + prevStandbyTasks +\n-            \") prevAssignedTasks: (\" + prevAssignedTasks +\n             \") prevOwnedPartitionsByConsumerId: (\" + ownedPartitions.keySet() +\n             \") changelogOffsetTotalsByTask: (\" + taskOffsetSums.entrySet() +\n             \") capacity: \" + capacity +\n+            \" assigned: \" + (activeTasks.size() + standbyTasks.size()) +\n             \"]\";\n     }\n \n     // Visible for testing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5MjMwNg==", "bodyText": "Just an idea: Is it worth to unit test them as an attempt to ensure that returned sets stay unmodifiable after possible refactorings?", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423592306", "createdAt": "2020-05-12T09:25:39Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {\n+        if (!standbyTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign standby task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        standbyTasks.remove(task);\n     }\n \n-    void assignStandbyTasks(final Collection<TaskId> tasks) {\n-        standbyTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    public void assignActiveTasks(final Collection<TaskId> tasks) {\n+        activeTasks.addAll(tasks);\n     }\n \n     public Set<TaskId> activeTasks() {\n-        return activeTasks;\n+        return unmodifiableSet(activeTasks);\n     }\n \n     public Set<TaskId> standbyTasks() {\n-        return standbyTasks;\n+        return unmodifiableSet(standbyTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNjIyNw==", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423616227", "createdAt": "2020-05-12T10:05:01Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 237}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNjUyMA==", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423616520", "createdAt": "2020-05-12T10:05:32Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();\n+        final double proposedAssignedTasksPerStreamThreadAtSource =\n+            (sourceClientState.assignedTasks().size() - 1.0) / sourceClientState.capacity();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNzI5Ng==", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423617296", "createdAt": "2020-05-12T10:06:50Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNzM4NQ==", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423617385", "createdAt": "2020-05-12T10:07:00Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYyMjM0OA==", "bodyText": "prop: I see assignedTasks().contains(task) a couple of times in the assignment algorithm. Should we provide a ClientState#containsAssignedTask() to avoid computing the union each time it is called?", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423622348", "createdAt": "2020-05-12T10:15:52Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY2MTA1MA==", "bodyText": "prop: It seems like we do not need statefulTasksToRankedCandidates anymore. We could directly build tasksToCaughtUpClients from statefulTasks, clients, and configs.acceptableRecoveryLag. Also in assignStandbyReplicaTasks(), we only use the keySet() of statefulTasksToRankedCandidates. So, we can remove statefulTasksToRankedCandidates.", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423661050", "createdAt": "2020-05-12T11:29:36Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 69}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c036caa6c06cb32e8811c918b0abb09767f658fe", "author": {"user": {"login": "vvcephei", "name": "John Roesler"}}, "url": "https://github.com/apache/kafka/commit/c036caa6c06cb32e8811c918b0abb09767f658fe", "committedDate": "2020-05-12T18:15:35Z", "message": "Merge branch 'trunk' into kafka-6145-validate-balanced-assignment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b0655e27b98733d69d4767b37dd0f570419c1a6", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/6b0655e27b98733d69d4767b37dd0f570419c1a6", "committedDate": "2020-05-12T19:16:07Z", "message": "more util tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7026acf24da182171ebf4bdb62aa08cc40cc75da", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/7026acf24da182171ebf4bdb62aa08cc40cc75da", "committedDate": "2020-05-12T19:22:10Z", "message": "unassign"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d443ca845e85631faed2644754b73b5a74f0ff2", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/0d443ca845e85631faed2644754b73b5a74f0ff2", "committedDate": "2020-05-12T20:04:23Z", "message": "CR regarding ClientState"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ddcbd7b56075581de8c22968de3c90c6efc299c", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/4ddcbd7b56075581de8c22968de3c90c6efc299c", "committedDate": "2020-05-12T20:17:09Z", "message": "remove RankedClient"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8bf161b94198a536d812ad553f0f92d75d0f154c", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/8bf161b94198a536d812ad553f0f92d75d0f154c", "committedDate": "2020-05-12T20:23:19Z", "message": "CR cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExMDIwMDAw", "url": "https://github.com/apache/kafka/pull/8588#pullrequestreview-411020000", "createdAt": "2020-05-13T15:04:50Z", "commit": {"oid": "8bf161b94198a536d812ad553f0f92d75d0f154c"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNTowNDo1MVrOGU2GgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNTowNDo1MVrOGU2GgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxMTEwNQ==", "bodyText": "Q: Why do we need this additional check?", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424511105", "createdAt": "2020-05-13T15:04:51Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        assertValidAssignment(\n+            numStandbyReplicas,\n+            0,\n+            statefulTasks,\n+            statelessTasks,\n+            assignedStates,\n+            failureContext\n+        );\n+    }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final int maxWarmupReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        final Map<TaskId, Set<UUID>> assignments = new TreeMap<>();\n+        for (final TaskId taskId : statefulTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final TaskId taskId : statelessTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final Map.Entry<UUID, ClientState> entry : assignedStates.entrySet()) {\n+            validateAndAddActiveAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+            validateAndAddStandbyAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+        }\n+\n+        final AtomicInteger remainingWarmups = new AtomicInteger(maxWarmupReplicas);\n+\n+        final TreeMap<TaskId, Set<UUID>> misassigned =\n+            assignments\n+                .entrySet()\n+                .stream()\n+                .filter(entry -> {\n+                    final int expectedActives = 1;\n+                    final boolean isStateless = statelessTasks.contains(entry.getKey());\n+                    final int expectedStandbys = isStateless ? 0 : numStandbyReplicas;\n+                    // We'll never assign even the expected number of standbys if they don't actually fit in the cluster\n+                    final int expectedAssignments = Math.min(\n+                        assignedStates.size(),\n+                        expectedActives + expectedStandbys\n+                    );\n+                    final int actualAssignments = entry.getValue().size();\n+                    if (actualAssignments == expectedAssignments) {\n+                        return false; // not misassigned\n+                    } else {\n+                        if (actualAssignments == expectedAssignments + 1 && remainingWarmups.get() > 0) {\n+                            remainingWarmups.getAndDecrement();\n+                            return false; // it's a warmup, so it's fine\n+                        } else {\n+                            return true; // misassigned\n+                        }\n+                    }\n+                })\n+                .collect(entriesToMap(TreeMap::new));\n+\n+        if (!misassigned.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8bf161b94198a536d812ad553f0f92d75d0f154c"}, "originalPosition": 118}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExMjk3OTA1", "url": "https://github.com/apache/kafka/pull/8588#pullrequestreview-411297905", "createdAt": "2020-05-13T21:03:54Z", "commit": {"oid": "8bf161b94198a536d812ad553f0f92d75d0f154c"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMTowMzo1NFrOGVDczg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMTo0NToxOVrOGVEpCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDcyOTgwNg==", "bodyText": "I'm not sure I see how the returned clients could ever be different using \"poll N clients\" vs \"poll N times\". Only the clients which are getting a new task assigned will have their weight changed while in the middle of an N poll, and once we assign this task to that client it no longer meets the criteria so we don't care about it anyway right?\nThe reason for the \"poll N clients\" method was to save on some of the poll-and-reoffer of clients that don't meet the criteria, but I don't think that's really worth worrying over. I'm fine with whatever code is easiest to read -- just want to understand why this affects the balance?", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424729806", "createdAt": "2020-05-13T21:03:54Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));\n     }\n \n     /**\n      * @return the next least loaded client that satisfies the given criteria, or null if none do\n      */\n-    UUID poll(final TaskId task) {\n-        final List<UUID> validClient = poll(task, 1);\n-        return validClient.isEmpty() ? null : validClient.get(0);\n-    }\n-\n-    /**\n-     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n-     */\n-    List<UUID> poll(final TaskId task, final int numClients) {\n-        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+    UUID poll(final TaskId task, final Function<UUID, Boolean> extraConstraint) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDA3Ng=="}, "originalCommit": {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDczNjY0Ng==", "bodyText": "nit: I know I named this in the first place but can we change it to caughtUpClientsByTaskLoad or something?", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424736646", "createdAt": "2020-05-13T21:17:44Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n                                        final Map<UUID, ClientState> clientStates,\n-                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n                                        final int maxWarmupReplicas) {\n-        boolean warmupReplicasAssigned = false;\n+        final BiFunction<UUID, TaskId, Boolean> caughtUpPredicate =\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n \n-        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        final ConstrainedPrioritySet clientsByTaskLoad = new ConstrainedPrioritySet(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8bf161b94198a536d812ad553f0f92d75d0f154c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0NTc5MA==", "bodyText": "IIRC this was covering an edge case where it might produce an unbalanced assignment. But it may be moot at this point (and besides, we don't necessarily need to produce a perfectly balanced assignment here)", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424745790", "createdAt": "2020-05-13T21:37:36Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovementTest.java", "diffHunk": "@@ -35,262 +44,161 @@\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.getClientStatesMap;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.hasProperty;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.assertFalse;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.SortedSet;\n-import java.util.UUID;\n-import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.is;\n \n public class TaskMovementTest {\n-    private final ClientState client1 = new ClientState(1);\n-    private final ClientState client2 = new ClientState(1);\n-    private final ClientState client3 = new ClientState(1);\n-\n-    private final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n-\n-    private final Map<UUID, List<TaskId>> emptyWarmupAssignment = mkMap(\n-        mkEntry(UUID_1, EMPTY_TASK_LIST),\n-        mkEntry(UUID_2, EMPTY_TASK_LIST),\n-        mkEntry(UUID_3, EMPTY_TASK_LIST)\n-    );\n-\n     @Test\n     public void shouldAssignTasksToClientsAndReturnFalseWhenAllClientsCaughtUp() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n         final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n-\n         final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n         for (final TaskId task : allTasks) {\n             tasksToCaughtUpClients.put(task, mkSortedSet(UUID_1, UUID_2, UUID_3));\n         }\n-        \n-        assertFalse(\n+\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n+\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)\n         );\n-\n-        verifyClientStateAssignments(balancedAssignment, emptyWarmupAssignment);\n     }\n \n     @Test\n     public void shouldAssignAllTasksToClientsAndReturnFalseIfNoClientsAreCaughtUp() {\n-        final int maxWarmupReplicas = 2;\n-        final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n+        final int maxWarmupReplicas = Integer.MAX_VALUE;\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n \n-        assertFalse(\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 emptyMap(),\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)\n         );\n-        verifyClientStateAssignments(balancedAssignment, emptyWarmupAssignment);\n     }\n \n     @Test\n     public void shouldMoveTasksToCaughtUpClientsAndAssignWarmupReplicasInTheirPlace() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n-        final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2);\n+        final ClientState client1 = getClientStateWithActiveAssignment(singletonList(TASK_0_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(singletonList(TASK_0_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(singletonList(TASK_0_2));\n+        final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, singletonList(TASK_0_0)),\n-            mkEntry(UUID_2, singletonList(TASK_0_1)),\n-            mkEntry(UUID_3, singletonList(TASK_0_2))\n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = mkMap(\n+            mkEntry(TASK_0_0, mkSortedSet(UUID_1)),\n+            mkEntry(TASK_0_1, mkSortedSet(UUID_3)),\n+            mkEntry(TASK_0_2, mkSortedSet(UUID_2))\n         );\n \n-        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n-        tasksToCaughtUpClients.put(TASK_0_0, mkSortedSet(UUID_1));\n-        tasksToCaughtUpClients.put(TASK_0_1, mkSortedSet(UUID_3));\n-        tasksToCaughtUpClients.put(TASK_0_2, mkSortedSet(UUID_2));\n-\n-        final Map<UUID, List<TaskId>> expectedActiveTaskAssignment = mkMap(\n-            mkEntry(UUID_1, singletonList(TASK_0_0)),\n-            mkEntry(UUID_2, singletonList(TASK_0_2)),\n-            mkEntry(UUID_3, singletonList(TASK_0_1))\n-        );\n-\n-        final Map<UUID, List<TaskId>> expectedWarmupTaskAssignment = mkMap(\n-            mkEntry(UUID_1, EMPTY_TASK_LIST),\n-            mkEntry(UUID_2, singletonList(TASK_0_1)),\n-            mkEntry(UUID_3, singletonList(TASK_0_2))\n-        );\n-\n-        assertTrue(\n+        assertThat(\n+            \"should have assigned movements\",\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n                 clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n-        );\n-        verifyClientStateAssignments(expectedActiveTaskAssignment, expectedWarmupTaskAssignment);\n-    }\n-\n-    @Test\n-    public void shouldProduceBalancedAndStateConstrainedAssignment() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjI2Mg=="}, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0OTMyMQ==", "bodyText": "This is a really nice touch \ud83d\udc4d Although without attempting some degree of stickiness in the standby task assignment it seems unlikely to actually find a standby on a caught-up client..", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424749321", "createdAt": "2020-05-13T21:45:19Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n                                        final Map<UUID, ClientState> clientStates,\n-                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n                                        final int maxWarmupReplicas) {\n-        boolean warmupReplicasAssigned = false;\n+        final BiFunction<UUID, TaskId, Boolean> caughtUpPredicate =\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n \n-        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        final ConstrainedPrioritySet clientsByTaskLoad = new ConstrainedPrioritySet(\n+            caughtUpPredicate,\n+            client -> clientStates.get(client).taskLoad()\n         );\n \n-        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n-            (movement, other) -> {\n-                final int numCaughtUpClients = movement.caughtUpClients.size();\n-                final int otherNumCaughtUpClients = other.caughtUpClients.size();\n-                if (numCaughtUpClients != otherNumCaughtUpClients) {\n-                    return Integer.compare(numCaughtUpClients, otherNumCaughtUpClients);\n-                } else {\n-                    return movement.task.compareTo(other.task);\n-                }\n-            }\n+        final Queue<TaskMovement> taskMovements = new PriorityQueue<>(\n+            Comparator.comparing(TaskMovement::numCaughtUpClients).thenComparing(TaskMovement::task)\n         );\n \n-        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID client = assignmentEntry.getKey();\n-            final ClientState state = clientStates.get(client);\n-            for (final TaskId task : assignmentEntry.getValue()) {\n-                if (taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n-                    state.assignActive(task);\n-                } else {\n-                    final TaskMovement taskMovement = new TaskMovement(task, client, tasksToCaughtUpClients.get(task));\n-                    taskMovements.add(taskMovement);\n+        for (final Map.Entry<UUID, ClientState> clientStateEntry : clientStates.entrySet()) {\n+            final UUID client = clientStateEntry.getKey();\n+            final ClientState state = clientStateEntry.getValue();\n+            for (final TaskId task : state.activeTasks()) {\n+                // if the desired client is not caught up, and there is another client that _is_ caught up, then\n+                // we schedule a movement, so we can move the active task to the caught-up client. We'll try to\n+                // assign a warm-up to the desired client so that we can move it later on.\n+                if (!taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n+                    taskMovements.add(new TaskMovement(task, client, tasksToCaughtUpClients.get(task)));\n                 }\n             }\n             clientsByTaskLoad.offer(client);\n         }\n \n+        final boolean movementsNeeded = !taskMovements.isEmpty();\n+\n         final AtomicInteger remainingWarmupReplicas = new AtomicInteger(maxWarmupReplicas);\n         for (final TaskMovement movement : taskMovements) {\n-            final UUID sourceClient = clientsByTaskLoad.poll(movement.task);\n-            if (sourceClient == null) {\n-                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");\n-            }\n-\n-            final ClientState sourceClientState = clientStates.get(sourceClient);\n-            sourceClientState.assignActive(movement.task);\n-            clientsByTaskLoad.offer(sourceClient);\n+            final UUID standbySourceClient = clientsByTaskLoad.poll(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNTQyNg=="}, "originalCommit": {"oid": "cbd08807fb74217207502606cfb86325515077f5"}, "originalPosition": 113}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cf8316444c205ea04f6fa0874619451f5b94d92", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/9cf8316444c205ea04f6fa0874619451f5b94d92", "committedDate": "2020-05-14T01:52:04Z", "message": "cr feedback"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1291, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}