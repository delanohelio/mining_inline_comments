{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4OTM5NjE0", "number": 9020, "title": "KAFKA-10271 Performance regression while fetching a key from a single partition", "bodyText": "StreamThreadStateStoreProvider excessive loop over calling internalTopologyBuilder.topicGroups(), which is synchronized, thus causing significant performance degradation to the caller, especially when store has many partitions.\nhttps://issues.apache.org/jira/browse/KAFKA-10271", "createdAt": "2020-07-14T15:05:22Z", "url": "https://github.com/apache/kafka/pull/9020", "merged": true, "mergeCommit": {"oid": "cc54000e722f2b258ad6e04643189327bebc5b77"}, "closed": true, "closedAt": "2020-10-08T17:12:34Z", "author": {"login": "dima5rr"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc03KfhgH2gAyNDQ4OTM5NjE0OjJmYjk2NmZmOWQzZjAzMGMxYjZjNTRlNTJiY2VlODAzMmZhZGE1ZmI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQdf-qAH2gAyNDQ4OTM5NjE0OjA0OWVjMjI5NzQ1ODBhY2QwMjNmNWQwNmU0Y2IzNGJmZDRhZDkzNWU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2fb966ff9d3f030c1b6c54e52bcee8032fada5fb", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/2fb966ff9d3f030c1b6c54e52bcee8032fada5fb", "committedDate": "2020-07-14T14:46:23Z", "message": "Performance issue, in case of withPartition parameter exists - do not return all state stores"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4MjYxNjEz", "url": "https://github.com/apache/kafka/pull/9020#pullrequestreview-448261613", "createdAt": "2020-07-14T16:18:45Z", "commit": {"oid": "2fb966ff9d3f030c1b6c54e52bcee8032fada5fb"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "839379de35fe41c044f84aeffcf414a3c3bb91ff", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/839379de35fe41c044f84aeffcf414a3c3bb91ff", "committedDate": "2020-07-15T11:35:55Z", "message": "Find task in-place, avoid synchorinized calling to topicsGroup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a15d28f518bdd008b393d9f280341da6fb564b56", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/a15d28f518bdd008b393d9f280341da6fb564b56", "committedDate": "2020-07-15T11:45:12Z", "message": "Find task in-place, avoid synchorinized calling to topicsGroup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1802752ed407e8a4e2a0ff00ea792b1466b3906", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/c1802752ed407e8a4e2a0ff00ea792b1466b3906", "committedDate": "2020-07-15T11:50:17Z", "message": "Find task in-place, avoid synchorinized calling to topicsGroup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4209ce29861c1fea6cfe6628ed54da7e772e7f8f", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/4209ce29861c1fea6cfe6628ed54da7e772e7f8f", "committedDate": "2020-07-15T15:54:45Z", "message": "Find task in-place, avoid synchorinized calling to topicsGroup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5f2e46fbd9ec3f39c6c1f140d220a6052fdccd4", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/f5f2e46fbd9ec3f39c6c1f140d220a6052fdccd4", "committedDate": "2020-07-15T17:01:00Z", "message": "remove extra loops over all stores of all providers as a sanity check before returning the WrappingStoreProvider"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7d38c6ed55dae174de77de429cb94fa1c26344c", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/b7d38c6ed55dae174de77de429cb94fa1c26344c", "committedDate": "2020-07-15T17:30:28Z", "message": "remove extra loops over all stores of all providers as a sanity check before returning the WrappingStoreProvider"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5OTk3MjYw", "url": "https://github.com/apache/kafka/pull/9020#pullrequestreview-449997260", "createdAt": "2020-07-16T16:05:54Z", "commit": {"oid": "b7d38c6ed55dae174de77de429cb94fa1c26344c"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNjowNTo1NFrOGyx-Ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNjoxMTo0MFrOGyyN8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg==", "bodyText": "Is this really a different condition than the one on L65? It seems like the failure is still probably that the store \"migrated\" instead of \"doesn't exist\", right?", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455900726", "createdAt": "2020-07-16T16:05:54Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java", "diffHunk": "@@ -46,11 +46,22 @@ public void setStoreQueryParameters(final StoreQueryParameters storeQueryParamet\n     public <T> List<T> stores(final String storeName,\n                               final QueryableStoreType<T> queryableStoreType) {\n         final List<T> allStores = new ArrayList<>();\n-        for (final StreamThreadStateStoreProvider provider : storeProviders) {\n-            final List<T> stores = provider.stores(storeQueryParameters);\n-            allStores.addAll(stores);\n+        for (final StreamThreadStateStoreProvider storeProvider : storeProviders) {\n+            final List<T> stores = storeProvider.stores(storeQueryParameters);\n+            if (!stores.isEmpty()) {\n+                allStores.addAll(stores);\n+                if (storeQueryParameters.partition() != null) {\n+                    break;\n+                }\n+            }\n         }\n         if (allStores.isEmpty()) {\n+            if (storeQueryParameters.partition() != null) {\n+                throw new InvalidStateStoreException(\n+                        String.format(\"The specified partition %d for store %s does not exist.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7d38c6ed55dae174de77de429cb94fa1c26344c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwNDc1Mg==", "bodyText": "The nested early-return pattern is pretty hard to follow. Do you mind rewriting it to use if/else blocks? I know it was previously doing some early returns; it'd be better to migrate to a more maintainable style when we update the code, though.", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455904752", "createdAt": "2020-07-16T16:11:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "diffHunk": "@@ -20,62 +20,49 @@\n import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.InternalTopologyBuilder;\n import org.apache.kafka.streams.processor.internals.StreamThread;\n import org.apache.kafka.streams.processor.internals.Task;\n import org.apache.kafka.streams.state.QueryableStoreType;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n import org.apache.kafka.streams.state.TimestampedKeyValueStore;\n import org.apache.kafka.streams.state.TimestampedWindowStore;\n \n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Set;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n \n public class StreamThreadStateStoreProvider {\n \n     private final StreamThread streamThread;\n-    private final InternalTopologyBuilder internalTopologyBuilder;\n \n-    public StreamThreadStateStoreProvider(final StreamThread streamThread,\n-                                          final InternalTopologyBuilder internalTopologyBuilder) {\n+    public StreamThreadStateStoreProvider(final StreamThread streamThread) {\n         this.streamThread = streamThread;\n-        this.internalTopologyBuilder = internalTopologyBuilder;\n     }\n \n     @SuppressWarnings(\"unchecked\")\n     public <T> List<T> stores(final StoreQueryParameters storeQueryParams) {\n         final String storeName = storeQueryParams.storeName();\n         final QueryableStoreType<T> queryableStoreType = storeQueryParams.queryableStoreType();\n-        final TaskId keyTaskId = createKeyTaskId(storeName, storeQueryParams.partition());\n         if (streamThread.state() == StreamThread.State.DEAD) {\n             return Collections.emptyList();\n         }\n         final StreamThread.State state = streamThread.state();\n         if (storeQueryParams.staleStoresEnabled() ? state.isAlive() : state == StreamThread.State.RUNNING) {\n             final Map<TaskId, ? extends Task> tasks = storeQueryParams.staleStoresEnabled() ? streamThread.allTasks() : streamThread.activeTaskMap();\n-            final List<T> stores = new ArrayList<>();\n-            if (keyTaskId != null) {\n-                final Task task = tasks.get(keyTaskId);\n-                if (task == null) {\n+            if (storeQueryParams.partition() != null) {\n+                final Task streamTask = findStreamTask(tasks, storeName, storeQueryParams.partition());\n+                if (streamTask == null) {\n                     return Collections.emptyList();\n                 }\n-                final T store = validateAndListStores(task.getStore(storeName), queryableStoreType, storeName, keyTaskId);\n-                if (store != null) {\n-                    return Collections.singletonList(store);\n-                }\n-            } else {\n-                for (final Task streamTask : tasks.values()) {\n-                    final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n-                    if (store != null) {\n-                        stores.add(store);\n-                    }\n-                }\n+                final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n+                return store != null ? Collections.singletonList(store) : Collections.emptyList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7d38c6ed55dae174de77de429cb94fa1c26344c"}, "originalPosition": 65}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89ac008bbe0763619a68d5cb455f73ec1aee54be", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/89ac008bbe0763619a68d5cb455f73ec1aee54be", "committedDate": "2020-07-16T16:58:24Z", "message": "refactor if/else flow to eliminate early-return"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMDU4NDI0", "url": "https://github.com/apache/kafka/pull/9020#pullrequestreview-450058424", "createdAt": "2020-07-16T17:22:31Z", "commit": {"oid": "89ac008bbe0763619a68d5cb455f73ec1aee54be"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzoyMjozMVrOGy05oA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzoyMjozMVrOGy05oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk0ODcwNA==", "bodyText": "Ah, sorry, I can see that my prior comment was ambiguous. This is what I meant:\n            if (storeQueryParams.partition() == null) {\n                return tasks.values().stream().\n                        map(streamTask -> validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id())).\n                        filter(Objects::nonNull).\n                        collect(Collectors.toList());\n            } else {\n                final Task streamTask = findStreamTask(tasks, storeName, storeQueryParams.partition());\n                if (streamTask == null) {\n                    return Collections.emptyList();\n                } else {\n                    final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n                    return store == null ? Collections.emptyList() : Collections.singletonList(store);\n                }\n            }\nThe reason this is better for maintenence is that you only have to trace a path through the nested conditionals into a single inner block to understand what gets returned. I.e., code comprehension complexity is only the depth of the conditional tree.\nIn contrast, if we do early returns, you have to fully read all the conditional blocks that lead up to the one you're interested (depth-first traversal), so code comprehension is linear instead of logarithmic. If we mutate the collection, you actually have to read all the conditionals to understand what is going to happen, so code comprehension is also linear instead of logarithmic.", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455948704", "createdAt": "2020-07-16T17:22:31Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "diffHunk": "@@ -51,18 +51,22 @@ public StreamThreadStateStoreProvider(final StreamThread streamThread) {\n         final StreamThread.State state = streamThread.state();\n         if (storeQueryParams.staleStoresEnabled() ? state.isAlive() : state == StreamThread.State.RUNNING) {\n             final Map<TaskId, ? extends Task> tasks = storeQueryParams.staleStoresEnabled() ? streamThread.allTasks() : streamThread.activeTaskMap();\n+            final List<T> stores = new ArrayList<>();\n             if (storeQueryParams.partition() != null) {\n                 final Task streamTask = findStreamTask(tasks, storeName, storeQueryParams.partition());\n-                if (streamTask == null) {\n-                    return Collections.emptyList();\n+                if (streamTask != null) {\n+                    final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n+                    if (store != null) {\n+                        stores.add(store);\n+                    }\n                 }\n-                final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n-                return store != null ? Collections.singletonList(store) : Collections.emptyList();\n+            } else {\n+                tasks.values().stream().\n+                        map(streamTask -> validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id())).\n+                        filter(Objects::nonNull).\n+                        forEach(stores::add);\n             }\n-            return tasks.values().stream().\n-                    map(streamTask -> validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id())).\n-                    filter(Objects::nonNull).\n-                    collect(Collectors.toList());\n+            return Collections.unmodifiableList(stores);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89ac008bbe0763619a68d5cb455f73ec1aee54be"}, "originalPosition": 40}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "158db402af5cfbbffeba297d3d9d3811ea75a979", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/158db402af5cfbbffeba297d3d9d3811ea75a979", "committedDate": "2020-07-16T18:03:51Z", "message": "concise if/else flow into functional"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b22417fee013cb18e19ad96d4f2d7c0845cf3452", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/b22417fee013cb18e19ad96d4f2d7c0845cf3452", "committedDate": "2020-07-17T08:53:50Z", "message": "optimize performance - avoid creating intermediate active tasks map"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da8554cff2f30fcb00f748cd81e7d676098850c9", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/da8554cff2f30fcb00f748cd81e7d676098850c9", "committedDate": "2020-07-17T09:06:55Z", "message": "optimize performance - avoid creating intermediate active tasks map"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fca67e47561ac63fb15c1fd13f3c19c3bead496e", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/fca67e47561ac63fb15c1fd13f3c19c3bead496e", "committedDate": "2020-07-17T10:45:51Z", "message": "[TESTS] remove expensive double-iteration from state provider"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fb7fe29473956dad54e73440152ec90df86ae78", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/0fb7fe29473956dad54e73440152ec90df86ae78", "committedDate": "2020-08-16T14:56:18Z", "message": "Merge branch 'trunk' into KAFKA-10271"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2MTEwMjg2", "url": "https://github.com/apache/kafka/pull/9020#pullrequestreview-496110286", "createdAt": "2020-09-25T04:57:13Z", "commit": {"oid": "0fb7fe29473956dad54e73440152ec90df86ae78"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNDo1NzoxM1rOHX0-cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNTowMDo1MVrOHX1Bpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0NzI1MA==", "bodyText": "Could you elaborate a bit more about this? If allStores.isEmpty() is empty, it is always possible that the specified store-partition or just store-\"null\" does not exist in this client. Why they are different failure cases?", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494747250", "createdAt": "2020-09-25T04:57:13Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java", "diffHunk": "@@ -46,11 +46,22 @@ public void setStoreQueryParameters(final StoreQueryParameters storeQueryParamet\n     public <T> List<T> stores(final String storeName,\n                               final QueryableStoreType<T> queryableStoreType) {\n         final List<T> allStores = new ArrayList<>();\n-        for (final StreamThreadStateStoreProvider provider : storeProviders) {\n-            final List<T> stores = provider.stores(storeQueryParameters);\n-            allStores.addAll(stores);\n+        for (final StreamThreadStateStoreProvider storeProvider : storeProviders) {\n+            final List<T> stores = storeProvider.stores(storeQueryParameters);\n+            if (!stores.isEmpty()) {\n+                allStores.addAll(stores);\n+                if (storeQueryParameters.partition() != null) {\n+                    break;\n+                }\n+            }\n         }\n         if (allStores.isEmpty()) {\n+            if (storeQueryParameters.partition() != null) {\n+                throw new InvalidStateStoreException(\n+                        String.format(\"The specified partition %d for store %s does not exist.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}, "originalCommit": {"oid": "b7d38c6ed55dae174de77de429cb94fa1c26344c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0NzMxOQ==", "bodyText": "A good coverage improvement! Thanks.", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494747319", "createdAt": "2020-09-25T04:57:38Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java", "diffHunk": "@@ -60,6 +60,34 @@ public void prepareTopology() throws InterruptedException {\n         rightStream = builder.stream(INPUT_TOPIC_RIGHT);\n     }\n \n+    @Test\n+    public void shouldNotAccessJoinStoresWhenGivingName() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fb7fe29473956dad54e73440152ec90df86ae78"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0ODA0OQ==", "bodyText": "This is a great find, thanks!", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494748049", "createdAt": "2020-09-25T05:00:45Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "diffHunk": "@@ -104,19 +95,11 @@ public StreamThreadStateStoreProvider(final StreamThread streamThread,\n         }\n     }\n \n-    private TaskId createKeyTaskId(final String storeName, final Integer partition) {\n-        if (partition == null) {\n-            return null;\n-        }\n-        final List<String> sourceTopics = internalTopologyBuilder.stateStoreNameToSourceTopics().get(storeName);\n-        final Set<String> sourceTopicsSet = new HashSet<>(sourceTopics);\n-        final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = internalTopologyBuilder.topicGroups();\n-        for (final Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> topicGroup : topicGroups.entrySet()) {\n-            if (topicGroup.getValue().sourceTopics.containsAll(sourceTopicsSet)) {\n-                return new TaskId(topicGroup.getKey(), partition);\n-            }\n-        }\n-        throw new InvalidStateStoreException(\"Cannot get state store \" + storeName + \" because the requested partition \" +\n-            partition + \" is not available on this instance\");\n+    private Optional<Task> findStreamTask(final Collection<Task> tasks, final String storeName, final int partition) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fb7fe29473956dad54e73440152ec90df86ae78"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0ODA3MA==", "bodyText": "LGTM.", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494748070", "createdAt": "2020-09-25T05:00:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java", "diffHunk": "@@ -56,25 +55,6 @@ public QueryableStoreProvider(final List<StreamThreadStateStoreProvider> storePr\n         if (!globalStore.isEmpty()) {\n             return queryableStoreType.create(globalStoreProvider, storeName);\n         }\n-        final List<T> allStores = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fb7fe29473956dad54e73440152ec90df86ae78"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9838391304ff1cb3d1eab7b9d4f0126c57a66a1b", "author": {"user": {"login": "albert02lowis", "name": null}}, "url": "https://github.com/apache/kafka/commit/9838391304ff1cb3d1eab7b9d4f0126c57a66a1b", "committedDate": "2020-10-05T09:08:29Z", "message": "KAFKA-9273: Extract testShouldAutoShutdownOnIncompleteMetadata from S\u2026 (#9108)\n\nThe main goal is to remove usage of embedded broker (EmbeddedKafkaCluster) in AbstractJoinIntegrationTest and its subclasses.\nThis is because the tests under this class are no longer using the embedded broker, except for two.\ntestShouldAutoShutdownOnIncompleteMetadata is one of such tests.\nFurthermore, this test does not actually perfom stream-table join; it is testing an edge case of joining with a non-existent topic, so it should be in a separate test.\n\nTesting strategy: run existing unit and integration test\n\nReviewers: Boyang Chen <boyang@confluent.io>, Bill Bejeck <bbejeck@apache.org>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d7c37fc50bb71e9ef9381fdb494b1d3b97571eb", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/5d7c37fc50bb71e9ef9381fdb494b1d3b97571eb", "committedDate": "2020-10-05T12:10:49Z", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73def50d582d4c90bc5a53fed7463aa0a3b0d0d0", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/73def50d582d4c90bc5a53fed7463aa0a3b0d0d0", "committedDate": "2020-10-05T15:02:59Z", "message": "fix test to throw exception on store request"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f4f9355c5dc1b8768d0a44a81b00c2b7076e7957", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/f4f9355c5dc1b8768d0a44a81b00c2b7076e7957", "committedDate": "2020-10-07T08:26:22Z", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "702cc065a2a1e723e3c84268d02983025c144f51", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/702cc065a2a1e723e3c84268d02983025c144f51", "committedDate": "2020-10-07T12:10:16Z", "message": "waiting state store readiness by probing non-existing value"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "274617cd232c8546ba68df6cdfe8e12bef67ed23", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/274617cd232c8546ba68df6cdfe8e12bef67ed23", "committedDate": "2020-10-07T15:18:24Z", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "049ec22974580acd023f5d06e4cb34bfd4ad935e", "author": {"user": {"login": "dima5rr", "name": "Dima Reznik"}}, "url": "https://github.com/apache/kafka/commit/049ec22974580acd023f5d06e4cb34bfd4ad935e", "committedDate": "2020-10-08T08:42:12Z", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1277, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}