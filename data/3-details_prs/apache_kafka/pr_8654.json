{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE2NzU3NTc4", "number": 8654, "title": "KAFKA-9931: Implement KIP-605 to expand support for Connect worker internal topic configurations", "bodyText": "KIP-605 has passed.\nExpanded the allowed values for the internal topics\u2019 replication factor and partitions from positive values to also include -1 to signify that the broker defaults should be used.\nThe Kafka storage classes were already constructing a NewTopic object (always with a replication factor and partitions) and sending it to Kafka when required. This change will avoid setting the replication factor and/or number of partitions on this NewTopic if the worker configuration uses -1 for the corresponding configuration value.\nQuite a few new tests were added to verify that the TopicAdmin utility class is correctly using the AdminClient, that the DistributedConfig validators for these configurations are correct, and that the DistributedConfig is correctly assembling the configuration properties that define the topic settings, which are now accessed by the three Kafka storage objects before they create the topics.\nAlso added support for additional topic settings used when creating the config, status, and offset internal topics.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-05-12T14:18:54Z", "url": "https://github.com/apache/kafka/pull/8654", "merged": true, "mergeCommit": {"oid": "981ef5166d2c95cacb6cdc1d52ed0c866b473868"}, "closed": true, "closedAt": "2020-05-23T14:00:32Z", "author": {"login": "rhauch"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABchk220gFqTQxMjc3NTg5NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcjjPcvgFqTQxNjQ0NzIyMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNzc1ODk1", "url": "https://github.com/apache/kafka/pull/8654#pullrequestreview-412775895", "createdAt": "2020-05-15T16:01:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowMTo0N1rOGWK0ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjozMjo0M1rOGWL6-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTE5NA==", "bodyText": "this check seems to replicate a bit what's happening in NewTopic. Should we just pass the value, given that we have validations too already?", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425899194", "createdAt": "2020-05-15T16:01:47Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -66,25 +71,55 @@\n         /**\n          * Specify the desired number of partitions for the topic.\n          *\n-         * @param numPartitions the desired number of partitions; must be positive\n+         * @param numPartitions the desired number of partitions; must be positive, or -1 to\n+         *                      signify using the broker's default\n          * @return this builder to allow methods to be chained; never null\n          */\n         public NewTopicBuilder partitions(int numPartitions) {\n+            if (numPartitions == NO_PARTITIONS) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTMwMA==", "bodyText": "same question as above", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425899300", "createdAt": "2020-05-15T16:02:00Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -66,25 +71,55 @@\n         /**\n          * Specify the desired number of partitions for the topic.\n          *\n-         * @param numPartitions the desired number of partitions; must be positive\n+         * @param numPartitions the desired number of partitions; must be positive, or -1 to\n+         *                      signify using the broker's default\n          * @return this builder to allow methods to be chained; never null\n          */\n         public NewTopicBuilder partitions(int numPartitions) {\n+            if (numPartitions == NO_PARTITIONS) {\n+                return defaultPartitions();\n+            }\n             this.numPartitions = numPartitions;\n             return this;\n         }\n \n+        /**\n+         * Specify the topic's number of partition should be the broker configuration for\n+         * {@code num.partitions}.\n+         *\n+         * @return this builder to allow methods to be chained; never null\n+         */\n+        public NewTopicBuilder defaultPartitions() {\n+            this.numPartitions = null;\n+            return this;\n+        }\n+\n         /**\n          * Specify the desired replication factor for the topic.\n          *\n-         * @param replicationFactor the desired replication factor; must be positive\n+         * @param replicationFactor the desired replication factor; must be positive, or -1 to\n+         *                          signify using the broker's default\n          * @return this builder to allow methods to be chained; never null\n          */\n         public NewTopicBuilder replicationFactor(short replicationFactor) {\n+            if (replicationFactor == NO_REPLICATION_FACTOR) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMDY3Ng==", "bodyText": "did you notice that this matters? Not suggesting to remove necessarily, but I wonder whether you noticed it didn't work otherwise.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425900676", "createdAt": "2020-05-15T16:04:33Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")\n+                                                      .workerProps(workerProps)\n+                                                      .numWorkers(numWorkers)\n+                                                      .numBrokers(numBrokers)\n+                                                      .brokerProps(brokerProps)\n+                                                      .build();\n+\n+        // Start the Connect cluster\n+        connect.start();\n+        connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, \"Brokers did not start in time.\");\n+        connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, \"Worker did not start in time.\");\n+        log.info(\"Completed startup of {} Kafka brokers and {} Connect workers\", numBrokers, numWorkers);\n+\n+        // Check the topics\n+        log.info(\"Verifying the internal topics for Connect\");\n+        connect.assertions().assertTopicsExist(configTopic(), offsetTopic(), statusTopic());\n+        assertInternalTopicSettings();\n+\n+        // Remove the Connect worker\n+        log.info(\"Stopping the Connect worker\");\n+        connect.removeWorker();\n+\n+        // Sleep for a bit\n+        Thread.sleep(3000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMjI5OQ==", "bodyText": "Is naming here to distinguish between runs of integration tests?\nI have a commit that will help us know the boundaries between ITs. I'll submit separately.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425902299", "createdAt": "2020-05-15T16:07:32Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkxMzc3Mw==", "bodyText": "topicSettings.forEach((k, v) -> {...}); is a better shorthand in cases like these here where you don't need to pass the entry between stream stages.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425913773", "createdAt": "2020-05-15T16:27:37Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedConfigTest.java", "diffHunk": "@@ -105,4 +106,204 @@ public void shouldValidateAllVerificationAlgorithms() {\n             algorithms.add(algorithms.remove(0));\n         }\n     }\n+\n+    @Test\n+    public void shouldAllowNegativeOneAndPositiveForPartitions() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, \"-1\");\n+        settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, \"-1\");\n+        new DistributedConfig(configs());\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+        settings.remove(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG);\n+\n+        for (int i = 1; i != 100; ++i) {\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+        }\n+    }\n+\n+    @Test\n+    public void shouldNotAllowZeroPartitions() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+        settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+    }\n+\n+    @Test\n+    public void shouldNotAllowNegativePartitionsLessThanNegativeOne() {\n+        Map<String, String> settings = configs();\n+        for (int i = -2; i > -100; --i) {\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldAllowNegativeOneAndPositiveForReplicationFactor() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, \"-1\");\n+        settings.put(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG, \"-1\");\n+        settings.put(DistributedConfig.STATUS_STORAGE_REPLICATION_FACTOR_CONFIG, \"-1\");\n+        new DistributedConfig(configs());\n+        settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+        settings.remove(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG);\n+\n+        for (int i = 1; i != 100; ++i) {\n+            settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+            settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+        }\n+    }\n+\n+    @Test\n+    public void shouldNotAllowZeroReplicationFactor() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+        settings.put(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+        settings.put(DistributedConfig.STATUS_STORAGE_REPLICATION_FACTOR_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+    }\n+\n+    @Test\n+    public void shouldNotAllowNegativeReplicationFactorLessThanNegativeOne() {\n+        Map<String, String> settings = configs();\n+        for (int i = -2; i > -100; --i) {\n+            settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+            settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+            settings.put(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldAllowSettingConfigTopicSettings() {\n+        Map<String, String> topicSettings = new HashMap<>();\n+        topicSettings.put(\"foo\", \"foo value\");\n+        topicSettings.put(\"bar\", \"bar value\");\n+        topicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {\n+            settings.put(DistributedConfig.CONFIG_STORAGE_PREFIX + e.getKey(), e.getValue());\n+        });\n+        DistributedConfig config = new DistributedConfig(settings);\n+        assertEquals(topicSettings, config.configStorageTopicSettings());\n+    }\n+\n+    @Test\n+    public void shouldAllowSettingOffsetTopicSettings() {\n+        Map<String, String> topicSettings = new HashMap<>();\n+        topicSettings.put(\"foo\", \"foo value\");\n+        topicSettings.put(\"bar\", \"bar value\");\n+        topicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PREFIX + e.getKey(), e.getValue());\n+        });\n+        DistributedConfig config = new DistributedConfig(settings);\n+        assertEquals(topicSettings, config.offsetStorageTopicSettings());\n+    }\n+\n+    @Test\n+    public void shouldAllowSettingStatusTopicSettings() {\n+        Map<String, String> topicSettings = new HashMap<>();\n+        topicSettings.put(\"foo\", \"foo value\");\n+        topicSettings.put(\"bar\", \"bar value\");\n+        topicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {\n+            settings.put(DistributedConfig.STATUS_STORAGE_PREFIX + e.getKey(), e.getValue());\n+        });\n+        DistributedConfig config = new DistributedConfig(settings);\n+        assertEquals(topicSettings, config.statusStorageTopicSettings());\n+    }\n+\n+    @Test\n+    public void shouldRemoveCompactionFromConfigTopicSettings() {\n+        Map<String, String> expectedTopicSettings = new HashMap<>();\n+        expectedTopicSettings.put(\"foo\", \"foo value\");\n+        expectedTopicSettings.put(\"bar\", \"bar value\");\n+        expectedTopicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> topicSettings = new HashMap<>(expectedTopicSettings);\n+        topicSettings.put(\"cleanup.policy\", \"something-else\");\n+        topicSettings.put(\"partitions\", \"3\");\n+\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkxNzE3OA==", "bodyText": "I didn't comment elsewhere, but that's where != has the potential of stackoverflow vs <=. I understand we trust ourselves to set maxDefaultRf to be >0 but that's a couple indirections away.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425917178", "createdAt": "2020-05-15T16:32:43Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java", "diffHunk": "@@ -97,12 +103,65 @@ public void shouldNotCreateTopicWhenItAlreadyExists() {\n     }\n \n     @Test\n-    public void shouldCreateTopicWhenItDoesNotExist() {\n-        NewTopic newTopic = TopicAdmin.defineTopic(\"myTopic\").partitions(1).compacted().build();\n-        Cluster cluster = createCluster(1);\n-        try (MockAdminClient mockAdminClient = new MockAdminClient(cluster.nodes(), cluster.nodeById(0))) {\n-            TopicAdmin admin = new TopicAdmin(null, mockAdminClient);\n-            assertTrue(admin.createTopic(newTopic));\n+    public void shouldCreateTopicWithPartitionsWhenItDoesNotExist() {\n+        for (int numBrokers = 1; numBrokers < 10; ++numBrokers) {\n+            int expectedReplicas = Math.min(3, numBrokers);\n+            int maxDefaultRf = Math.min(numBrokers, 5);\n+            for (int numPartitions = 1; numPartitions != 30; ++numPartitions) {\n+                NewTopic newTopic = TopicAdmin.defineTopic(\"myTopic\").partitions(numPartitions).compacted().build();\n+\n+                // Try clusters with no default replication factor or default partitions\n+                assertTopicCreation(numBrokers, newTopic, null, null, expectedReplicas, numPartitions);\n+\n+                // Try clusters with different default partitions\n+                for (int defaultPartitions = 1; defaultPartitions != 20; ++defaultPartitions) {\n+                    assertTopicCreation(numBrokers, newTopic, defaultPartitions, null, expectedReplicas, numPartitions);\n+                }\n+\n+                // Try clusters with different default replication factors\n+                for (int defaultRF = 1; defaultRF != maxDefaultRf; ++defaultRF) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE1ODczMDAy", "url": "https://github.com/apache/kafka/pull/8654#pullrequestreview-415873002", "createdAt": "2020-05-21T04:37:53Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDozNzo1M1rOGYl7Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDo1MDoxMVrOGYmFyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ0MDM3OA==", "bodyText": "It's one more import statement, but I don't see a reason not to get the actual config from:\nTopicConfig.CLEANUP_POLICY_CONFIG. I think you'll agree that a variable is better than a string, but just a nit.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428440378", "createdAt": "2020-05-21T04:37:53Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java", "diffHunk": "@@ -400,6 +424,33 @@ public KeyGenerator getInternalRequestKeyGenerator() {\n         }\n     }\n \n+    private Map<String, Object> topicSettings(String prefix) {\n+        Map<String, Object> result = originalsWithPrefix(prefix);\n+        if (CONFIG_STORAGE_PREFIX.equals(prefix) && result.containsKey(PARTITIONS_SUFFIX)) {\n+            log.warn(\"Ignoring '{}{}={}' setting, since config topic partitions is always 1\", prefix, PARTITIONS_SUFFIX, result.get(\"partitions\"));\n+        }\n+        Object removedPolicy = result.remove(\"cleanup.policy\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ0MjM3OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Map<String, Object> topicSettings = null;\n          \n          \n            \n                    if (config instanceof DistributedConfig) {\n          \n          \n            \n                        topicSettings = ((DistributedConfig) config).configStorageTopicSettings();\n          \n          \n            \n                    }\n          \n          \n            \n                    Map<String, Object> topicSettings = config instanceof DistributedConfig\n          \n          \n            \n                            ? ((DistributedConfig) config).configStorageTopicSettings()\n          \n          \n            \n                            : Collections.emptyMap();\n          \n      \n    \n    \n  \n\nI know that TopicAdmin#defineTopic checks for null, but I think using null with collections is better to do when such optimization matters. Wdyt?\n(btw you don't have to use the ternary operator, I just added it to make the suggestion clear).\nAlso, if you change here, please change in the other files too.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428442379", "createdAt": "2020-05-21T04:46:39Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java", "diffHunk": "@@ -453,11 +453,17 @@ public void putSessionKey(SessionKey sessionKey) {\n         consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n \n         Map<String, Object> adminProps = new HashMap<>(originals);\n-        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n-                compacted().\n-                partitions(1).\n-                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n-                build();\n+\n+        Map<String, Object> topicSettings = null;\n+        if (config instanceof DistributedConfig) {\n+            topicSettings = ((DistributedConfig) config).configStorageTopicSettings();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ0MzA4MA==", "bodyText": "Makes sense.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428443080", "createdAt": "2020-05-21T04:50:11Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMjI5OQ=="}, "originalCommit": null, "originalPosition": 69}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2eb1ac205bce3fc5ce016263ca9bf6c655b9669", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/b2eb1ac205bce3fc5ce016263ca9bf6c655b9669", "committedDate": "2020-05-21T18:06:24Z", "message": "KAFKA-9931: Added support for -1 replication factor and partitions for distributed worker internal topics\n\nExpanded the allowed values for the internal topics\u2019 replication factor and partitions from positive values to also include -1 to signify that the broker defaults should be used.\n\nThe Kafka storage classes were already constructing a `NewTopic` object (always with a replication factor and partitions) and sending it to Kafka when required. This change will avoid setting the replication factor and/or number of partitions on this `NewTopic` if the worker configuration uses -1 for the corresponding configuration value.\n\nQuite a few new tests were added to verify that the `TopicAdmin` utility class is correctly using the AdminClient, and that the `DistributedConfig` validators for these configurations are correct."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "250096181fd6163cfbe7a7a26a08c16ca7904933", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/250096181fd6163cfbe7a7a26a08c16ca7904933", "committedDate": "2020-05-21T18:06:24Z", "message": "KAFKA-9931: Added support for extra settings for internal topics on distributed config\n\nAdded support for additional topic settings used when creating the config, status, and offset internal topics."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57e68740389729497a3505c186849e18bc999f22", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/57e68740389729497a3505c186849e18bc999f22", "committedDate": "2020-05-21T18:06:24Z", "message": "KAFKA-9931: Added integration tests for internal topic creation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b645a05b25106ea036c886b918e4bb1d803ae432", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/b645a05b25106ea036c886b918e4bb1d803ae432", "committedDate": "2020-05-21T18:06:24Z", "message": "KAFKA-9931: Incorporated suggestions from review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a07644cb8749bad36538fbbaf42f10d7cefe2268", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/a07644cb8749bad36538fbbaf42f10d7cefe2268", "committedDate": "2020-05-21T18:06:25Z", "message": "KAFKA-9931: Fix checkstyle failures"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "831a0e9a5a5420ab94d8aeed614d84d8c3498722", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/831a0e9a5a5420ab94d8aeed614d84d8c3498722", "committedDate": "2020-05-21T19:02:50Z", "message": "KAFKA-9931: Used existing constants in TopicAdmin"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "278ca9ca75e4a6ec9d0229c0454168f58542b831", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/278ca9ca75e4a6ec9d0229c0454168f58542b831", "committedDate": "2020-05-21T19:03:41Z", "message": "KAFKA-9931: Applied recommendations from code reviews"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e2d988af9e8e685cf33b281c094fe6b72065317", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/4e2d988af9e8e685cf33b281c094fe6b72065317", "committedDate": "2020-05-21T19:03:57Z", "message": "KAFKA-9931: Improved comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "4e2d988af9e8e685cf33b281c094fe6b72065317", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/4e2d988af9e8e685cf33b281c094fe6b72065317", "committedDate": "2020-05-21T19:03:57Z", "message": "KAFKA-9931: Improved comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2NDQ3MjIz", "url": "https://github.com/apache/kafka/pull/8654#pullrequestreview-416447223", "createdAt": "2020-05-21T19:56:59Z", "commit": {"oid": "4e2d988af9e8e685cf33b281c094fe6b72065317"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 955, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}