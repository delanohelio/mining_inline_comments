{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAwNDk2Nzkx", "number": 8440, "reviewThreads": {"totalCount": 33, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NDo0M1rODvvOTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMToyNDo1MlrODxZOLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzMzczOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NDo0M1rOGCVv7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NDo0M1rOGCVv7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNjY3MA==", "bodyText": "This is an actually bug-fix. StandbyTasks did not set the eos flag to true for eos-beta and thus did not wipe out their stores in case of failure.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405106670", "createdAt": "2020-04-07T20:54:43Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -72,7 +72,7 @@\n \n         processorContext = new StandbyContextImpl(id, config, stateMgr, metrics);\n         closeTaskSensor = ThreadMetrics.closeTaskSensor(Thread.currentThread().getName(), metrics);\n-        this.eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+        eosEnabled = StreamThread.eosEnabled(config);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzNjEzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NToyNlrOGCVxaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzo0Mzo0M1rOGDi40Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzA0OA==", "bodyText": "This is an actually bug fix: consumedOffsetsAndMetadataPerTask could be empty, if only standby tasks (but no active tasks) are assigned to a thread.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107048", "createdAt": "2020-04-07T20:55:26Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -758,7 +758,9 @@ private int commitInternal(final Collection<Task> tasks) {\n                 }\n             }\n \n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NDUxOA==", "bodyText": "Could you elaborate more on why committing an empty map will fail?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405144518", "createdAt": "2020-04-07T22:13:28Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -758,7 +758,9 @@ private int commitInternal(final Collection<Task> tasks) {\n                 }\n             }\n \n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzA0OA=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM3MDUxMw==", "bodyText": "If we only have StandbyTasks assigned, the RecordCollector would not be initialized and thus the KafkaProducer would not initialize transactions and hence the offset commit would fail as we cannot begin a new transaction.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r406370513", "createdAt": "2020-04-09T17:43:43Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -758,7 +758,9 @@ private int commitInternal(final Collection<Task> tasks) {\n                 }\n             }\n \n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzA0OA=="}, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzNjk1OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTozOVrOGCVx3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTozOVrOGCVx3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzE2NA==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107164", "createdAt": "2020-04-07T20:55:39Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -75,26 +71,20 @@ public void start() {\n                 uncaughtException = false;\n \n                 streams = createKafkaStreams(properties);\n-                streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzNzI0OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTo0NFrOGCVyAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTo0NFrOGCVyAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzIwMg==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107202", "createdAt": "2020-04-07T20:55:44Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -75,26 +71,20 @@ public void start() {\n                 uncaughtException = false;\n \n                 streams = createKafkaStreams(properties);\n-                streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {\n-                    @Override\n-                    public void uncaughtException(final Thread t, final Throwable e) {\n-                        System.out.println(System.currentTimeMillis());\n-                        System.out.println(\"EOS-TEST-CLIENT-EXCEPTION\");\n-                        e.printStackTrace();\n-                        System.out.flush();\n-                        uncaughtException = true;\n-                    }\n+                streams.setUncaughtExceptionHandler((t, e) -> {\n+                    System.out.println(System.currentTimeMillis());\n+                    System.out.println(\"EOS-TEST-CLIENT-EXCEPTION\");\n+                    e.printStackTrace();\n+                    System.out.flush();\n+                    uncaughtException = true;\n                 });\n-                streams.setStateListener(new KafkaStreams.StateListener() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzODc0OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoxMlrOGCVy8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoxMlrOGCVy8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzQ0Mg==", "bodyText": "We set processing guarantee \"external\" now, via the system test properties file", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107442", "createdAt": "2020-04-07T20:56:12Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzOTIwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoyMlrOGCVzSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMjowMDoxM1rOGE22xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzUyOQ==", "bodyText": "Small side improvement.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107529", "createdAt": "2020-04-07T20:56:22Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n         props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 5000); // increase commit interval to make sure a client is killed having an open transaction", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY3NjMwNg==", "bodyText": "Could you elaborate a bit? The comment doesn't seem readable.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405676306", "createdAt": "2020-04-08T17:01:28Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n         props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 5000); // increase commit interval to make sure a client is killed having an open transaction", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzUyOQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4NjY5MQ==", "bodyText": "Comment says:\n// increase commit interval to make sure a client is killed having an open transaction\n\nIf we commit with small commit interval, the probability that there is no pending transaction when we kill the instance is high.\nWe delay to start a new transaction until we do the first send() and would commit quickly afterwards. If we \"stall\" in between waiting for new data (what is not uncommon in this test) there will be no open tx for some time.\nDuring debugging I did some segment dumps and could not find a single aborted transaction.\nDoes this make sense?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405886691", "createdAt": "2020-04-09T00:15:44Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n         props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 5000); // increase commit interval to make sure a client is killed having an open transaction", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzUyOQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc0NjI0Nw==", "bodyText": "kk, I think I get the motivation, but still feels a client is killed and having an open transaction could not be connected as a full sentence here. Maybe we should call a killed client?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r407746247", "createdAt": "2020-04-13T22:00:13Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n         props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 5000); // increase commit interval to make sure a client is killed having an open transaction", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzUyOQ=="}, "originalCommit": null, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzOTQ3OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoyNlrOGCVzeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoyNlrOGCVzeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzU3Nw==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107577", "createdAt": "2020-04-07T20:56:26Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -127,41 +117,17 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         // min\n         groupedData\n             .aggregate(\n-                new Initializer<Integer>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzOTY4OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozMVrOGCVzpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozMVrOGCVzpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzYyMQ==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107621", "createdAt": "2020-04-07T20:56:31Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -127,41 +117,17 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         // min\n         groupedData\n             .aggregate(\n-                new Initializer<Integer>() {\n-                    @Override\n-                    public Integer apply() {\n-                        return Integer.MAX_VALUE;\n-                    }\n-                },\n-                new Aggregator<String, Integer, Integer>() {\n-                    @Override\n-                    public Integer apply(final String aggKey,\n-                                         final Integer value,\n-                                         final Integer aggregate) {\n-                        return (value < aggregate) ? value : aggregate;\n-                    }\n-                },\n-                Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>with(null, intSerde))\n+                () -> Integer.MAX_VALUE,\n+                (aggKey, value, aggregate) -> (value < aggregate) ? value : aggregate,\n+                Materialized.with(null, intSerde))\n             .toStream()\n             .to(\"min\", Produced.with(stringSerde, intSerde));\n \n         // sum\n         groupedData.aggregate(\n-            new Initializer<Long>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzgzOTk0OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozNVrOGCVz0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozNVrOGCVz0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzY2NA==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107664", "createdAt": "2020-04-07T20:56:35Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -174,21 +140,9 @@ public Long apply(final String aggKey,\n             // max\n             groupedDataAfterRepartitioning\n                 .aggregate(\n-                    new Initializer<Integer>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg0NzM1OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1ODozOVrOGCV4Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1ODozOVrOGCV4Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwODgwNw==", "bodyText": "The previous shutdown hook did not wait until the \"main loop\" breaks and exits. Hence, the code after the loop was never executed making debugging harder. We introduce the terminated flag to delay the termination of the JVM until the method finished.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405108807", "createdAt": "2020-04-07T20:58:39Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg1MTE2OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1OTo0NFrOGCV6rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1OTo0NFrOGCV6rA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwOTQyMA==", "bodyText": "This is the \"main loop\" as mentioned above", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405109420", "createdAt": "2020-04-07T20:59:44Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg1MjUwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDoxMFrOGCV7iQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDoxMFrOGCV7iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwOTY0MQ==", "bodyText": "This is the code after the \"main loop\" that was never executed", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405109641", "createdAt": "2020-04-07T21:00:10Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());\n                     }\n+                });\n+\n+                updateNumRecordsProduces(1);\n+                if (numRecordsProduced % 1000 == 0) {\n+                    System.out.println(numRecordsProduced + \" records produced\");\n+                    System.out.flush();\n                 }\n-            });\n+                Utils.sleep(rand.nextInt(10));\n+            }\n+            producer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg1NTI2OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDo1OFrOGCV9NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDo1OFrOGCV9NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMDA2OQ==", "bodyText": "We use a try-catch to set the flag to make sure the shutdown hook can exit quickly even in case of failure", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405110069", "createdAt": "2020-04-07T21:00:58Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());\n                     }\n+                });\n+\n+                updateNumRecordsProduces(1);\n+                if (numRecordsProduced % 1000 == 0) {\n+                    System.out.println(numRecordsProduced + \" records produced\");\n+                    System.out.flush();\n                 }\n-            });\n+                Utils.sleep(rand.nextInt(10));\n+            }\n+            producer.close();\n+            System.out.println(\"Producer closed: \" + numRecordsProduced + \" records produced\");\n+            System.out.flush();\n \n-            updateNumRecordsProduces(1);\n-            if (numRecordsProduced % 1000 == 0) {\n-                System.out.println(numRecordsProduced + \" records produced\");\n-                System.out.flush();\n+            // verify offsets\n+            for (Map.Entry<Integer, List<Long>> offsetsOfPartition : offsets.entrySet()) {\n+                offsetsOfPartition.getValue().sort(Long::compareTo);\n+                for (int i = 0; i < offsetsOfPartition.getValue().size() - 1; ++i) {\n+                    if (offsetsOfPartition.getValue().get(i) != i) {\n+                        System.err.println(\"Offset for partition \" + offsetsOfPartition.getKey() + \" is not \" + i + \" as expected but \" + offsetsOfPartition.getValue().get(i));\n+                        System.err.flush();\n+                    }\n+                }\n+                System.out.println(\"Max offset of partition \" + offsetsOfPartition.getKey() + \" is \" + offsetsOfPartition.getValue().get(offsetsOfPartition.getValue().size() - 1));\n             }\n-            Utils.sleep(rand.nextInt(10));\n-        }\n-        producer.close();\n-        System.out.println(\"Producer closed: \" + numRecordsProduced + \" records produced\");\n \n-        final Properties props = new Properties();\n-        props.put(ConsumerConfig.CLIENT_ID_CONFIG, \"verifier\");\n-        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n-        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n \n-        try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {\n-            final List<TopicPartition> partitions = getAllPartitions(consumer, \"data\");\n-            System.out.println(\"Partitions: \" + partitions);\n-            consumer.assign(partitions);\n-            consumer.seekToEnd(partitions);\n+            final Properties props = new Properties();\n+            props.put(ConsumerConfig.CLIENT_ID_CONFIG, \"verifier\");\n+            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n+            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n+            props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n \n-            for (final TopicPartition tp : partitions) {\n-                System.out.println(\"End-offset for \" + tp + \" is \" + consumer.position(tp));\n+            try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {\n+                final List<TopicPartition> partitions = getAllPartitions(consumer, \"data\");\n+                System.out.println(\"Partitions: \" + partitions);\n+                System.out.flush();\n+                consumer.assign(partitions);\n+                consumer.seekToEnd(partitions);\n+\n+                for (final TopicPartition tp : partitions) {\n+                    System.out.println(\"End-offset for \" + tp + \" is \" + consumer.position(tp));\n+                    System.out.flush();\n+                }\n             }\n+            System.out.flush();\n+        } finally {\n+            terminated = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg2OTA3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNDo1MVrOGCWFWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNDo1MVrOGCWFWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMjE1NA==", "bodyText": "To make the verification step work, we first need to check that all transactions are finished. With EOS-alpha we never had pending transactions that would eventually be aborted by the tx-coordinator, because while we crash some instanced in between the final shutdown phase is always clean. Hence, for eos-alpha all pending transactions would be aborted by initTransaction() calls.\nFor eos-beta, thread that are killed leave open transaction that will be eventually expired by the tx-coordinator though, as we (also on restart of a thread) would generate a new transactonal.id.\nHaving no pending transactions is a requirement for the following code to do a correct verification of the result.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405112154", "createdAt": "2020-04-07T21:04:51Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -144,6 +188,14 @@ public static void verify(final String kafka, final boolean withRepartitioning)\n         props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n         props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n \n+        try (final KafkaConsumer<byte[], byte[]> committedConsumer = new KafkaConsumer<>(props)) {\n+            verifyAllTransactionFinished(committedConsumer, kafka, withRepartitioning);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg3MTg3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNTozOVrOGCWHFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNTozOVrOGCWHFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMjU5Nw==", "bodyText": "Just increasing the wait time as small side improvement to spin less and reduce the output for debugging.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405112597", "createdAt": "2020-04-07T21:05:39Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -263,15 +303,15 @@ private static void ensureStreamsApplicationDown(final Admin adminClient) {\n                                                                                                      final Map<TopicPartition, Long> readEndOffsets,\n                                                                                                      final boolean withRepartitioning,\n                                                                                                      final boolean isInputTopic) {\n-        System.err.println(\"read end offset: \" + readEndOffsets);\n+        System.out.println(\"read end offset: \" + readEndOffsets);\n         final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> recordPerTopicPerPartition = new HashMap<>();\n         final Map<TopicPartition, Long> maxReceivedOffsetPerPartition = new HashMap<>();\n         final Map<TopicPartition, Long> maxConsumerPositionPerPartition = new HashMap<>();\n \n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n         boolean allRecordsReceived = false;\n         while (!allRecordsReceived && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> receivedRecords = consumer.poll(Duration.ofMillis(100));\n+            final ConsumerRecords<byte[], byte[]> receivedRecords = consumer.poll(Duration.ofSeconds(1L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 263}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg3NDQ2OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNjozNlrOGCWI0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxODowMToyMFrOGDjgnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg==", "bodyText": "Because we do the verification for pending transactions first now, we have one additional record that is not part of the result and that we need to exclude (similar below)", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405113042", "createdAt": "2020-04-07T21:06:36Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5MzUyNw==", "bodyText": "The logic looks fragile when the partitionRecords is empty. For all -1 cases, we add one more dummy record to the array being checked, or just remove the last element from the derived array so that we could maintain the same verification.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405693527", "createdAt": "2020-04-08T17:29:38Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg=="}, "originalCommit": null, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTcwMTgzNw==", "bodyText": "I'm wondering if we have to produce another record in the verifyAllTransactionFinished; for example, could we just check the end value of the newly added offsets map maintained by producer? If yes then we can remove this extra logic here and below.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405701837", "createdAt": "2020-04-08T17:43:58Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg=="}, "originalCommit": null, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4ODk2OQ==", "bodyText": "The logic looks fragile when the partitionRecords is empty\n\npartitionRecords would never be empty; it would at least contain the \"dummy\" record that we wrote previously\n(We could of course strip the dummy record somewhere else; I picked this solution because it was the least line of code to be changed.)\n\nfor example, could we just check the end value of the newly added offsets map maintained by producer?\n\nWell, we can only maintain this new offset map if we write those dummy records. If we don't write anything, the producer would not put anything into the map but it would stay empty?\n(Or do you refer to the offsets map from the producer that write the input data? This would not help because the generate() and verify() methods are not executed in the same JVM -- also, we are interested in pending transaction of repartition and output topic -- for input topics, there are not transaction.)\nHowever,  maybe we could use two consumers (and get rid of the producer): one in read_uncommitted mode to get the endOffset and a second one with read_committed mode that also get the end-offsets in a loop. Only if the \"read committed\" consumer returns the same end-offset as the \"read uncommitted\" consumer did, we know that there is no pending transaction?\nThoughts?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405888969", "createdAt": "2020-04-09T00:24:14Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg=="}, "originalCommit": null, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM4MDcwMA==", "bodyText": "Yes I was thinking about the added offsets map from the generate() function -- you're right they would not be shared for the other. Bummer..\nI think using two consumer is slightly better than using a producer to write a dummy record.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r406380700", "createdAt": "2020-04-09T18:01:20Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg=="}, "originalCommit": null, "originalPosition": 281}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg3ODQ0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNzo0NVrOGCWLTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzo0NDo1MVrOGC6GrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzY3Ng==", "bodyText": "As there might be pending transaction, we need to improve the way how we verify that all transaction are finished. For this, we need to remember the offsets of our \"topic end marker messages\".", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405113676", "createdAt": "2020-04-07T21:07:45Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -550,6 +590,8 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n         producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n+        final Map<TopicPartition, Long> endMarkerOffset = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 357}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTcwMjMxNw==", "bodyText": "See my question above: could we just rely on the maintained offsets map last values?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405702317", "createdAt": "2020-04-08T17:44:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -550,6 +590,8 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n         producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n+        final Map<TopicPartition, Long> endMarkerOffset = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzY3Ng=="}, "originalCommit": null, "originalPosition": 357}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg4NTkzOnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowOTo1MFrOGCWP3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowOTo1MFrOGCWP3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNDg0Nw==", "bodyText": "Instead of looking for the end-marker message per content (ie, comparing key and value), we now use the offset (that we now know) to see if we can get the endOffset() as expected in \"read_committed\" mode.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405114847", "createdAt": "2020-04-07T21:09:50Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 375}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg4ODY1OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMDo0NlrOGCWRmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMDo0NlrOGCWRmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNTI4OQ==", "bodyText": "seekToEnd() will only reach the end-marker in read-committed mode, if there is no pending transaction.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405115289", "createdAt": "2020-04-07T21:10:46Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 391}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg5MTAzOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMTozM1rOGCWTIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDozODowM1rOGDFtfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNTY4MA==", "bodyText": "Strictly, position should be exactly endMarkerOffset + 1 -- it seems ok to just check for >", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405115680", "createdAt": "2020-04-07T21:11:33Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while(iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 410}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTcwMzIxMQ==", "bodyText": "Why we want to relax this check here?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405703211", "createdAt": "2020-04-08T17:46:29Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while(iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNTY4MA=="}, "originalCommit": null, "originalPosition": 410}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MjQ3OA==", "bodyText": "No reason. I can make it strict, too.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405892478", "createdAt": "2020-04-09T00:38:03Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while(iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNTY4MA=="}, "originalCommit": null, "originalPosition": 410}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg5NDgyOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjozOFrOGCWVWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjozOFrOGCWVWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjI0OA==", "bodyText": "For debugging purpose,  we now track the smallest and largest processed offset, too. This helps to understand which task during which phase processed which part of the data.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116248", "createdAt": "2020-04-07T21:12:38Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -45,12 +44,17 @@\n             public Processor<Object, Object> get() {\n                 return new AbstractProcessor<Object, Object>() {\n                     private int numRecordsProcessed = 0;\n+                    private long smallestOffset = Long.MAX_VALUE;\n+                    private long largestOffset = Long.MIN_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg5NjAyOnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjo1OFrOGCWWEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjo1OFrOGCWWEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjQzNA==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116434", "createdAt": "2020-04-07T21:12:58Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -76,39 +101,19 @@ public K apply(final Windowed<K> winKey, final V value) {\n     public static class Agg {\n \n         KeyValueMapper<String, Long, KeyValue<String, Long>> selector() {\n-            return new KeyValueMapper<String, Long, KeyValue<String, Long>>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg5NzE1OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMzoxN1rOGCWWww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMzoxN1rOGCWWww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjYxMQ==", "bodyText": "Removing unused method.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116611", "createdAt": "2020-04-07T21:13:17Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -120,14 +125,6 @@ public Long apply(final String aggKey, final Long value, final Long aggregate) {\n \n     static Serde<Double> doubleSerde = Serdes.Double();\n \n-    static File createDir(final File parent, final String child) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzg5OTU3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/StreamsEosTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDowM1rOGCWYRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDowM1rOGCWYRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjk5OQ==", "bodyText": "Config verification step.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116999", "createdAt": "2020-04-07T21:14:03Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/StreamsEosTest.java", "diffHunk": "@@ -39,12 +39,22 @@ public static void main(final String[] args) throws IOException {\n \n         final Properties streamsProperties = Utils.loadProps(propFileName);\n         final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);\n+        final String processingGuarantee = streamsProperties.getProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG);\n \n         if (kafka == null) {\n             System.err.println(\"No bootstrap kafka servers specified in \" + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);\n             System.exit(1);\n         }\n \n+        if (\"process\".equals(command) || \"process-complex\".equals(command)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzkwMTI2OnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/services/streams.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDo0MVrOGCWZWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDo0MVrOGCWZWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNzI3NA==", "bodyText": "We know set the processing guarantee via the properties file (that allows us to easily parametrize the test)", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405117274", "createdAt": "2020-04-07T21:14:41Z", "author": {"login": "mjsax"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -322,11 +322,20 @@ class StreamsEosTestBaseService(StreamsTestBaseService):\n \n     clean_node_enabled = True\n \n-    def __init__(self, test_context, kafka, command):\n+    def __init__(self, test_context, kafka, processing_guarantee, command):\n         super(StreamsEosTestBaseService, self).__init__(test_context,\n                                                         kafka,\n                                                         \"org.apache.kafka.streams.tests.StreamsEosTest\",\n                                                         command)\n+        self.PROCESSING_GUARANTEES = processing_guarantee\n+\n+    def prop_file(self):\n+        properties = {streams_property.STATE_DIR: self.PERSISTENT_ROOT,\n+                      streams_property.KAFKA_SERVERS: self.kafka.bootstrap_servers(),\n+                      streams_property.PROCESSING_GUARANTEES: self.PROCESSING_GUARANTEES}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDMzMTg5OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwMDowNTozNFrOGCadQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDoyNjo1MVrOGDFiWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MzgwOQ==", "bodyText": "Why do we need to add cleanUp now? Is this missing originally?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405183809", "createdAt": "2020-04-08T00:05:34Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java", "diffHunk": "@@ -102,17 +119,19 @@ public void surviveWithOneTaskAsStandby() throws ExecutionException, Interrupted\n \n             streamInstanceOne.close(Duration.ZERO);\n             streamInstanceTwo.close(Duration.ZERO);\n+\n+            streamInstanceOne.cleanUp();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4OTYyNw==", "bodyText": "Previously, the test was executed only once. Now it's executed twice. Because we use the same appId for both runs it seems better to add the cleanup (even if we might get two different temp directories anyway, so it might not be required)", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405889627", "createdAt": "2020-04-09T00:26:51Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java", "diffHunk": "@@ -102,17 +119,19 @@ public void surviveWithOneTaskAsStandby() throws ExecutionException, Interrupted\n \n             streamInstanceOne.close(Duration.ZERO);\n             streamInstanceTwo.close(Duration.ZERO);\n+\n+            streamInstanceOne.cleanUp();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MzgwOQ=="}, "originalCommit": null, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNzUyNjU1OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzoyMDo0M1rOGC5Orw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzoyMDo0M1rOGC5Orw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4Nzk4Mw==", "bodyText": "Can we use Util.waitUntilCondition here, or even simply using a latch and wait on it instead?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405687983", "createdAt": "2020-04-08T17:20:43Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNzUzMDcyOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzoyMTo1MVrOGC5RWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDozMDo1NFrOGDFmQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4ODY2NQ==", "bodyText": "How does this logic work? Are we updating the offsets map when the partition is not present?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405688665", "createdAt": "2020-04-08T17:21:51Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MDYyNQ==", "bodyText": "If the partition is not present, we first create a new entry in the map for the partition with an empty list. Afterward we add to the empty list. Ie, the code is short for:\nif (!offsets.containsKey(metadata.partition()) {\n  offsets.put(metadata.partition(), new LinkedList<>());\n}\noffsets.get(metadata.partition()).add(metadata.offset());", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405890625", "createdAt": "2020-04-09T00:30:54Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4ODY2NQ=="}, "originalCommit": null, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNzUzODY0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzoyMzo1NVrOGC5WMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDozNDowOVrOGDFpsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4OTkwNA==", "bodyText": "Does this verification reflect in the actual system test?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405689904", "createdAt": "2020-04-08T17:23:55Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());\n                     }\n+                });\n+\n+                updateNumRecordsProduces(1);\n+                if (numRecordsProduced % 1000 == 0) {\n+                    System.out.println(numRecordsProduced + \" records produced\");\n+                    System.out.flush();\n                 }\n-            });\n+                Utils.sleep(rand.nextInt(10));\n+            }\n+            producer.close();\n+            System.out.println(\"Producer closed: \" + numRecordsProduced + \" records produced\");\n+            System.out.flush();\n \n-            updateNumRecordsProduces(1);\n-            if (numRecordsProduced % 1000 == 0) {\n-                System.out.println(numRecordsProduced + \" records produced\");\n-                System.out.flush();\n+            // verify offsets\n+            for (final Map.Entry<Integer, List<Long>> offsetsOfPartition : offsets.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MTUwNA==", "bodyText": "It's a sanity check that we did not loose or duplicate any write.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405891504", "createdAt": "2020-04-09T00:34:09Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());\n                     }\n+                });\n+\n+                updateNumRecordsProduces(1);\n+                if (numRecordsProduced % 1000 == 0) {\n+                    System.out.println(numRecordsProduced + \" records produced\");\n+                    System.out.flush();\n                 }\n-            });\n+                Utils.sleep(rand.nextInt(10));\n+            }\n+            producer.close();\n+            System.out.println(\"Producer closed: \" + numRecordsProduced + \" records produced\");\n+            System.out.flush();\n \n-            updateNumRecordsProduces(1);\n-            if (numRecordsProduced % 1000 == 0) {\n-                System.out.println(numRecordsProduced + \" records produced\");\n-                System.out.flush();\n+            // verify offsets\n+            for (final Map.Entry<Integer, List<Long>> offsetsOfPartition : offsets.entrySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4OTkwNA=="}, "originalCommit": null, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNzU2NTE0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzozMDozOVrOGC5mow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDo0MjoyNVrOGDFx3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDExNQ==", "bodyText": "I think the Exit.addShutdownHook block can be outside of the largest try/finally block of the method.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405694115", "createdAt": "2020-04-08T17:30:39Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4OTc4NA==", "bodyText": "Maybe. Does it matter?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405889784", "createdAt": "2020-04-09T00:27:38Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDExNQ=="}, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MzU5OA==", "bodyText": "I should added in my previous comment it's a nit.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405893598", "createdAt": "2020-04-09T00:42:25Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDExNQ=="}, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNzU2NTI1OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzozMDo0MlrOGC5muw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDozOTozMVrOGDFvCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDEzOQ==", "bodyText": "What does this sleep do?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405694139", "createdAt": "2020-04-08T17:30:42Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n-        long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        final long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while (iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {\n+                    iterator.remove();\n+                    endMarkerOffset.remove(topicPartition);\n+                    System.out.println(\"Removing \" + topicPartition + \" at position \" + consumer.position(topicPartition));\n+                } else {\n+                    System.out.println(\"Retry \" + topicPartition + \" at position \" + consumer.position(topicPartition));\n                 }\n-\n             }\n+            sleep(1000L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 420}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5Mjg3Mg==", "bodyText": "We don't want to \"busy loop\" calling consumer.position() -- the tx-timeout is 10 seconds and thus if we update position() (that is a broker round trip) once a seconds it seems sufficient.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405892872", "createdAt": "2020-04-09T00:39:31Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n-        long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        final long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while (iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {\n+                    iterator.remove();\n+                    endMarkerOffset.remove(topicPartition);\n+                    System.out.println(\"Removing \" + topicPartition + \" at position \" + consumer.position(topicPartition));\n+                } else {\n+                    System.out.println(\"Retry \" + topicPartition + \" at position \" + consumer.position(topicPartition));\n                 }\n-\n             }\n+            sleep(1000L);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDEzOQ=="}, "originalCommit": null, "originalPosition": 420}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTE0NzY0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMTowNzo0N1rOGE1YJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMTowNzo0N1rOGE1YJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzcyMjAyMg==", "bodyText": "If we hit a TaskCorruptedException, we know that only a task in restore mode could be affected and those don't have anything to be committed (their commitNeeded flag should be set to false). Hence, we just commit all non-corrupted tasks. Afterwards we can safely call handleCorruption() (if we don't commit, we might abort a pending transaction for eos-beta incorrectly within handleCorruption())\n\\cc @abbccdda @guozhangwang", "url": "https://github.com/apache/kafka/pull/8440#discussion_r407722022", "createdAt": "2020-04-13T21:07:47Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -569,7 +570,13 @@ private void runLoop() {\n                 log.warn(\"Detected the states of tasks \" + e.corruptedTaskWithChangelogs() + \" are corrupted. \" +\n                              \"Will close the task as dirty and re-create and bootstrap from scratch.\", e);\n \n-                taskManager.commitAll();\n+                taskManager.commitInternal(\n+                    taskManager.tasks()\n+                        .values()\n+                        .stream()\n+                        .filter(t -> !e.corruptedTaskWithChangelogs().containsKey(t.id()))\n+                        .collect(Collectors.toSet())\n+                );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTIwMDQ0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMToyNDo1MlrOGE15Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMzoxNjoxMlrOGE4fYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczMDQ3MQ==", "bodyText": "Should we consider adding a unit test here, since this call is externalized?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r407730471", "createdAt": "2020-04-13T21:24:52Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -741,7 +741,7 @@ int commitAll() {\n         return commitInternal(tasks.values());\n     }\n \n-    private int commitInternal(final Collection<Task> tasks) {\n+    int commitInternal(final Collection<Task> tasks) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc3MzAyNQ==", "bodyText": "Good call.\nI also need to add a unit test that we actually commit all other tasks if a TaskCorruptedException is thrown. Just wanted to get the suggested fix reviewed first (also tested via system test run) before I close the unit test gaps.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r407773025", "createdAt": "2020-04-13T23:16:12Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -741,7 +741,7 @@ int commitAll() {\n         return commitInternal(tasks.values());\n     }\n \n-    private int commitInternal(final Collection<Task> tasks) {\n+    int commitInternal(final Collection<Task> tasks) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczMDQ3MQ=="}, "originalCommit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2924, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}