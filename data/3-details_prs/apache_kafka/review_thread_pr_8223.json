{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzODgxNjE1", "number": 8223, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMToxNjo0OVrODlNKww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQyMToyMzowNVrODq-cjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwMzM5NjUxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMToxNjo0OVrOFx97hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMToxNjo0OVrOFx97hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzkzOTIwNw==", "bodyText": "the tests in ReassignPartitionsCommandTest cover this PR. However, ReassignPartitionsCommandTest is still not stable even if this PR fixes the ReplicaAlterLogDirsThread since ReassignPartitionsCommand does not have correct \"wait\" for replica folder change.\nInstead of checking return value of Admin#alterReplicaLogDirs, this PR get the replica folder via Admin#describeLogDirs and make sure the target folder is existent and isFuture = false", "url": "https://github.com/apache/kafka/pull/8223#discussion_r387939207", "createdAt": "2020-03-04T21:16:49Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -634,18 +634,43 @@ class ReassignPartitionsCommand(zkClient: KafkaZkClient,\n         // Create reassignment znode so that controller will send LeaderAndIsrRequest to create replica in the broker\n         zkClient.createPartitionReassignment(validPartitions.map({case (key, value) => (new TopicPartition(key.topic, key.partition), value)}).toMap)\n \n-        // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n-        // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n-        var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n-        val replicasAssignedToFutureDir = mutable.Set.empty[TopicPartitionReplica]\n-        while (remainingTimeMs > 0 && replicasAssignedToFutureDir.size < proposedReplicaAssignment.size) {\n-          replicasAssignedToFutureDir ++= alterReplicaLogDirsIgnoreReplicaNotAvailable(\n-            proposedReplicaAssignment.filter { case (replica, _) => !replicasAssignedToFutureDir.contains(replica) },\n-            adminClientOpt.get, remainingTimeMs)\n-          Thread.sleep(100)\n-          remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+        if (proposedReplicaAssignment.nonEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwNzY1Mzk4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQyMjozMzo0M1rOFymo_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwNDo0MDoyMFrOF1RBvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODYwNjIwNg==", "bodyText": "Hmm, not sure about this. A given topic partition is always hashed into the same ReplicaAlterLogDirsThread. So, if that thread is dead, adding the same partition to the same thread won't help.\nDo you know why the ReplicaAlterLogDirsThread died? If it's a bug there, we probably should fix the logic in ReplicaAlterLogDirsThread.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r388606206", "createdAt": "2020-03-05T22:33:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -278,7 +278,7 @@ class Partition(val topicPartition: TopicPartition,\n             if (futureLogDir != logDir)\n               throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                 s\"different from the requested log dir $logDir\")\n-            false\n+            true", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcyNjkzMA==", "bodyText": "@junrao thanks for the feedback!\n\nDo you know why the ReplicaAlterLogDirsThread died? If it's a bug there, we probably should fix the logic in ReplicaAlterLogDirsThread.\n\n\nsend request to alter replica dir\nfuture log is created\nReplicaAlterLogDirsThread is created and it gets epoch N\nsend request to change partition reassignment (but the new reassignment is same to current reassignment)\nKafkaController assumes the reassignment is completed\nLeaderAndIsrRequest is sent\nReplicaManager#becomeLeaderOrFollower updates epoch of partition (from N to N + 1)\nReplicaAlterLogDirsThread encounters FENCED_LEADER_EPOCH since its fetch request is stale\nReplicaAlterLogDirsThread move the partition to failedPartitions\nReplicaAlterLogDirsThread get idle\nReplicaManager#becomeLeaderOrFollower shutdowns idle ReplicaAlterLogDirsThread\n\nSince the Partition.futureLog is existent, RepliaManager#alterReplicaLogDirs does NOT add ReplicaAlterLogDirsThread for requests.\n          if (partition.maybeCreateFutureReplica(destinationDir, highWatermarkCheckpoints)) {\n            val futureLog = futureLocalLogOrException(topicPartition)\n            logManager.abortAndPauseCleaning(topicPartition)\n\n            val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n              partition.getLeaderEpoch, futureLog.highWatermark)\n            replicaAlterLogDirsManager.addFetcherForPartitions(Map(topicPartition -> initialFetchState))\n          }", "url": "https://github.com/apache/kafka/pull/8223#discussion_r388726930", "createdAt": "2020-03-06T06:05:32Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -278,7 +278,7 @@ class Partition(val topicPartition: TopicPartition,\n             if (futureLogDir != logDir)\n               throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                 s\"different from the requested log dir $logDir\")\n-            false\n+            true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODYwNjIwNg=="}, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIxMDQxNw==", "bodyText": "@chia7712 : Thanks for the reply. Great finding! This seems to be a bug. So, in step 7, whenever we bump up the leader epoch in ReplicaManager#becomeLeaderOrFollower(). It seems that we should update the leader epoch for existing partitions in ReplicaAlterLogDirsThread. One way to do that is to first call replicaAlterLogDirsManager.removeFetcherForPartitions() and then addFetcherForPartitions() on that partition.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r389210417", "createdAt": "2020-03-07T01:05:49Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -278,7 +278,7 @@ class Partition(val topicPartition: TopicPartition,\n             if (futureLogDir != logDir)\n               throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                 s\"different from the requested log dir $logDir\")\n-            false\n+            true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODYwNjIwNg=="}, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTI5NTA2Nw==", "bodyText": "@junrao thanks for the suggestions!\nThis issue is still existent even if we updates the partitions in ReplicaManager#becomeLeaderOrFollower. For example, the following execution order causes the updated partition is removed.\n\nReplicaManager#updates the leader epoch of partition\nReplicaAlterLogDirsThread gets epoch error due to (local) stale leader epoch\nReplicaManager#updates the leader epoch of ReplicaAlterLogDirsThread\nReplicaAlterLogDirsThread remove fenced partition and then get idle\n\nIt seems to me the root cause is that we do not change both parittion epoch and ReplicaAlterLogDirsThread epoch atomically. Hence, removing the fenced partition from ReplicaAlterLogDirsThread is dangerous to this issue. If we want to make ReplicaAlterLogDirsThread be recoverable from fenced error, the fenced partition should be added back as \"delay\" partition. Also, ReplicaManager#becomeLeaderOrFollower should update the epoch of ReplicaAlterLogDirsThread.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r389295067", "createdAt": "2020-03-07T17:32:58Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -278,7 +278,7 @@ class Partition(val topicPartition: TopicPartition,\n             if (futureLogDir != logDir)\n               throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                 s\"different from the requested log dir $logDir\")\n-            false\n+            true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODYwNjIwNg=="}, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMjY4NQ==", "bodyText": "@chia7712 : Thanks for the reply. If a partition is removed (or re-inserted) from AbstractFetcherThread (which ReplicaAlterLogDirsThread inherits from), processFetchRequest() will ignore any pending fetch response for that partition. So, the scenario that you described should be handled even though we don't have the logic to change epoch atomically in multiple places.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r391312685", "createdAt": "2020-03-11T22:46:00Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -278,7 +278,7 @@ class Partition(val topicPartition: TopicPartition,\n             if (futureLogDir != logDir)\n               throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                 s\"different from the requested log dir $logDir\")\n-            false\n+            true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODYwNjIwNg=="}, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM5NzgyMw==", "bodyText": "If a partition is removed (or re-inserted) from AbstractFetcherThread (which ReplicaAlterLogDirsThread inherits from), processFetchRequest() will ignore any pending fetch response for that partition\n\nThanks for the feedback! This comment is totally right but the key point related to this issue is the partition is removed by AbstractFetcherThread#processFetchRequest().\n                case Errors.FENCED_LEADER_EPOCH =>\n                  onPartitionFenced(topicPartition)\nAfter the partition is removed, the thread becomes idle and then ReplicaManager#becomeLeaderOrFollower remove it.\n        replicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset)\n\n        replicaFetcherManager.shutdownIdleFetcherThreads()\n        replicaAlterLogDirsManager.shutdownIdleFetcherThreads()  // this line\nAnd noted that futureReplicasAndInitialOffset is NOT empty only if there is \"new\" partition. In this case, there is no new partition so nothing is re-added to replicaAlterLogDirsManager.\nIn short, the thread is shutdown and it leaves a future folder.\nAnother issue is ReplicaManager#alterReplicaLogDirs add new partition to ReplicaAlterLogDirsManager only if the future folder is NOT existent (and it is created successfully)\n          if (partition.maybeCreateFutureReplica(destinationDir, highWatermarkCheckpoints)) {\n            ...\n            replicaAlterLogDirsManager.addFetcherForPartitions(Map(topicPartition -> initialFetchState))\n          }\nSo the following request to alter replica folder don't work util the future replica is removed.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r391397823", "createdAt": "2020-03-12T04:40:20Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -278,7 +278,7 @@ class Partition(val topicPartition: TopicPartition,\n             if (futureLogDir != logDir)\n               throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                 s\"different from the requested log dir $logDir\")\n-            false\n+            true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODYwNjIwNg=="}, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMzU3NTUyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwNjo1NTo0OFrOFzcbJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwNjo1NTo0OFrOFzcbJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ4NzM5Ng==", "bodyText": "Moving the replica folder to another location has chance to produce this issue so I separate the origin test case to two cases. The first case always move the folder to same location. Another case does move the folder to different location.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r389487396", "createdAt": "2020-03-09T06:55:48Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala", "diffHunk": "@@ -161,19 +161,31 @@ class ReassignPartitionsClusterTest extends ZooKeeperTestHarness with Logging {\n   }\n \n   @Test\n-  def shouldMoveSinglePartitionWithinBroker(): Unit = {\n+  def shouldMoveSinglePartitionToSameFolderWithinBroker(): Unit = shouldMoveSinglePartitionWithinBroker(true)\n+\n+  @Test\n+  def shouldMoveSinglePartitionToDifferentFolderWithinBroker(): Unit = shouldMoveSinglePartitionWithinBroker(false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyODY5Mjc4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMDozMjo1M1rOF1uYBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNTo1MjozNlrOF2I7bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg3ODY2MA==", "bodyText": "For the normal fetcher, we force the replica into a truncating phase after each epoch change. I'm trying to convince myself whether we need to do this for the log dir fetcher or not. Although we have similar log reconciliation logic implemented for the log dir fetcher, it seems we only exercise it when the log dir fetcher is first initialized for a partition. Outside of that, we have an asynchronous truncation path which gets executed after the current log is truncated upon becoming follower. If we think this is sufficient to guarantee consistency, then I'm wondering whether the epoch validation when the log dir fetcher is fetching against the active log is actually buying us anything. An alternative would be that we leave the epoch uninitialized which would cause us to skip epoch validation.\n@junrao Any thoughts?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r391878660", "createdAt": "2020-03-12T20:32:53Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1297,7 +1299,10 @@ class ReplicaManager(val config: KafkaConfig,\n             }\n           }\n         }\n+        // handle the new partition\n         replicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset)\n+        // handle the partitions having new epoch\n+        replicaAlterLogDirsManager.updateEpoch(updatedPartitions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzOTg3Mw==", "bodyText": "@hachikuji : Another thing is that epoch could potentially help in the case of unclean leader election.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r391939873", "createdAt": "2020-03-12T22:29:46Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1297,7 +1299,10 @@ class ReplicaManager(val config: KafkaConfig,\n             }\n           }\n         }\n+        // handle the new partition\n         replicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset)\n+        // handle the partitions having new epoch\n+        replicaAlterLogDirsManager.updateEpoch(updatedPartitions)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg3ODY2MA=="}, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMxMzcxMA==", "bodyText": "I gave this a bit more thought. I think it would be safer to have the log dir fetcher mimic the regular fetcher when it comes to the truncation protocol. That said, I don't think we need to address this here. The fix in this patch addresses the main problem at the moment.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392313710", "createdAt": "2020-03-13T15:52:36Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1297,7 +1299,10 @@ class ReplicaManager(val config: KafkaConfig,\n             }\n           }\n         }\n+        // handle the new partition\n         replicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset)\n+        // handle the partitions having new epoch\n+        replicaAlterLogDirsManager.updateEpoch(updatedPartitions)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg3ODY2MA=="}, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMTM5NDk3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNTozMTo1OFrOF2ILvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxNjozNjowNVrOF28YAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwMTUwMQ==", "bodyText": "I'm not sure about this. If we know the partition has been fenced, why retry until the epoch has been updated? Alternatively, we could let it be marked failed and remove it when the epoch gets updated. The downside of the approach here is that a partition which is in a permanently failed state due to a stale epoch will not get reflected in the metric.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392301501", "createdAt": "2020-03-13T15:31:58Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -255,7 +255,7 @@ abstract class AbstractFetcherThread(name: String,\n             fetchOffsets.put(tp, offsetTruncationState)\n \n         case Errors.FENCED_LEADER_EPOCH =>\n-          onPartitionFenced(tp)\n+          partitionsWithError += onPartitionFenced(tp)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjYwNDIyMw==", "bodyText": "If the partition in failed epoch is \"removed\", the thread get idle and is able to be \"shutdown\" by ReplicaManager#becomeLeaderOrFollower.\n        replicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset)\n\n        replicaFetcherManager.shutdownIdleFetcherThreads()\n        replicaAlterLogDirsManager.shutdownIdleFetcherThreads()\nAnd we can't create/update thread for the partition having future log...\n          if (partition.maybeCreateFutureReplica(destinationDir, highWatermarkCheckpoints)) {\n            val futureLog = futureLocalLogOrException(topicPartition)\n            logManager.abortAndPauseCleaning(topicPartition)\n\n            val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n              partition.getLeaderEpoch, futureLog.highWatermark)\n            replicaAlterLogDirsManager.addFetcherForPartitions(Map(topicPartition -> initialFetchState))\n          }\nI had uploaded a patch to display above case (see https://github.com/apache/kafka/files/4322767/test_kafka_9654.txt)\nIt seems to me that it is hard to make sure whether the failed state is permanent or not. Instead of making alter thread recoverable from the failed state since, my previous patch is to make sure the alter thread is capable to be updated even if the target partition has future log.\n  def maybeCreateFutureReplica(logDir: String, highWatermarkCheckpoints: OffsetCheckpoints): Boolean = {\n    // The writeLock is needed to make sure that while the caller checks the log directory of the\n    // current replica and the existence of the future replica, no other thread can update the log directory of the\n    // current replica or remove the future replica.\n    inWriteLock(leaderIsrUpdateLock) {\n      val currentLogDir = localLogOrException.dir.getParent\n      if (currentLogDir == logDir) {\n        info(s\"Current log directory $currentLogDir is same as requested log dir $logDir. \" +\n          s\"Skipping future replica creation.\")\n        false\n      } else {\n        futureLog match {\n          case Some(partitionFutureLog) =>\n            val futureLogDir = partitionFutureLog.dir.getParent\n            if (futureLogDir != logDir)\n              throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                s\"different from the requested log dir $logDir\")\n            false  // change this to true\n          case None =>\n            createLogIfNotExists(Request.FutureLocalReplicaId, isNew = false, isFutureReplica = true, highWatermarkCheckpoints)\n            true\n        }\n      }\n    }\n  }", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392604223", "createdAt": "2020-03-14T17:19:47Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -255,7 +255,7 @@ abstract class AbstractFetcherThread(name: String,\n             fetchOffsets.put(tp, offsetTruncationState)\n \n         case Errors.FENCED_LEADER_EPOCH =>\n-          onPartitionFenced(tp)\n+          partitionsWithError += onPartitionFenced(tp)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwMTUwMQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjczMDY4Mw==", "bodyText": "I understand the complication, but it does not seem like a big problem. When we update the epoch in ReplicaAlterLogDirsManager, we can first make a call to addFetcherForPartitions, which creates the fetcher thread if needed and removes the partition from the failed state.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392730683", "createdAt": "2020-03-16T00:16:10Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -255,7 +255,7 @@ abstract class AbstractFetcherThread(name: String,\n             fetchOffsets.put(tp, offsetTruncationState)\n \n         case Errors.FENCED_LEADER_EPOCH =>\n-          onPartitionFenced(tp)\n+          partitionsWithError += onPartitionFenced(tp)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwMTUwMQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgwMDM1OA==", "bodyText": "@hachikuji  thanks for the feedback.\n\nwe can first make a call to addFetcherForPartitions, which creates the fetcher thread if needed and removes the partition from the failed state.\n\nDo you mean it should re-add the updated partition to fetcher thread instead of updating only epoch?\nreplicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset ++ updatedPartitions)", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392800358", "createdAt": "2020-03-16T06:18:45Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -255,7 +255,7 @@ abstract class AbstractFetcherThread(name: String,\n             fetchOffsets.put(tp, offsetTruncationState)\n \n         case Errors.FENCED_LEADER_EPOCH =>\n-          onPartitionFenced(tp)\n+          partitionsWithError += onPartitionFenced(tp)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwMTUwMQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE1NjYwOA==", "bodyText": "Yes. That is how it works for the replica fetcher, for example. If a follower's epoch gets fenced, the partition is temporarily marked as failed. After we receive the LeaderAndIsr request and update the epoch, the partition is re-added to the fetcher manager.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r393156608", "createdAt": "2020-03-16T16:36:05Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -255,7 +255,7 @@ abstract class AbstractFetcherThread(name: String,\n             fetchOffsets.put(tp, offsetTruncationState)\n \n         case Errors.FENCED_LEADER_EPOCH =>\n-          onPartitionFenced(tp)\n+          partitionsWithError += onPartitionFenced(tp)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwMTUwMQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMTQ0MDEyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNTo0NDoyMlrOF2Iocw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNzo0NzowNlrOF2axbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwODg1MQ==", "bodyText": "Would it be reasonable to pull this change into a separate PR? I think the fix in the log dir fetcher is more important and a more isolated change will be easier to backport to older versions.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392308851", "createdAt": "2020-03-13T15:44:22Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -634,18 +634,43 @@ class ReassignPartitionsCommand(zkClient: KafkaZkClient,\n         // Create reassignment znode so that controller will send LeaderAndIsrRequest to create replica in the broker\n         zkClient.createPartitionReassignment(validPartitions.map({case (key, value) => (new TopicPartition(key.topic, key.partition), value)}).toMap)\n \n-        // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n-        // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n-        var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n-        val replicasAssignedToFutureDir = mutable.Set.empty[TopicPartitionReplica]\n-        while (remainingTimeMs > 0 && replicasAssignedToFutureDir.size < proposedReplicaAssignment.size) {\n-          replicasAssignedToFutureDir ++= alterReplicaLogDirsIgnoreReplicaNotAvailable(\n-            proposedReplicaAssignment.filter { case (replica, _) => !replicasAssignedToFutureDir.contains(replica) },\n-            adminClientOpt.get, remainingTimeMs)\n-          Thread.sleep(100)\n-          remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+        if (proposedReplicaAssignment.nonEmpty) {\n+          // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n+          // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n+          var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+          def nonFutureReplicas() = adminClientOpt.get", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjYwNjA2MQ==", "bodyText": "Got it. trace this by https://issues.apache.org/jira/browse/KAFKA-9721", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392606061", "createdAt": "2020-03-14T17:47:06Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -634,18 +634,43 @@ class ReassignPartitionsCommand(zkClient: KafkaZkClient,\n         // Create reassignment znode so that controller will send LeaderAndIsrRequest to create replica in the broker\n         zkClient.createPartitionReassignment(validPartitions.map({case (key, value) => (new TopicPartition(key.topic, key.partition), value)}).toMap)\n \n-        // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n-        // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n-        var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n-        val replicasAssignedToFutureDir = mutable.Set.empty[TopicPartitionReplica]\n-        while (remainingTimeMs > 0 && replicasAssignedToFutureDir.size < proposedReplicaAssignment.size) {\n-          replicasAssignedToFutureDir ++= alterReplicaLogDirsIgnoreReplicaNotAvailable(\n-            proposedReplicaAssignment.filter { case (replica, _) => !replicasAssignedToFutureDir.contains(replica) },\n-            adminClientOpt.get, remainingTimeMs)\n-          Thread.sleep(100)\n-          remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+        if (proposedReplicaAssignment.nonEmpty) {\n+          // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n+          // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n+          var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+          def nonFutureReplicas() = adminClientOpt.get", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwODg1MQ=="}, "originalCommit": null, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0NTY5MTAyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxODo1NDozM1rOF4Ss6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQwNToxMDowOVrOF4gUNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3MDk4Ng==", "bodyText": "Could this be requestEpoch.contains(currentLeaderEpoch)? If we got this error and we didn't provide an epoch, then it suggests an error on the leader. Maybe retrying is best in that case?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394570986", "createdAt": "2020-03-18T18:54:33Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -266,12 +271,22 @@ abstract class AbstractFetcherThread(name: String,\n     ResultWithPartitions(fetchOffsets, partitionsWithError)\n   }\n \n-  private def onPartitionFenced(tp: TopicPartition): Unit = inLock(partitionMapLock) {\n-    Option(partitionStates.stateValue(tp)).foreach { currentFetchState =>\n+  /**\n+   * remove the partition if the partition state is NOT updated. Otherwise, keep the partition active.\n+   * @return true if the epoch is updated\n+   */\n+  private def onPartitionFenced(tp: TopicPartition, requestEpoch: Option[Int]): Boolean = inLock(partitionMapLock) {\n+    Option(partitionStates.stateValue(tp)).exists { currentFetchState =>\n       val currentLeaderEpoch = currentFetchState.currentLeaderEpoch\n-      info(s\"Partition $tp has an older epoch ($currentLeaderEpoch) than the current leader. Will await \" +\n-        s\"the new LeaderAndIsr state before resuming fetching.\")\n-      markPartitionFailed(tp)\n+      if (requestEpoch.forall(_ == currentLeaderEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc5NDAzOA==", "bodyText": "you are right !", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394794038", "createdAt": "2020-03-19T05:10:09Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -266,12 +271,22 @@ abstract class AbstractFetcherThread(name: String,\n     ResultWithPartitions(fetchOffsets, partitionsWithError)\n   }\n \n-  private def onPartitionFenced(tp: TopicPartition): Unit = inLock(partitionMapLock) {\n-    Option(partitionStates.stateValue(tp)).foreach { currentFetchState =>\n+  /**\n+   * remove the partition if the partition state is NOT updated. Otherwise, keep the partition active.\n+   * @return true if the epoch is updated\n+   */\n+  private def onPartitionFenced(tp: TopicPartition, requestEpoch: Option[Int]): Boolean = inLock(partitionMapLock) {\n+    Option(partitionStates.stateValue(tp)).exists { currentFetchState =>\n       val currentLeaderEpoch = currentFetchState.currentLeaderEpoch\n-      info(s\"Partition $tp has an older epoch ($currentLeaderEpoch) than the current leader. Will await \" +\n-        s\"the new LeaderAndIsr state before resuming fetching.\")\n-      markPartitionFailed(tp)\n+      if (requestEpoch.forall(_ == currentLeaderEpoch)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3MDk4Ng=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0NTcxOTc0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxOTowMzoxM1rOF4S_xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQwNToxMDowOFrOF4gUMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3NTgxNA==", "bodyText": "Shouldn't we return the result of onPartitionFenced?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394575814", "createdAt": "2020-03-18T19:03:13Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -531,7 +548,7 @@ abstract class AbstractFetcherThread(name: String,\n       true\n     } catch {\n       case _: FencedLeaderEpochException =>\n-        onPartitionFenced(topicPartition)\n+        onPartitionFenced(topicPartition, requestEpoch)\n         true", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc5NDAzMg==", "bodyText": "you are right again :)", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394794032", "createdAt": "2020-03-19T05:10:08Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -531,7 +548,7 @@ abstract class AbstractFetcherThread(name: String,\n       true\n     } catch {\n       case _: FencedLeaderEpochException =>\n-        onPartitionFenced(topicPartition)\n+        onPartitionFenced(topicPartition, requestEpoch)\n         true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3NTgxNA=="}, "originalCommit": null, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0NTczNzY1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxOTowODozN1rOF4TK6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQwNToxMDowNlrOF4gULA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3ODY2NQ==", "bodyText": "Should this be an assertion? Is the test not deterministic for some reason?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394578665", "createdAt": "2020-03-18T19:08:37Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,56 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")\n+      // the partition is added to failedPartitions if fenced error happens\n+      if (replicaManager.replicaAlterLogDirsManager.failedPartitions.size != 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc5NDAyOA==", "bodyText": "If the thread is done before ReplicaManager#becomeLeaderOrFollower, the faced error does not happen.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394794028", "createdAt": "2020-03-19T05:10:06Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,56 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")\n+      // the partition is added to failedPartitions if fenced error happens\n+      if (replicaManager.replicaAlterLogDirsManager.failedPartitions.size != 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3ODY2NQ=="}, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2Mzg5OTAwOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQyMToyMzowNVrOF7DnYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxNzo1MToyNlrOF7nRnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ2OTUzNw==", "bodyText": "@chia7712 In our IC we are consistently getting a failure in this check. Do you have any suggestion on what is happening and how to fix it?\nError Message\norg.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending state\n\nStacktrace\norg.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending state\n\tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)\n\tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)\n\tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)\n\tat org.scalatest.Assertions.fail(Assertions.scala:1091)\n\tat org.scalatest.Assertions.fail$(Assertions.scala:1087)\n\tat org.scalatest.Assertions$.fail(Assertions.scala:1389)\n\tat kafka.server.ReplicaManagerTest.testFencedErrorCausedByBecomeLeader(ReplicaManagerTest.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\n\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\n\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)\n\tat org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)\n\tat sun.reflect.GeneratedMethodAccessor112.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat com.sun.proxy.$Proxy2.processTestClass(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)\n\tat sun.reflect.GeneratedMethodAccessor111.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)\n\tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)\n\tat org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)\n\tat java.lang.Thread.run(Thread.java:748)\n\nStandard Output\n[2020-03-24 17:39:19,377] ERROR [ReplicaAlterLogDirsThread-0]: Error due to (kafka.server.ReplicaAlterLogDirsThread:76)\norg.apache.kafka.common.errors.ReplicaNotAvailableException: Future log for partition test-topic-0 is not available on broker 0\n[2020-03-24 17:39:38,537] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-topic-0 (kafka.server.ReplicaManager:76)\norg.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producerId 234 at offset 3 in partition test-topic-0: 13 (incoming seq. number), 2 (current end sequence number)", "url": "https://github.com/apache/kafka/pull/8223#discussion_r397469537", "createdAt": "2020-03-24T21:23:05Z", "author": {"login": "jsancio"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,58 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ3MTA5MQ==", "bodyText": "@jsancio Thanks for this report. Let me dig in it :)", "url": "https://github.com/apache/kafka/pull/8223#discussion_r397471091", "createdAt": "2020-03-24T21:26:25Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,58 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ2OTUzNw=="}, "originalCommit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ3MjQyNg==", "bodyText": "seems there is a ticket (https://issues.apache.org/jira/browse/KAFKA-9750)", "url": "https://github.com/apache/kafka/pull/8223#discussion_r397472426", "createdAt": "2020-03-24T21:28:46Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,58 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ2OTUzNw=="}, "originalCommit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA1Mzc4OA==", "bodyText": "Thanks. I update the Jira with the same information above.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r398053788", "createdAt": "2020-03-25T17:51:26Z", "author": {"login": "jsancio"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,58 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ2OTUzNw=="}, "originalCommit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a"}, "originalPosition": 41}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3424, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}