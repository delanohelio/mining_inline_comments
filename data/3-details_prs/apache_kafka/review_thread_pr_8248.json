{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1MTA4MTg4", "number": 8248, "reviewThreads": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1MzoyOVrODnTzFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNDozNDozNFrOEA6r9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyNTQ1NDI4OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1MzoyOVrOF1O3lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1MzoyOVrOF1O3lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MjQ1NQ==", "bodyText": "Factored out the following to be reused in the case of transitioning from standby; a standby has only the state manager, everything else must be created just like a new task", "url": "https://github.com/apache/kafka/pull/8248#discussion_r391362455", "createdAt": "2020-03-12T01:53:29Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -132,47 +130,74 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                 partitions\n             );\n \n-            if (threadProducer == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyNTQ1Njc3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1NToxNFrOF1O5Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMTo0ODowOFrOGLqbKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MjgzOA==", "bodyText": "Pretty sure we should still count a task transition as a task creation event for the purposes of metrics", "url": "https://github.com/apache/kafka/pull/8248#discussion_r391362838", "createdAt": "2020-03-12T01:55:14Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -132,47 +130,74 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                 partitions\n             );\n \n-            if (threadProducer == null) {\n-                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n-                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n-                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n-                log.info(\"Creating producer client for task {}\", taskId);\n-                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n-            }\n-\n-            final RecordCollector recordCollector = new RecordCollectorImpl(\n-                logContext,\n-                taskId,\n-                consumer,\n-                threadProducer != null ?\n-                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n-                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n-                config.defaultProductionExceptionHandler(),\n-                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n-                streamsMetrics\n-            );\n-\n-            final Task task = new StreamTask(\n+            createdTasks.add(createStreamTask(\n                 taskId,\n                 partitions,\n-                topology,\n                 consumer,\n-                config,\n-                streamsMetrics,\n-                stateDirectory,\n-                cache,\n-                time,\n+                logContext,\n                 stateManager,\n-                recordCollector\n-            );\n-\n-            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n-            createdTasks.add(task);\n-            createTaskSensor.record();\n+                topology));\n         }\n         return createdTasks;\n     }\n \n+    private StreamTask createStreamTask(final TaskId taskId,\n+                                        final Set<TopicPartition> partitions,\n+                                        final Consumer<byte[], byte[]> consumer,\n+                                        final LogContext logContext,\n+                                        final ProcessorStateManager stateManager,\n+                                        final ProcessorTopology topology) {\n+        if (threadProducer == null) {\n+            final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+            producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+            log.info(\"Creating producer client for task {}\", taskId);\n+            taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+        }\n+\n+        final RecordCollector recordCollector = new RecordCollectorImpl(\n+            logContext,\n+            taskId,\n+            consumer,\n+            threadProducer != null ?\n+                new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                                                                                          new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+            config.defaultProductionExceptionHandler(),\n+            EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+            streamsMetrics\n+        );\n+\n+        final StreamTask task = new StreamTask(\n+            taskId,\n+            partitions,\n+            topology,\n+            consumer,\n+            config,\n+            streamsMetrics,\n+            stateDirectory,\n+            cache,\n+            time,\n+            stateManager,\n+            recordCollector\n+        );\n+\n+        log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+        createTaskSensor.record();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg4MjYwMA==", "bodyText": "I agree.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414882600", "createdAt": "2020-04-24T21:48:08Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -132,47 +130,74 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                 partitions\n             );\n \n-            if (threadProducer == null) {\n-                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n-                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n-                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n-                log.info(\"Creating producer client for task {}\", taskId);\n-                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n-            }\n-\n-            final RecordCollector recordCollector = new RecordCollectorImpl(\n-                logContext,\n-                taskId,\n-                consumer,\n-                threadProducer != null ?\n-                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n-                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n-                config.defaultProductionExceptionHandler(),\n-                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n-                streamsMetrics\n-            );\n-\n-            final Task task = new StreamTask(\n+            createdTasks.add(createStreamTask(\n                 taskId,\n                 partitions,\n-                topology,\n                 consumer,\n-                config,\n-                streamsMetrics,\n-                stateDirectory,\n-                cache,\n-                time,\n+                logContext,\n                 stateManager,\n-                recordCollector\n-            );\n-\n-            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n-            createdTasks.add(task);\n-            createTaskSensor.record();\n+                topology));\n         }\n         return createdTasks;\n     }\n \n+    private StreamTask createStreamTask(final TaskId taskId,\n+                                        final Set<TopicPartition> partitions,\n+                                        final Consumer<byte[], byte[]> consumer,\n+                                        final LogContext logContext,\n+                                        final ProcessorStateManager stateManager,\n+                                        final ProcessorTopology topology) {\n+        if (threadProducer == null) {\n+            final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+            producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+            log.info(\"Creating producer client for task {}\", taskId);\n+            taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+        }\n+\n+        final RecordCollector recordCollector = new RecordCollectorImpl(\n+            logContext,\n+            taskId,\n+            consumer,\n+            threadProducer != null ?\n+                new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                                                                                          new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+            config.defaultProductionExceptionHandler(),\n+            EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+            streamsMetrics\n+        );\n+\n+        final StreamTask task = new StreamTask(\n+            taskId,\n+            partitions,\n+            topology,\n+            consumer,\n+            config,\n+            streamsMetrics,\n+            stateDirectory,\n+            cache,\n+            time,\n+            stateManager,\n+            recordCollector\n+        );\n+\n+        log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+        createTaskSensor.record();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MjgzOA=="}, "originalCommit": null, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDg3MDAwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskCreationIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMjozMToxM1rOGLDpJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjo0MzoyMlrOGPXjiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI0NzIwNA==", "bodyText": "Wasn't really sure where to put this test, but this class seemed close enough. Verified that this does indeed fail on trunk without this fix", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414247204", "createdAt": "2020-04-24T02:31:13Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskCreationIntegrationTest.java", "diffHunk": "@@ -148,6 +161,82 @@ public void shouldCreateStandByTasksForMaterializedAndOptimizedSourceTables() th\n         );\n     }\n \n+    @Test\n+    public void shouldRecycleStateFromStandbyTaskPromotedToActiveTask() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2Nzc1Mw==", "bodyText": "This seems fine to me.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418767753", "createdAt": "2020-05-01T22:43:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskCreationIntegrationTest.java", "diffHunk": "@@ -148,6 +161,82 @@ public void shouldCreateStandByTasksForMaterializedAndOptimizedSourceTables() th\n         );\n     }\n \n+    @Test\n+    public void shouldRecycleStateFromStandbyTaskPromotedToActiveTask() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI0NzIwNA=="}, "originalCommit": null, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODg2NDM5OnYy", "diffSide": "RIGHT", "path": "checkstyle/suppressions.xml", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMDoxNjozNVrOGLnrPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwMzowMToxNlrOGPcnoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ==", "bodyText": "Ok, it seems like this one at least is avoidable.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414837565", "createdAt": "2020-04-24T20:16:35Z", "author": {"login": "vvcephei"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc4MDI0MA==", "bodyText": "Man, this is like being the person to finish the tequila bottle and having to eat the worm. Everyone's been adding to TaskManager#handleAssignment a few lines at a time and I was the lucky one to push it over the checkstyle edge \ud83c\udf40", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418780240", "createdAt": "2020-05-01T23:39:38Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc4MDg5MA==", "bodyText": "Kidding aside, Guozhang keeps telling me that @mjsax is (or is about to be) working on a PR to clean up the TaskManager. I wanted to keep the conflicts to a minimum, especially if it'll all be rewritten soon anyway. Would you be satisfied if I made a ticket for myself to loop back around later?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418780890", "createdAt": "2020-05-01T23:42:55Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc4MjU0Mg==", "bodyText": "Given the current progress on KIP-447 testing, I would start working on the refactoring next week. keeping fingers crossed", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418782542", "createdAt": "2020-05-01T23:51:47Z", "author": {"login": "mjsax"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODgzMDA3OQ==", "bodyText": "Fine by me!\nBy the way, I think you mean getting to eat the worm. ;)", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418830079", "createdAt": "2020-05-02T02:06:26Z", "author": {"login": "vvcephei"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODgzMTk3OA==", "bodyText": "I think we can actually remove the suppression for StreamsPartitionAssigor though. So net neutral style violations \ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418831978", "createdAt": "2020-05-02T02:11:21Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg1MDcyMQ==", "bodyText": "Oh, awesome! Maybe we should try to remove them all at some point, to figure out which ones we really still need.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418850721", "createdAt": "2020-05-02T03:01:16Z", "author": {"login": "vvcephei"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3OTM0NTA2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjo1Mzo0NFrOGLr3Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwMjoxNDoxN1rOGPbinQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwNjE3NQ==", "bodyText": "It seems like duelling formatters here and above. Do you want to propose that these it's better this way?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414906175", "createdAt": "2020-04-24T22:53:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -235,9 +264,16 @@ void closeAndRemoveTaskProducerIfNeeded(final TaskId id) {\n             return Collections.singleton(getThreadProducerClientId(threadId));\n         } else {\n             return taskProducers.keySet()\n-                                .stream()\n-                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n-                                .collect(Collectors.toSet());\n+                .stream()\n+                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                .collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODgzMzA1Mw==", "bodyText": "I don't really have skin in the game, but I would argue that this is more consistent with the rest of the codebase.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418833053", "createdAt": "2020-05-02T02:14:17Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -235,9 +264,16 @@ void closeAndRemoveTaskProducerIfNeeded(final TaskId id) {\n             return Collections.singleton(getThreadProducerClientId(threadId));\n         } else {\n             return taskProducers.keySet()\n-                                .stream()\n-                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n-                                .collect(Collectors.toSet());\n+                .stream()\n+                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                .collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwNjE3NQ=="}, "originalCommit": null, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTY2ODcxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMTo1OTozMFrOGPW1WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMTo1OTozMFrOGPW1WA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1NTkyOA==", "bodyText": "This comment needs to move into isLoggingEnabled. It doesn't make sense in this context anymore.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418755928", "createdAt": "2020-05-01T21:59:30Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +271,25 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {\n+        // if the store name does not exist in the changelog map, it means the underlying store\n+        // is not log enabled (including global stores), and hence it does not need to be restored", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTY3NDUwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjowMjoyNVrOGPW4vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjowMjoyNVrOGPW4vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1Njc5Nw==", "bodyText": "And this comment should move into getStorePartition", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418756797", "createdAt": "2020-05-01T22:02:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +271,25 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {\n+        // if the store name does not exist in the changelog map, it means the underlying store\n+        // is not log enabled (including global stores), and hence it does not need to be restored\n+        if (isLoggingEnabled(storeName)) {\n+            // NOTE we assume the partition of the topic can always be inferred from the task id;\n+            // if user ever use a custom partition grouper (deprecated in KIP-528) this would break and\n+            // it is not a regression (it would always break anyways)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTY4MjExOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjowNjoyOFrOGPW9dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjowNjoyOFrOGPW9dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1ODAwNw==", "bodyText": "Seems like this comment has also become displaced from its intended location before L83, stateMgr.initializeStoreOffsetsFromCheckpoint(storeDirsEmpty)", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418758007", "createdAt": "2020-05-01T22:06:28Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -70,18 +69,16 @@ static void registerStateStores(final Logger log,\n                 e\n             );\n         }\n+\n         log.debug(\"Acquired state directory lock\");\n \n         final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n \n         // We should only load checkpoint AFTER the corresponding state directory lock has been acquired and\n         // the state stores have been registered; we should not try to load at the state manager construction time.\n         // See https://issues.apache.org/jira/browse/KAFKA-8574", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTY5MjUxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjoxMjoyN1rOGPXD9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwMzowMjo0M1rOGPcp_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1OTY2OA==", "bodyText": "It seems like the intent here is to skip initializing the store again if it's already been initialized, but still register it to the changelog reader. Which is ok, since we unregisterAllStoresWithChangelogReader in the recycleState, right?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418759668", "createdAt": "2020-05-01T22:12:27Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -70,18 +69,16 @@ static void registerStateStores(final Logger log,\n                 e\n             );\n         }\n+\n         log.debug(\"Acquired state directory lock\");\n \n         final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n \n         // We should only load checkpoint AFTER the corresponding state directory lock has been acquired and\n         // the state stores have been registered; we should not try to load at the state manager construction time.\n         // See https://issues.apache.org/jira/browse/KAFKA-8574\n-        for (final StateStore store : topology.stateStores()) {\n-            processorContext.uninitialize();\n-            store.init(processorContext, store);\n-            log.trace(\"Registered state store {}\", store.name());\n-        }\n+        stateMgr.registerStateStores(topology.stateStores(), processorContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODgzNzg2Nw==", "bodyText": "That's the idea", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418837867", "createdAt": "2020-05-02T02:27:41Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -70,18 +69,16 @@ static void registerStateStores(final Logger log,\n                 e\n             );\n         }\n+\n         log.debug(\"Acquired state directory lock\");\n \n         final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n \n         // We should only load checkpoint AFTER the corresponding state directory lock has been acquired and\n         // the state stores have been registered; we should not try to load at the state manager construction time.\n         // See https://issues.apache.org/jira/browse/KAFKA-8574\n-        for (final StateStore store : topology.stateStores()) {\n-            processorContext.uninitialize();\n-            store.init(processorContext, store);\n-            log.trace(\"Registered state store {}\", store.name());\n-        }\n+        stateMgr.registerStateStores(topology.stateStores(), processorContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1OTY2OA=="}, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODgzODUyNg==", "bodyText": "We need to unregister and register the changelogs during task type conversion, as standby changelogs are handled differently  than active ones during registration (to disable standby processing during restoration)", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418838526", "createdAt": "2020-05-02T02:29:24Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -70,18 +69,16 @@ static void registerStateStores(final Logger log,\n                 e\n             );\n         }\n+\n         log.debug(\"Acquired state directory lock\");\n \n         final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n \n         // We should only load checkpoint AFTER the corresponding state directory lock has been acquired and\n         // the state stores have been registered; we should not try to load at the state manager construction time.\n         // See https://issues.apache.org/jira/browse/KAFKA-8574\n-        for (final StateStore store : topology.stateStores()) {\n-            processorContext.uninitialize();\n-            store.init(processorContext, store);\n-            log.trace(\"Registered state store {}\", store.name());\n-        }\n+        stateMgr.registerStateStores(topology.stateStores(), processorContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1OTY2OA=="}, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg1MTMyNg==", "bodyText": "Ah. Makes sense.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418851326", "createdAt": "2020-05-02T03:02:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -70,18 +69,16 @@ static void registerStateStores(final Logger log,\n                 e\n             );\n         }\n+\n         log.debug(\"Acquired state directory lock\");\n \n         final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n \n         // We should only load checkpoint AFTER the corresponding state directory lock has been acquired and\n         // the state stores have been registered; we should not try to load at the state manager construction time.\n         // See https://issues.apache.org/jira/browse/KAFKA-8574\n-        for (final StateStore store : topology.stateStores()) {\n-            processorContext.uninitialize();\n-            store.init(processorContext, store);\n-            log.trace(\"Registered state store {}\", store.name());\n-        }\n+        stateMgr.registerStateStores(topology.stateStores(), processorContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1OTY2OA=="}, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTcwNDczOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjoxOToyMFrOGPXLMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwNDoyNzozNVrOGPevoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2MTUyMg==", "bodyText": "It seems a bit \"bold\" here to assume that we want to flip the active/standby state. The TaskManager objectively knows what type it wants the task to become, so it seems it should just inform us of the desired task type in prepareForRecycle.\nOr, better yet, just null it out and we can set it during createStandbyTaskFromActive and createActiveTaskFromStandby. This kind of thing might also be nice, since we could have a more explicit lifecycle for this object:\n\nI'm ready (created, valid, good to go)\nI'm getting recycled\nI'm closed\n\nThen, we can ensure that these \"createXFromY\" methods cleanly take the state manager from \"closed\" to \"ready\".\nI'm not saying to add another state enum, but having a clearly defined lifecycle will help us later in maintenance.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418761522", "createdAt": "2020-05-01T22:19:20Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -505,4 +550,35 @@ private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n \n         return found.isEmpty() ? null : found.get(0);\n     }\n+\n+    void prepareForRecycle() {\n+        log.debug(\"Preparing to recycle state for {} task {}.\", taskType, taskId);\n+\n+        if (recyclingState) {\n+            throw new IllegalStateException(\"Attempted to re-recycle state without completing first recycle\");\n+        }\n+        recyclingState = true;\n+    }\n+\n+    void recycleState() {\n+        log.debug(\"Completed recycling state for formerly {} task {}.\", taskType, taskId);\n+\n+        if (!recyclingState) {\n+            throw new IllegalStateException(\"Attempted to complete recycle but state is not currently being recycled\");\n+        }\n+        recyclingState = false;\n+\n+        if (taskType == TaskType.ACTIVE) {\n+            taskType = TaskType.STANDBY;\n+        } else {\n+            taskType = TaskType.ACTIVE;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg1MDA2OA==", "bodyText": "That makes sense to me, I'll try it out", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418850068", "createdAt": "2020-05-02T02:59:37Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -505,4 +550,35 @@ private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n \n         return found.isEmpty() ? null : found.get(0);\n     }\n+\n+    void prepareForRecycle() {\n+        log.debug(\"Preparing to recycle state for {} task {}.\", taskType, taskId);\n+\n+        if (recyclingState) {\n+            throw new IllegalStateException(\"Attempted to re-recycle state without completing first recycle\");\n+        }\n+        recyclingState = true;\n+    }\n+\n+    void recycleState() {\n+        log.debug(\"Completed recycling state for formerly {} task {}.\", taskType, taskId);\n+\n+        if (!recyclingState) {\n+            throw new IllegalStateException(\"Attempted to complete recycle but state is not currently being recycled\");\n+        }\n+        recyclingState = false;\n+\n+        if (taskType == TaskType.ACTIVE) {\n+            taskType = TaskType.STANDBY;\n+        } else {\n+            taskType = TaskType.ACTIVE;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2MTUyMg=="}, "originalCommit": null, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg1ODY1Mg==", "bodyText": "Alright I reworked things slightly here. With nulling out the task type during close/recycle we can technically get away with not even having the recyclingState flag at all. But I thought it might be a good idea to keep the two distinct to avoid future messes", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418858652", "createdAt": "2020-05-02T03:19:56Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -505,4 +550,35 @@ private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n \n         return found.isEmpty() ? null : found.get(0);\n     }\n+\n+    void prepareForRecycle() {\n+        log.debug(\"Preparing to recycle state for {} task {}.\", taskType, taskId);\n+\n+        if (recyclingState) {\n+            throw new IllegalStateException(\"Attempted to re-recycle state without completing first recycle\");\n+        }\n+        recyclingState = true;\n+    }\n+\n+    void recycleState() {\n+        log.debug(\"Completed recycling state for formerly {} task {}.\", taskType, taskId);\n+\n+        if (!recyclingState) {\n+            throw new IllegalStateException(\"Attempted to complete recycle but state is not currently being recycled\");\n+        }\n+        recyclingState = false;\n+\n+        if (taskType == TaskType.ACTIVE) {\n+            taskType = TaskType.STANDBY;\n+        } else {\n+            taskType = TaskType.ACTIVE;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2MTUyMg=="}, "originalCommit": null, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg4NTUzNg==", "bodyText": "Actually turns out nulling it during close causes some tests to fail \ud83d\ude15 Probably the changelog reader.  I'll just revert that part for now and look into it next week", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418885536", "createdAt": "2020-05-02T04:27:35Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -505,4 +550,35 @@ private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n \n         return found.isEmpty() ? null : found.get(0);\n     }\n+\n+    void prepareForRecycle() {\n+        log.debug(\"Preparing to recycle state for {} task {}.\", taskType, taskId);\n+\n+        if (recyclingState) {\n+            throw new IllegalStateException(\"Attempted to re-recycle state without completing first recycle\");\n+        }\n+        recyclingState = true;\n+    }\n+\n+    void recycleState() {\n+        log.debug(\"Completed recycling state for formerly {} task {}.\", taskType, taskId);\n+\n+        if (!recyclingState) {\n+            throw new IllegalStateException(\"Attempted to complete recycle but state is not currently being recycled\");\n+        }\n+        recyclingState = false;\n+\n+        if (taskType == TaskType.ACTIVE) {\n+            taskType = TaskType.STANDBY;\n+        } else {\n+            taskType = TaskType.ACTIVE;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2MTUyMg=="}, "originalCommit": null, "originalPosition": 209}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTcyNjE2OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjozMTo0NVrOGPXYEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwMjozODo1N1rOGPcGWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2NDgxOQ==", "bodyText": "I lost track of how this is now handled. Can you enlighten me?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418764819", "createdAt": "2020-05-01T22:31:45Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -155,9 +155,6 @@ void handleCorruption(final Map<TaskId, Collection<TopicPartition>> taskWithChan\n             final TaskId taskId = entry.getKey();\n             final Task task = tasks.get(taskId);\n \n-            // this call is idempotent so even if the task is only CREATED we can still call it\n-            changelogReader.remove(task.changelogPartitions());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg0MjIwMg==", "bodyText": "Yeah, so remove is basically the opposite of register in the ChangelogReader and previously the registration was done by the ProcessorStateManager  while the TaskManager  was responsible for unregistering changelogs (via remove).\nI renamed remove to unregister and moved the unregistration to the ProcessorStateManager as well, so the changelog lifecycle is entirely managed by the state manager now.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418842202", "createdAt": "2020-05-02T02:38:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -155,9 +155,6 @@ void handleCorruption(final Map<TaskId, Collection<TopicPartition>> taskWithChan\n             final TaskId taskId = entry.getKey();\n             final Task task = tasks.get(taskId);\n \n-            // this call is idempotent so even if the task is only CREATED we can still call it\n-            changelogReader.remove(task.changelogPartitions());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2NDgxOQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNjMzMzk0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwMjozMzoyNFrOGPb-JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMzo1ODo1NVrOGQWT0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg0MDEwMQ==", "bodyText": "I know this might make the changes a bit harder to follow in the review, but I think this renaming will make things less confusing in the long run to be symmetrical to register", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418840101", "createdAt": "2020-05-02T02:33:24Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -789,7 +789,7 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n     }\n \n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg1MjE4OQ==", "bodyText": "Thanks! We just have to review this once, but maintain it \"forever\", so I'm always happy to see improvements like this.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418852189", "createdAt": "2020-05-02T03:04:37Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -789,7 +789,7 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n     }\n \n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg0MDEwMQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5NTkyMg==", "bodyText": "Some callers may be passing null into the collection, see my other comments.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419795922", "createdAt": "2020-05-04T23:58:55Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -789,7 +789,7 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n     }\n \n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg0MDEwMQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzEyMzU1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMzo1NzoxMVrOGQWR9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzoxOToxNFrOGS2QEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5NTQ0NA==", "bodyText": "The storeMetadata.changelogPartition maybe null, while ArrayList would still put that null into the array.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419795444", "createdAt": "2020-05-04T23:57:11Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +270,20 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {\n+        if (isLoggingEnabled(storeName)) {\n+            changelogReader.register(getStorePartition(storeName), this);\n+        }\n+    }\n+\n+    private void unregisterAllStoresWithChangelogReader() {\n+        final List<TopicPartition> allChangelogPartitions = new ArrayList<>();\n+        for (final StateStoreMetadata storeMetadata : stores.values()) {\n+            allChangelogPartitions.add(storeMetadata.changelogPartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjQwMg==", "bodyText": "Ah, good catch", "url": "https://github.com/apache/kafka/pull/8248#discussion_r422416402", "createdAt": "2020-05-08T23:19:14Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +270,20 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {\n+        if (isLoggingEnabled(storeName)) {\n+            changelogReader.register(getStorePartition(storeName), this);\n+        }\n+    }\n+\n+    private void unregisterAllStoresWithChangelogReader() {\n+        final List<TopicPartition> allChangelogPartitions = new ArrayList<>();\n+        for (final StateStoreMetadata storeMetadata : stores.values()) {\n+            allChangelogPartitions.add(storeMetadata.changelogPartition);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5NTQ0NA=="}, "originalCommit": null, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzEyNzQxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMzo1OToxNlrOGQWUSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMzo1OToxNlrOGQWUSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5NjA0MA==", "bodyText": "nit: maybeRegisterStoreWithChangelogReader.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419796040", "createdAt": "2020-05-04T23:59:16Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +270,20 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzIwMTc3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDozNzozMVrOGQW_aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzoxMzo1N1rOGS2LyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwNzA4Mw==", "bodyText": "This is a meta comment: I'm a bit inclined to suggest that we do this recycle on the task-manager, i.e. do not call task.close() inside task-manager and let the close() internally check a boolean whether it is prepareRecycle.\nInstead, we can do the following:\n\n\nAdd a Task#convertTo(TaskType) interface which would return an active / standby task copying the fields of the original task, originated in RESTORING state.\n\n\nFor active task, the implementation would be:\n\n\n\n\nfirst transit to the RESTORING state (we would allow SUSPENDED to transit to RESTORING too, so if it is not in CREATED, we can first suspend it and then transit to RESTORING).\n\n\nand then return a newly created standby task initialized state as RESTORING.\n\n\n\nFor standby task, the implementation would be:\n\n\n\nfirst transit to RESTORING state (which would usually be a no-op).\n\n\nand then return a newly created active task initialized state as RESTORING.\n\n\nAlso I realized that recordCollector.initialize(); in active task should be moved from initializeIfNeeded to completeRestoration. This is a minor bug that may block this proposal --- I will prepare a PR fixing this.\n\nThen on task manager, for those convertible tasks we would call convertTo instead of close / re-create via the task-creators.\n\nThe key behind this proposal is that:\n\nSuspended and Restoring states are actually the same for active and standby tasks.\nIn the future when we remove Suspended state we would just have Restoring.\nActive and Standby's Restoring state are actually the same in terms of functionality.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419807083", "createdAt": "2020-05-05T00:37:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -204,7 +203,14 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 task.resume();\n                 standbyTasksToCreate.remove(task.id());\n-            } else /* we previously owned this task, and we don't have it anymore, or it has changed active/standby state */ {\n+            } else {\n+                // check for tasks that were owned previously but have changed active/standby status\n+                final boolean isTransitioningType = activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id());\n+                if (isTransitioningType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNTMwNQ==", "bodyText": "Yeah this does sound appealing, and is more or else what I first tried out when I started work on this (a long long time ago). My take at the time was that it added some complexity to the already-complicated TaskManager#handleAssignment while gaining little benefit, and was less \"future-proof\" compared to the current approach. For example it seems safer to just reuse the bare minimum of the task needed to recycle the state rather than rely on assumptions about which task states might be functionally equivalent to which states of the other type. It also seemed weird to require each task know how and be able to create a task of the opposite type, I felt this was more appropriate for the task creators.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r422415305", "createdAt": "2020-05-08T23:13:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -204,7 +203,14 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 task.resume();\n                 standbyTasksToCreate.remove(task.id());\n-            } else /* we previously owned this task, and we don't have it anymore, or it has changed active/standby state */ {\n+            } else {\n+                // check for tasks that were owned previously but have changed active/standby status\n+                final boolean isTransitioningType = activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id());\n+                if (isTransitioningType) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwNzA4Mw=="}, "originalCommit": null, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA1MzU4OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTozMjoxMVrOGS3j_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTozMjoxMVrOGS3j_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNzg4Ng==", "bodyText": "Alright I tried to shore up the test to verify that we actually do not restore anything in addition to not closing the store itself. This should be closer to testing the specific behavior pointed out in the ticket", "url": "https://github.com/apache/kafka/pull/8248#discussion_r422437886", "createdAt": "2020-05-09T01:32:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -311,9 +325,98 @@ public void shouldProcessDataFromStoresWithLoggingDisabled() throws InterruptedE\n         latch.await(30, TimeUnit.SECONDS);\n \n         assertTrue(processorLatch.await(30, TimeUnit.SECONDS));\n+    }\n+\n+    @Test\n+    public void shouldRecycleStateFromStandbyTaskPromotedToActiveTaskAndNotRestore() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODI3OTU1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDowNjo0NlrOGYlhcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMTowODo0NVrOGa1U8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzc3OQ==", "bodyText": "I tried to reduce the number of unnecessary local variables that could potentially get out of sync", "url": "https://github.com/apache/kafka/pull/8248#discussion_r428433779", "createdAt": "2020-05-21T04:06:46Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -109,7 +94,7 @@ public void logChange(final String storeName,\n                           final long timestamp) {\n         throwUnsupportedOperationExceptionIfStandby(\"logChange\");\n         // Sending null headers to changelog topics (KIP-244)\n-        collector.send(\n+        streamTask.recordCollector().send(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NDc3NQ==", "bodyText": "maybe we can refactor the interfaces to transitToActive and transitToStandby such that:\n\ntransitToActive: takes the streamTask (in the future I'd suggest we just pass in the record-collector after removing schedule and refactored request-commit) and the cache as parameters, check the current state manager's taskType is active.\ntransitToStandby: takes no parameter, reset streamTask and cache, check the current state manager's taskType is active.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429694775", "createdAt": "2020-05-25T00:57:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -109,7 +94,7 @@ public void logChange(final String storeName,\n                           final long timestamp) {\n         throwUnsupportedOperationExceptionIfStandby(\"logChange\");\n         // Sending null headers to changelog topics (KIP-244)\n-        collector.send(\n+        streamTask.recordCollector().send(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzc3OQ=="}, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc4OTg3NQ==", "bodyText": "Sure, if the plan is to ultimately remove streamTask from the context then it makes sense to just save the collector reference instead", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430789875", "createdAt": "2020-05-27T01:08:45Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -109,7 +94,7 @@ public void logChange(final String storeName,\n                           final long timestamp) {\n         throwUnsupportedOperationExceptionIfStandby(\"logChange\");\n         // Sending null headers to changelog topics (KIP-244)\n-        collector.send(\n+        streamTask.recordCollector().send(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzc3OQ=="}, "originalCommit": null, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM1ODMxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMDo0MzoxMlrOGZyYcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDo1ODoxNVrOGa1K4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzA0MQ==", "bodyText": "newType is not needed since it is overridden via state manager?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429693041", "createdAt": "2020-05-25T00:43:12Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NDM5Mg==", "bodyText": "Thinking about this a bit more, seems we do not need transition but just a registerCache? Could this be consolidated with the function below?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429694392", "createdAt": "2020-05-25T00:54:09Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzA0MQ=="}, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc4NzI5OQ==", "bodyText": "good point, all we need is registerNewTaskAndCache", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430787299", "createdAt": "2020-05-27T00:58:15Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzA0MQ=="}, "originalCommit": null, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM2MjM2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMDo0Nzo1OFrOGZyaoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzoxMjo1OVrOGb-4xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzYwMQ==", "bodyText": "Not a comment for this PR: since in 2.6+ we always try to commit all tasks when user request commits via context, we do not need to maintain the flag per-task, but per-thread. And then when we removed the deprecated schedule function we can remove the stream-task reference inside.\nCan we add a TODO marker to do that in the future?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429693601", "createdAt": "2020-05-25T00:47:58Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;\n+        streamTask = null;\n     }\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamsConfig config,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics) {\n-        this(\n-            id,\n-            null,\n-            config,\n-            null,\n-            stateMgr,\n-            metrics,\n-            new ThreadCache(\n-                new LogContext(String.format(\"stream-thread [%s] \", Thread.currentThread().getName())),\n-                0,\n-                metrics\n-            )\n-        );\n+    @Override\n+    public void registerNewTask(final Task task) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc5MTc1NQ==", "bodyText": "Sure, but there's also a non-deprecated schedule method. Won't that still need the reference to streamTask or do we somehow plan to remove that as well?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430791755", "createdAt": "2020-05-27T01:14:13Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;\n+        streamTask = null;\n     }\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamsConfig config,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics) {\n-        this(\n-            id,\n-            null,\n-            config,\n-            null,\n-            stateMgr,\n-            metrics,\n-            new ThreadCache(\n-                new LogContext(String.format(\"stream-thread [%s] \", Thread.currentThread().getName())),\n-                0,\n-                metrics\n-            )\n-        );\n+    @Override\n+    public void registerNewTask(final Task task) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzYwMQ=="}, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk5NTA3OQ==", "bodyText": "Ah yes, we still need the reference of a task in the future.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431995079", "createdAt": "2020-05-28T17:12:59Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;\n+        streamTask = null;\n     }\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamsConfig config,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics) {\n-        this(\n-            id,\n-            null,\n-            config,\n-            null,\n-            stateMgr,\n-            metrics,\n-            new ThreadCache(\n-                new LogContext(String.format(\"stream-thread [%s] \", Thread.currentThread().getName())),\n-                0,\n-                metrics\n-            )\n-        );\n+    @Override\n+    public void registerNewTask(final Task task) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzYwMQ=="}, "originalCommit": null, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM3NzY0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMTowNTo1MVrOGZyjAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMTowNTo1MVrOGZyjAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NTc0NQ==", "bodyText": "nit: I think now it's better to move the debug line 464 before unregisterAllStoresWithChangelogReader.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429695745", "createdAt": "2020-05-25T01:05:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -425,12 +449,14 @@ public void flush() {\n \n     /**\n      * {@link StateStore#close() Close} all stores (even in case of failure).\n-     * Log all exception and re-throw the first exception that did occur at the end.\n+     * Log all exceptions and re-throw the first exception that occurred at the end.\n      *\n      * @throws ProcessorStateException if any error happens when closing the state stores\n      */\n     @Override\n     public void close() throws ProcessorStateException {\n+        unregisterAllStoresWithChangelogReader();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM4NDMyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMToxMjo0MlrOGZym3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QyMjo0NjowMlrOGbfwCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NjczMg==", "bodyText": "nit: I'd suggest we have a separate recycle() function rather than piggy-backing on close with additional flag, since the clean is always true, i.e. we would always just 1) write checkpoints, 2) call stateMgr.recycle(), 3) then transit to CLOSED and if it is not in CREATED / RUNNING then do nothing. Though it has some duplicated lines but it is a bit straight-forward.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429696732", "createdAt": "2020-05-25T01:12:42Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -189,38 +190,52 @@ private void prepareClose(final boolean clean) {\n     @Override\n     public void closeClean(final Map<TopicPartition, Long> checkpoint) {\n         Objects.requireNonNull(checkpoint);\n-        close(true);\n+        close(true, false);\n \n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n-        close(false);\n+        close(false, false);\n \n         log.info(\"Closed dirty\");\n     }\n \n-    private void close(final boolean clean) {\n+    @Override\n+    public void closeAndRecycleState() {\n+        prepareClose(true);\n+        close(true, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgwNzc2NA==", "bodyText": "I was debating this a lot actually. Ultimately I decided to reuse the existing close methods because I was worried we might change or fix something in close and forget to add it to closeAndRecycle.  Since closeAndRecycle should always be a subset of the actions taken in close I thought this would be more future proof...WDYT?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430807764", "createdAt": "2020-05-27T01:33:29Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -189,38 +190,52 @@ private void prepareClose(final boolean clean) {\n     @Override\n     public void closeClean(final Map<TopicPartition, Long> checkpoint) {\n         Objects.requireNonNull(checkpoint);\n-        close(true);\n+        close(true, false);\n \n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n-        close(false);\n+        close(false, false);\n \n         log.info(\"Closed dirty\");\n     }\n \n-    private void close(final boolean clean) {\n+    @Override\n+    public void closeAndRecycleState() {\n+        prepareClose(true);\n+        close(true, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NjczMg=="}, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI1MTY1Nw==", "bodyText": "When I introduced closeClean and closeDirty, I resisted the urge to inline close(boolean) only to control the LOC of the change. Having more branches and flags is generally more of a liability than a few duplicated statements.\nNow that we have two boolean flags (again) and new branch in the internal close method, I'd be much more inclined to inline it. But we can do this in a follow-on PR, if you prefer.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431251657", "createdAt": "2020-05-27T15:53:56Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -189,38 +190,52 @@ private void prepareClose(final boolean clean) {\n     @Override\n     public void closeClean(final Map<TopicPartition, Long> checkpoint) {\n         Objects.requireNonNull(checkpoint);\n-        close(true);\n+        close(true, false);\n \n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n-        close(false);\n+        close(false, false);\n \n         log.info(\"Closed dirty\");\n     }\n \n-    private void close(final boolean clean) {\n+    @Override\n+    public void closeAndRecycleState() {\n+        prepareClose(true);\n+        close(true, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NjczMg=="}, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ4NDkzOQ==", "bodyText": "Well, I think it still makes sense to have closeDirty and closeClean fall back to the same close method since the only difference is whether to rethrow exceptions, whereas closeAndRecycle actually has different contents.\nBut I guess for that reason right there it makes sense to duplicate only the relevant parts of close.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431484939", "createdAt": "2020-05-27T22:46:02Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -189,38 +190,52 @@ private void prepareClose(final boolean clean) {\n     @Override\n     public void closeClean(final Map<TopicPartition, Long> checkpoint) {\n         Objects.requireNonNull(checkpoint);\n-        close(true);\n+        close(true, false);\n \n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n-        close(false);\n+        close(false, false);\n \n         log.info(\"Closed dirty\");\n     }\n \n-    private void close(final boolean clean) {\n+    @Override\n+    public void closeAndRecycleState() {\n+        prepareClose(true);\n+        close(true, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NjczMg=="}, "originalCommit": null, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM4Nzg0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMToxNjoyM1rOGZyo3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMTozMzo1MVrOGa2cBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NzI0Ng==", "bodyText": "Are these new lines intentional?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429697246", "createdAt": "2020-05-25T01:16:23Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -112,6 +103,7 @@ static void closeStateManager(final Logger log,\n         final AtomicReference<ProcessorStateException> firstException = new AtomicReference<>(null);\n         try {\n             if (stateDirectory.lock(id)) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgwODA2OQ==", "bodyText": "nope \ud83d\ude42", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430808069", "createdAt": "2020-05-27T01:33:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -112,6 +103,7 @@ static void closeStateManager(final Logger log,\n         final AtomicReference<ProcessorStateException> firstException = new AtomicReference<>(null);\n         try {\n             if (stateDirectory.lock(id)) {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NzI0Ng=="}, "originalCommit": null, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM4OTk1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMToxOTowNVrOGZyqLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMToxOTowNVrOGZyqLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NzU4MQ==", "bodyText": "Ditto here, I'd suggest add cycle as a separate function.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429697581", "createdAt": "2020-05-25T01:19:05Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -482,7 +488,8 @@ public void closeDirty() {\n      * </pre>\n      */\n     private void close(final boolean clean,\n-                       final Map<TopicPartition, Long> checkpoint) {\n+                       final Map<TopicPartition, Long> checkpoint,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM5NTMyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMToyNTowNlrOGZytYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjowNzowNFrOGa3BdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5ODQwMQ==", "bodyText": "This will add all current tasks, is that right?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429698401", "createdAt": "2020-05-25T01:25:06Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -272,6 +274,36 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n+        if (taskCloseExceptions.isEmpty()) {\n+            Task oldTask = null;\n+            final Iterator<Task> transitioningTasksIter = tasksToRecycle.iterator();\n+            try {\n+                while (transitioningTasksIter.hasNext()) {\n+                    oldTask = transitioningTasksIter.next();\n+                    final Task newTask;\n+                    if (oldTask.isActive()) {\n+                        final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n+                        newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    } else {\n+                        final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n+                        newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n+                    }\n+                    tasks.remove(oldTask.id());\n+                    addNewTask(newTask);\n+                    transitioningTasksIter.remove();\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(oldTask.id(), e);\n+\n+                dirtyTasks.addAll(tasksToRecycle); // contains the tasks we have not yet tried to transition\n+                dirtyTasks.addAll(tasks.values());         // contains the new tasks we just created", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgxNzY1Mw==", "bodyText": "Hm. The current logic is to only close dirty those tasks which were in a failed action (eg close, or in the case of a failed commit we will close all currently assigned active tasks), so I guess we never end up closing standbys, which makes sense.\nSo maybe we shouldn't close any successfully recycled tasks at all, and just let them be closed during handleLostAll if necessary?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430817653", "createdAt": "2020-05-27T02:07:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -272,6 +274,36 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n+        if (taskCloseExceptions.isEmpty()) {\n+            Task oldTask = null;\n+            final Iterator<Task> transitioningTasksIter = tasksToRecycle.iterator();\n+            try {\n+                while (transitioningTasksIter.hasNext()) {\n+                    oldTask = transitioningTasksIter.next();\n+                    final Task newTask;\n+                    if (oldTask.isActive()) {\n+                        final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n+                        newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    } else {\n+                        final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n+                        newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n+                    }\n+                    tasks.remove(oldTask.id());\n+                    addNewTask(newTask);\n+                    transitioningTasksIter.remove();\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(oldTask.id(), e);\n+\n+                dirtyTasks.addAll(tasksToRecycle); // contains the tasks we have not yet tried to transition\n+                dirtyTasks.addAll(tasks.values());         // contains the new tasks we just created", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5ODQwMQ=="}, "originalCommit": null, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NjM5NzYxOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMToyNzo1NlrOGZyuxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjoxMzo1N1rOGa3H4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5ODc1OA==", "bodyText": "nit: I feel it is not necessary to remove the extra reference on the cache and then call context.cache() every time we need it, but I think this is really a very very nit point so your call.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429698758", "createdAt": "2020-05-25T01:27:56Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "diffHunk": "@@ -46,7 +46,6 @@\n     private final SessionKeySchema keySchema;\n     private final SegmentedCacheFunction cacheFunction;\n     private String cacheName;\n-    private ThreadCache cache;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgxOTI5Ng==", "bodyText": "Well, the underlying ThreadCache changes when we transition task type (as standbys have a dummy size-zero cache). So we need to go through the context to make sure we call the right cache", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430819296", "createdAt": "2020-05-27T02:13:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "diffHunk": "@@ -46,7 +46,6 @@\n     private final SessionKeySchema keySchema;\n     private final SegmentedCacheFunction cacheFunction;\n     private String cacheName;\n-    private ThreadCache cache;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5ODc1OA=="}, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzU1NzI1OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzozNTo0NFrOGa4UPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzozNTo0NFrOGa4UPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzODg0Ng==", "bodyText": "Renamed to unregister and moved to ChangelogRegister interface", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430838846", "createdAt": "2020-05-27T03:35:44Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java", "diffHunk": "@@ -45,12 +44,6 @@\n      */\n     Set<TopicPartition> completedChangelogs();\n \n-    /**\n-     * Removes the passed in partitions from the set of changelogs\n-     * @param revokedPartitions the set of partitions to remove\n-     */\n-    void remove(Collection<TopicPartition> revokedPartitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzU5NTI0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNDowMjoyNlrOGa4rBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzoyMjoxM1rOGb_NoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng==", "bodyText": "Alright, the situation here is that we need to make sure we toggle bulk loading off for any active restoring tasks that convert to standby. Rather than try and force a direct call to toggleForBulkLoading on the store itself I figured we should just call onRestoreEnd. Technically, restoration is ending. It just happens to be due to type transition, rather than restore completion.\nI figured this might be relevant for users of custom stores, which might do something similar to bulk loading that they wish to turn off for standbys. But since this is only relevant to the underlying store, and doesn't mean we have actually finished restoring a task, we should only call the specific store's listener -- and not the user registered global listener.\nWDYT?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430844676", "createdAt": "2020-05-27T04:02:26Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyNDU3Mw==", "bodyText": "I think this makes sense. The restore listener / bulk loading interaction is a bit wacky, but it seems reasonable to just work around it for now.\nJust to play devil's advocate briefly, though, is it not true for all listeners that the restore has ended, for exactly the reason you cited above?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431324573", "createdAt": "2020-05-27T17:40:36Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ4MTgxMg==", "bodyText": "I assume you're referring to the user registered global listener? I was trying to imagine what users actually might be using this for, and figured a major reason was to alert when a store is ready for IQ. Obviously, the store is not in fact ready for IQ in this case. I assume the worst that could happen is they'll just get an exception saying the store is not in fact ready if they do try to query it, but it still seems likely to cause confusion.\nIf you're also wondering about the seemingly arbitrary distinction made between the store-specific listener and the global one, it seems like the intent of the store-specific listener is to take action on a particular store as it transitions between restoring and not. The store-specific listener has a reference to the actual store, and can for example toggle it for bulk loading.\nBut IIUC the global listener does not have a reference to any actual stores and thus it's purpose seems more for alerting on the completion of restoration rather than taking some action on the store as restoration begins/ends.\nRestoration completion =/= restoration ending, but unfortunately we just have the one callback", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431481812", "createdAt": "2020-05-27T22:37:18Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUwNTcyMw==", "bodyText": "Yeah, that last sentence is what I was thinking would make sense to just call all the listeners, not just the inner ones. If you implemented the listener so that you could log or collect metrics to watch the restore process, it seems like it would be strange just to see the restoration disappear, and then a new restoration start (for the post-recycled task), without the prior one ever having \"ended\". It just seems natural to see an end for every start, even if the \"end\" doesn't mean \"completed\". But I can see how that could also be confusing.\nI'm not totally sure what the best call here is, so maybe we should just defer to your judgement, since you're the closest to this code right now.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431505723", "createdAt": "2020-05-27T23:49:14Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxMzIzMA==", "bodyText": "a new restoration start (for the post-recycled task), without the prior one ever having \"ended\".\n\nThis could happen if you have a restoring task that transitions to standby and then back to restoring. But invoking onRestoreStart twice in a row in that case is the current behavior, so your listener should presumably already be handling the situation. My impression is that users understand the global restore listener's onRestoreEnd to mean that restoration has completed, and invoking it before this would be an unexpected behavior change.\nI think in an ideal world we would decouple these two callbacks to make the distinction more apparent.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431513230", "createdAt": "2020-05-28T00:15:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxMzQ4Mg==", "bodyText": "Ah actually @guozhangwang filed a ticket recently for exactly that: https://issues.apache.org/jira/browse/KAFKA-10005", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431513482", "createdAt": "2020-05-28T00:15:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTg3OTMxNA==", "bodyText": "Ok, I'm satisfied with this conclusion.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431879314", "createdAt": "2020-05-28T14:29:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAwMDQxNw==", "bodyText": "Yup, I propose to decouple these two and only allow restore callback at the per-store level and restore listener at the global level. We will always open the store with compaction disabled etc when we are transiting to restoring, and after we've done the restoration (for active) we will do a one-off compaction, and then reopen the store with configs overridden.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432000417", "createdAt": "2020-05-28T17:22:13Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4NzM1ODY5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QyMTo0NzowNVrOGbeUqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QyMjozOTo0NVrOGbfnWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ2MTU0NQ==", "bodyText": "This seems like an odd case. Am I right in reading this as, \"something went wrong, and we don't know what it was, so we're just going to assume the worst and dump all the tasks that we were hoping to recycle\"?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431461545", "createdAt": "2020-05-27T21:47:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -272,6 +274,30 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n+        if (taskCloseExceptions.isEmpty()) {\n+            for (final Task oldTask : tasksToRecycle) {\n+                final Task newTask;\n+                try {\n+                    if (oldTask.isActive()) {\n+                        final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n+                        newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    } else {\n+                        final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n+                        newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n+                    }\n+                    tasks.remove(oldTask.id());\n+                    addNewTask(newTask);\n+                } catch (final RuntimeException e) {\n+                    final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n+                    log.error(uncleanMessage, e);\n+                    taskCloseExceptions.put(oldTask.id(), e);\n+                    dirtyTasks.add(oldTask);\n+                }\n+            }\n+        } else {\n+            dirtyTasks.addAll(tasksToRecycle);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ4MjcxNA==", "bodyText": "Yeah, it still seems to me like if we have to close any tasks as dirty we will ultimately have to do so for them all (as in handleLostAll) But that's a big assumption and even if true now, it may not always be...I'll remove this", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431482714", "createdAt": "2020-05-27T22:39:45Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -272,6 +274,30 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n+        if (taskCloseExceptions.isEmpty()) {\n+            for (final Task oldTask : tasksToRecycle) {\n+                final Task newTask;\n+                try {\n+                    if (oldTask.isActive()) {\n+                        final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n+                        newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    } else {\n+                        final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n+                        newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n+                    }\n+                    tasks.remove(oldTask.id());\n+                    addNewTask(newTask);\n+                } catch (final RuntimeException e) {\n+                    final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n+                    log.error(uncleanMessage, e);\n+                    taskCloseExceptions.put(oldTask.id(), e);\n+                    dirtyTasks.add(oldTask);\n+                }\n+            }\n+        } else {\n+            dirtyTasks.addAll(tasksToRecycle);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ2MTU0NQ=="}, "originalCommit": null, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4Nzg3NTc5OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQwMjowNzoxMVrOGbjQMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo0NDo1OFrOGcD7bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MjMyMg==", "bodyText": "@guozhangwang was there a reason for these to be TreeMaps?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431542322", "createdAt": "2020-05-28T02:07:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -183,12 +180,15 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new TreeMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new TreeMap<>(standbyTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA3NzY3OA==", "bodyText": "I don't think there was a reason besides determinism for debugging, etc.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432077678", "createdAt": "2020-05-28T19:44:58Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -183,12 +180,15 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new TreeMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new TreeMap<>(standbyTasks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MjMyMg=="}, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTE5MjA0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1MjozNVrOGcETAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMzozMDoxNlrOGcKGkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzcxNQ==", "bodyText": "Why do we need to remove the Rule annotation?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432083715", "createdAt": "2020-05-28T19:52:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -79,25 +88,27 @@\n \n     private static final String APPID = \"restore-test\";\n \n-    @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n+    public final EmbeddedKafkaCluster cluster = new EmbeddedKafkaCluster(NUM_BROKERS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEyMTI3Ng==", "bodyText": "I made the cluster non-static because there was poor isolation between the tests and, for example, setCommittedOffset would often fail. This was just the \"lazy\" solution, we could also make sure to give the mall unique application ids", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432121276", "createdAt": "2020-05-28T21:01:52Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -79,25 +88,27 @@\n \n     private static final String APPID = \"restore-test\";\n \n-    @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n+    public final EmbeddedKafkaCluster cluster = new EmbeddedKafkaCluster(NUM_BROKERS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzcxNQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE0MzE0Mw==", "bodyText": "I see. Maybe making it a Rule instead of a ClassRule is the solution you're looking for?\nBut yes, there's a middle ground, which is to use uniquely named resources for each test, for which we have org.apache.kafka.streams.integration.utils.IntegrationTestUtils#safeUniqueTestName.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432143143", "createdAt": "2020-05-28T21:48:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -79,25 +88,27 @@\n \n     private static final String APPID = \"restore-test\";\n \n-    @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n+    public final EmbeddedKafkaCluster cluster = new EmbeddedKafkaCluster(NUM_BROKERS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzcxNQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NTE1OQ==", "bodyText": "What's the advantage of a Rule over just...nothing?\nMostly I just didn't want to hard code a bunch of names in myself. But safeUniqueTestName sounds like what I need", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432155159", "createdAt": "2020-05-28T22:17:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -79,25 +88,27 @@\n \n     private static final String APPID = \"restore-test\";\n \n-    @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n+    public final EmbeddedKafkaCluster cluster = new EmbeddedKafkaCluster(NUM_BROKERS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzcxNQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE3MTkzMQ==", "bodyText": "Nevermind, I take it the answer is \"it calls start for you since the embedded cluster extends ExternalResource ?\"", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432171931", "createdAt": "2020-05-28T23:07:34Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -79,25 +88,27 @@\n \n     private static final String APPID = \"restore-test\";\n \n-    @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n+    public final EmbeddedKafkaCluster cluster = new EmbeddedKafkaCluster(NUM_BROKERS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzcxNQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE3ODgzMg==", "bodyText": "I remember now, the main reason for the poor test isolation was different tests writing various amounts of data to the different topics at specific offsets. Honestly I'm not sure why this wasn't causing problems before \ud83e\udd14\nAlthough...I notice that the first test writes 10000 keys to the input topic starting at offset 0, while the second test to write to the input topic does so starting at offset 10000. Clearly I should have looked at all the other tests in the class, counted them up, and then just started writing data at the highest offset", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432178832", "createdAt": "2020-05-28T23:30:16Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -79,25 +88,27 @@\n \n     private static final String APPID = \"restore-test\";\n \n-    @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n+    public final EmbeddedKafkaCluster cluster = new EmbeddedKafkaCluster(NUM_BROKERS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzcxNQ=="}, "originalCommit": null, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTM0ODM5OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDozNzo0NVrOGcF2lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxODowNlrOGcIquw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwOTIwNw==", "bodyText": "This isn't threadsafe, but it looks like we're using it from multiple threads during the tests.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432109207", "createdAt": "2020-05-28T20:37:45Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java", "diffHunk": "@@ -1247,4 +1270,33 @@ public void waitForNextStableAssignment(final long maxWaitMs) throws Interrupted\n             );\n         }\n     }\n+\n+    public static class TrackingStateRestoreListener implements StateRestoreListener {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NTMyMw==", "bodyText": "Whoops", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432155323", "createdAt": "2020-05-28T22:18:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java", "diffHunk": "@@ -1247,4 +1270,33 @@ public void waitForNextStableAssignment(final long maxWaitMs) throws Interrupted\n             );\n         }\n     }\n+\n+    public static class TrackingStateRestoreListener implements StateRestoreListener {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwOTIwNw=="}, "originalCommit": null, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTM3NTg2OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDo0Njo1M1rOGcGIFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNDozMDo0M1rOGcfS0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExMzY4NQ==", "bodyText": "I feel like I missed something here. We don't expect the changelog to get unregistered during close anymore?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432113685", "createdAt": "2020-05-28T20:46:53Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1468,9 +1456,6 @@ public void shouldCloseActiveTasksAndIgnoreExceptionsOnUncleanShutdown() {\n \n         resetToStrict(changeLogReader);\n         expect(changeLogReader.completedChangelogs()).andReturn(emptySet());\n-        // make sure we also remove the changelog partitions from the changelog reader\n-        changeLogReader.remove(eq(singletonList(changelog)));\n-        expectLastCall();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE4MzY3OA==", "bodyText": "The \"unregistration\" is now handled by the ProcessorStateManager, which is mocked in this test. There are new tests in ProcessorStateManagerTest that verify we unregister the partitions during close/recycle", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432183678", "createdAt": "2020-05-28T23:47:23Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1468,9 +1456,6 @@ public void shouldCloseActiveTasksAndIgnoreExceptionsOnUncleanShutdown() {\n \n         resetToStrict(changeLogReader);\n         expect(changeLogReader.completedChangelogs()).andReturn(emptySet());\n-        // make sure we also remove the changelog partitions from the changelog reader\n-        changeLogReader.remove(eq(singletonList(changelog)));\n-        expectLastCall();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExMzY4NQ=="}, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjUyNjAzMg==", "bodyText": "Thanks.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432526032", "createdAt": "2020-05-29T14:30:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1468,9 +1456,6 @@ public void shouldCloseActiveTasksAndIgnoreExceptionsOnUncleanShutdown() {\n \n         resetToStrict(changeLogReader);\n         expect(changeLogReader.completedChangelogs()).andReturn(emptySet());\n-        // make sure we also remove the changelog partitions from the changelog reader\n-        changeLogReader.remove(eq(singletonList(changelog)));\n-        expectLastCall();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExMzY4NQ=="}, "originalCommit": null, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTQwMTI5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDo1NDo1MFrOGcGXnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNDozMDoyNlrOGcfRyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExNzY2MA==", "bodyText": "One high-level concern I've developed during this review is whether there's any possibility that something like this could leak into the public API. I.e., is it possible that a Processor, Transformer, or StateStore could have cached some reference from the context that would become invalid when the context gets recycled, similar to the way this recordCollector (which is not public, I know) did.\nWhat's your take on that, @ableegoldman ?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432117660", "createdAt": "2020-05-28T20:54:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -263,27 +259,28 @@ private void logValue(final Bytes key, final BufferKey bufferKey, final BufferVa\n         final ByteBuffer buffer = value.serialize(sizeOfBufferTime);\n         buffer.putLong(bufferKey.time());\n \n-        collector.send(\n-            changelogTopic,\n-            key,\n-            buffer.array(),\n-            V_2_CHANGELOG_HEADERS,\n-            partition,\n-            null,\n-            KEY_SERIALIZER,\n-            VALUE_SERIALIZER\n+        ((RecordCollector.Supplier) context).recordCollector().send(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5MDA5MQ==", "bodyText": "Well, only active tasks have a topology, and we don't initialize the topology until everything has been cleanly recycled. So by the time init is being called and the context is being used, it should be all up-to-date with the active task references.\nOf course that only applies to the Processor/Transformer half of the question. With StateStores we're obviously still calling init for standby tasks, and more. But nothing in the public ProcessorContext interface gets recycled. Only the cache, record collector, and StreamTask have to be updated, so this should all be totally transparent (unless they're doing something they shouldn't be)", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432190091", "createdAt": "2020-05-29T00:10:37Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -263,27 +259,28 @@ private void logValue(final Bytes key, final BufferKey bufferKey, final BufferVa\n         final ByteBuffer buffer = value.serialize(sizeOfBufferTime);\n         buffer.putLong(bufferKey.time());\n \n-        collector.send(\n-            changelogTopic,\n-            key,\n-            buffer.array(),\n-            V_2_CHANGELOG_HEADERS,\n-            partition,\n-            null,\n-            KEY_SERIALIZER,\n-            VALUE_SERIALIZER\n+        ((RecordCollector.Supplier) context).recordCollector().send(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExNzY2MA=="}, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjUyNTc2OA==", "bodyText": "Ok. Thanks.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432525768", "createdAt": "2020-05-29T14:30:26Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -263,27 +259,28 @@ private void logValue(final Bytes key, final BufferKey bufferKey, final BufferVa\n         final ByteBuffer buffer = value.serialize(sizeOfBufferTime);\n         buffer.putLong(bufferKey.time());\n \n-        collector.send(\n-            changelogTopic,\n-            key,\n-            buffer.array(),\n-            V_2_CHANGELOG_HEADERS,\n-            partition,\n-            null,\n-            KEY_SERIALIZER,\n-            VALUE_SERIALIZER\n+        ((RecordCollector.Supplier) context).recordCollector().send(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExNzY2MA=="}, "originalCommit": null, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5Mzk2OTgxOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNDozNDozNVrOGcfepA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNDozNDozNVrOGcfepA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjUyOTA2MA==", "bodyText": "Note for the future: it's not necessary to prefix the temp directory.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432529060", "createdAt": "2020-05-29T14:34:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -77,29 +89,30 @@\n public class RestoreIntegrationTest {\n     private static final int NUM_BROKERS = 1;\n \n-    private static final String APPID = \"restore-test\";\n-\n     @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n-    private static final String INPUT_STREAM = \"input-stream\";\n-    private static final String INPUT_STREAM_2 = \"input-stream-2\";\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(NUM_BROKERS);\n+\n+    @Rule\n+    public final TestName testName = new TestName();\n+    private String appId;\n+    private String inputStream;\n+\n     private final int numberOfKeys = 10000;\n     private KafkaStreams kafkaStreams;\n \n-    @BeforeClass\n-    public static void createTopics() throws InterruptedException {\n-        CLUSTER.createTopic(INPUT_STREAM, 2, 1);\n-        CLUSTER.createTopic(INPUT_STREAM_2, 2, 1);\n-        CLUSTER.createTopic(APPID + \"-store-changelog\", 2, 1);\n+    @Before\n+    public void createTopics() throws InterruptedException {\n+        appId = safeUniqueTestName(RestoreIntegrationTest.class, testName);\n+        inputStream = appId + \"-input-stream\";\n+        CLUSTER.createTopic(inputStream, 2, 1);\n     }\n \n-    private Properties props(final String applicationId) {\n+    private Properties props() {\n         final Properties streamsConfiguration = new Properties();\n-        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);\n+        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, appId);\n         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n-        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(applicationId).getPath());\n+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(appId).getPath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30ac7b3ccd47063497c17ac148d90f9b29683e82"}, "originalPosition": 99}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3255, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}