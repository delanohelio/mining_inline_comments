{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1NTgzMjE0", "number": 8256, "title": "KAFKA-9675: Fix bug that prevents RocksDB metrics to be updated", "bodyText": "The root cause of the bug is that the statistics object is passed to the RocksDB options after the database is opened. Apparently, the options object is copied during the opening process. The solution is to pass the statistics object before the database is opened.\nAdditionally, I added some unit tests to check if the Kafka Streams' metrics are updated when the measurements in RocksDB's statistics object change.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-03-09T13:18:42Z", "url": "https://github.com/apache/kafka/pull/8256", "merged": true, "mergeCommit": {"oid": "0c256e16ab33b58c8306502401e2c992018a1781"}, "closed": true, "closedAt": "2020-03-12T18:51:34Z", "author": {"login": "cadonna"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcL8_eUgH2gAyMzg1NTgzMjE0OmE2NTJiMzc4ZDUxYzNhODNiNzE3NThkMTkxZDQzOTQ1NzEyM2I2YmE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcM8fm0AFqTM3MzU5OTE3Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a652b378d51c3a83b71758d191d439457123b6ba", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/a652b378d51c3a83b71758d191d439457123b6ba", "committedDate": "2020-03-09T12:22:53Z", "message": "KAFKA-9675: Fix bug that prevents RocksDB metrics to be updated"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxMTk1MzQw", "url": "https://github.com/apache/kafka/pull/8256#pullrequestreview-371195340", "createdAt": "2020-03-09T13:46:50Z", "commit": {"oid": "a652b378d51c3a83b71758d191d439457123b6ba"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxMzo0Njo1MFrOFzocnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxMzo1MjoxMlrOFzo0mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4NDM4MQ==", "bodyText": "This is the fix. For an explanation read the commit message.", "url": "https://github.com/apache/kafka/pull/8256#discussion_r389684381", "createdAt": "2020-03-09T13:46:50Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -188,11 +188,12 @@ void openDB(final ProcessorContext context) {\n             throw new ProcessorStateException(fatal);\n         }\n \n+        // Setup metrics before the database is opened, otherwise the metrics are not updated\n+        // with the measurements from Rocks DB\n+        maybeSetUpMetricsRecorder(context, configs);\n+\n         openRocksDB(dbOptions, columnFamilyOptions);\n         open = true;\n-\n-        // Do this last because the prior operations could throw exceptions.\n-        maybeSetUpMetricsRecorder(context, configs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a652b378d51c3a83b71758d191d439457123b6ba"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4OTkyNw==", "bodyText": "This verifies that the metrics are updated when the measurements in the statistics object change. Unfortunately, the tests the use this verification run more than a minute, because the thread that triggers the recordings of the RocksDB metrics takes some time to record its first value.", "url": "https://github.com/apache/kafka/pull/8256#discussion_r389689927", "createdAt": "2020-03-09T13:51:37Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -283,13 +333,38 @@ private void verifyRocksDBMetrics(final KafkaStreams kafkaStreams, final String\n         checkMetricByName(listMetricStore, NUMBER_OF_FILE_ERRORS, 1);\n     }\n \n-    private void checkMetricByName(final List<Metric> listMetric, final String metricName, final int numMetric) {\n+    private void checkMetricByName(final List<Metric> listMetric,\n+                                   final String metricName,\n+                                   final int numMetric) {\n         final List<Metric> metrics = listMetric.stream()\n             .filter(m -> m.metricName().name().equals(metricName))\n             .collect(Collectors.toList());\n-        Assert.assertEquals(\"Size of metrics of type:'\" + metricName + \"' must be equal to \" + numMetric + \" but it's equal to \" + metrics.size(), numMetric, metrics.size());\n-        for (final Metric m : metrics) {\n-            Assert.assertNotNull(\"Metric:'\" + m.metricName() + \"' must be not null\", m.metricValue());\n+        assertThat(\n+            \"Size of metrics of type:'\" + metricName + \"' must be equal to \" + numMetric + \" but it's equal to \" + metrics.size(),\n+            metrics.size(),\n+            is(numMetric)\n+        );\n+        for (final Metric metric : metrics) {\n+            assertThat(\"Metric:'\" + metric.metricName() + \"' must be not null\", metric.metricValue(), is(notNullValue()));\n         }\n     }\n-}\n+\n+    private void verifyThatBytesWrittenTotalIncreases(final KafkaStreams kafkaStreams,\n+                                                      final String metricsScope) throws InterruptedException {\n+        final List<Metric> metric = getRocksDBMetrics(kafkaStreams, metricsScope).stream()\n+            .filter(m -> BYTES_WRITTEN_TOTAL.equals(m.metricName().name()))\n+            .collect(Collectors.toList());\n+        TestUtils.waitForCondition(\n+            () -> (double) metric.get(0).metricValue() > 0,\n+            TIMEOUT,\n+            () -> \"RocksDB metric bytes.written.total did not increase in \" + TIMEOUT + \" ms\"\n+        );\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a652b378d51c3a83b71758d191d439457123b6ba"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY5MDUyMQ==", "bodyText": "This is one of the tests that verifies the fix. See my comment in verifyThatBytesWrittenTotalIncreases().", "url": "https://github.com/apache/kafka/pull/8256#discussion_r389690521", "createdAt": "2020-03-09T13:52:12Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -142,21 +155,58 @@ public void shouldExposeRocksDBMetricsForSegmentedStateStoreBeforeAndAfterFailur\n         final Properties streamsConfiguration = streamsConfig();\n         IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n         final StreamsBuilder builder = builderForSegmentedStateStore();\n+        final String metricsScope = \"rocksdb-window-state-id\";\n+\n+        cleanUpStateRunAndVerify(\n+            builder,\n+            streamsConfiguration,\n+            LongDeserializer.class,\n+            LongDeserializer.class,\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );\n \n         cleanUpStateRunAndVerify(\n             builder,\n             streamsConfiguration,\n             LongDeserializer.class,\n             LongDeserializer.class,\n-            \"rocksdb-window-state-id\"\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );\n+    }\n+\n+    @Test\n+    public void shouldVerifyThatMetricsGetMeasurementsFromRocksDBForNonSegmentedStateStore() throws Exception {\n+        final Properties streamsConfiguration = streamsConfig();\n+        IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n+        final StreamsBuilder builder = builderForNonSegmentedStateStore();\n+        final String metricsScope = \"rocksdb-state-id\";\n+\n+        cleanUpStateRunAndVerify(\n+            builder,\n+            streamsConfiguration,\n+            IntegerDeserializer.class,\n+            StringDeserializer.class,\n+            this::verifyThatBytesWrittenTotalIncreases,\n+            metricsScope\n         );\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a652b378d51c3a83b71758d191d439457123b6ba"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyMzYyNDEx", "url": "https://github.com/apache/kafka/pull/8256#pullrequestreview-372362411", "createdAt": "2020-03-10T22:22:31Z", "commit": {"oid": "a652b378d51c3a83b71758d191d439457123b6ba"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQyMjoyMjozMlrOF0jItQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQyMjoyMjozMlrOF0jItQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NTk0MQ==", "bodyText": "Do I read this right, that we're doing the exact same thing twice?", "url": "https://github.com/apache/kafka/pull/8256#discussion_r390645941", "createdAt": "2020-03-10T22:22:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -142,21 +155,58 @@ public void shouldExposeRocksDBMetricsForSegmentedStateStoreBeforeAndAfterFailur\n         final Properties streamsConfiguration = streamsConfig();\n         IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n         final StreamsBuilder builder = builderForSegmentedStateStore();\n+        final String metricsScope = \"rocksdb-window-state-id\";\n+\n+        cleanUpStateRunAndVerify(\n+            builder,\n+            streamsConfiguration,\n+            LongDeserializer.class,\n+            LongDeserializer.class,\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );\n \n         cleanUpStateRunAndVerify(\n             builder,\n             streamsConfiguration,\n             LongDeserializer.class,\n             LongDeserializer.class,\n-            \"rocksdb-window-state-id\"\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a652b378d51c3a83b71758d191d439457123b6ba"}, "originalPosition": 90}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e145830422a24c6802bb1d7cef9e80008025c182", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/e145830422a24c6802bb1d7cef9e80008025c182", "committedDate": "2020-03-12T09:38:38Z", "message": "Clarify some tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNTk5MTcz", "url": "https://github.com/apache/kafka/pull/8256#pullrequestreview-373599173", "createdAt": "2020-03-12T14:22:00Z", "commit": {"oid": "e145830422a24c6802bb1d7cef9e80008025c182"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 178, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}