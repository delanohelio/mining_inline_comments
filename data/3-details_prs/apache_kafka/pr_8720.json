{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIyMTg2Njg0", "number": 8720, "title": "KAFKA-9971: Error Reporting in Sink Connectors (KIP-610)", "bodyText": "Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors\nThis PR adds the ErrantRecordReporter interface as well as its implementation - WorkerErrantRecordReporter. The WorkerErrantRecordReporter is created in Worker and brought up through WorkerSinkTask to WorkerSinkTaskContext. An integration test and unit test has been added.", "createdAt": "2020-05-23T01:00:41Z", "url": "https://github.com/apache/kafka/pull/8720", "merged": true, "mergeCommit": {"oid": "38c1e96d2c2084c7f3c3f9e5309ca91953e7c88f"}, "closed": true, "closedAt": "2020-05-28T06:49:57Z", "author": {"login": "aakashnshah"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcklCBTgFqTQxNzQwNTQ5OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcloD3HAFqTQxOTgxNjk4NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3NDA1NDk5", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-417405499", "createdAt": "2020-05-25T00:12:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMDoxMjo0NVrOGZyLLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMDozNToxMVrOGZyUxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY4OTY0NQ==", "bodyText": "this is separate from the Connect DLQ - maybe we should avoid mentioning it in the docs to make sure its a different concept?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               * Report a problematic record and the corresponding error to be written to the sink\n          \n          \n            \n               * connector's dead letter queue (DLQ).\n          \n          \n            \n               * Report a problematic record and the corresponding error to be written to the sink\n          \n          \n            \n               * connector's error topic.\n          \n      \n    \n    \n  \n\nor something similar", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429689645", "createdAt": "2020-05-25T00:12:45Z", "author": {"login": "levzem"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+public interface ErrantRecordReporter {\n+\n+\n+  /**\n+   * Report a problematic record and the corresponding error to be written to the sink\n+   * connector's dead letter queue (DLQ).", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY4OTY1Mg==", "bodyText": "nit 2x", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429689652", "createdAt": "2020-05-25T00:12:51Z", "author": {"login": "levzem"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+public interface ErrantRecordReporter {\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY4OTY5OQ==", "bodyText": "same as above", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429689699", "createdAt": "2020-05-25T00:13:16Z", "author": {"login": "levzem"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+public interface ErrantRecordReporter {\n+\n+\n+  /**\n+   * Report a problematic record and the corresponding error to be written to the sink\n+   * connector's dead letter queue (DLQ).\n+   *\n+   * <p>This call is asynchronous and returns a {@link java.util.concurrent.Future Future}.\n+   * Invoking {@link java.util.concurrent.Future#get() get()} on this future will block until the\n+   * record has been written or throw any exception that occurred while sending the record.\n+   * If you want to simulate a simple blocking call you can call the <code>get()</code> method\n+   * immediately.\n+   *\n+   * @param record the problematic record; may not be null\n+   * @param error  the error capturing the problem with the record; may not be null\n+   * @return a future that can be used to block until the record and error are reported\n+   *         to the DLQ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MDI0OQ==", "bodyText": "2x", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429690249", "createdAt": "2020-05-25T00:18:30Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MDM4MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            log.error(\"Error processing record: \" + record.toString(), error);\n          \n          \n            \n                            log.error(\"Error processing record: {}\", record.toString(), error);", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429690381", "createdAt": "2020-05-25T00:19:45Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MDYwNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            log.error(\n          \n          \n            \n                                \"Error processing record in topic \"\n          \n          \n            \n                                    + record.topic()\n          \n          \n            \n                                    + \"at offset \"\n          \n          \n            \n                                    + record.kafkaOffset(),\n          \n          \n            \n                                error\n          \n          \n            \n                            );\n          \n          \n            \n                            log.error(\n          \n          \n            \n                                \"Error processing record in topic {} partition {} at offset {}\", \n          \n          \n            \n                                 record.topic(),\n          \n          \n            \n                                 record.partition(),\n          \n          \n            \n                                 record.kafkaOffset(),\n          \n          \n            \n                                error\n          \n          \n            \n                            );", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429690605", "createdAt": "2020-05-25T00:21:47Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTA1Mw==", "bodyText": "should you remove the future from the list when you successfully get()?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429691053", "createdAt": "2020-05-25T00:25:45Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );\n+            }\n+        }\n+\n+        Future<RecordMetadata> producerFuture = null;\n+\n+        if (useDlq) {\n+\n+            Headers headers = record.headers();\n+            RecordHeaders result = new RecordHeaders();\n+            if (headers != null) {\n+                String topic = record.topic();\n+                for (Header header : headers) {\n+                    String key = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, key, header.schema(), header.value());\n+                    result.add(key, rawHeader);\n+                }\n+            }\n+\n+            ProducerRecord<byte[], byte[]> errantRecord = new ProducerRecord<>(\n+                dlqTopic,\n+                null,\n+                record.timestamp() == RecordBatch.NO_TIMESTAMP ? record.timestamp() : null,\n+                keyConverter.fromConnectData(dlqTopic, record.keySchema(), record.key()),\n+                valueConverter.fromConnectData(dlqTopic, record.valueSchema(), record.value()),\n+                result\n+            );\n+\n+            producerFuture = producer.send(errantRecord);\n+        }\n+\n+        ErrantRecordFuture errantRecordFuture = new ErrantRecordFuture(producerFuture);\n+        errantRecordFutures.add(errantRecordFuture);\n+        return errantRecordFuture;\n+    }\n+\n+    public void waitForAllFutures() {\n+        for (ErrantRecordFuture future : errantRecordFutures) {\n+            try {\n+                future.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTEwOQ==", "bodyText": "would be nice if this was a method in the config", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429691109", "createdAt": "2020-05-25T00:26:18Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -695,6 +705,32 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n         return reporters;\n     }\n \n+    private WorkerErrantRecordReporter createWorkerErrantRecordReporter(\n+        ConnectorTaskId id,\n+        SinkConnectorConfig connConfig,\n+        Class<? extends Connector> connectorClass,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        // check if errant record reporter topic is configured\n+        String topic = connConfig.dlqTopicName();\n+        if ((topic != null && !topic.isEmpty()) || connConfig.enableErrorLog()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTIxMQ==", "bodyText": "any reason for this to ever be null?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429691211", "createdAt": "2020-05-25T00:27:10Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );\n+            }\n+        }\n+\n+        Future<RecordMetadata> producerFuture = null;\n+\n+        if (useDlq) {\n+\n+            Headers headers = record.headers();\n+            RecordHeaders result = new RecordHeaders();\n+            if (headers != null) {\n+                String topic = record.topic();\n+                for (Header header : headers) {\n+                    String key = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, key, header.schema(), header.value());\n+                    result.add(key, rawHeader);\n+                }\n+            }\n+\n+            ProducerRecord<byte[], byte[]> errantRecord = new ProducerRecord<>(\n+                dlqTopic,\n+                null,\n+                record.timestamp() == RecordBatch.NO_TIMESTAMP ? record.timestamp() : null,\n+                keyConverter.fromConnectData(dlqTopic, record.keySchema(), record.key()),\n+                valueConverter.fromConnectData(dlqTopic, record.valueSchema(), record.value()),\n+                result\n+            );\n+\n+            producerFuture = producer.send(errantRecord);\n+        }\n+\n+        ErrantRecordFuture errantRecordFuture = new ErrantRecordFuture(producerFuture);\n+        errantRecordFutures.add(errantRecordFuture);\n+        return errantRecordFuture;\n+    }\n+\n+    public void waitForAllFutures() {\n+        for (ErrantRecordFuture future : errantRecordFutures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                throw new ConnectException(e);\n+            }\n+        }\n+    }\n+\n+    // Visible for testing\n+    public class ErrantRecordFuture implements Future<Void> {\n+\n+        Future<RecordMetadata> future;\n+\n+        public ErrantRecordFuture(Future<RecordMetadata> producerFuture) {\n+            future = producerFuture;\n+        }\n+\n+        public boolean cancel(boolean mayInterruptIfRunning) {\n+            throw new UnsupportedOperationException(\"Reporting an errant record cannot be cancelled.\");\n+        }\n+\n+        public boolean isCancelled() {\n+            return false;\n+        }\n+\n+        public boolean isDone() {\n+            return future == null || future.isDone();\n+        }\n+\n+        public Void get() throws InterruptedException, ExecutionException {\n+            if (future != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTY5MQ==", "bodyText": "extract to variable", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429691691", "createdAt": "2020-05-25T00:31:17Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -214,6 +218,72 @@ public void testSourceConnector() throws Exception {\n         connect.deleteConnector(CONNECTOR_NAME);\n     }\n \n+    @Test\n+    public void testErrantRecordReporter() throws Exception {\n+        connect.kafka().createTopic(DLQ_TOPIC, 1);\n+        // create test topic\n+        connect.kafka().createTopic(\"test-topic\", NUM_TOPIC_PARTITIONS);\n+\n+        // setup up props for the sink connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(CONNECTOR_CLASS_CONFIG, ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME);\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n+        props.put(TOPICS_CONFIG, \"test-topic\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTgwMA==", "bodyText": "do we need this invalid config step here", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429691800", "createdAt": "2020-05-25T00:32:23Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -214,6 +218,72 @@ public void testSourceConnector() throws Exception {\n         connect.deleteConnector(CONNECTOR_NAME);\n     }\n \n+    @Test\n+    public void testErrantRecordReporter() throws Exception {\n+        connect.kafka().createTopic(DLQ_TOPIC, 1);\n+        // create test topic\n+        connect.kafka().createTopic(\"test-topic\", NUM_TOPIC_PARTITIONS);\n+\n+        // setup up props for the sink connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(CONNECTOR_CLASS_CONFIG, ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME);\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n+        props.put(TOPICS_CONFIG, \"test-topic\");\n+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);\n+\n+        // validate the intended connector configuration, a config that errors\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 1,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // add missing configuration to make the config valid\n+        props.put(\"name\", CONNECTOR_NAME);\n+\n+        // validate the intended connector configuration, a valid config\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 0,\n+            \"Validating connector configuration produced an unexpected number or errors.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTg5NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    waitForCondition(this::checkForPartitionAssignment,\n          \n          \n            \n                        CONNECTOR_SETUP_DURATION_MS,\n          \n          \n            \n                        \"Connector tasks were not assigned a partition each.\");\n          \n          \n            \n                    waitForCondition(\n          \n          \n            \n                        this::checkForPartitionAssignment,\n          \n          \n            \n                        CONNECTOR_SETUP_DURATION_MS,\n          \n          \n            \n                        \"Connector tasks were not assigned a partition each.\"\n          \n          \n            \n                     );", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429691895", "createdAt": "2020-05-25T00:33:15Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -214,6 +218,72 @@ public void testSourceConnector() throws Exception {\n         connect.deleteConnector(CONNECTOR_NAME);\n     }\n \n+    @Test\n+    public void testErrantRecordReporter() throws Exception {\n+        connect.kafka().createTopic(DLQ_TOPIC, 1);\n+        // create test topic\n+        connect.kafka().createTopic(\"test-topic\", NUM_TOPIC_PARTITIONS);\n+\n+        // setup up props for the sink connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(CONNECTOR_CLASS_CONFIG, ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME);\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n+        props.put(TOPICS_CONFIG, \"test-topic\");\n+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);\n+\n+        // validate the intended connector configuration, a config that errors\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 1,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // add missing configuration to make the config valid\n+        props.put(\"name\", CONNECTOR_NAME);\n+\n+        // validate the intended connector configuration, a valid config\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 0,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // start a sink connector\n+        connect.configureConnector(CONNECTOR_NAME, props);\n+\n+        waitForCondition(this::checkForPartitionAssignment,\n+            CONNECTOR_SETUP_DURATION_MS,\n+            \"Connector tasks were not assigned a partition each.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTk3NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n      \n    \n    \n  \n\nnit", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429691974", "createdAt": "2020-05-25T00:34:27Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporterTest.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+@RunWith(PowerMockRunner.class)\n+@PowerMockIgnore(\"javax.management.*\")\n+public class WorkerErrantRecordReporterTest {\n+\n+    private WorkerErrantRecordReporter reporter;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MjEwMg==", "bodyText": "maybe make this into a mock as well and at least assert on some method calls using verify()", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429692102", "createdAt": "2020-05-25T00:35:11Z", "author": {"login": "levzem"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporterTest.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+@RunWith(PowerMockRunner.class)\n+@PowerMockIgnore(\"javax.management.*\")\n+public class WorkerErrantRecordReporterTest {\n+\n+    private WorkerErrantRecordReporter reporter;\n+\n+    private KafkaProducer<byte[], byte[]> producer = mock(KafkaProducer.class);\n+    private SinkConnectorConfig sinkConnectorConfig = mock(SinkConnectorConfig.class);\n+    private Converter converter = mock(Converter.class);\n+    private HeaderConverter headerConverter = mock(HeaderConverter.class);\n+    private SinkRecord record = mock(SinkRecord.class);\n+\n+    @Before\n+    public void setup() {\n+      reporter = new WorkerErrantRecordReporter(\n+            producer,\n+            sinkConnectorConfig,\n+            converter,\n+            converter,\n+            headerConverter\n+        );\n+    }\n+\n+    @Test\n+    public void testReport() {\n+      when(sinkConnectorConfig.dlqTopicName()).thenReturn(\"dlq-topic\");\n+      when(sinkConnectorConfig.enableErrorLog()).thenReturn(false);\n+      reporter.report(record, new Throwable());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3NzUzMTA4", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-417753108", "createdAt": "2020-05-25T14:39:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxNDozOToyN1rOGaDPDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxNToyNTo1M1rOGaEe_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk2OTE2NA==", "bodyText": "Please add JavaDoc for the interface, with @since 2.6.0.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429969164", "createdAt": "2020-05-25T14:39:27Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+public interface ErrantRecordReporter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3MjQyNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               *\n          \n          \n            \n               * \n          \n          \n            \n               * Connect guarantees that sink records reported through this reporter will be written to the error topic\n          \n          \n            \n               * before the framework calls the {@link SinkTask#preCommit(Map)} method and therefore before\n          \n          \n            \n               * committing the consumer offsets. SinkTask implementations can use the Future when stronger guarantees\n          \n          \n            \n               * are required.\n          \n          \n            \n               *", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429972427", "createdAt": "2020-05-25T14:46:23Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+public interface ErrantRecordReporter {\n+\n+\n+  /**\n+   * Report a problematic record and the corresponding error to be written to the sink\n+   * connector's dead letter queue (DLQ).\n+   *\n+   * <p>This call is asynchronous and returns a {@link java.util.concurrent.Future Future}.\n+   * Invoking {@link java.util.concurrent.Future#get() get()} on this future will block until the\n+   * record has been written or throw any exception that occurred while sending the record.\n+   * If you want to simulate a simple blocking call you can call the <code>get()</code> method\n+   * immediately.\n+   *", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3NTIzMQ==", "bodyText": "Also, what exceptions can this method throw? Should we add something like:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               *         to the DLQ\n          \n          \n            \n               *         to the DLQ\n          \n          \n            \n               * @throws ConnectException if the error reporter and DLQ fail to write a \n          \n          \n            \n               *         reported record and are configured with {@code error.tolerance=NONE}", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429975231", "createdAt": "2020-05-25T14:52:21Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+public interface ErrantRecordReporter {\n+\n+\n+  /**\n+   * Report a problematic record and the corresponding error to be written to the sink\n+   * connector's dead letter queue (DLQ).\n+   *\n+   * <p>This call is asynchronous and returns a {@link java.util.concurrent.Future Future}.\n+   * Invoking {@link java.util.concurrent.Future#get() get()} on this future will block until the\n+   * record has been written or throw any exception that occurred while sending the record.\n+   * If you want to simulate a simple blocking call you can call the <code>get()</code> method\n+   * immediately.\n+   *\n+   * @param record the problematic record; may not be null\n+   * @param error  the error capturing the problem with the record; may not be null\n+   * @return a future that can be used to block until the record and error are reported\n+   *         to the DLQ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY4OTY5OQ=="}, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3NzYxMQ==", "bodyText": "Nit formatting:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        WorkerErrantRecordReporter workerErrantRecordReporter =\n          \n          \n            \n                            createWorkerErrantRecordReporter(\n          \n          \n            \n                                id,\n          \n          \n            \n                                sinkConfig,\n          \n          \n            \n                                connectorClass,\n          \n          \n            \n                                keyConverter,\n          \n          \n            \n                                valueConverter,\n          \n          \n            \n                                headerConverter\n          \n          \n            \n                            );\n          \n          \n            \n                        WorkerErrantRecordReporter workerErrantRecordReporter = createWorkerErrantRecordReporter(\n          \n          \n            \n                                id, sinkConfig, connectorClass, keyConverter, valueConverter, headerConverter);", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429977611", "createdAt": "2020-05-25T14:57:33Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -531,13 +531,22 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,\n             log.info(\"Initializing: {}\", transformationChain);\n             SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());\n             retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n+            WorkerErrantRecordReporter workerErrantRecordReporter =\n+                createWorkerErrantRecordReporter(\n+                    id,\n+                    sinkConfig,\n+                    connectorClass,\n+                    keyConverter,\n+                    valueConverter,\n+                    headerConverter\n+                );", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3Nzk2OQ==", "bodyText": "Nit: let's avoid adding new lines in code otherwise unaffected in the PR.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429977969", "createdAt": "2020-05-25T14:58:20Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -680,6 +689,7 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n                                                                 connectorClientConfigOverridePolicy);\n             Map<String, Object> adminProps = adminConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);\n             DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(adminProps, id, connConfig, producerProps, errorHandlingMetrics);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3ODQ2Mw==", "bodyText": "Nit formatting:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return WorkerErrantRecordReporter.createAndSetup(\n          \n          \n            \n                            adminProps,\n          \n          \n            \n                            producerProps,\n          \n          \n            \n                            connConfig,\n          \n          \n            \n                            keyConverter,\n          \n          \n            \n                            valueConverter,\n          \n          \n            \n                            headerConverter\n          \n          \n            \n                        );\n          \n          \n            \n                        return WorkerErrantRecordReporter.createAndSetup(adminProps, producerProps,\n          \n          \n            \n                            connConfig, keyConverter, valueConverter, headerConverter);", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429978463", "createdAt": "2020-05-25T14:59:31Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -695,6 +705,32 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n         return reporters;\n     }\n \n+    private WorkerErrantRecordReporter createWorkerErrantRecordReporter(\n+        ConnectorTaskId id,\n+        SinkConnectorConfig connConfig,\n+        Class<? extends Connector> connectorClass,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        // check if errant record reporter topic is configured\n+        String topic = connConfig.dlqTopicName();\n+        if ((topic != null && !topic.isEmpty()) || connConfig.enableErrorLog()) {\n+            Map<String, Object> producerProps = producerConfigs(id, \"connector-dlq-producer-\" + id, config, connConfig, connectorClass,\n+                connectorClientConfigOverridePolicy);\n+            Map<String, Object> adminProps = adminConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);\n+            return WorkerErrantRecordReporter.createAndSetup(\n+                adminProps,\n+                producerProps,\n+                connConfig,\n+                keyConverter,\n+                valueConverter,\n+                headerConverter\n+            );", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3ODkyMA==", "bodyText": "Nit: static methods should appear before the non-static fields.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429978920", "createdAt": "2020-05-25T15:00:33Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3OTY1OQ==", "bodyText": "All fields that can be final should be marked as such. This provides semantic intent to future developers and helps prevent unintentionally changing the fields in the future.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429979659", "createdAt": "2020-05-25T15:02:14Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk4MTE3OA==", "bodyText": "Do we really want to pass the ExecutionException to the ConnectException, or would it be better to pass that exception's cause to the ConnectException?\nHow about log messages here?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429981178", "createdAt": "2020-05-25T15:05:44Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );\n+            }\n+        }\n+\n+        Future<RecordMetadata> producerFuture = null;\n+\n+        if (useDlq) {\n+\n+            Headers headers = record.headers();\n+            RecordHeaders result = new RecordHeaders();\n+            if (headers != null) {\n+                String topic = record.topic();\n+                for (Header header : headers) {\n+                    String key = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, key, header.schema(), header.value());\n+                    result.add(key, rawHeader);\n+                }\n+            }\n+\n+            ProducerRecord<byte[], byte[]> errantRecord = new ProducerRecord<>(\n+                dlqTopic,\n+                null,\n+                record.timestamp() == RecordBatch.NO_TIMESTAMP ? record.timestamp() : null,\n+                keyConverter.fromConnectData(dlqTopic, record.keySchema(), record.key()),\n+                valueConverter.fromConnectData(dlqTopic, record.valueSchema(), record.value()),\n+                result\n+            );\n+\n+            producerFuture = producer.send(errantRecord);\n+        }\n+\n+        ErrantRecordFuture errantRecordFuture = new ErrantRecordFuture(producerFuture);\n+        errantRecordFutures.add(errantRecordFuture);\n+        return errantRecordFuture;\n+    }\n+\n+    public void waitForAllFutures() {\n+        for (ErrantRecordFuture future : errantRecordFutures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                throw new ConnectException(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk4NTk2NA==", "bodyText": "At a higher level, why are we not reusing the RetryWithToleranceOperator here? I thought that was kind of the intent of the KIP, that this new report(...) method is just more way to capture problematic records using the existing DLQ functionality. I understand that might require other refactoring of that class (like returning a Future from the produce-like methods), but it seems like it would simplify things substantially by avoiding having to create our own producer and reuse a lot more of the functionality, such as metrics, retry count, logging, using the same producers, etc.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429985964", "createdAt": "2020-05-25T15:16:57Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -531,13 +531,22 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,\n             log.info(\"Initializing: {}\", transformationChain);\n             SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());\n             retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n+            WorkerErrantRecordReporter workerErrantRecordReporter =\n+                createWorkerErrantRecordReporter(\n+                    id,\n+                    sinkConfig,\n+                    connectorClass,\n+                    keyConverter,\n+                    valueConverter,\n+                    headerConverter\n+                );", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3NzYxMQ=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk4OTExMA==", "bodyText": "I suggested earlier about reusing the RetryWithToleranceOperator, and that doing so might require adding a produce-like method to that class that simply reports a new error. If that method took a Callback here and passed it to its producer.send(...) call, then we could provide a callback that removed the (completed) future from our list, helping to keep that list as small as possible with only the incomplete futures.\nIf we did that, we'd want to use a LinkedList rather than an ArrayList, since we're no longer removing futures only from the ends of the list.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429989110", "createdAt": "2020-05-25T15:24:35Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );\n+            }\n+        }\n+\n+        Future<RecordMetadata> producerFuture = null;\n+\n+        if (useDlq) {\n+\n+            Headers headers = record.headers();\n+            RecordHeaders result = new RecordHeaders();\n+            if (headers != null) {\n+                String topic = record.topic();\n+                for (Header header : headers) {\n+                    String key = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, key, header.schema(), header.value());\n+                    result.add(key, rawHeader);\n+                }\n+            }\n+\n+            ProducerRecord<byte[], byte[]> errantRecord = new ProducerRecord<>(\n+                dlqTopic,\n+                null,\n+                record.timestamp() == RecordBatch.NO_TIMESTAMP ? record.timestamp() : null,\n+                keyConverter.fromConnectData(dlqTopic, record.keySchema(), record.key()),\n+                valueConverter.fromConnectData(dlqTopic, record.valueSchema(), record.value()),\n+                result\n+            );\n+\n+            producerFuture = producer.send(errantRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk4OTMyNQ==", "bodyText": "Couldn't this be final?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429989325", "createdAt": "2020-05-25T15:25:07Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );\n+            }\n+        }\n+\n+        Future<RecordMetadata> producerFuture = null;\n+\n+        if (useDlq) {\n+\n+            Headers headers = record.headers();\n+            RecordHeaders result = new RecordHeaders();\n+            if (headers != null) {\n+                String topic = record.topic();\n+                for (Header header : headers) {\n+                    String key = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, key, header.schema(), header.value());\n+                    result.add(key, rawHeader);\n+                }\n+            }\n+\n+            ProducerRecord<byte[], byte[]> errantRecord = new ProducerRecord<>(\n+                dlqTopic,\n+                null,\n+                record.timestamp() == RecordBatch.NO_TIMESTAMP ? record.timestamp() : null,\n+                keyConverter.fromConnectData(dlqTopic, record.keySchema(), record.key()),\n+                valueConverter.fromConnectData(dlqTopic, record.valueSchema(), record.value()),\n+                result\n+            );\n+\n+            producerFuture = producer.send(errantRecord);\n+        }\n+\n+        ErrantRecordFuture errantRecordFuture = new ErrantRecordFuture(producerFuture);\n+        errantRecordFutures.add(errantRecordFuture);\n+        return errantRecordFuture;\n+    }\n+\n+    public void waitForAllFutures() {\n+        for (ErrantRecordFuture future : errantRecordFutures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                throw new ConnectException(e);\n+            }\n+        }\n+    }\n+\n+    // Visible for testing\n+    public class ErrantRecordFuture implements Future<Void> {\n+\n+        Future<RecordMetadata> future;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk4OTYzMA==", "bodyText": "If we ensure that the producer future is never null, then we can remove the if (future == null) kind of checks in this class' methods.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r429989630", "createdAt": "2020-05-25T15:25:53Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );\n+            }\n+        }\n+\n+        Future<RecordMetadata> producerFuture = null;\n+\n+        if (useDlq) {\n+\n+            Headers headers = record.headers();\n+            RecordHeaders result = new RecordHeaders();\n+            if (headers != null) {\n+                String topic = record.topic();\n+                for (Header header : headers) {\n+                    String key = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, key, header.schema(), header.value());\n+                    result.add(key, rawHeader);\n+                }\n+            }\n+\n+            ProducerRecord<byte[], byte[]> errantRecord = new ProducerRecord<>(\n+                dlqTopic,\n+                null,\n+                record.timestamp() == RecordBatch.NO_TIMESTAMP ? record.timestamp() : null,\n+                keyConverter.fromConnectData(dlqTopic, record.keySchema(), record.key()),\n+                valueConverter.fromConnectData(dlqTopic, record.valueSchema(), record.value()),\n+                result\n+            );\n+\n+            producerFuture = producer.send(errantRecord);\n+        }\n+\n+        ErrantRecordFuture errantRecordFuture = new ErrantRecordFuture(producerFuture);\n+        errantRecordFutures.add(errantRecordFuture);\n+        return errantRecordFuture;\n+    }\n+\n+    public void waitForAllFutures() {\n+        for (ErrantRecordFuture future : errantRecordFutures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                throw new ConnectException(e);\n+            }\n+        }\n+    }\n+\n+    // Visible for testing\n+    public class ErrantRecordFuture implements Future<Void> {\n+\n+        Future<RecordMetadata> future;\n+\n+        public ErrantRecordFuture(Future<RecordMetadata> producerFuture) {\n+            future = producerFuture;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 168}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NTYzMTk5", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-418563199", "createdAt": "2020-05-26T18:24:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzQzODQ5", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-418743849", "createdAt": "2020-05-26T23:23:22Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMzoyMzoyMlrOGazhxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMzoyNzozNVrOGazmfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MDM4OQ==", "bodyText": "Does this test ever encounter this exception? I don't think we will be able to backport this test to < 2.6 because the method won't exist at all, much less generate the exception that is being caught here.\nIf anything, this generates a less informative NPE later in put, and hides the actual root cause.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r430760389", "createdAt": "2020-05-26T23:23:22Z", "author": {"login": "gharris1727"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Task;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class ErrantRecordSinkConnector extends MonitorableSinkConnector {\n+\n+    @Override\n+    public Class<? extends Task> taskClass() {\n+        return ErrantRecordSinkTask.class;\n+    }\n+\n+    public static class ErrantRecordSinkTask extends MonitorableSinkTask {\n+        private ErrantRecordReporter reporter;\n+\n+        public ErrantRecordSinkTask() {\n+            super();\n+        }\n+\n+        @Override\n+        public void start(Map<String, String> props) {\n+            super.start(props);\n+            try {\n+                reporter = context.errantRecordReporter(); // may be null if DLQ not enabled\n+            } catch (NoClassDefFoundError e) {\n+                // Will occur in Connect runtimes earlier than 2.6", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MTU5OQ==", "bodyText": "Does this have an unbounded waiting time? How does this interact with task.shutdown.graceful.timeout.ms? What is the delivery guarantee of these error reports?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r430761599", "createdAt": "2020-05-26T23:27:35Z", "author": {"login": "gharris1727"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private KafkaProducer<byte[], byte[]> producer;\n+    private String dlqTopic;\n+    private boolean useDlq;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private List<ErrantRecordFuture> errantRecordFutures;\n+    private SinkConnectorConfig sinkConfig;\n+    private HeaderConverter headerConverter;\n+\n+\n+    public static WorkerErrantRecordReporter createAndSetup(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+\n+        KafkaProducer<byte[], byte[]> kafkaProducer = DeadLetterQueueReporter.setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConnectorConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );\n+\n+        return new WorkerErrantRecordReporter(\n+            kafkaProducer,\n+            sinkConnectorConfig,\n+            workerKeyConverter,\n+            workerValueConverter,\n+            workerHeaderConverter\n+        );\n+    }\n+\n+    // Visible for testing purposes\n+    public WorkerErrantRecordReporter(\n+        KafkaProducer<byte[], byte[]> kafkaProducer,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        Converter workerKeyConverter,\n+        Converter workerValueConverter,\n+        HeaderConverter workerHeaderConverter\n+    ) {\n+        producer = kafkaProducer;\n+        dlqTopic = sinkConnectorConfig.dlqTopicName();\n+        useDlq = dlqTopic != null && !dlqTopic.isEmpty();\n+        keyConverter = workerKeyConverter;\n+        valueConverter = workerValueConverter;\n+        errantRecordFutures = new ArrayList<>();\n+        sinkConfig = sinkConnectorConfig;\n+        headerConverter = workerHeaderConverter;\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+\n+        if (sinkConfig.enableErrorLog()) {\n+            if (sinkConfig.includeRecordDetailsInErrorLog()) {\n+                log.error(\"Error processing record: \" + record.toString(), error);\n+            } else {\n+                log.error(\n+                    \"Error processing record in topic \"\n+                        + record.topic()\n+                        + \"at offset \"\n+                        + record.kafkaOffset(),\n+                    error\n+                );\n+            }\n+        }\n+\n+        Future<RecordMetadata> producerFuture = null;\n+\n+        if (useDlq) {\n+\n+            Headers headers = record.headers();\n+            RecordHeaders result = new RecordHeaders();\n+            if (headers != null) {\n+                String topic = record.topic();\n+                for (Header header : headers) {\n+                    String key = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, key, header.schema(), header.value());\n+                    result.add(key, rawHeader);\n+                }\n+            }\n+\n+            ProducerRecord<byte[], byte[]> errantRecord = new ProducerRecord<>(\n+                dlqTopic,\n+                null,\n+                record.timestamp() == RecordBatch.NO_TIMESTAMP ? record.timestamp() : null,\n+                keyConverter.fromConnectData(dlqTopic, record.keySchema(), record.key()),\n+                valueConverter.fromConnectData(dlqTopic, record.valueSchema(), record.value()),\n+                result\n+            );\n+\n+            producerFuture = producer.send(errantRecord);\n+        }\n+\n+        ErrantRecordFuture errantRecordFuture = new ErrantRecordFuture(producerFuture);\n+        errantRecordFutures.add(errantRecordFuture);\n+        return errantRecordFuture;\n+    }\n+\n+    public void waitForAllFutures() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 152}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5MzE1MTU2", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-419315156", "createdAt": "2020-05-27T15:03:18Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTowMzoxOFrOGbPD-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNjowMDowNlrOGbR1wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxMTUxMw==", "bodyText": "This is not legal JavaDoc:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Component that the sink task can use as it {@link SinkTask#put(Collection<SinkRecord>)}.\n          \n          \n            \n             * Component that the sink task can use as it {@link SinkTask#put(java.util.Collection)}.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431211513", "createdAt": "2020-05-27T15:03:18Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+/**\n+ * Component that the sink task can use as it {@link SinkTask#put(Collection<SinkRecord>)}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxMjY1MA==", "bodyText": "You need to qualify Map or import it:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               * before the framework calls the {@link SinkTask#preCommit(Map)} method and therefore before\n          \n          \n            \n               * before the framework calls the {@link SinkTask#preCommit(java.util.Map)} method and therefore before", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431212650", "createdAt": "2020-05-27T15:04:42Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+/**\n+ * Component that the sink task can use as it {@link SinkTask#put(Collection<SinkRecord>)}.\n+ * Reporter of problematic records and the corresponding problems.\n+ *\n+ * @since 2.6\n+ */\n+public interface ErrantRecordReporter {\n+\n+  /**\n+   * Report a problematic record and the corresponding error to be written to the sink\n+   * connector's dead letter queue (DLQ).\n+   *\n+   * <p>This call is asynchronous and returns a {@link java.util.concurrent.Future Future}.\n+   * Invoking {@link java.util.concurrent.Future#get() get()} on this future will block until the\n+   * record has been written or throw any exception that occurred while sending the record.\n+   * If you want to simulate a simple blocking call you can call the <code>get()</code> method\n+   * immediately.\n+   *\n+   * Connect guarantees that sink records reported through this reporter will be written to the error topic\n+   * before the framework calls the {@link SinkTask#preCommit(Map)} method and therefore before", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxMzA4OQ==", "bodyText": "You need to qualify ConnectException or import it. The latter is probably better in this case to make the JavaDoc more readable in the code.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431213089", "createdAt": "2020-05-27T15:05:15Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+\n+/**\n+ * Component that the sink task can use as it {@link SinkTask#put(Collection<SinkRecord>)}.\n+ * Reporter of problematic records and the corresponding problems.\n+ *\n+ * @since 2.6\n+ */\n+public interface ErrantRecordReporter {\n+\n+  /**\n+   * Report a problematic record and the corresponding error to be written to the sink\n+   * connector's dead letter queue (DLQ).\n+   *\n+   * <p>This call is asynchronous and returns a {@link java.util.concurrent.Future Future}.\n+   * Invoking {@link java.util.concurrent.Future#get() get()} on this future will block until the\n+   * record has been written or throw any exception that occurred while sending the record.\n+   * If you want to simulate a simple blocking call you can call the <code>get()</code> method\n+   * immediately.\n+   *\n+   * Connect guarantees that sink records reported through this reporter will be written to the error topic\n+   * before the framework calls the {@link SinkTask#preCommit(Map)} method and therefore before\n+   * committing the consumer offsets. SinkTask implementations can use the Future when stronger guarantees\n+   * are required.\n+   *\n+   * @param record the problematic record; may not be null\n+   * @param error  the error capturing the problem with the record; may not be null\n+   * @return a future that can be used to block until the record and error are reported\n+   *         to the DLQ\n+   * @throws ConnectException if the error reporter and DLQ fails to write a reported record", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxMzkzMg==", "bodyText": "You can't change this public API. InternalSinkRecord needs to be in the runtime module.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431213932", "createdAt": "2020-05-27T15:06:23Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/SinkRecord.java", "diffHunk": "@@ -68,6 +70,14 @@ public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySche\n         return new SinkRecord(topic, kafkaPartition, keySchema, key, valueSchema, value, kafkaOffset(), timestamp, timestampType, headers);\n     }\n \n+    public InternalSinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key, Schema valueSchema, Object value,\n+                                        long kafkaOffset, Long timestamp,\n+                                        TimestampType timestampType, Iterable<Header> headers,\n+                                        ConsumerRecord<byte[], byte[]> originalRecord) {\n+        return new InternalSinkRecord(topic, kafkaPartition, keySchema, key, valueSchema, value,\n+            kafkaOffset, timestamp, timestampType, headers, originalRecord);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxNDI3NA==", "bodyText": "When you move InternalSinkRecord to the runtime module, be sure to make this private final.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431214274", "createdAt": "2020-05-27T15:06:52Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/SinkRecord.java", "diffHunk": "@@ -100,4 +110,29 @@ public String toString() {\n                 \", timestampType=\" + timestampType +\n                 \"} \" + super.toString();\n     }\n+\n+    public class InternalSinkRecord extends SinkRecord {\n+\n+        ConsumerRecord<byte[], byte[]> originalRecord;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxNTY2NQ==", "bodyText": "If we're going to add JavaDoc, which I think is helpful, then make it complete by adding a description:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    /**\n          \n          \n            \n                     *\n          \n          \n            \n                     * @return the original consumer record that was converted to this sink record.\n          \n          \n            \n                     */\n          \n          \n            \n                    /**\n          \n          \n            \n                     * Return the original consumer record that this sink record represents.\n          \n          \n            \n                     *\n          \n          \n            \n                     * @return the original consumer record; never null\n          \n          \n            \n                     */", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431215665", "createdAt": "2020-05-27T15:08:45Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/SinkRecord.java", "diffHunk": "@@ -100,4 +110,29 @@ public String toString() {\n                 \", timestampType=\" + timestampType +\n                 \"} \" + super.toString();\n     }\n+\n+    public class InternalSinkRecord extends SinkRecord {\n+\n+        ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+        public InternalSinkRecord(String topic, int partition, Schema keySchema, Object key,\n+                                  Schema valueSchema, Object value, long kafkaOffset,\n+                                  Long timestamp, TimestampType timestampType,\n+                                  Iterable<Header> headers,\n+                                  ConsumerRecord<byte[], byte[]> originalRecord) {\n+            super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp,\n+                timestampType, headers);\n+            this.originalRecord = originalRecord;\n+\n+        }\n+\n+        /**\n+         *\n+         * @return the original consumer record that was converted to this sink record.\n+         */", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxODk0OQ==", "bodyText": "First of all, let's avoid adding unnecessary blank lines.\nSecond, when you move this to runtime, you won't need to use this constructor and instead could use a much more straightforward one:\n         public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n            super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n                    record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n                    record.timestampType(), record.headers());\n            this.originalRecord = Objects.requireNonNull(originalRecord);\n        }", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431218949", "createdAt": "2020-05-27T15:13:12Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/SinkRecord.java", "diffHunk": "@@ -100,4 +110,29 @@ public String toString() {\n                 \", timestampType=\" + timestampType +\n                 \"} \" + super.toString();\n     }\n+\n+    public class InternalSinkRecord extends SinkRecord {\n+\n+        ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+        public InternalSinkRecord(String topic, int partition, Schema keySchema, Object key,\n+                                  Schema valueSchema, Object value, long kafkaOffset,\n+                                  Long timestamp, TimestampType timestampType,\n+                                  Iterable<Header> headers,\n+                                  ConsumerRecord<byte[], byte[]> originalRecord) {\n+            super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp,\n+                timestampType, headers);\n+            this.originalRecord = originalRecord;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxOTYxMg==", "bodyText": "This is invalid:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * passed to the {@link SinkTask#put(Collection)} method. When reporting a failed record,\n          \n          \n            \n                 * passed to the {@link SinkTask#put(java.util.Collection)} method. When reporting a failed record,", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431219612", "createdAt": "2020-05-27T15:13:58Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java", "diffHunk": "@@ -95,4 +95,30 @@\n      */\n     void requestCommit();\n \n+    /**\n+     * Get the reporter to which the sink task can report problematic or failed {@link SinkRecord records}\n+     * passed to the {@link SinkTask#put(Collection)} method. When reporting a failed record,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIyMDMxOA==", "bodyText": "This is invalid in JavaDoc:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * the sink task will receive a {@link Future} that the task can optionally use to wait until\n          \n          \n            \n                 * the sink task will receive a {@link java.util.concurrent.Future} that the task can optionally use to wait until", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431220318", "createdAt": "2020-05-27T15:14:52Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java", "diffHunk": "@@ -95,4 +95,30 @@\n      */\n     void requestCommit();\n \n+    /**\n+     * Get the reporter to which the sink task can report problematic or failed {@link SinkRecord records}\n+     * passed to the {@link SinkTask#put(Collection)} method. When reporting a failed record,\n+     * the sink task will receive a {@link Future} that the task can optionally use to wait until", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIyMTQ1NA==", "bodyText": "I don't think this line was actually changed other than formatting. Please remove it to avoid changing lines we don't have to.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431221454", "createdAt": "2020-05-27T15:16:25Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -552,14 +553,18 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,\n             TransformationChain<SinkRecord> transformationChain = new TransformationChain<>(connConfig.<SinkRecord>transformations(), retryWithToleranceOperator);\n             log.info(\"Initializing: {}\", transformationChain);\n             SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());\n-            retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n+            retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig,\n+                errorHandlingMetrics, connectorClass));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIyNjQwNg==", "bodyText": "Let's not create the InternalSinkRecord until after the transformation chain has been applied. That way we're not affected by any SMT that creates a new SinkRecord via a constructor (where we'd lose our InternalSinkRecord) rather than newRecord(...) (where we'd keep the InternalSinkRecord).\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n                    InternalSinkRecord internalSinkRecord = origRecord.newRecord(origRecord.topic(),\n          \n          \n            \n                        origRecord.kafkaPartition(), origRecord.keySchema(), origRecord.key(),\n          \n          \n            \n                        origRecord.valueSchema(), origRecord.value(), origRecord.kafkaOffset(),\n          \n          \n            \n                        origRecord.timestamp(), origRecord.timestampType(), origRecord.headers(), msg);", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431226406", "createdAt": "2020-05-27T15:22:56Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -497,12 +506,18 @@ private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]\n                 timestamp,\n                 msg.timestampType(),\n                 headers);\n+\n+        InternalSinkRecord internalSinkRecord = origRecord.newRecord(origRecord.topic(),\n+            origRecord.kafkaPartition(), origRecord.keySchema(), origRecord.key(),\n+            origRecord.valueSchema(), origRecord.value(), origRecord.kafkaOffset(),\n+            origRecord.timestamp(), origRecord.timestampType(), origRecord.headers(), msg);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIyNjY1Mg==", "bodyText": "This line would not need to be affected.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        recordActiveTopic(internalSinkRecord.topic());\n          \n          \n            \n                        recordActiveTopic(sinkRecord.topic());", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431226652", "createdAt": "2020-05-27T15:23:14Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -497,12 +506,18 @@ private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]\n                 timestamp,\n                 msg.timestampType(),\n                 headers);\n+\n+        InternalSinkRecord internalSinkRecord = origRecord.newRecord(origRecord.topic(),\n+            origRecord.kafkaPartition(), origRecord.keySchema(), origRecord.key(),\n+            origRecord.valueSchema(), origRecord.value(), origRecord.kafkaOffset(),\n+            origRecord.timestamp(), origRecord.timestampType(), origRecord.headers(), msg);\n+\n         log.trace(\"{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}\",\n                 this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());\n         if (isTopicTrackingEnabled) {\n-            recordActiveTopic(origRecord.topic());\n+            recordActiveTopic(internalSinkRecord.topic());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIyNjg1Mw==", "bodyText": "This line would change to:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return transformationChain.apply(internalSinkRecord);\n          \n          \n            \n                    // Apply the transformations\n          \n          \n            \n                    SinkRecord transformedRecord = transformationChain.apply(sinkRecord);\n          \n          \n            \n                    if (transformedRecord == null) {\n          \n          \n            \n                        // The record is being dropped\n          \n          \n            \n                        return null;\n          \n          \n            \n                    }\n          \n          \n            \n                    // Error reporting will need to correlate each sink record with the original consumer record\n          \n          \n            \n                    return new InternalSinkRecord(msg, transformedRecord);", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431226853", "createdAt": "2020-05-27T15:23:31Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -497,12 +506,18 @@ private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]\n                 timestamp,\n                 msg.timestampType(),\n                 headers);\n+\n+        InternalSinkRecord internalSinkRecord = origRecord.newRecord(origRecord.topic(),\n+            origRecord.kafkaPartition(), origRecord.keySchema(), origRecord.key(),\n+            origRecord.valueSchema(), origRecord.value(), origRecord.kafkaOffset(),\n+            origRecord.timestamp(), origRecord.timestampType(), origRecord.headers(), msg);\n+\n         log.trace(\"{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}\",\n                 this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());\n         if (isTopicTrackingEnabled) {\n-            recordActiveTopic(origRecord.topic());\n+            recordActiveTopic(internalSinkRecord.topic());\n         }\n-        return transformationChain.apply(origRecord);\n+        return transformationChain.apply(internalSinkRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIzMjA2Mw==", "bodyText": "We don't need to make this change, do we? Let's try to minimize the changes to the existing code.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431232063", "createdAt": "2020-05-27T15:28:06Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -67,30 +72,22 @@\n     private final SinkConnectorConfig connConfig;\n     private final ConnectorTaskId connectorTaskId;\n     private final ErrorHandlingMetrics errorHandlingMetrics;\n+    private final String dlqTopicName;\n \n     private KafkaProducer<byte[], byte[]> kafkaProducer;\n \n     public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminProps,\n                                                          ConnectorTaskId id,\n                                                          SinkConnectorConfig sinkConfig, Map<String, Object> producerProps,\n                                                          ErrorHandlingMetrics errorHandlingMetrics) {\n-        String topic = sinkConfig.dlqTopicName();\n \n-        try (Admin admin = Admin.create(adminProps)) {\n-            if (!admin.listTopics().names().get().contains(topic)) {\n-                log.error(\"Topic {} doesn't exist. Will attempt to create topic.\", topic);\n-                NewTopic schemaTopicRequest = new NewTopic(topic, DLQ_NUM_DESIRED_PARTITIONS, sinkConfig.dlqTopicReplicationFactor());\n-                admin.createTopics(singleton(schemaTopicRequest)).all().get();\n-            }\n-        } catch (InterruptedException e) {\n-            throw new ConnectException(\"Could not initialize dead letter queue with topic=\" + topic, e);\n-        } catch (ExecutionException e) {\n-            if (!(e.getCause() instanceof TopicExistsException)) {\n-                throw new ConnectException(\"Could not initialize dead letter queue with topic=\" + topic, e);\n-            }\n-        }\n \n-        KafkaProducer<byte[], byte[]> dlqProducer = new KafkaProducer<>(producerProps);\n+        KafkaProducer<byte[], byte[]> dlqProducer = setUpTopicAndProducer(\n+            adminProps,\n+            producerProps,\n+            sinkConfig,\n+            DLQ_NUM_DESIRED_PARTITIONS\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIzMzM4Mw==", "bodyText": "I think we don't really need to make this change anymore, since it's only refactoring the existing code that we don't need to actually change anymore. (This PR is no longer using this logic in multiple places.)", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431233383", "createdAt": "2020-05-27T15:29:56Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -176,6 +182,43 @@ void populateContextHeaders(ProducerRecord<byte[], byte[]> producerRecord, Proce\n         }\n     }\n \n+    public static KafkaProducer<byte[], byte[]> setUpTopicAndProducer(\n+        Map<String, Object> adminProps,\n+        Map<String, Object> producerProps,\n+        SinkConnectorConfig sinkConnectorConfig,\n+        int dlqTopicNumPartitions\n+    ) {\n+        String dlqTopic = sinkConnectorConfig.dlqTopicName();\n+\n+        if (dlqTopic != null && !dlqTopic.isEmpty()) {\n+            try (Admin admin = Admin.create(adminProps)) {\n+                if (!admin.listTopics().names().get().contains(dlqTopic)) {\n+                    log.error(\"Topic {} doesn't exist. Will attempt to create topic.\", dlqTopic);\n+                    NewTopic schemaTopicRequest = new NewTopic(\n+                        dlqTopic,\n+                        dlqTopicNumPartitions,\n+                        sinkConnectorConfig.dlqTopicReplicationFactor()\n+                    );\n+                    admin.createTopics(singleton(schemaTopicRequest)).all().get();\n+                }\n+            } catch (InterruptedException e) {\n+                throw new ConnectException(\n+                    \"Could not initialize errant record reporter with topic = \" + dlqTopic,\n+                    e\n+                );\n+            } catch (ExecutionException e) {\n+                if (!(e.getCause() instanceof TopicExistsException)) {\n+                    throw new ConnectException(\n+                        \"Could not initialize errant record reporter with topic = \" + dlqTopic,\n+                        e\n+                    );\n+                }\n+            }\n+            return new KafkaProducer<>(producerProps);\n+        }\n+        return null;\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIzNDY2MQ==", "bodyText": "Missing JavaDoc details:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * @return\n          \n          \n            \n                 * @return the future associated with the writing of this record; never null", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431234661", "createdAt": "2020-05-27T15:31:44Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -119,38 +117,46 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n      */\n     public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+        Callback callback = (metadata, exception) -> {\n+            if (exception != null) {\n+                log.error(\"Could not produce message to dead letter queue. topic=\" + dlqTopicName, exception);\n+                errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n+            }\n+        };\n+        report(context, callback);\n+    }\n+\n+    /**\n+     * Write the raw records into a Kafka topic. This methods allows for a custom callback to be\n+     * passed to the producer.\n+     *\n+     * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @param callback callback to be invoked by the producer when the record is sent to Kafka.\n+     * @return", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIzODU0OA==", "bodyText": "Why do we need to overload this method to pass in a callback? The only place we're using this new method is via the reporter, and WorkerErrantRecordReporter.callback doesn't seem to provide any value and in fact is not able to call errorHandlingMetrics.recordDeadLetterQueueProduceFailed() like this class.\nWouldn't it be much simpler to just add a return type to the existing method?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431238548", "createdAt": "2020-05-27T15:36:59Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -119,38 +117,46 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n      */\n     public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+        Callback callback = (metadata, exception) -> {\n+            if (exception != null) {\n+                log.error(\"Could not produce message to dead letter queue. topic=\" + dlqTopicName, exception);\n+                errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n+            }\n+        };\n+        report(context, callback);\n+    }\n+\n+    /**\n+     * Write the raw records into a Kafka topic. This methods allows for a custom callback to be\n+     * passed to the producer.\n+     *\n+     * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @param callback callback to be invoked by the producer when the record is sent to Kafka.\n+     * @return", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIzNDY2MQ=="}, "originalCommit": null, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIzOTIwNA==", "bodyText": "It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431239204", "createdAt": "2020-05-27T15:37:51Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -119,38 +117,46 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n      */\n     public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+        Callback callback = (metadata, exception) -> {\n+            if (exception != null) {\n+                log.error(\"Could not produce message to dead letter queue. topic=\" + dlqTopicName, exception);\n+                errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n+            }\n+        };\n+        report(context, callback);\n+    }\n+\n+    /**\n+     * Write the raw records into a Kafka topic. This methods allows for a custom callback to be\n+     * passed to the producer.\n+     *\n+     * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @param callback callback to be invoked by the producer when the record is sent to Kafka.\n+     * @return\n+     */\n+    public Future<RecordMetadata> report(ProcessingContext context, Callback callback) {\n         if (dlqTopicName.isEmpty()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n-\n         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();\n \n         ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();\n         if (originalMessage == null) {\n             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n-        ProducerRecord<byte[], byte[]> producerRecord;\n-        if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n-            producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n-        } else {\n-            producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.timestamp(),\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n-        }\n+        ProducerRecord<byte[], byte[]> producerRecord =\n+            new ProducerRecord<>(dlqTopicName, null,\n+                originalMessage.timestamp() != RecordBatch.NO_TIMESTAMP ?\n+                    originalMessage.timestamp() : null,\n+                originalMessage.key(), originalMessage.value(), originalMessage.headers());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI0MDExMg==", "bodyText": "I don't think we need to overload this method, and instead we can just change the return type. After all, the ErrorReporter is not part of the public API, and is merely an abstraction we use within the runtime itself.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431240112", "createdAt": "2020-05-27T15:39:05Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java", "diffHunk": "@@ -28,6 +34,18 @@\n      */\n     void report(ProcessingContext context);\n \n+    /**\n+     * Report an error with a specified callback.\n+     *\n+     * @param context the processing context (cannot be null).\n+     * @param callback callback to be invoked by a producer when sending a record to Kafka.\n+     * @return future result from the producer sending a record to Kafka\n+     */\n+    default Future<RecordMetadata> report(ProcessingContext context, Callback callback) {\n+        report(context);\n+        return CompletableFuture.completedFuture(null);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI0NDk0Mw==", "bodyText": "Since we often have just one reporter, it is probably worth avoiding the unnecessary allocations:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    List<Future<RecordMetadata>> futures = new ArrayList<>();\n          \n          \n            \n                    for (ErrorReporter reporter: reporters) {\n          \n          \n            \n                        Future<RecordMetadata> future = reporter.report(this, callback);\n          \n          \n            \n                        if (!future.isDone()) {\n          \n          \n            \n                            futures.add(future);\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    return new ErrantRecordFuture(futures);\n          \n          \n            \n                    if (reporters.size() == 1) {\n          \n          \n            \n                        return reporters.get(0).report(this);\n          \n          \n            \n                    }\n          \n          \n            \n                    List<Future<RecordMetadata>> futures = new LinkedList<>();\n          \n          \n            \n                    for (ErrorReporter reporter: reporters) {\n          \n          \n            \n                        Future<RecordMetadata> future = reporter.report(this, callback);\n          \n          \n            \n                        if (!future.isDone()) {\n          \n          \n            \n                            futures.add(future);\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    if (futures.isEmpty()) {\n          \n          \n            \n                        return CompletableFuture.completedFuture(null);\n          \n          \n            \n                    }\n          \n          \n            \n                    return new ErrantRecordFuture(futures);\n          \n      \n    \n    \n  \n\nAnd since we don't know how many futures we'll add to the list (and it will likely be just zero if the DLQ is not configured or just one for the DLQ), let's use a LinkedList instead to avoid excessive allocation when adding the first element to the ArrayList.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431244943", "createdAt": "2020-05-27T15:45:19Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java", "diffHunk": "@@ -139,6 +145,18 @@ public void report() {\n         }\n     }\n \n+    public Future<Void> report(Callback callback) {\n+        List<Future<RecordMetadata>> futures = new ArrayList<>();\n+        for (ErrorReporter reporter: reporters) {\n+            Future<RecordMetadata> future = reporter.report(this, callback);\n+            if (!future.isDone()) {\n+                futures.add(future);\n+            }\n+        }\n+        return new ErrantRecordFuture(futures);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI0NzIxOA==", "bodyText": "No new line is needed here.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431247218", "createdAt": "2020-05-27T15:48:13Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java", "diffHunk": "@@ -83,6 +87,17 @@ public RetryWithToleranceOperator(long errorRetryTimeout, long errorMaxDelayInMi\n         this.time = time;\n     }\n \n+    public Future<Void> executeFailed(Function<SinkRecord, ConsumerRecord<byte[], byte[]>> function,\n+                                      Stage stage, Class<?> executingClass, SinkRecord record,\n+                                      Throwable error, Callback callback) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI0ODc4Ng==", "bodyText": "Why use a function here? We can use a simple variable here.\n(I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431248786", "createdAt": "2020-05-27T15:50:10Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.sink.SinkRecord.InternalSinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Function;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final Callback callback = (metadata, exception) -> {\n+        if (exception != null) {\n+            throw new ConnectException(\"Failed to send the errant record to Kafka\",\n+                exception.getCause());\n+        }\n+    };\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        Function<SinkRecord, ConsumerRecord<byte[], byte[]>> function;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI0OTU1OA==", "bodyText": "How about adding it only if the future is not done already?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431249558", "createdAt": "2020-05-27T15:51:11Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.sink.SinkRecord.InternalSinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Function;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final Callback callback = (metadata, exception) -> {\n+        if (exception != null) {\n+            throw new ConnectException(\"Failed to send the errant record to Kafka\",\n+                exception.getCause());\n+        }\n+    };\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        Function<SinkRecord, ConsumerRecord<byte[], byte[]>> function;\n+\n+        if (record instanceof InternalSinkRecord) {\n+            function = sinkRecord -> ((InternalSinkRecord) sinkRecord).originalRecord();\n+        } else {\n+            function = sinkRecord -> {\n+\n+                String topic = record.topic();\n+                byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+                byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(),\n+                    record.value());\n+\n+                RecordHeaders headers = new RecordHeaders();\n+                if (record.headers() != null) {\n+                    for (Header header : record.headers()) {\n+                        String headerKey = header.key();\n+                        byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                            header.schema(), header.value());\n+                        headers.add(headerKey, rawHeader);\n+                    }\n+                }\n+\n+                return new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                    record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                    -1, key, value, headers);\n+\n+            };\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(function, Stage.TASK_PUT,\n+            SinkTask.class, record, error, callback);\n+\n+        futures.add(future);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI0OTk5NA==", "bodyText": "Let's avoid unnecessary blank lines.\n\n  \n    \n      \n        Suggested change", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431249994", "createdAt": "2020-05-27T15:51:42Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.sink.SinkRecord.InternalSinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Function;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final Callback callback = (metadata, exception) -> {\n+        if (exception != null) {\n+            throw new ConnectException(\"Failed to send the errant record to Kafka\",\n+                exception.getCause());\n+        }\n+    };\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        Function<SinkRecord, ConsumerRecord<byte[], byte[]>> function;\n+\n+        if (record instanceof InternalSinkRecord) {\n+            function = sinkRecord -> ((InternalSinkRecord) sinkRecord).originalRecord();\n+        } else {\n+            function = sinkRecord -> {\n+\n+                String topic = record.topic();\n+                byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+                byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(),\n+                    record.value());\n+\n+                RecordHeaders headers = new RecordHeaders();\n+                if (record.headers() != null) {\n+                    for (Header header : record.headers()) {\n+                        String headerKey = header.key();\n+                        byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                            header.schema(), header.value());\n+                        headers.add(headerKey, rawHeader);\n+                    }\n+                }\n+\n+                return new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                    record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                    -1, key, value, headers);\n+\n+            };\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(function, Stage.TASK_PUT,\n+            SinkTask.class, record, error, callback);\n+\n+        futures.add(future);\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void getAllFutures() {\n+        for (Future<Void> future : futures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while calling \");\n+                throw new ConnectException(e);\n+            }\n+        }\n+        futures.clear();\n+    }\n+\n+    /**\n+     * Wrapper class to aggregate producer futures and abstract away the record metadata from the\n+     * Connect user.\n+     */\n+    public static class ErrantRecordFuture implements Future<Void> {\n+\n+        private final List<Future<RecordMetadata>> futures;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI1NzAyNQ==", "bodyText": "Rather than have a list of futures, why not have a single Future delegate that is either a CompletableFuture.allOf(...) or a single feature? This makes the constructor a little more complex, but it would simplify all of the other methods tremendously since they merely have to delegate (except for cancel() and isCancelled(), which can stay the same:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) {\n          \n          \n            \n                        futures = producerFutures;\n          \n          \n            \n                    }\n          \n          \n            \n                    public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) {\n          \n          \n            \n                        if (producerFutures == null || producerFutures.isEmpty()) {\n          \n          \n            \n                            future = CompletableFuture.completedFuture(null);\n          \n          \n            \n                        } else {\n          \n          \n            \n                            futures = CompletableFutures.allOf(producerFutures);\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n      \n    \n    \n  \n\nThis will make get(long, TimeUnit) behave more correctly by requiring that all futures complete within the stated time.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431257025", "createdAt": "2020-05-27T16:00:06Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.sink.SinkRecord.InternalSinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Function;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final Callback callback = (metadata, exception) -> {\n+        if (exception != null) {\n+            throw new ConnectException(\"Failed to send the errant record to Kafka\",\n+                exception.getCause());\n+        }\n+    };\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        Function<SinkRecord, ConsumerRecord<byte[], byte[]>> function;\n+\n+        if (record instanceof InternalSinkRecord) {\n+            function = sinkRecord -> ((InternalSinkRecord) sinkRecord).originalRecord();\n+        } else {\n+            function = sinkRecord -> {\n+\n+                String topic = record.topic();\n+                byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+                byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(),\n+                    record.value());\n+\n+                RecordHeaders headers = new RecordHeaders();\n+                if (record.headers() != null) {\n+                    for (Header header : record.headers()) {\n+                        String headerKey = header.key();\n+                        byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                            header.schema(), header.value());\n+                        headers.add(headerKey, rawHeader);\n+                    }\n+                }\n+\n+                return new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                    record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                    -1, key, value, headers);\n+\n+            };\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(function, Stage.TASK_PUT,\n+            SinkTask.class, record, error, callback);\n+\n+        futures.add(future);\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void getAllFutures() {\n+        for (Future<Void> future : futures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while calling \");\n+                throw new ConnectException(e);\n+            }\n+        }\n+        futures.clear();\n+    }\n+\n+    /**\n+     * Wrapper class to aggregate producer futures and abstract away the record metadata from the\n+     * Connect user.\n+     */\n+    public static class ErrantRecordFuture implements Future<Void> {\n+\n+        private final List<Future<RecordMetadata>> futures;\n+\n+\n+        public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) {\n+            futures = producerFutures;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 141}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NTAxMjg0", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-419501284", "createdAt": "2020-05-27T18:31:46Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxODozMTo0NlrOGbX9fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxOTowNzo1MlrOGbZLew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM1NzMwOQ==", "bodyText": "This leftover line should be removed.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            import org.apache.kafka.clients.consumer.ConsumerRecord;", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431357309", "createdAt": "2020-05-27T18:31:46Z", "author": {"login": "rhauch"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/SinkRecord.java", "diffHunk": "@@ -16,6 +16,7 @@\n  */\n package org.apache.kafka.connect.sink;\n \n+import org.apache.kafka.clients.consumer.ConsumerRecord;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM2NzQ2Nw==", "bodyText": "We need to override the newRecord(...) that has all the parameters:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n          \n          \n            \n                    super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n          \n          \n            \n                        record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n          \n          \n            \n                        record.timestampType(), record.headers());\n          \n          \n            \n                    this.originalRecord = originalRecord;\n          \n          \n            \n                }\n          \n          \n            \n                public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n          \n          \n            \n                    this(originalRecord, record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n          \n          \n            \n                            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n          \n          \n            \n                            record.timestampType(), record.headers());\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord,\n          \n          \n            \n                    String topic, int partition, Schema keySchema, Object key, Schema valueSchema, Object value, long kafkaOffset,\n          \n          \n            \n                    Long timestamp, TimestampType timestampType, Iterable<Header> headers\n          \n          \n            \n                ) {\n          \n          \n            \n                    super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers);\n          \n          \n            \n                    this.originalRecord = originalRecord;\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                @Override\n          \n          \n            \n                public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key, Schema valueSchema, Object value,\n          \n          \n            \n                        Long timestamp, Iterable<Header> headers) {\n          \n          \n            \n                    return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,\n          \n          \n            \n                            valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers());\n          \n          \n            \n                }", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431367467", "createdAt": "2020-05-27T18:50:16Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM2ODg0Nw==", "bodyText": "Let's add a trace log message before and after this call.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431368847", "createdAt": "2020-05-27T18:52:40Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -360,6 +364,10 @@ private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean cl\n     }\n \n     private void commitOffsets(long now, boolean closing) {\n+        if (workerErrantRecordReporter != null) {\n+            workerErrantRecordReporter.getAllFutures();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM2OTE4Mw==", "bodyText": "Nit: let's remove this blank line, since there already are quite a few.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431369183", "createdAt": "2020-05-27T18:53:14Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -497,12 +505,21 @@ private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]\n                 timestamp,\n                 msg.timestampType(),\n                 headers);\n+\n         log.trace(\"{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}\",\n                 this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());\n         if (isTopicTrackingEnabled) {\n             recordActiveTopic(origRecord.topic());\n         }\n-        return transformationChain.apply(origRecord);\n+\n+        // Apply the transformations\n+        SinkRecord transformedRecord = transformationChain.apply(origRecord);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM2OTI4OA==", "bodyText": "Nit: let's remove this blank line, since it's unrelated to other changes.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431369288", "createdAt": "2020-05-27T18:53:27Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -497,12 +505,21 @@ private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]\n                 timestamp,\n                 msg.timestampType(),\n                 headers);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM2OTU4Mg==", "bodyText": "Nit: let's remove this blank line, since it's unrelated to other changes.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431369582", "createdAt": "2020-05-27T18:53:58Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -21,11 +21,13 @@\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n import org.apache.kafka.common.errors.TopicExistsException;\n import org.apache.kafka.common.header.Headers;\n import org.apache.kafka.common.record.RecordBatch;\n import org.apache.kafka.connect.errors.ConnectException;\n import org.apache.kafka.connect.runtime.SinkConnectorConfig;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3MDE2Mg==", "bodyText": "Should we trim this?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    this.dlqTopicName = connConfig.dlqTopicName();\n          \n          \n            \n                    this.dlqTopicName = connConfig.dlqTopicName().trim();", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431370162", "createdAt": "2020-05-27T18:54:56Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -111,6 +116,7 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n         this.errorHandlingMetrics = errorHandlingMetrics;\n+        this.dlqTopicName = connConfig.dlqTopicName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3MTQxNA==", "bodyText": "This is an internal API, so why can we not just change the existing report(...) method to return Future<?>?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431371414", "createdAt": "2020-05-27T18:57:12Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java", "diffHunk": "@@ -28,6 +33,17 @@\n      */\n     void report(ProcessingContext context);\n \n+    /**\n+     * Report an error and return the producer future.\n+     *\n+     * @param context the processing context (cannot be null).\n+     * @return future result from the producer sending a record to Kafka\n+     */\n+    default Future<RecordMetadata> reportAndReturnFuture(ProcessingContext context) {\n+        report(context);\n+        return CompletableFuture.completedFuture(null);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3Mjk0MA==", "bodyText": "This can be package protected and final:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public LinkedList<Future<Void>> futures;\n          \n          \n            \n                final LinkedList<Future<Void>> futures;", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431372940", "createdAt": "2020-05-27T18:59:47Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3MzIyNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(),\n          \n          \n            \n                            record.value());\n          \n          \n            \n                        byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(), record.value());", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431373225", "createdAt": "2020-05-27T19:00:17Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(),\n+                record.value());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3Mzk1Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (record instanceof InternalSinkRecord) {\n          \n          \n            \n                    // Most of the records will be an internal sink record, but the task could potentially\n          \n          \n            \n                    // report modified or new records, so handle both cases\n          \n          \n            \n                    if (record instanceof InternalSinkRecord) {", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431373953", "createdAt": "2020-05-27T19:01:37Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        if (record instanceof InternalSinkRecord) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3NDQ2MA==", "bodyText": "Let's rename this to awaitAllFutures() since this really is not a getter method.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431374460", "createdAt": "2020-05-27T19:02:34Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(),\n+                record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                -1, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void getAllFutures() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3NjkzNA==", "bodyText": "Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    for (Future<Void> future : futures) {\n          \n          \n            \n                        try {\n          \n          \n            \n                            future.get();\n          \n          \n            \n                        } catch (InterruptedException | ExecutionException e) {\n          \n          \n            \n                            log.error(\"Encountered an error while calling \");\n          \n          \n            \n                            throw new ConnectException(e);\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    futures.clear();\n          \n          \n            \n                    Future<?> future = null;\n          \n          \n            \n                    while ((future = futures.poll()) != null) {\n          \n          \n            \n                        try {\n          \n          \n            \n                            future.get();\n          \n          \n            \n                        } catch (InterruptedException | ExecutionException e) {\n          \n          \n            \n                            log.error(\"Encountered an error while calling \");\n          \n          \n            \n                            throw new ConnectException(e);\n          \n          \n            \n                        }\n          \n          \n            \n                    }", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431376934", "createdAt": "2020-05-27T19:07:13Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    public LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(),\n+                record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                -1, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void getAllFutures() {\n+        for (Future<Void> future : futures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while calling \");\n+                throw new ConnectException(e);\n+            }\n+        }\n+        futures.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM3NzI3NQ==", "bodyText": "@aakashnshah let's remove this comment", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431377275", "createdAt": "2020-05-27T19:07:52Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Task;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class ErrantRecordSinkConnector extends MonitorableSinkConnector {\n+\n+    @Override\n+    public Class<? extends Task> taskClass() {\n+        return ErrantRecordSinkTask.class;\n+    }\n+\n+    public static class ErrantRecordSinkTask extends MonitorableSinkTask {\n+        private ErrantRecordReporter reporter;\n+\n+        public ErrantRecordSinkTask() {\n+            super();\n+        }\n+\n+        @Override\n+        public void start(Map<String, String> props) {\n+            super.start(props);\n+            try {\n+                reporter = context.errantRecordReporter(); // may be null if DLQ not enabled\n+            } catch (NoClassDefFoundError e) {\n+                // Will occur in Connect runtimes earlier than 2.6", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MDM4OQ=="}, "originalCommit": null, "originalPosition": 49}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26034508ea5c799768484c70df2a06fe0f8c53c9", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/26034508ea5c799768484c70df2a06fe0f8c53c9", "committedDate": "2020-05-27T19:26:35Z", "message": "KAFKA-9971: Error Reporting in Sink Connectors\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1134e990335001d732678c4def54738a4d71473d", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/1134e990335001d732678c4def54738a4d71473d", "committedDate": "2020-05-27T19:26:35Z", "message": "addressed comments\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd043161ad3a430f4c341fb86b4a4f31aded0e5b", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/cd043161ad3a430f4c341fb86b4a4f31aded0e5b", "committedDate": "2020-05-27T19:26:35Z", "message": "addressed more comments\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/b91a8f989aa3dcfccf9d3498717c0517b013c677", "committedDate": "2020-05-27T19:26:35Z", "message": "addressed some more comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/b91a8f989aa3dcfccf9d3498717c0517b013c677", "committedDate": "2020-05-27T19:26:35Z", "message": "addressed some more comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NTQ4MDI5", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-419548029", "createdAt": "2020-05-27T19:34:37Z", "commit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxOTozNDozN1rOGbaS-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxOTo1ODo0NFrOGbbC7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5NTU3OQ==", "bodyText": "We should not make this method a default method, since both implementations of the interface define this method.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431395579", "createdAt": "2020-05-27T19:34:37Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java", "diffHunk": "@@ -16,17 +16,25 @@\n  */\n package org.apache.kafka.connect.runtime.errors;\n \n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+\n /**\n  * Report an error using the information contained in the {@link ProcessingContext}.\n  */\n public interface ErrorReporter extends AutoCloseable {\n \n     /**\n-     * Report an error.\n+     * Report an error and return the producer future.\n      *\n      * @param context the processing context (cannot be null).\n+     * @return future result from the producer sending a record to Kafka.\n      */\n-    void report(ProcessingContext context);\n+    default Future<RecordMetadata> report(ProcessingContext context) {\n+        return CompletableFuture.completedFuture(null);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5NzA1NA==", "bodyText": "What about creating:\n    private static final Future<RecordMetadata> COMPLETED = CompletableFuture.completedFuture(null);\n\nand then returning that instance in all of these places. Since it's already completed, immutable, and we don't allow cancellation, it should be fine to reuse in this LogReporter.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431397054", "createdAt": "2020-05-27T19:37:27Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java", "diffHunk": "@@ -50,17 +53,18 @@ public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig, ErrorHandling\n      * @param context the processing context.\n      */\n     @Override\n-    public void report(ProcessingContext context) {\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (!connConfig.enableErrorLog()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         if (!context.failed()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         log.error(message(context), context.error());\n         errorHandlingMetrics.recordErrorLogged();\n+        return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwMDc4Nw==", "bodyText": "How about clarifying this a bit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        String topic = record.topic();\n          \n          \n            \n                        // Generate a new consumer record from the modified sink record. We prefer\n          \n          \n            \n                        // to send the original consumer record (pre-transformed) to the DLQ, \n          \n          \n            \n                        // but in this case we don't have one and send the potentially transformed\n          \n          \n            \n                        // record instead\n          \n          \n            \n                        String topic = record.topic();", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431400787", "createdAt": "2020-05-27T19:44:44Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwMjEyMw==", "bodyText": "We should use the length of the key and value in the record:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n          \n          \n            \n                            record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n          \n          \n            \n                            -1, key, value, headers);\n          \n          \n            \n                        int keyLength = key != null ? key.length : -1;\n          \n          \n            \n                        int valLength = value != null ? value.length : -1;\n          \n          \n            \n                        consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n          \n          \n            \n                            record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n          \n          \n            \n                            valLength, key, value, headers);", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431402123", "createdAt": "2020-05-27T19:47:20Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                -1, key, value, headers);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwNzg1NA==", "bodyText": "Once again, please add trace log messages before an after this line.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431407854", "createdAt": "2020-05-27T19:58:44Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -360,6 +364,10 @@ private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean cl\n     }\n \n     private void commitOffsets(long now, boolean closing) {\n+        if (workerErrantRecordReporter != null) {\n+            workerErrantRecordReporter.awaitAllFutures();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677"}, "originalPosition": 37}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab1af2bd15f4a05974ec55bcae34ad39ece1aad3", "author": {"user": {"login": "rhauch", "name": "Randall Hauch"}}, "url": "https://github.com/apache/kafka/commit/ab1af2bd15f4a05974ec55bcae34ad39ece1aad3", "committedDate": "2020-05-27T20:02:55Z", "message": "KAFKA-9971: Added trace logging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "committedDate": "2020-05-27T21:24:31Z", "message": "more comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NjQ4MDQ2", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-419648046", "createdAt": "2020-05-27T22:16:37Z", "commit": {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QyMjoxNjozN1rOGbfFxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QyMjoyMjoxOVrOGbfOLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NDExNg==", "bodyText": "Hmm, let's just have this delegate to the super method. It's internal, so we need not include the original record details.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return \"InternalSinkRecord{\" +\n          \n          \n            \n                            \"consumerRecord=\" + originalRecord.toString() +\n          \n          \n            \n                            \"} \" + super.toString();\n          \n          \n            \n                    return super.toString();", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431474116", "createdAt": "2020-05-27T22:16:37Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {\n+        super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers);\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    @Override\n+    public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key,\n+                                Schema valueSchema, Object value, Long timestamp,\n+                                Iterable<Header> headers) {\n+        return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,\n+            valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers());\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        return super.equals(o);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return super.hashCode();\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"InternalSinkRecord{\" +\n+                \"consumerRecord=\" + originalRecord.toString() +\n+                \"} \" + super.toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NDI5Nw==", "bodyText": "Let's make this protected.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n          \n          \n            \n                protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431474297", "createdAt": "2020-05-27T22:17:06Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NTg0Ng==", "bodyText": "These can be final.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431475846", "createdAt": "2020-05-27T22:21:14Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NjI3MA==", "bodyText": "Nit: new line is unnecessary, and there's a misspelling:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            log.error(\"Encountered an error while awaiting an errant record future's \" +\n          \n          \n            \n                                \"completition.\");\n          \n          \n            \n                            log.error(\"Encountered an error while awaiting an errant record future's completion.\");", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431476270", "createdAt": "2020-05-27T22:22:19Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;\n+        while ((future = futures.poll()) != null) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while awaiting an errant record future's \" +\n+                    \"completition.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a"}, "originalPosition": 123}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/126b04c568381dc108db87da27ddd4862a5e0d2b", "committedDate": "2020-05-27T22:58:05Z", "message": "more comments again"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NjkzNjI0", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-419693624", "createdAt": "2020-05-28T00:10:55Z", "commit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQwMDoxMDo1NVrOGbhZ0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQwMjowMzoyMlrOGbjMKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxMjAxOQ==", "bodyText": "nit: indentation is a bit off here.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431512019", "createdAt": "2020-05-28T00:10:55Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDAxNQ==", "bodyText": "these overrides don't seem to add much.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431514015", "createdAt": "2020-05-28T00:17:44Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {\n+        super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers);\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    @Override\n+    public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key,\n+                                Schema valueSchema, Object value, Long timestamp,\n+                                Iterable<Header> headers) {\n+        return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,\n+            valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers());\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        return super.equals(o);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return super.hashCode();\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return super.toString();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDMzNg==", "bodyText": "nit: I think we always keep = with the left operand.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431514336", "createdAt": "2020-05-28T00:18:41Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -553,13 +554,16 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,\n             log.info(\"Initializing: {}\", transformationChain);\n             SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());\n             retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n+            WorkerErrantRecordReporter workerErrantRecordReporter\n+                = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNjcyNA==", "bodyText": "should be final right?", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431516724", "createdAt": "2020-05-28T00:27:30Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -94,6 +95,7 @@\n     private int commitFailures;\n     private boolean pausedForRedelivery;\n     private boolean committing;\n+    private WorkerErrantRecordReporter workerErrantRecordReporter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNzIzNg==", "bodyText": "Also, topic can never be null if it's coming from a parsed config value that doesn't have null as its default value. (another way to think of that is that you can't pass a null value from properties)", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431517236", "createdAt": "2020-05-28T00:29:32Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -695,6 +705,32 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n         return reporters;\n     }\n \n+    private WorkerErrantRecordReporter createWorkerErrantRecordReporter(\n+        ConnectorTaskId id,\n+        SinkConnectorConfig connConfig,\n+        Class<? extends Connector> connectorClass,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        // check if errant record reporter topic is configured\n+        String topic = connConfig.dlqTopicName();\n+        if ((topic != null && !topic.isEmpty()) || connConfig.enableErrorLog()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTEwOQ=="}, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMjUzNA==", "bodyText": "let's add protected here to be symmetric to what other fields that are accessed by the context have as scope", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431522534", "createdAt": "2020-05-28T00:49:31Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -518,6 +535,10 @@ private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {\n         return result;\n     }\n \n+    WorkerErrantRecordReporter workerErrantRecordReporter() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMjkwOQ==", "bodyText": "typo. Not sure how you want to say it.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431522909", "createdAt": "2020-05-28T00:50:57Z", "author": {"login": "kkonstantine"}, "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+import org.apache.kafka.connect.errors.ConnectException;\n+\n+/**\n+ * Component that the sink task can use as it {@link SinkTask#put(java.util.Collection)}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMzYxOA==", "bodyText": "A class javadoc would be helpful, in order to understand why this is introduced.", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431523618", "createdAt": "2020-05-28T00:53:45Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNDgyNw==", "bodyText": "nit: it was correct before", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431534827", "createdAt": "2020-05-28T01:38:01Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -111,41 +115,41 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n         this.errorHandlingMetrics = errorHandlingMetrics;\n+        this.dlqTopicName = connConfig.dlqTopicName().trim();\n     }\n \n     /**\n-     * Write the raw records into a Kafka topic.\n+     * Write the raw records into a Kafka topic and return the producer future.\n      *\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @return the future associated with the writing of this record; never null\n      */\n-    public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (dlqTopicName.isEmpty()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n-\n         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();\n \n         ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();\n         if (originalMessage == null) {\n             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNDg0Mw==", "bodyText": "nit: it was correct before", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431534843", "createdAt": "2020-05-28T01:38:07Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -111,41 +115,41 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n         this.errorHandlingMetrics = errorHandlingMetrics;\n+        this.dlqTopicName = connConfig.dlqTopicName().trim();\n     }\n \n     /**\n-     * Write the raw records into a Kafka topic.\n+     * Write the raw records into a Kafka topic and return the producer future.\n      *\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @return the future associated with the writing of this record; never null\n      */\n-    public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (dlqTopicName.isEmpty()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n-\n         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();\n \n         ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();\n         if (originalMessage == null) {\n             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n+                originalMessage.key(), originalMessage.value(), originalMessage.headers());\n         } else {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.timestamp(),\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNTQ2Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final LinkedList<Future<Void>> futures;\n          \n          \n            \n                protected final List<Future<Void>> futures;", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431535467", "createdAt": "2020-05-28T01:40:39Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzODgzMg==", "bodyText": "Suggestion (can't add because of the deleted line):\n        List<Future<RecordMetadata>> futures = reporters.stream()\n                .map(r -> r.report(this))\n                .filter(Future::isDone)\n                .collect(Collectors.toCollection(LinkedList::new));", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431538832", "createdAt": "2020-05-28T01:53:49Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java", "diffHunk": "@@ -132,11 +138,25 @@ public void currentContext(Stage stage, Class<?> klass) {\n \n     /**\n      * Report errors. Should be called only if an error was encountered while executing the operation.\n+     *\n+     * @return a errant record future that potentially aggregates the producer futures\n      */\n-    public void report() {\n+    public Future<Void> report() {\n+        if (reporters.size() == 1) {\n+            return new ErrantRecordFuture(Collections.singletonList(reporters.iterator().next().report(this)));\n+        }\n+\n+        List<Future<RecordMetadata>> futures = new LinkedList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzOTQxNA==", "bodyText": "nit: initialization is not required", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431539414", "createdAt": "2020-05-28T01:55:58Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDI3Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        for (Future<RecordMetadata> future: futures) {\n          \n          \n            \n                            if (!future.isDone()) {\n          \n          \n            \n                                return false;\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                        return true;\n          \n          \n            \n                        return futures.stream().allMatch(Future::isDone);", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431540276", "createdAt": "2020-05-28T01:59:32Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;\n+        while ((future = futures.poll()) != null) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while awaiting an errant record future's completion.\");\n+                throw new ConnectException(e);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Wrapper class to aggregate producer futures and abstract away the record metadata from the\n+     * Connect user.\n+     */\n+    public static class ErrantRecordFuture implements Future<Void> {\n+\n+        private final List<Future<RecordMetadata>> futures;\n+\n+        public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) {\n+            futures = producerFutures;\n+        }\n+\n+        public boolean cancel(boolean mayInterruptIfRunning) {\n+            throw new UnsupportedOperationException(\"Reporting an errant record cannot be cancelled.\");\n+        }\n+\n+        public boolean isCancelled() {\n+            return false;\n+        }\n+\n+        public boolean isDone() {\n+            for (Future<RecordMetadata> future: futures) {\n+                if (!future.isDone()) {\n+                    return false;\n+                }\n+            }\n+            return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDk3MA==", "bodyText": "nit: extra blank line\n\n  \n    \n      \n        Suggested change", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431540970", "createdAt": "2020-05-28T02:02:04Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Task;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class ErrantRecordSinkConnector extends MonitorableSinkConnector {\n+\n+    @Override\n+    public Class<? extends Task> taskClass() {\n+        return ErrantRecordSinkTask.class;\n+    }\n+\n+    public static class ErrantRecordSinkTask extends MonitorableSinkTask {\n+        private ErrantRecordReporter reporter;\n+\n+        public ErrantRecordSinkTask() {\n+            super();\n+        }\n+\n+        @Override\n+        public void start(Map<String, String> props) {\n+            super.start(props);\n+            reporter = context.errantRecordReporter();\n+        }\n+\n+        @Override\n+        public void put(Collection<SinkRecord> records) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MTI5MQ==", "bodyText": "Suggested change", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431541291", "createdAt": "2020-05-28T02:03:22Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -219,6 +223,72 @@ public void testSourceConnector() throws Exception {\n         connect.deleteConnector(CONNECTOR_NAME);\n     }\n \n+    @Test\n+    public void testErrantRecordReporter() throws Exception {\n+        connect.kafka().createTopic(DLQ_TOPIC, 1);\n+        // create test topic\n+        connect.kafka().createTopic(\"test-topic\", NUM_TOPIC_PARTITIONS);\n+\n+        // setup up props for the sink connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(CONNECTOR_CLASS_CONFIG, ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME);\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n+        props.put(TOPICS_CONFIG, \"test-topic\");\n+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);\n+\n+        // validate the intended connector configuration, a config that errors\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 1,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // add missing configuration to make the config valid\n+        props.put(\"name\", CONNECTOR_NAME);\n+\n+        // validate the intended connector configuration, a valid config\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 0,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // start a sink connector\n+        connect.configureConnector(CONNECTOR_NAME, props);\n+\n+        waitForCondition(this::checkForPartitionAssignment,\n+            CONNECTOR_SETUP_DURATION_MS,\n+            \"Connector tasks were not assigned a partition each.\");\n+\n+        // produce some messages into source topic partitions\n+        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n+            connect.kafka().produce(\"test-topic\", i % NUM_TOPIC_PARTITIONS, \"key\", \"simple-message-value-\" + i);\n+        }\n+\n+        // consume all records from the source topic or fail, to ensure that they were correctly produced.\n+        assertEquals(\"Unexpected number of records consumed\", NUM_RECORDS_PRODUCED,\n+            connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic\").count());\n+\n+        // wait for the connector tasks to consume all records.\n+        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);\n+\n+        // wait for the connector tasks to commit all records.\n+        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);\n+\n+        // consume all records from the dlq topic or fail, to ensure that they were correctly produced\n+        int recordNum = connect.kafka().consume(\n+            NUM_RECORDS_PRODUCED,\n+            RECORD_TRANSFER_DURATION_MS,\n+            DLQ_TOPIC\n+        ).count();\n+\n+        // delete connector\n+        connect.deleteConnector(CONNECTOR_NAME);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c4442fffdc3b163458bf42b7af29183062c636f", "author": {"user": {"login": "aakashnshah", "name": "Aakash Shah"}}, "url": "https://github.com/apache/kafka/commit/1c4442fffdc3b163458bf42b7af29183062c636f", "committedDate": "2020-05-28T03:55:43Z", "message": "addressed some more comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NzY1Mzc1", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-419765375", "createdAt": "2020-05-28T04:15:40Z", "commit": {"oid": "1c4442fffdc3b163458bf42b7af29183062c636f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5ODE2OTg0", "url": "https://github.com/apache/kafka/pull/8720#pullrequestreview-419816984", "createdAt": "2020-05-28T06:41:42Z", "commit": {"oid": "1c4442fffdc3b163458bf42b7af29183062c636f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1159, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}