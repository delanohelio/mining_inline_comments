{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEwNDc2NjIx", "number": 9508, "title": "KAFKA-10648: Add Prefix Scan support to State Stores", "bodyText": "More detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.\nSummary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-10-27T05:18:06Z", "url": "https://github.com/apache/kafka/pull/9508", "merged": true, "mergeCommit": {"oid": "51833bf37c7ab5ce24aed82749d45f5c8550d12f"}, "closed": true, "closedAt": "2021-02-03T18:26:30Z", "author": {"login": "vamossagar12"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdY0eI2ABqjM5NTE1NTcwMjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABd2eCfUgH2gAyNTEwNDc2NjIxOjhlY2EzYzljMjg1MjE3Mjg5NjAwMTE3OGY4ZTdhMTE1ZmQzOTJhZWI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MDU3OTY5", "url": "https://github.com/apache/kafka/pull/9508#pullrequestreview-525057969", "createdAt": "2020-11-06T11:08:24Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMTowODoyNFrOHup3hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNDowMjowOFrOHuvM9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MjUwMg==", "bodyText": "Please leave an empty line above to separate parameter description from the description of the method.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518682502", "createdAt": "2020-11-06T11:08:24Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java", "diffHunk": "@@ -107,4 +108,18 @@\n      * @throws InvalidStateStoreException if the store is not initialized\n      */\n     long approximateNumEntries();\n+\n+    /**\n+     * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n+     * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n+     * format in which the keys are stored underneath in the stores\n+     * @param prefix The prefix.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4ODE0Mw==", "bodyText": "We usually put every sentence on its own line In javadocs. Unfortunately, we are not always consistent as you can see in the javadocs in this file.\nI guess this iterator has similar requirements as the other iterators, i.e., it must be closed, it must be safe from java.util.ConcurrentModificationException, and it must not return null values. If so, please add this requirements to the javadocs.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n          \n          \n            \n                 * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n          \n          \n            \n                 * format in which the keys are stored underneath in the stores\n          \n          \n            \n                 * Return an iterator over all keys with the specified prefix. \n          \n          \n            \n                 * Since the type of the prefix can be different from that of the key, a serializer to convert the\n          \n          \n            \n                 * prefix into the format in which the keys are stored in the stores needs to be passed to this method.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518688143", "createdAt": "2020-11-06T11:19:56Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java", "diffHunk": "@@ -107,4 +108,18 @@\n      * @throws InvalidStateStoreException if the store is not initialized\n      */\n     long approximateNumEntries();\n+\n+    /**\n+     * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n+     * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n+     * format in which the keys are stored underneath in the stores", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4OTAzMA==", "bodyText": "I would move this method above approximateNumEntries, so that all methods that return iterators are in one block.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518689030", "createdAt": "2020-11-06T11:21:41Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java", "diffHunk": "@@ -107,4 +108,18 @@\n      * @throws InvalidStateStoreException if the store is not initialized\n      */\n     long approximateNumEntries();\n+\n+    /**\n+     * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n+     * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n+     * format in which the keys are stored underneath in the stores\n+     * @param prefix The prefix.\n+     * @param prefixKeySerializer Serializer for the Prefix key type\n+     * @param <PS> Prefix Serializer type\n+     * @param <P> Prefix Type.\n+     * @return The iterator for keys having the specified prefix.\n+     */\n+    default <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(P prefix, PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5NzMwNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n          \n          \n            \n                    return wrapped().prefixScan(prefix, prefixKeySerializer);\n          \n          \n            \n                }\n          \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, \n          \n          \n            \n                                                                                                final PS prefixKeySerializer) {\n          \n          \n            \n                    return wrapped().prefixScan(prefix, prefixKeySerializer);\n          \n          \n            \n                }", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518697304", "createdAt": "2020-11-06T11:39:18Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStore.java", "diffHunk": "@@ -97,6 +98,11 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+        return wrapped().prefixScan(prefix, prefixKeySerializer);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5ODc3OA==", "bodyText": "Unit test for this method is missing.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518698778", "createdAt": "2020-11-06T11:42:27Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStore.java", "diffHunk": "@@ -97,6 +98,11 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcwMDYyOA==", "bodyText": "This method needs unit testing.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518700628", "createdAt": "2020-11-06T11:46:26Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,19 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcwMjEyNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                            name,\n          \n          \n            \n                            new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));\n          \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                        name,\n          \n          \n            \n                        new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true)\n          \n          \n            \n                    );", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518702127", "createdAt": "2020-11-06T11:49:38Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,19 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);\n+\n+        return new DelegatingPeekingKeyValueIterator<>(\n+                name,\n+                new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODczMjE4OA==", "bodyText": "This method needs unit testing.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518732188", "createdAt": "2020-11-06T12:53:10Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +230,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODczNzcwNA==", "bodyText": "Please use meaningful names for the paramters instead of x and y.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518737704", "createdAt": "2020-11-06T13:04:08Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {\n+    private byte[] rawPrefix;\n+\n+    RocksDBPrefixIterator(final String name,\n+                          final RocksIterator newIterator,\n+                          final Set<KeyValueIterator<Bytes, byte[]>> openIterators,\n+                          final Bytes prefix) {\n+        super(name, newIterator, openIterators, true);\n+        this.rawPrefix = prefix.get();\n+        newIterator.seek(rawPrefix);\n+    }\n+\n+    private boolean prefixEquals(final byte[] x, final byte[] y) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0MTA1Ng==", "bodyText": "This class needs unit testing.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518741056", "createdAt": "2020-11-06T13:10:52Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0NDExMw==", "bodyText": "Please put the second parameter on a new line and align the two parameters like:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n          \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, \n          \n          \n            \n                                                                                                final PS prefixKeySerializer) {\n          \n      \n    \n    \n  \n\n(On GitHub the suggestions seems a bit strange)", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518744113", "createdAt": "2020-11-06T13:16:48Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -308,6 +309,20 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0NzkwNQ==", "bodyText": "Why is the prefix scan not supported here? The timstamped stores are the default stores in Kafka Streams.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518747905", "createdAt": "2020-11-06T13:23:59Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -256,6 +256,11 @@ public void close() {\n             oldColumnFamily.close();\n             newColumnFamily.close();\n         }\n+\n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            throw new UnsupportedOperationException();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1ODE4Mg==", "bodyText": "You do not need to test the prefix scan by using the topology test driver because you actually do not need a topology. Your addition is limited to the state stores and those you should unit test. Furthermore, this test is misplaced because you did not change anything in ProcessorTopology. Please remove this test and add unit tests where I indicated the need for them.\nAlthough, you will find a lot of test named test*, we name new tests with should* followed by the expected result, e.g., shouldThrowIllegalStateException or shouldGetRecordsWithPrefixedKey.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518758182", "createdAt": "2020-11-06T13:42:26Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorTopologyTest.java", "diffHunk": "@@ -273,6 +275,43 @@ public void testDrivingStatefulTopology() {\n         assertNull(store.get(\"key4\"));\n     }\n \n+    @Test\n+    public void testPrefixScanStatefulTopology() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1OTUyOA==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518759528", "createdAt": "2020-11-06T13:44:43Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertEquals(3, numberOfKeysReturned);\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertEquals(valuesWithPrefix[0], \"f\");\n+        assertEquals(valuesWithPrefix[1], \"d\");\n+        assertEquals(valuesWithPrefix[2], \"b\");\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertEquals(\n+                \"c\",\n+                stringDeserializer.deserialize(\n+                        null,\n+                        rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\")))));\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2Mjk2OA==", "bodyText": "Please fix the indentation and the newlines also in the other tests as follows:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"k1\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"a\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"b\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"k2\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"c\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"d\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"k3\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"e\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"f\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"k1\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"a\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"b\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"k2\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"c\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"d\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"k3\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"e\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"f\"))\n          \n          \n            \n                    );", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518762968", "createdAt": "2020-11-06T13:50:29Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2NTM5MQ==", "bodyText": "We prefer to use assertThat() in new test, because it is better readable. This line would then become:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertEquals(3, numberOfKeysReturned);\n          \n          \n            \n                   assertThat(numberOfKeysReturned, is(3));\n          \n      \n    \n    \n  \n\nPlease replace also the other verifications with assertThat() and fix the indentation (should be 4 spaces, not 8).", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518765391", "createdAt": "2020-11-06T13:54:36Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertEquals(3, numberOfKeysReturned);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2NTc0MQ==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518765741", "createdAt": "2020-11-06T13:55:13Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertEquals(3, numberOfKeysReturned);\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertEquals(valuesWithPrefix[0], \"f\");\n+        assertEquals(valuesWithPrefix[1], \"d\");\n+        assertEquals(valuesWithPrefix[2], \"b\");\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertEquals(\n+                \"c\",\n+                stringDeserializer.deserialize(\n+                        null,\n+                        rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\")))));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+                new Bytes(uuidSerializer.serialize(null, uuid1)),\n+                stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+                new Bytes(uuidSerializer.serialize(null, uuid2)),\n+                stringSerializer.serialize(null, \"b\")));\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2OTkxMA==", "bodyText": "I would rather use a List instead of the array and then verify if the length of the list is three. That will give us a more meaningful assertion error than the IndexOutOfBoundsException that we would get with the array in case of failure.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518769910", "createdAt": "2020-11-06T14:02:08Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1Mjg4MjY1", "url": "https://github.com/apache/kafka/pull/9508#pullrequestreview-535288265", "createdAt": "2020-11-20T10:04:18Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMDowNDoxOVrOH3JHgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMTo0ODowN1rOH3MiZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4MzEwNw==", "bodyText": "This method still needs unit testing.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527583107", "createdAt": "2020-11-20T10:04:19Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,19 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcwMDYyOA=="}, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4MzYxNA==", "bodyText": "nit: The last parenthesis should go to a new line. I know that we are not consistent throughout the code base (that is why this comment is prefixed with \"nit\") but that is actually the code style, we agreed upon. If you need to push another commit you can fix this, otherwise it's fine.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                        name,\n          \n          \n            \n                        new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));\n          \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                        name,\n          \n          \n            \n                        new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true)\n          \n          \n            \n                    );", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527583614", "createdAt": "2020-11-20T10:05:09Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,19 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);\n+\n+        return new DelegatingPeekingKeyValueIterator<>(\n+            name,\n+            new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4NzUyMA==", "bodyText": "nit: Sorry, I overlooked this last time. We actually use 4 space and not 8.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                            wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                            rangeSensor\n          \n          \n            \n                    );\n          \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                        wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                        rangeSensor\n          \n          \n            \n                    );", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527587520", "createdAt": "2020-11-20T10:12:07Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +230,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+                wrapped().prefixScan(prefix, prefixKeySerializer),\n+                rangeSensor\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5NTQ3Ng==", "bodyText": "I am not sure whether we should use the rangeSensor here or introduce a new prefixScanSensor. I looked into the KIP discussion but could not find any reference to metrics. Either I missed it or we missed it in the KIP discussion. What do the reviewers of the KIP think @ableegoldman @vvcephei @guozhangwang?", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527595476", "createdAt": "2020-11-20T10:25:41Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +230,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+                wrapped().prefixScan(prefix, prefixKeySerializer),\n+                rangeSensor", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5OTk2Ng==", "bodyText": "The following request is not a requirement to get the PR approved, but rather optional extra work to improve the code base. Could you rewrite this test with a mock for the inner state store as in MeteredKeyValueStoreTest?", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527599966", "createdAt": "2020-11-20T10:33:19Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java", "diffHunk": "@@ -359,6 +361,31 @@ public void shouldReverseIterateOverRange() {\n         ), results);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(bytesKey(\"k1\"), bytesValue(\"1\")));\n+        entries.add(new KeyValue<>(bytesKey(\"k2\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"p2\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"p1\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"p0\"), bytesValue(\"2\")));\n+        store.putAll(entries);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"p\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(keys, is(Arrays.asList(\"p0\", \"p1\", \"p2\")));\n+        assertThat(values, is(Arrays.asList(\"2\", \"2\", \"2\")));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwMTUyMA==", "bodyText": "See my comment in CachingInMemoryKeyValueStoreTest. Also this not required.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527601520", "createdAt": "2020-11-20T10:35:27Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +200,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(hello, world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"h\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(2));\n+        assertThat(keys, is(Arrays.asList(\"hello\", \"hi\")));\n+        assertThat(values, is(Arrays.asList(\"world\", \"there\")));\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwMzA4Nw==", "bodyText": "Why not simply:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n          \n          \n            \n                    if (toInclusive)\n          \n          \n            \n                        keyRange(from, to);\n          \n          \n            \n                    return keySetIterator(cache.navigableKeySet().subSet(from, true, to, false), true);\n          \n          \n            \n                }\n          \n          \n            \n                synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n          \n          \n            \n                    return keySetIterator(cache.navigableKeySet().subSet(from, true, to, toInclusive), true);\n          \n          \n            \n                }", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527603087", "createdAt": "2020-11-20T10:38:02Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "diffHunk": "@@ -284,6 +284,12 @@ public boolean isEmpty() {\n         return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true), true);\n     }\n \n+    synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n+        if (toInclusive)\n+            keyRange(from, to);\n+        return keySetIterator(cache.navigableKeySet().subSet(from, true, to, false), true);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwODAxNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (next == null) return allDone();\n          \n          \n            \n                    else {\n          \n          \n            \n                        if (prefixEquals(this.rawPrefix, next.key.get())) return next;\n          \n          \n            \n                        else return allDone();\n          \n          \n            \n                    }\n          \n          \n            \n                    if (next == null || !prefixEquals(this.rawPrefix, next.key.get())) {\n          \n          \n            \n                        return allDone();\n          \n          \n            \n                    } else {\n          \n          \n            \n                        return next;\n          \n          \n            \n                    }", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527608014", "createdAt": "2020-11-20T10:47:07Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {\n+    private final byte[] rawPrefix;\n+\n+    RocksDBPrefixIterator(final String name,\n+                          final RocksIterator newIterator,\n+                          final Set<KeyValueIterator<Bytes, byte[]>> openIterators,\n+                          final Bytes prefix) {\n+        super(name, newIterator, openIterators, true);\n+        this.rawPrefix = prefix.get();\n+        newIterator.seek(rawPrefix);\n+    }\n+\n+    private boolean prefixEquals(final byte[] prefix1, final byte[] prefix2) {\n+        final int min = Math.min(prefix1.length, prefix2.length);\n+        final ByteBuffer prefix1Slice = ByteBuffer.wrap(prefix1, 0, min);\n+        final ByteBuffer prefix2Slice = ByteBuffer.wrap(prefix2, 0, min);\n+        return prefix1Slice.equals(prefix2Slice);\n+    }\n+\n+    @Override\n+    public KeyValue<Bytes, byte[]> makeNext() {\n+        final KeyValue<Bytes, byte[]> next = super.makeNext();\n+        if (next == null) return allDone();\n+        else {\n+            if (prefixEquals(this.rawPrefix, next.key.get())) return next;\n+            else return allDone();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYxMjk0Mg==", "bodyText": "Yes, you need explicit unit tests. If there are no unit tests for RocksDbRangeIterator then that is a hole in our test coverage. You do not need to add the tests for RocksDbRangeIterator but could you please create a ticket to document the missing unit tests? Of course, you are very welcome to assign the ticket to yourself and add the missing tests in a separate PR.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527612942", "createdAt": "2020-11-20T10:56:13Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0MTA1Ng=="}, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYxNTE4OA==", "bodyText": "With this code abcd would be a prefix of abc. Is this intended? Those cases should be tested in the missing unit tests for this class.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527615188", "createdAt": "2020-11-20T11:00:26Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {\n+    private final byte[] rawPrefix;\n+\n+    RocksDBPrefixIterator(final String name,\n+                          final RocksIterator newIterator,\n+                          final Set<KeyValueIterator<Bytes, byte[]>> openIterators,\n+                          final Bytes prefix) {\n+        super(name, newIterator, openIterators, true);\n+        this.rawPrefix = prefix.get();\n+        newIterator.seek(rawPrefix);\n+    }\n+\n+    private boolean prefixEquals(final byte[] prefix1, final byte[] prefix2) {\n+        final int min = Math.min(prefix1.length, prefix2.length);\n+        final ByteBuffer prefix1Slice = ByteBuffer.wrap(prefix1, 0, min);\n+        final ByteBuffer prefix2Slice = ByteBuffer.wrap(prefix2, 0, min);\n+        return prefix1Slice.equals(prefix2Slice);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyMzg3MA==", "bodyText": "Do we need this verification? This is covered by other unit test methods, isn't it? Could you please remove it from here?", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527623870", "createdAt": "2020-11-20T11:17:10Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,115 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertThat(numberOfKeysReturned, is(3));\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertThat(\"c\", is(stringDeserializer.deserialize(null,\n+            rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\"))))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyNjY4Mg==", "bodyText": "Could you please remove the inline comments? They do not add too much information.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527626682", "createdAt": "2020-11-20T11:22:59Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,115 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertThat(numberOfKeysReturned, is(3));\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertThat(\"c\", is(stringDeserializer.deserialize(null,\n+            rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\"))))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyNzczMA==", "bodyText": "Please remove this comment.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527627730", "createdAt": "2020-11-20T11:25:01Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,115 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertThat(numberOfKeysReturned, is(3));\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertThat(\"c\", is(stringDeserializer.deserialize(null,\n+            rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\"))))));\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid2)),\n+            stringSerializer.serialize(null, \"b\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(prefix, stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(valuesWithPrefix.get(0), is(\"a\"));\n+    }\n+\n+    @Test\n+    public void shouldReturnNoKeys() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"a\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"b\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"c\")),\n+            stringSerializer.serialize(null, \"e\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"d\", stringSerializer);\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            keysWithPrefix.next();\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are no keys prefixed with d, the count should be 0", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyOTY5Mg==", "bodyText": "Please remove comment.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527629692", "createdAt": "2020-11-20T11:28:51Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -431,4 +441,36 @@ public Bytes peekNextKey() {\n             }\n         }\n     }\n+\n+    private class RocksDBDualCFPrefixIterator extends RocksDBDualCFIterator {\n+        private final byte[] rawPrefix;\n+\n+        // In Prefix scan mode, we always move in forward direction.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzMzUyMw==", "bodyText": "This method need unit testing. I realized that also here we missed to cover all methods with unit tests. For example, there are also no unit tests for all(). Could you please add the ones for prefixScan()? Also here you are really welcome to add unit tests for the other methods in a separate PR, but it is not a requirement for the approval of this PR.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527633523", "createdAt": "2020-11-20T11:36:53Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -201,6 +201,14 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzOTE0Mg==", "bodyText": "I guess, it would be more consistent to add your verifications to iteratorsShouldNotMigrateData() where all other iterators are tested.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527639142", "createdAt": "2020-11-20T11:48:07Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java", "diffHunk": "@@ -130,6 +134,8 @@ public void shouldMigrateDataFromDefaultToTimestampColumnFamily() throws Excepti\n         // approx: 7 entries on old CF, 0 in new CF\n         assertThat(rocksDBStore.approximateNumEntries(), is(7L));\n \n+        // prefix scan should return 7 keys with prefix \"key\"\n+        assertThat(runPrefixScan(\"key\"), is(7));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "committedDate": "2020-12-01T13:03:55Z", "message": "KAFKA-10648: Add Prefix Scan support to State Stores"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzODI3MjUx", "url": "https://github.com/apache/kafka/pull/9508#pullrequestreview-553827251", "createdAt": "2020-12-16T15:53:05Z", "commit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNTo1MzowNVrOIHMQNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjozNDoxOFrOIHzkMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQxMTcwMQ==", "bodyText": "This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544411701", "createdAt": "2020-12-16T15:53:05Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        assertFalse(iterator.hasNext());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwMTQ1NQ==", "bodyText": "Please change prefixScan to prefix-scan. For metrics name we do not use camel case. See put-all or put-if-absent for examples.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544501455", "createdAt": "2020-12-16T17:50:51Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefixScan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefixScan\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNDc5MQ==", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                        wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                        prefixScanSensor\n          \n          \n            \n                    );\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544504791", "createdAt": "2020-12-16T17:55:34Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+            wrapped().prefixScan(prefix, prefixKeySerializer),\n+            prefixScanSensor\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNTc1OA==", "bodyText": "nit: Please use 4 instead of 8 spaces indentation.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544505758", "createdAt": "2020-12-16T17:56:59Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUxNzAxMg==", "bodyText": "Sorry, if I haven't noticed it before. Could you move this method up before approximateNumEntries() to have all operations in one block?", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544517012", "createdAt": "2020-12-16T18:13:55Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -627,6 +645,11 @@ public void addToBatch(final byte[] key,\n         public void close() {\n             columnFamily.close();\n         }\n+\n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            return new RocksDBPrefixIterator(name, db.newIterator(columnFamily), openIterators, prefix);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMzk5Ng==", "bodyText": "nit: I would just add parameter toInclusive to the existing method instead of creating an overload.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545033996", "createdAt": "2020-12-17T11:56:54Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "diffHunk": "@@ -284,6 +284,10 @@ public boolean isEmpty() {\n         return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true), true);\n     }\n \n+    synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n+        return keySetIterator(cache.navigableKeySet().subSet(from, true, to, toInclusive), true);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ==", "bodyText": "I actually do not completely understand why we need a specific iterator for the prefix scan. We could just as good extend RocksDBRangeIterator to consider or not consider the end result of the range. We can do that because those iterator implementations are internal and the public API does not care which iterator is used as long as it implements interface KeyValueIterator and the behavior is correct. Extending and re-using RocksDBRangeIterator would lead to less code to maintain. Note, I agree that we need the public method prefixScan(), but what the implementation uses internally is not relevant for the KIP as long as it is correct. Did I miss something that imposes the implementation for a separate iterator?", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545040031", "createdAt": "2020-12-17T12:07:11Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDQwMg==", "bodyText": "If you consider my comment in NamedCache, you could also just call range() here. Or -- if you want to be more descriptive -- having prefixScan() and range() in this class calling a private overload of range() with a flag that excludes or includes the end of the range. That would deduplicate code.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545044402", "createdAt": "2020-12-17T12:14:50Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -201,6 +201,14 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {\n+        final NamedCache cache = getCache(namespace);\n+        if (cache == null) {\n+            return new MemoryLRUCacheBytesIterator(Collections.emptyIterator(), new NamedCache(namespace, this.metrics));\n+        }\n+        return new MemoryLRUCacheBytesIterator(cache.keyRange(from, to, false), cache);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1MzI5OQ==", "bodyText": "For new sensors like this, we only need to consider built-in metrics version LATEST. Hence, you should not call throughputAndLatencySensor(), but only call the parts that are relevant for LATEST and not for FROM_0100_TO_24. You also need to adapt the corresponding unit test for that. See also KIP-444 for more details.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545053299", "createdAt": "2020-12-17T12:30:03Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,26 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String threadId,\n+                                     final String taskId,\n+                                     final String storeType,\n+                                     final String storeName,\n+                                     final StreamsMetricsImpl streamsMetrics) {\n+        return throughputAndLatencySensor(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTc5NA==", "bodyText": "In all the tests for the prefix scan you should also verify boundary conditions, e.g., if you have a prefix abcd, you should verify that abce is not matched since this is the first key that should not be matched.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545055794", "createdAt": "2020-12-17T12:34:18Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java", "diffHunk": "@@ -359,6 +361,34 @@ public void shouldReverseIterateOverRange() {\n         ), results);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5"}, "originalPosition": 21}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "committedDate": "2020-12-01T13:03:55Z", "message": "KAFKA-10648: Add Prefix Scan support to State Stores"}, "afterCommit": {"oid": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/1210e2475f23648bbcdd338fc9a7beb38541c0c4", "committedDate": "2021-01-13T12:47:23Z", "message": "KAFKA-10648: Add Prefix Scan support to State Stores"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dd4c426a2db3f7fd2a82a5bd01a730e0654aba76", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/dd4c426a2db3f7fd2a82a5bd01a730e0654aba76", "committedDate": "2021-01-13T12:56:18Z", "message": "Delete TransactionLogValue.java"}, "afterCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "committedDate": "2021-01-13T13:01:01Z", "message": "KAFKA-10648: Add Prefix Scan support to State Stores"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5MTM0NTk0", "url": "https://github.com/apache/kafka/pull/9508#pullrequestreview-569134594", "createdAt": "2021-01-15T10:21:52Z", "commit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "state": "COMMENTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxMDoyMTo1MlrOIUV5FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxMzozMTo1NlrOIUcaUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIwMTEwOQ==", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n          \n          \n            \n                        prefixKeySerializer), prefixScanSensor);\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558201109", "createdAt": "2021-01-15T10:21:52Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,13 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n+            prefixKeySerializer), prefixScanSensor);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzEyMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false);\n          \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217121", "createdAt": "2021-01-15T10:33:38Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzM1NQ==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217355", "createdAt": "2021-01-15T10:33:50Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMDE3OQ==", "bodyText": "I guess you forgot to remove this. \ud83d\ude42", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558220179", "createdAt": "2021-01-15T10:37:23Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -308,6 +309,24 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n+                                                                                    final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        validateStoreOpen();\n+        final Bytes prefixBytes = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+\n+        /*final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);*/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMTk3NA==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558221974", "createdAt": "2021-01-15T10:40:29Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid2)),\n+            stringSerializer.serialize(null, \"b\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(prefix, stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(valuesWithPrefix.get(0), is(\"a\"));\n+    }\n+\n+    @Test\n+    public void shouldReturnNoKeys() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"a\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"b\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"c\")),\n+            stringSerializer.serialize(null, \"e\")));\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMjA2Nw==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558222067", "createdAt": "2021-01-15T10:40:39Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzAwMg==", "bodyText": "Could you split up this unit test into two, one for prefix prefix and one for prefix abcd? The former tests the general case whereas the latter tests the corner case where the the end of the range should not be returned.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223002", "createdAt": "2021-01-15T10:42:29Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzcwOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            true);\n          \n          \n            \n                            true\n          \n          \n            \n                        );", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223708", "createdAt": "2021-01-15T10:43:50Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -580,7 +601,8 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                 openIterators,\n                 from,\n                 to,\n-                forward);\n+                forward,\n+                true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNTAxNA==", "bodyText": "I think here IntelliJ's formatting confused you. \ud83d\ude42\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            to,\n          \n          \n            \n                    true,\n          \n          \n            \n                  false);\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558225014", "createdAt": "2021-01-15T10:46:08Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -218,6 +219,19 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDBDualCFIterator(name, innerIterWithTimestamp, innerIterNoTimestamp, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBDualCFRangeIterator(\n+                name,\n+                db.newIterator(newColumnFamily),\n+                db.newIterator(oldColumnFamily),\n+                prefix,\n+                to,\n+        true,\n+      false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNjg4Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it =\n          \n          \n            \n                                 rocksDBStore.prefixScan(\"key1\", stringSerializer)) {\n          \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it = rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558226883", "createdAt": "2021-01-15T10:49:21Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java", "diffHunk": "@@ -337,6 +341,22 @@ private void iteratorsShouldNotMigrateData() {\n             }\n             assertFalse(it.hasNext());\n         }\n+\n+        try (final KeyValueIterator<Bytes, byte[]> it =\n+                     rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMTk0NQ==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558231945", "createdAt": "2021-01-15T10:58:55Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,34 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+\n+        final String latencyMetricName = PREFIX_SCAN + LATENCY_SUFFIX;\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(taskId, storeType, storeName);\n+\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(taskId, storeName, PREFIX_SCAN, RecordingLevel.DEBUG);\n+        addInvocationRateToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,\n+            tagMap,\n+            PREFIX_SCAN,\n+            PREFIX_SCAN_RATE_DESCRIPTION\n+        );\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMjIyMA==", "bodyText": "This description is not needed since there is no total metric.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558232220", "createdAt": "2021-01-15T10:59:24Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefix-scan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefix-scan\";\n+    private static final String PREFIX_SCAN_TOTAL_DESCRIPTION = TOTAL_DESCRIPTION + PREFIX_SCAN_DESCRIPTION;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDYzNw==", "bodyText": "I do not think you need to put an entry if you use mocks.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234637", "createdAt": "2021-01-15T11:04:08Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDg3NA==", "bodyText": "This line is not needed in this case. A method call without a return value is expected on the mock if you simply call the method on the mock in the replay phase.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234874", "createdAt": "2021-01-15T11:04:32Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNDAwMg==", "bodyText": "I think we do not need this method. We can just call range() in CachingKeyValueStore.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558304002", "createdAt": "2021-01-15T13:25:15Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -218,6 +222,10 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzIyOA==", "bodyText": "If we remove prefixScan(), you can remove this test, but we need a test for range() that excludes the end limit.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307228", "createdAt": "2021-01-15T13:30:37Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java", "diffHunk": "@@ -285,6 +285,25 @@ public void shouldPeekAndIterateOverRange() {\n         assertEquals(5, bytesIndex);\n     }\n \n+    @Test\n+    public void testPrefixScan() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc1Ng==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307756", "createdAt": "2021-01-15T13:31:32Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc5MQ==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307791", "createdAt": "2021-01-15T13:31:37Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzkyMw==", "bodyText": "Please remove empty line.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307923", "createdAt": "2021-01-15T13:31:56Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e"}, "originalPosition": 10}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "committedDate": "2021-01-20T12:27:29Z", "message": "KAFKA-10648: Add Prefix Scan support to State Stores"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "98989a889e6f1ba404527aa9749d2dfa26230b09", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/98989a889e6f1ba404527aa9749d2dfa26230b09", "committedDate": "2021-01-18T17:28:10Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}, "afterCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "committedDate": "2021-01-20T12:27:29Z", "message": "KAFKA-10648: Add Prefix Scan support to State Stores"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTc2MjAxMDE1", "url": "https://github.com/apache/kafka/pull/9508#pullrequestreview-576201015", "createdAt": "2021-01-26T10:11:06Z", "commit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yNlQxMDoxMTowN1rOIaP57A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yNlQxMDo1NTowMlrOIaRlMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng==", "bodyText": "This method needs unit testing. Try to use a mock for the cache in the test.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564394476", "createdAt": "2021-01-26T10:11:07Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java", "diffHunk": "@@ -291,6 +292,16 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         return new MergedSortedCacheKeyValueBytesStoreIterator(cacheIterator, storeIterator, true);\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNTMwOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n          \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n          \n      \n    \n    \n  \n\nIn such a way, you can reuse variable hi and there. Similar is true for my suggestions below.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564405308", "createdAt": "2021-01-26T10:27:55Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjI2OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final List<String> keys = new ArrayList<>();\n          \n          \n            \n                    final List<String> values = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> keys = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> values = new ArrayList<>();", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406268", "createdAt": "2021-01-26T10:29:29Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjUxMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        keys.add(next.key.toString());\n          \n          \n            \n                        values.add(new String(next.value));\n          \n          \n            \n                        keys.add(next.key);\n          \n          \n            \n                        values.add(Bytes.wrap(next.value));", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406512", "createdAt": "2021-01-26T10:29:53Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjg4Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertThat(keys, is(Collections.singletonList(\"hi\")));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(\"there\")));\n          \n          \n            \n                    assertThat(keys, is(Collections.singletonList(hi)));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(Bytes.wrap(there))));", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406883", "createdAt": "2021-01-26T10:30:22Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(keys, is(Collections.singletonList(\"hi\")));\n+        assertThat(values, is(Collections.singletonList(\"there\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw==", "bodyText": "@vamossagar12 I can still not find the unit test for this method.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564409937", "createdAt": "2021-01-26T10:35:35Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,20 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQxNTY4Mw==", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564415683", "createdAt": "2021-01-26T10:44:39Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQyMTkzNg==", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final Sensor sensor =\n          \n          \n            \n                            StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);\n          \n          \n            \n                    final Sensor sensor = StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564421936", "createdAt": "2021-01-26T10:55:02Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,39 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+        StreamsMetricsImpl.addAvgAndMaxToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            latencyMetricName(metricName),\n+            descriptionOfAvg,\n+            descriptionOfMax\n+        );\n+        replay(StreamsMetricsImpl.class, streamsMetrics);\n+\n+        final Sensor sensor =\n+                StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17be91a37214bf77430c65d9300a5120e4348df9"}, "originalPosition": 31}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/4a41206daa0ffbc7516059d29a7ddda109f64b5e", "committedDate": "2021-01-28T16:16:52Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33be9113c6225063a1af489c5eca62f7645250ab", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/33be9113c6225063a1af489c5eca62f7645250ab", "committedDate": "2021-01-28T16:17:12Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "committedDate": "2021-01-28T16:17:21Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "committedDate": "2021-01-28T16:17:32Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dddad17ad5102e937150bd7115c215b92807e734", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/dddad17ad5102e937150bd7115c215b92807e734", "committedDate": "2021-01-28T16:17:49Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "committedDate": "2021-01-28T16:18:01Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTc4OTc2NzQx", "url": "https://github.com/apache/kafka/pull/9508#pullrequestreview-578976741", "createdAt": "2021-01-29T06:40:26Z", "commit": {"oid": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTgxMjE5NjMx", "url": "https://github.com/apache/kafka/pull/9508#pullrequestreview-581219631", "createdAt": "2021-02-02T10:55:13Z", "commit": {"oid": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxMDo1NToxM1rOIeK-rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxMDo1NToxM1rOIeK-rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODUwODA3OQ==", "bodyText": "To get rid of the test failure, you need to change this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");\n          \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n      \n    \n    \n  \n\nSorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.", "url": "https://github.com/apache/kafka/pull/9508#discussion_r568508079", "createdAt": "2021-02-02T10:55:13Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metric(\"prefix-scan-rate\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb"}, "originalPosition": 23}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8eca3c9c2852172896001178f8e7a115fd392aeb", "author": {"user": {"login": "vamossagar12", "name": null}}, "url": "https://github.com/apache/kafka/commit/8eca3c9c2852172896001178f8e7a115fd392aeb", "committedDate": "2021-02-03T10:49:01Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2890, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}