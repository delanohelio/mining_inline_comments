{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2NzgxMjAw", "number": 9001, "reviewThreads": {"totalCount": 233, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMDoyNjowNlrOEoThuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwNjo1ODo0M1rOEreQsQ==", "hasNextPage": false, "hasPreviousPage": true}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjk4NDI1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMDoyNjowNlrOHZUJmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODozMjozMVrOHZh2XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNjU4Ng==", "bodyText": "indentation", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496306586", "createdAt": "2020-09-29T00:26:06Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1893,203 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 397}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzMTAzNw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496531037", "createdAt": "2020-09-29T08:32:31Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1893,203 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNjU4Ng=="}, "originalCommit": null, "originalPosition": 397}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzAwNjM1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMDozODozNVrOHZUWSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODozNjowM1rOHZiFgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwOTgzMw==", "bodyText": "featureCache => finalizedFeatureCache ?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496309833", "createdAt": "2020-09-29T00:38:35Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -112,8 +112,9 @@ class KafkaApis(val requestChannel: RequestChannel,\n                 brokerTopicStats: BrokerTopicStats,\n                 val clusterId: String,\n                 time: Time,\n-                val tokenManager: DelegationTokenManager)\n-  extends ApiRequestHandler with Logging {\n+                val tokenManager: DelegationTokenManager,\n+                val brokerFeatures: BrokerFeatures,\n+                val featureCache: FinalizedFeatureCache) extends ApiRequestHandler with Logging {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzNDkxNA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496534914", "createdAt": "2020-09-29T08:36:03Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -112,8 +112,9 @@ class KafkaApis(val requestChannel: RequestChannel,\n                 brokerTopicStats: BrokerTopicStats,\n                 val clusterId: String,\n                 time: Time,\n-                val tokenManager: DelegationTokenManager)\n-  extends ApiRequestHandler with Logging {\n+                val tokenManager: DelegationTokenManager,\n+                val brokerFeatures: BrokerFeatures,\n+                val featureCache: FinalizedFeatureCache) extends ApiRequestHandler with Logging {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwOTgzMw=="}, "originalCommit": null, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzAxNzY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMDo0NToyM1rOHZUc_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODozNjozN1rOHZiH7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxMTU1MA==", "bodyText": "I think the convention is that if there is a top level error, the second level will just be empty since there is not need to process them individually.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496311550", "createdAt": "2020-09-29T00:45:23Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3109,6 +3110,37 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+\n+    def sendResponseCallback(errors: Either[ApiError, Map[String, ApiError]]): Unit = {\n+      def createResponse(throttleTimeMs: Int): UpdateFeaturesResponse = {\n+        errors match {\n+          case Left(topLevelError) => {\n+            val featureUpdateNoErrors = updateFeaturesRequest\n+              .data().featureUpdates().asScala", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzNTUzNA==", "bodyText": "Done. Great point.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496535534", "createdAt": "2020-09-29T08:36:37Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3109,6 +3110,37 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+\n+    def sendResponseCallback(errors: Either[ApiError, Map[String, ApiError]]): Unit = {\n+      def createResponse(throttleTimeMs: Int): UpdateFeaturesResponse = {\n+        errors match {\n+          case Left(topLevelError) => {\n+            val featureUpdateNoErrors = updateFeaturesRequest\n+              .data().featureUpdates().asScala", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxMTU1MA=="}, "originalCommit": null, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzAyNTI0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMDo1MDowOVrOHZUhcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODozNjo0MlrOHZiIXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxMjY4OA==", "bodyText": "-1 =>  -1L?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496312688", "createdAt": "2020-09-29T00:50:09Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -43,7 +43,7 @@\n  */\n public class ApiVersionsResponse extends AbstractResponse {\n \n-    public static final int UNKNOWN_FINALIZED_FEATURES_EPOCH = -1;\n+    public static final long UNKNOWN_FINALIZED_FEATURES_EPOCH = -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzNTY0Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496535646", "createdAt": "2020-09-29T08:36:42Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -43,7 +43,7 @@\n  */\n public class ApiVersionsResponse extends AbstractResponse {\n \n-    public static final int UNKNOWN_FINALIZED_FEATURES_EPOCH = -1;\n+    public static final long UNKNOWN_FINALIZED_FEATURES_EPOCH = -1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxMjY4OA=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzA0NDM1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowMDo1NlrOHZUsZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODozOToyNlrOHZiT-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNTQ5Mw==", "bodyText": "This package is not part of the javadoc and thus is not part of the public interface.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496315493", "createdAt": "2020-09-29T01:00:56Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FeatureUpdate.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzODYxNg==", "bodyText": "Done. I have now moved it to the package: org.apache.kafka.clients.admin.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496538616", "createdAt": "2020-09-29T08:39:26Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FeatureUpdate.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNTQ5Mw=="}, "originalCommit": null, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzA1NjUxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowODoyM1rOHZUzkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODo0NDowM1rOHZinxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzMyOA==", "bodyText": "Since we are including the timeout in the UpdateFeature request, perhaps we could just use that timeout here.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496317328", "createdAt": "2020-09-29T01:08:23Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.defaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU0MzY4NQ==", "bodyText": "I agree. But note that in this method, we do not process an UpdateFeaturesRequest. This method is only called during controller election to setup feature versioning. So, I have incorporated your suggestion at the point where we process the request, look for def processFeatureUpdatesWithActiveController in this file where now I set the ZK write timeout to be min(timeoutMs, config.zkConnectionTimeoutMs).", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496543685", "createdAt": "2020-09-29T08:44:03Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.defaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzMyOA=="}, "originalCommit": null, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzA2MzQ4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToxMjoyNVrOHZU3oA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODo0NDozM1rOHZip7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxODM2OA==", "bodyText": "featureCache => finalizedFeatureCache?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496318368", "createdAt": "2020-09-29T01:12:25Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -34,7 +34,7 @@ import scala.concurrent.TimeoutException\n  *\n  * @param zkClient     the Zookeeper client\n  */\n-class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+class FinalizedFeatureChangeListener(private val featureCache: FinalizedFeatureCache, private val zkClient: KafkaZkClient) extends Logging {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU0NDIzOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496544239", "createdAt": "2020-09-29T08:44:33Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -34,7 +34,7 @@ import scala.concurrent.TimeoutException\n  *\n  * @param zkClient     the Zookeeper client\n  */\n-class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+class FinalizedFeatureChangeListener(private val featureCache: FinalizedFeatureCache, private val zkClient: KafkaZkClient) extends Logging {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxODM2OA=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzA2OTAyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToxNToyNFrOHZU61Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODo1NDowNFrOHZjCnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxOTE4OQ==", "bodyText": "Should we just verify the range [first_active_version, max]?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496319189", "createdAt": "2020-09-29T01:15:24Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "diffHunk": "@@ -40,14 +40,16 @@ public static FinalizedVersionRange fromMap(Map<String, Short> versionRangeMap)\n \n     /**\n      * Checks if the [min, max] version level range of this object does *NOT* fall within the\n-     * [min, max] version range of the provided SupportedVersionRange parameter.\n+     * [min, first_active_version, max] range of the provided SupportedVersionRange parameter.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU1MDU1Nw==", "bodyText": "We need to keep the existing validation. Here is a case where minVersionLevel < firstActiveVersion is true, but still there are no incompatibilities:\nSupportedVersionRange={minVersion=1, firstActiveVersion=4, maxVersion=7}\nFinalizedVersionRange={minVersionLevel=2, maxVersionLevel=6}\n\nFor example, the above can happen during step 1 of feature verison level deprecation. Imagine the following:\n\nA supported feature exists with SupportedVersionRange={minVersion=1, firstActiveVersion=4, maxVersion=7}\nThe above feature is finalized at {minVersionLevel=2, maxVersionLevel=6} in ZK already.\n\nThen imagine a new Kafka release is deployed that raises firstActiveVersion for the supported feature from 1 -> 4 (in order to deprecate versions: 1,2,3). In such a case, during Kafka server startup (where we check for feature incompatibilities), we would run into the comparison cited above between the new SupportedVersionRange and existing FinalizedVersionRange. But it is not considered to be a case of incompatibility.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496550557", "createdAt": "2020-09-29T08:54:04Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "diffHunk": "@@ -40,14 +40,16 @@ public static FinalizedVersionRange fromMap(Map<String, Short> versionRangeMap)\n \n     /**\n      * Checks if the [min, max] version level range of this object does *NOT* fall within the\n-     * [min, max] version range of the provided SupportedVersionRange parameter.\n+     * [min, first_active_version, max] range of the provided SupportedVersionRange parameter.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxOTE4OQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDcyNTAyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzowNjo0NVrOHZ4d1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNTo0NjowOFrOHaOF1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkwMTU4OA==", "bodyText": "Do we want to have a different name from org.apache.kafka.common.feature.FinalizedVersionRange, such as FinalizedVersionLevels? Same case for SupportedVersionRange, personally I feel the same class name makes the navigation harder.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496901588", "createdAt": "2020-09-29T17:06:45Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Represents a range of version levels supported by every broker in a cluster for some feature.\n+ */\n+public class FinalizedVersionRange {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI1NTg5NA==", "bodyText": "Yes, but can I do it in a follow-up PR? The reason is if I were to refactor it now, this PR will bloat up.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497255894", "createdAt": "2020-09-30T05:46:08Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Represents a range of version levels supported by every broker in a cluster for some feature.\n+ */\n+public class FinalizedVersionRange {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkwMTU4OA=="}, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDgxNzUwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzozMDozNFrOHZ5XXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNTo0NzowMVrOHaOG4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxNjMxNg==", "bodyText": "I think we could just make the firstActiveVersion = minVersion by default, to avoid the requirement for configuring firstActiveVersion", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496916316", "createdAt": "2020-09-29T17:30:34Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,187 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:\n+ * =======\n+ *\n+ * In the first step, a major Kafka release is made with a Broker code change (explained later\n+ * below) that establishes the intent to deprecate certain versions of one or more features\n+ * cluster-wide. When this new Kafka release is deployed to the cluster, deprecated finalized\n+ * feature versions are no longer advertised to the client, but they can still be used by existing\n+ * connections. The way it works is that the feature versioning system (via the controller) will\n+ * automatically persist the new minVersionLevel for the feature in ZK to propagate the deprecation\n+ * of certain versions. After this happens, any external client that queries the Broker to learn the\n+ * feature versions will at some point start to see the new value for the finalized minVersionLevel\n+ * for the feature. The external clients are expected to stop using the deprecated versions at least\n+ * by the time that they learn about it.\n+ *\n+ * Here is how the above code change needs to be done:\n+ * In order to deprecate feature version levels, in the supportedFeatures map you need to supply a\n+ * specific firstActiveVersion value that's higher than the minVersion for the feature. The\n+ * value for firstActiveVersion should be 1 beyond the highest version that you intend to deprecate\n+ * for that feature. Whenever the controller is elected or the features are finalized via the\n+ * ApiKeys.UPDATE_FEATURES api, the feature version levels in the closed range:\n+ * [minVersion, firstActiveVersion - 1] are automatically deprecated in ZK by the controller logic.\n+ *\n+ * Example:\n+ * - Let us assume the existing finalized feature in ZK:\n+ *   {\n+ *      \"feature\" -> FinalizedVersionRange(minVersionLevel=1, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to deprecate feature version levels: [1, 2].\n+ *   Then, in the supportedFeatures map you should supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=3, maxVersion=5)\n+ *   }\n+ * - If you do NOT want to deprecate a version level for a feature, then, in the supportedFeatures", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI1NjE2Mg==", "bodyText": "Done. I've provided an overloaded c'tor now in org.apache.kafka.common.feature.SupportedVersionRange that only takes minVersion and maxVersion as parameters.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497256162", "createdAt": "2020-09-30T05:47:01Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,187 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:\n+ * =======\n+ *\n+ * In the first step, a major Kafka release is made with a Broker code change (explained later\n+ * below) that establishes the intent to deprecate certain versions of one or more features\n+ * cluster-wide. When this new Kafka release is deployed to the cluster, deprecated finalized\n+ * feature versions are no longer advertised to the client, but they can still be used by existing\n+ * connections. The way it works is that the feature versioning system (via the controller) will\n+ * automatically persist the new minVersionLevel for the feature in ZK to propagate the deprecation\n+ * of certain versions. After this happens, any external client that queries the Broker to learn the\n+ * feature versions will at some point start to see the new value for the finalized minVersionLevel\n+ * for the feature. The external clients are expected to stop using the deprecated versions at least\n+ * by the time that they learn about it.\n+ *\n+ * Here is how the above code change needs to be done:\n+ * In order to deprecate feature version levels, in the supportedFeatures map you need to supply a\n+ * specific firstActiveVersion value that's higher than the minVersion for the feature. The\n+ * value for firstActiveVersion should be 1 beyond the highest version that you intend to deprecate\n+ * for that feature. Whenever the controller is elected or the features are finalized via the\n+ * ApiKeys.UPDATE_FEATURES api, the feature version levels in the closed range:\n+ * [minVersion, firstActiveVersion - 1] are automatically deprecated in ZK by the controller logic.\n+ *\n+ * Example:\n+ * - Let us assume the existing finalized feature in ZK:\n+ *   {\n+ *      \"feature\" -> FinalizedVersionRange(minVersionLevel=1, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to deprecate feature version levels: [1, 2].\n+ *   Then, in the supportedFeatures map you should supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=3, maxVersion=5)\n+ *   }\n+ * - If you do NOT want to deprecate a version level for a feature, then, in the supportedFeatures", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxNjMxNg=="}, "originalCommit": null, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDgyMjAzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzozMTo0MVrOHZ5aJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNTo0ODowM1rOHaOIGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxNzAyOQ==", "bodyText": "Similar here to make firstActiveVersion = minVersion as default.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496917029", "createdAt": "2020-09-29T17:31:41Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,187 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:\n+ * =======\n+ *\n+ * In the first step, a major Kafka release is made with a Broker code change (explained later\n+ * below) that establishes the intent to deprecate certain versions of one or more features\n+ * cluster-wide. When this new Kafka release is deployed to the cluster, deprecated finalized\n+ * feature versions are no longer advertised to the client, but they can still be used by existing\n+ * connections. The way it works is that the feature versioning system (via the controller) will\n+ * automatically persist the new minVersionLevel for the feature in ZK to propagate the deprecation\n+ * of certain versions. After this happens, any external client that queries the Broker to learn the\n+ * feature versions will at some point start to see the new value for the finalized minVersionLevel\n+ * for the feature. The external clients are expected to stop using the deprecated versions at least\n+ * by the time that they learn about it.\n+ *\n+ * Here is how the above code change needs to be done:\n+ * In order to deprecate feature version levels, in the supportedFeatures map you need to supply a\n+ * specific firstActiveVersion value that's higher than the minVersion for the feature. The\n+ * value for firstActiveVersion should be 1 beyond the highest version that you intend to deprecate\n+ * for that feature. Whenever the controller is elected or the features are finalized via the\n+ * ApiKeys.UPDATE_FEATURES api, the feature version levels in the closed range:\n+ * [minVersion, firstActiveVersion - 1] are automatically deprecated in ZK by the controller logic.\n+ *\n+ * Example:\n+ * - Let us assume the existing finalized feature in ZK:\n+ *   {\n+ *      \"feature\" -> FinalizedVersionRange(minVersionLevel=1, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to deprecate feature version levels: [1, 2].\n+ *   Then, in the supportedFeatures map you should supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=3, maxVersion=5)\n+ *   }\n+ * - If you do NOT want to deprecate a version level for a feature, then, in the supportedFeatures\n+ *   map you should supply the firstActiveVersion to be the same as the minVersion supplied for that\n+ *   feature.\n+ *   Example:\n+ *   supportedFeatures = {\n+ *     \"feature\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=1, maxVersion=5)\n+ *   }\n+ *   The above indicates no intent to deprecate any version levels for the feature.\n+ *\n+ * STEP 2:\n+ * =======\n+ *\n+ * After the first step is over, you may (at some point) want to permanently remove the code/logic\n+ * for the functionality offered by the deprecated feature versions. This is the second step. Here a\n+ * subsequent major Kafka release is made with another Broker code change that removes the code for\n+ * the functionality offered by the deprecated feature versions. This would completely drop support\n+ * for the deprecated versions. Such a code change needs to be supplemented by supplying a\n+ * suitable higher minVersion value for the feature in the supportedFeatures map.\n+ * Example:\n+ * - In the example above in step 1, we showed how to deprecate version levels [1, 2] for\n+ *   \"feature\". Now let us assume the following finalized feature in ZK (after the deprecation\n+ *   has been carried out):\n+ *   {\n+ *     \"feature\" -> FinalizedVersionRange(minVersionLevel=3, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to permanently remove support for feature versions: [1, 2].\n+ *   Then, in the supportedFeatures map you should now supply the following:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI1NjQ3NQ==", "bodyText": "As mentioned in above response to a different comment, I've provided an overloaded c'tor now in org.apache.kafka.common.feature.SupportedVersionRange that only takes minVersion and maxVersion as parameters.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497256475", "createdAt": "2020-09-30T05:48:03Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,187 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:\n+ * =======\n+ *\n+ * In the first step, a major Kafka release is made with a Broker code change (explained later\n+ * below) that establishes the intent to deprecate certain versions of one or more features\n+ * cluster-wide. When this new Kafka release is deployed to the cluster, deprecated finalized\n+ * feature versions are no longer advertised to the client, but they can still be used by existing\n+ * connections. The way it works is that the feature versioning system (via the controller) will\n+ * automatically persist the new minVersionLevel for the feature in ZK to propagate the deprecation\n+ * of certain versions. After this happens, any external client that queries the Broker to learn the\n+ * feature versions will at some point start to see the new value for the finalized minVersionLevel\n+ * for the feature. The external clients are expected to stop using the deprecated versions at least\n+ * by the time that they learn about it.\n+ *\n+ * Here is how the above code change needs to be done:\n+ * In order to deprecate feature version levels, in the supportedFeatures map you need to supply a\n+ * specific firstActiveVersion value that's higher than the minVersion for the feature. The\n+ * value for firstActiveVersion should be 1 beyond the highest version that you intend to deprecate\n+ * for that feature. Whenever the controller is elected or the features are finalized via the\n+ * ApiKeys.UPDATE_FEATURES api, the feature version levels in the closed range:\n+ * [minVersion, firstActiveVersion - 1] are automatically deprecated in ZK by the controller logic.\n+ *\n+ * Example:\n+ * - Let us assume the existing finalized feature in ZK:\n+ *   {\n+ *      \"feature\" -> FinalizedVersionRange(minVersionLevel=1, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to deprecate feature version levels: [1, 2].\n+ *   Then, in the supportedFeatures map you should supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=3, maxVersion=5)\n+ *   }\n+ * - If you do NOT want to deprecate a version level for a feature, then, in the supportedFeatures\n+ *   map you should supply the firstActiveVersion to be the same as the minVersion supplied for that\n+ *   feature.\n+ *   Example:\n+ *   supportedFeatures = {\n+ *     \"feature\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=1, maxVersion=5)\n+ *   }\n+ *   The above indicates no intent to deprecate any version levels for the feature.\n+ *\n+ * STEP 2:\n+ * =======\n+ *\n+ * After the first step is over, you may (at some point) want to permanently remove the code/logic\n+ * for the functionality offered by the deprecated feature versions. This is the second step. Here a\n+ * subsequent major Kafka release is made with another Broker code change that removes the code for\n+ * the functionality offered by the deprecated feature versions. This would completely drop support\n+ * for the deprecated versions. Such a code change needs to be supplemented by supplying a\n+ * suitable higher minVersion value for the feature in the supportedFeatures map.\n+ * Example:\n+ * - In the example above in step 1, we showed how to deprecate version levels [1, 2] for\n+ *   \"feature\". Now let us assume the following finalized feature in ZK (after the deprecation\n+ *   has been carried out):\n+ *   {\n+ *     \"feature\" -> FinalizedVersionRange(minVersionLevel=3, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to permanently remove support for feature versions: [1, 2].\n+ *   Then, in the supportedFeatures map you should now supply the following:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxNzAyOQ=="}, "originalCommit": null, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDgyNzQ1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzozMzowOVrOHZ5dkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNTo1MToxMlrOHaOLzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxNzkwNw==", "bodyText": "So we are saving the ZK epoch in a long, which was supposed to be an int field?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496917907", "createdAt": "2020-09-29T17:33:09Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -20,26 +20,31 @@ package kafka.server\n import kafka.utils.Logging\n import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n \n+import scala.concurrent.TimeoutException\n+import scala.math.max\n+\n // Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n }\n \n // Helper class that represents finalized features along with an epoch value.\n-case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Int) {\n+case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Long) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI1NzQyMA==", "bodyText": "We would like to avoid overflow issues once ZK is gone in the future. This change is being done based on Colin's suggestion in the KIP-584 voting thread:\n\nHere is Colin's comment\nHere is my response", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497257420", "createdAt": "2020-09-30T05:51:12Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -20,26 +20,31 @@ package kafka.server\n import kafka.utils.Logging\n import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n \n+import scala.concurrent.TimeoutException\n+import scala.math.max\n+\n // Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n }\n \n // Helper class that represents finalized features along with an epoch value.\n-case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Int) {\n+case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Long) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxNzkwNw=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjM2OTUxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1MzoyOFrOHau6-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwODoyOTo0M1rOHa_6Xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5Mzc4NA==", "bodyText": "Hmm, why do we need to take the min? If the ZK data is propagated quickly, waitUntilEpochOrThrow() will just return early.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497793784", "createdAt": "2020-09-30T20:53:28Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1910,204 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                    existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain Errors.NONE.\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone match {\n+            case Some(newVersionRange) => targetFeatures += (update.feature() -> newVersionRange)\n+            case None => targetFeatures -= update.feature()\n+          }\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, request.data().timeoutMs().min(config.zkConnectionTimeoutMs))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 527}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODA3MjE1OA==", "bodyText": "Done. Good point, removed the min now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498072158", "createdAt": "2020-10-01T08:29:43Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1910,204 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                    existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain Errors.NONE.\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone match {\n+            case Some(newVersionRange) => targetFeatures += (update.feature() -> newVersionRange)\n+            case None => targetFeatures -= update.feature()\n+          }\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, request.data().timeoutMs().min(config.zkConnectionTimeoutMs))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5Mzc4NA=="}, "originalCommit": null, "originalPosition": 527}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjQ5ODUwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMTozNDowNlrOHawJJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwOTowMzowOVrOHbBJyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgxMzc5OA==", "bodyText": "I was looking at existing classes fro the return value. For example, CreateAclsResult deliberately makes the constructor non-public.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497813798", "createdAt": "2020-09-30T21:34:06Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Map<String, FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Long> finalizedFeaturesEpoch;\n+\n+    private final Map<String, SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Map<String, FinalizedVersionRange> finalizedFeatures,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODA5MjQ4OQ==", "bodyText": "Done. Good catch. Also I've modified org.apache.kafka.clients.admin.{Supported|Finalized}VersionRange classes to make constructors non-public.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498092489", "createdAt": "2020-10-01T09:03:09Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Map<String, FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Long> finalizedFeaturesEpoch;\n+\n+    private final Map<String, SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Map<String, FinalizedVersionRange> finalizedFeatures,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgxMzc5OA=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjcyNzU1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzowNTowNFrOHayQsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwOTowNDo0OFrOHbBNWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0ODQ5OA==", "bodyText": "It's useful to return an error message too.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497848498", "createdAt": "2020-09-30T23:05:04Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1910,204 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                    existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain Errors.NONE.\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone match {\n+            case Some(newVersionRange) => targetFeatures += (update.feature() -> newVersionRange)\n+            case None => targetFeatures -= update.feature()\n+          }\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, request.data().timeoutMs().min(config.zkConnectionTimeoutMs))\n+      }\n+    } catch {\n+      // For all features that correspond to valid FeatureUpdate (i.e. error is Errors.NONE),\n+      // we set the error as Errors.FEATURE_UPDATE_FAILED since the FeatureZNode update has failed\n+      // for these. For the rest, the existing error is left untouched.\n+      case e: Exception =>\n+        warn(s\"Processing of feature updates: $request failed due to error: $e\")\n+        errors.foreach { case (feature, apiError) =>\n+          if (apiError.error() == Errors.NONE) {\n+            errors(feature) = new ApiError(Errors.FEATURE_UPDATE_FAILED)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 537}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODA5MzQwMA==", "bodyText": "Would the default error message suffice?: Unable to update finalized features due to an unexpected server error.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498093400", "createdAt": "2020-10-01T09:04:48Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1910,204 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                    existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain Errors.NONE.\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone match {\n+            case Some(newVersionRange) => targetFeatures += (update.feature() -> newVersionRange)\n+            case None => targetFeatures -= update.feature()\n+          }\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, request.data().timeoutMs().min(config.zkConnectionTimeoutMs))\n+      }\n+    } catch {\n+      // For all features that correspond to valid FeatureUpdate (i.e. error is Errors.NONE),\n+      // we set the error as Errors.FEATURE_UPDATE_FAILED since the FeatureZNode update has failed\n+      // for these. For the rest, the existing error is left untouched.\n+      case e: Exception =>\n+        warn(s\"Processing of feature updates: $request failed due to error: $e\")\n+        errors.foreach { case (feature, apiError) =>\n+          if (apiError.error() == Errors.NONE) {\n+            errors(feature) = new ApiError(Errors.FEATURE_UPDATE_FAILED)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0ODQ5OA=="}, "originalCommit": null, "originalPosition": 537}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjc0MDI5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzoxMTowMlrOHayYFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwOTowNTo1M1rOHbBP2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1MDM5MQ==", "bodyText": "Could we use Collections.emptyMap()?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497850391", "createdAt": "2020-09-30T23:11:02Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3109,6 +3109,36 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+\n+    def sendResponseCallback(errors: Either[ApiError, Map[String, ApiError]]): Unit = {\n+      def createResponse(throttleTimeMs: Int): UpdateFeaturesResponse = {\n+        errors match {\n+          case Left(topLevelError) =>\n+            UpdateFeaturesResponse.createWithErrors(\n+              topLevelError,\n+              new util.HashMap[String, ApiError](),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODA5NDA0Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498094043", "createdAt": "2020-10-01T09:05:53Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3109,6 +3109,36 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+\n+    def sendResponseCallback(errors: Either[ApiError, Map[String, ApiError]]): Unit = {\n+      def createResponse(throttleTimeMs: Int): UpdateFeaturesResponse = {\n+        errors match {\n+          case Left(topLevelError) =>\n+            UpdateFeaturesResponse.createWithErrors(\n+              topLevelError,\n+              new util.HashMap[String, ApiError](),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1MDM5MQ=="}, "originalCommit": null, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjc4MzA5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzozMjoxMVrOHayxWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwOTozODowMlrOHbCbHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1Njg1OA==", "bodyText": "This can throw an exception due to feature mismatch. Currently, this forces the controller to move but keeps the broker alive. Should we force the broker to exit in this case?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497856858", "createdAt": "2020-09-30T23:32:11Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -219,6 +226,8 @@ class KafkaController(val config: KafkaConfig,\n    * This ensures another controller election will be triggered and there will always be an actively serving controller\n    */\n   private def onControllerFailover(): Unit = {\n+    maybeSetupFeatureVersioning()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODExMzMwOA==", "bodyText": "Done. Good point. It looks appropriate to me that we exit the broker in this case. I've captured the exception and added a call to Exit.exit(1), is there a better way to do it?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498113308", "createdAt": "2020-10-01T09:38:02Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -219,6 +226,8 @@ class KafkaController(val config: KafkaConfig,\n    * This ensures another controller election will be triggered and there will always be an actively serving controller\n    */\n   private def onControllerFailover(): Unit = {\n+    maybeSetupFeatureVersioning()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1Njg1OA=="}, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDMzMDgzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzo1NTozMVrOHbVMFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMToxNTozMlrOHb-yOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyMDc1OA==", "bodyText": "Thinking about this a bit more. It seems that the intention of firstActiveVersion is to avoid deploying a wrong version of the broker that causes the deprecation of a finalized feature version unexpectedly. However, the same mistake can happen with firstActiveVersion since the deprecation of a finalized feature version is based on firstActiveVersion. So, I am not sure if firstActiveVersion addresses a real problem.\nIn general, we tend to deprecate a version very slowly in AK. So, if the mistake is to deploy a new release that actually deprecates a supported version. Old clients are likely all gone. So, moving finalized min version to supported min version may not cause a big problem. We can just document that people should make sure old versions are no longer used before deploying new releases.\nIf the mistake is to deploy an old version of the broker whose maxSupportedVersion is < maxFinalizedVersion, we will fail the broker. So, this mistake can be prevented.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498420758", "createdAt": "2020-10-01T17:55:31Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "diffHunk": "@@ -17,9 +17,16 @@\n package org.apache.kafka.common.feature;\n \n import java.util.Map;\n+import java.util.Objects;\n+import org.apache.kafka.common.utils.Utils;\n \n /**\n- * An extended {@link BaseVersionRange} representing the min/max versions for supported features.\n+ * An extended {@link BaseVersionRange} representing the min, max and first active versions for a\n+ * supported feature:\n+ *  - minVersion: This is the minimum supported version for the feature.\n+ *  - maxVersion: This the maximum supported version for the feature.\n+ *  - firstActiveVersion: This is the first active version for the feature. Versions in the range", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ5NTQ2NA==", "bodyText": "@junrao :\nI'd like to discuss an example that cites a problem I'm concerned about.\n\nIn general, we tend to deprecate a version very slowly in AK. So, if the mistake is to deploy a new release that actually deprecates a supported version. Old clients are likely all gone. So, moving finalized min version to supported min version may not cause a big problem. We can just document that people should make sure old versions are no longer used before deploying new releases.\n\nLet's say we have some feature F whose:\n\nSupported version range is: [minVersion=1, maxVersion=6]\nExisting finalized version range in the cluster is: [minVersionLevel=1, maxVersionLevel=6]\n\nNow, let us say a point in time arrives when we need to deprecate the feature version 1.\nLet us say we bump up supported minVersion to 2 in a subsequent major Kafka release.\nBefore this new release is deployed, let us assume the cluster operator knows 100% that old clients that were using the feature at version 1 are gone, so this is not a problem.\nPROBLEM: Still, if we deploy this new release, the broker will consider the following as a feature version incompatibility.\n\nSupported version range is: [minVersion=2, maxVersion=6]\nExisting finalized version range in the cluster is: [minVersionLevel=1, maxVersionLevel=6]\n\nUpon startup of a broker thats using the new release binary, the above combination will crash the broker since supported minVersion=2 is greater than minVersionLevel=1. Basically the versioning system thinks that there is now a broker that does not support minVersionLevel=1, which does not adhere to the rules of the system. We currently do feature version incompatibility checks during KafkaServer startup sequence, here is the code.\nHere is my thought: This is where firstActiveVersion becomes useful. By bumping it up during a release (instead of the supported feature's minVersion), we are able to get past this situation. When firstActiveVersionis advanced in the code, and the cluster is deployed, the controller (and all brokers) know that the advancement acts a request to the controller to act upon the feature deprecation (by writing the advanced value to the FeatureZNode). So, in this case we would release the broker with the supported feature version range: [minVersion=1, firstActiveVersion=2, maxVersion=6], and the broker release wouldn't fail (because the intent is clearly expressed to the versioning system).\nWhat are your thoughts on the above?\nIs there a different way to solve it better that I'm missing, without compromising the versioning checks enforced by the system?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498495464", "createdAt": "2020-10-01T20:27:42Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "diffHunk": "@@ -17,9 +17,16 @@\n package org.apache.kafka.common.feature;\n \n import java.util.Map;\n+import java.util.Objects;\n+import org.apache.kafka.common.utils.Utils;\n \n /**\n- * An extended {@link BaseVersionRange} representing the min/max versions for supported features.\n+ * An extended {@link BaseVersionRange} representing the min, max and first active versions for a\n+ * supported feature:\n+ *  - minVersion: This is the minimum supported version for the feature.\n+ *  - maxVersion: This the maximum supported version for the feature.\n+ *  - firstActiveVersion: This is the first active version for the feature. Versions in the range", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyMDc1OA=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUxMjU3OQ==", "bodyText": "@kowshik : I was thinking what if we relax the current check by just making sure that maxVersion of finalized is within the supported range. Basically in your example, if supported minVersion goes to 2, it's still allowed since it's less than maxVersion of finalized. However, if supported minVersion goes to 7, this fails the broker since it's more than maxVersion of finalized.\nYour concern for the relaxed check seems to be around deploying a wrong version of the broker by mistake. I am not sure if that's a big concern. If the wrong broker affects maxVersion of finalized, the broker won't start. If the wrong broker affects minVersion of finalized, if we deprecated slowly, it won't impact the existing clients.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498512579", "createdAt": "2020-10-01T21:04:51Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "diffHunk": "@@ -17,9 +17,16 @@\n package org.apache.kafka.common.feature;\n \n import java.util.Map;\n+import java.util.Objects;\n+import org.apache.kafka.common.utils.Utils;\n \n /**\n- * An extended {@link BaseVersionRange} representing the min/max versions for supported features.\n+ * An extended {@link BaseVersionRange} representing the min, max and first active versions for a\n+ * supported feature:\n+ *  - minVersion: This is the minimum supported version for the feature.\n+ *  - maxVersion: This the maximum supported version for the feature.\n+ *  - firstActiveVersion: This is the first active version for the feature. Versions in the range", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyMDc1OA=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODU3NDkxMQ==", "bodyText": "@junrao Does the below feel right to you?\nThe key thing seems to be that you feel it is rare to deprecate feature versions in AK. I agree with the same. So, I propose we just do not have to solve the deprecation problem in this PR, until we find a clear route that the AK community agrees with. In this PR I propose to revert the firstActiveVersion change, leaving the rest of the things the way they are. In the future, we can develop a concrete solution for version deprecation i.e. the part on how to advance minVersion of supported feature, may be (or may not be) using firstActiveVersion or other ways (it is up for discussion, maybe in a separate KIP). I have made this proposed change in the most recent commit: 4218f95904989028a469930d0c266362bf173ece.\nRegarding your thought:\n\nI was thinking what if we relax the current check by just making sure that maxVersion of finalized is within the supported range. Basically in your example, if supported minVersion goes to 2, it's still allowed since it's less than maxVersion of finalized. However, if supported minVersion goes to 7, this fails the broker since it's more than maxVersion of finalized.\n\nThere is a consequence to relaxing the current check:\nThe controller can not effectively finalize minVersionLevel for the feature, because, with a relaxed check we do not know whether all brokers in the cluster support a particular minVersion when the controller finalizes the minVersionLevel at a particular value. It seems useful to keep the concept of minVersionLevel like the way it is now (i.e. it is the lowest version guaranteed to be supported by any broker in the cluster for a feature). And as I said above, in the future, we can decide on ways to mutate it safely (maybe through firstActiveVersion or other means).", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498574911", "createdAt": "2020-10-02T00:38:05Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "diffHunk": "@@ -17,9 +17,16 @@\n package org.apache.kafka.common.feature;\n \n import java.util.Map;\n+import java.util.Objects;\n+import org.apache.kafka.common.utils.Utils;\n \n /**\n- * An extended {@link BaseVersionRange} representing the min/max versions for supported features.\n+ * An extended {@link BaseVersionRange} representing the min, max and first active versions for a\n+ * supported feature:\n+ *  - minVersion: This is the minimum supported version for the feature.\n+ *  - maxVersion: This the maximum supported version for the feature.\n+ *  - firstActiveVersion: This is the first active version for the feature. Versions in the range", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyMDc1OA=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzNDM3Mg==", "bodyText": "\"we do not know whether all brokers in the cluster support a particular minVersion when the controller finalizes the minVersionLevel at a particular value.\" The controller knows the minSupportedVersion for all brokers, right? What if we do the following? When finalizing a feature, the controllers uses the highest minSupportedVersion across all brokers as finalizedMinVersion, as long as it's <= finalizedMaxVersion. On broker restart, we also advance finalizedMinVersion if the new broker's minSupportedVersion has advanced (assuming still <= finalizedMaxVersion).", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499034372", "createdAt": "2020-10-02T20:25:16Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "diffHunk": "@@ -17,9 +17,16 @@\n package org.apache.kafka.common.feature;\n \n import java.util.Map;\n+import java.util.Objects;\n+import org.apache.kafka.common.utils.Utils;\n \n /**\n- * An extended {@link BaseVersionRange} representing the min/max versions for supported features.\n+ * An extended {@link BaseVersionRange} representing the min, max and first active versions for a\n+ * supported feature:\n+ *  - minVersion: This is the minimum supported version for the feature.\n+ *  - maxVersion: This the maximum supported version for the feature.\n+ *  - firstActiveVersion: This is the first active version for the feature. Versions in the range", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyMDc1OA=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEwMjI2Ng==", "bodyText": "@junrao Awesome. This is a very good point. The approach you proposed is very elegant, and we should shoot for it, when we\u2019re giving the benefit of the doubt on deprecation to the broker binary version. I\u2019ll update the KIP with details and share with community for feedback. As soon as that is done, I'll follow up in separate PR implementing this logic.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499102266", "createdAt": "2020-10-03T01:15:32Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "diffHunk": "@@ -17,9 +17,16 @@\n package org.apache.kafka.common.feature;\n \n import java.util.Map;\n+import java.util.Objects;\n+import org.apache.kafka.common.utils.Utils;\n \n /**\n- * An extended {@link BaseVersionRange} representing the min/max versions for supported features.\n+ * An extended {@link BaseVersionRange} representing the min, max and first active versions for a\n+ * supported feature:\n+ *  - minVersion: This is the minimum supported version for the feature.\n+ *  - maxVersion: This the maximum supported version for the feature.\n+ *  - firstActiveVersion: This is the first active version for the feature. Versions in the range", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyMDc1OA=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzY1ODMzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNzozOToxNVrOHb2Ekg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMToxNDoxN1rOHb-xwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk1OTUwNg==", "bodyText": "This this case, existingFeatureZNode.features is expected to be empty? Could we log a warn if this is not the case and always set finalized to empty?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498959506", "createdAt": "2020-10-02T17:39:15Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,147 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        val newVersion = updateFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEwMjE0Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499102146", "createdAt": "2020-10-03T01:14:17Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,147 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        val newVersion = updateFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk1OTUwNg=="}, "originalCommit": null, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzY2NTMwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNzo0MTo0MFrOHb2I-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMToxNDoyNFrOHb-x0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2MDYzMw==", "bodyText": "Should we call updateFeatureZNode() so that we can get the logging?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498960633", "createdAt": "2020-10-02T17:41:40Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1840,204 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.min, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" supported minVersion:${supportedVersionRange.min}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                    existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain Errors.NONE.\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone match {\n+            case Some(newVersionRange) => targetFeatures += (update.feature() -> newVersionRange)\n+            case None => targetFeatures -= update.feature()\n+          }\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 465}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEwMjE2Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499102163", "createdAt": "2020-10-03T01:14:24Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1840,204 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.min, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" supported minVersion:${supportedVersionRange.min}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                    existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain Errors.NONE.\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone match {\n+            case Some(newVersionRange) => targetFeatures += (update.feature() -> newVersionRange)\n+            case None => targetFeatures -= update.feature()\n+          }\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2MDYzMw=="}, "originalCommit": null, "originalPosition": 465}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzc4MTAxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODoxOTozMVrOHb3UuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMToxNDozNlrOHb-x4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk4MDAyNQ==", "bodyText": "This test may not be enough. The issue is that when a controller fails over, it's possible that new brokers have joined the cluster during the failover. So, if existingFeatureZNode is enabled, it may not be reflecting the state in those newly joined brokers. So, it seems that we need to do the validation for every broker during controller failover in that case.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r498980025", "createdAt": "2020-10-02T18:19:31Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,147 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEwMjE3Ng==", "bodyText": "Done. Excellent catch.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499102176", "createdAt": "2020-10-03T01:14:36Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,147 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk4MDAyNQ=="}, "originalCommit": null, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDEzMDI1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesResult.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQyMDoyOTo1NFrOHb6v6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMToxNDowMVrOHb-xpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzNjEzOA==", "bodyText": "Could we make the constructor non-public?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499036138", "createdAt": "2020-10-02T20:29:54Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesResult.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.KafkaFuture;\n+\n+/**\n+ * The result of the {@link Admin#describeFeatures(DescribeFeaturesOptions)} call.\n+ *\n+ * The API of this class is evolving, see {@link Admin} for details.\n+ */\n+public class DescribeFeaturesResult {\n+\n+    private final KafkaFuture<FeatureMetadata> future;\n+\n+    public DescribeFeaturesResult(KafkaFuture<FeatureMetadata> future) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEwMjExOA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499102118", "createdAt": "2020-10-03T01:14:01Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesResult.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.KafkaFuture;\n+\n+/**\n+ * The result of the {@link Admin#describeFeatures(DescribeFeaturesOptions)} call.\n+ *\n+ * The API of this class is evolving, see {@link Admin} for details.\n+ */\n+public class DescribeFeaturesResult {\n+\n+    private final KafkaFuture<FeatureMetadata> future;\n+\n+    public DescribeFeaturesResult(KafkaFuture<FeatureMetadata> future) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzNjEzOA=="}, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDEzMzQ3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQyMDozMTowM1rOHb6xvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMToxNDowNVrOHb-xsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzNjYwNQ==", "bodyText": "Could we make the constructor non-public?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499036605", "createdAt": "2020-10-02T20:31:03Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Map;\n+import org.apache.kafka.common.KafkaFuture;\n+\n+/**\n+ * The result of the {@link Admin#updateFeatures(Map, UpdateFeaturesOptions)} call.\n+ *\n+ * The API of this class is evolving, see {@link Admin} for details.\n+ */\n+public class UpdateFeaturesResult {\n+    private final Map<String, KafkaFuture<Void>> futures;\n+\n+    /**\n+     * @param futures   a map from feature name to future, which can be used to check the status of\n+     *                  individual feature updates.\n+     */\n+    public UpdateFeaturesResult(final Map<String, KafkaFuture<Void>> futures) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEwMjEzMA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499102130", "createdAt": "2020-10-03T01:14:05Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Map;\n+import org.apache.kafka.common.KafkaFuture;\n+\n+/**\n+ * The result of the {@link Admin#updateFeatures(Map, UpdateFeaturesOptions)} call.\n+ *\n+ * The API of this class is evolving, see {@link Admin} for details.\n+ */\n+public class UpdateFeaturesResult {\n+    private final Map<String, KafkaFuture<Void>> futures;\n+\n+    /**\n+     * @param futures   a map from feature name to future, which can be used to check the status of\n+     *                  individual feature updates.\n+     */\n+    public UpdateFeaturesResult(final Map<String, KafkaFuture<Void>> futures) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzNjYwNQ=="}, "originalCommit": null, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyODk4MjMxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNjo1NDoyNVrOHcls8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTowNToyNFrOHcqAng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTczOTg4OQ==", "bodyText": "It's a bit weird that FeatureZNode.status is defined as FeatureZNodeStatus.Value. It seems that it should be defined as just FeatureZNodeStatus?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499739889", "createdAt": "2020-10-05T16:54:25Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,161 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      val newFeatures = existingFeatureZNode.status match {\n+        case FeatureZNodeStatus.Enabled => existingFeatureZNode.features\n+        case FeatureZNodeStatus.Disabled =>\n+          if (!existingFeatureZNode.features.empty()) {\n+            warn(s\"FeatureZNode at path: ${FeatureZNode.path} with disabled status\" +\n+              \" contains non-empty features.\")\n+          }\n+          Features.emptyFinalizedFeatures\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMDQ2Mg==", "bodyText": "Done. I have improved it now introducing a type definition called FeatureZNodeStatus that points to Value.\nIIUC you were referring to this LOC, correct?  \n  \n    \n      kafka/core/src/main/scala/kafka/zk/ZkData.scala\n    \n    \n         Line 851\n      in\n      4f96c5b\n    \n    \n    \n    \n\n        \n          \n           case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) { \n        \n    \n  \n\n\nHere the enum: FeatureZNodeStatus is defined and used in the same file.  I thought I'd add an import to fix it like the below, but it was a little unusual to add an import statement right above the class definition:\nimport FeatureZNodeStatus._\ncase class FeatureZNode(status: FeatureZNodeStatus, features: Features[FinalizedVersionRange]) {\n}\n\nWith my recent change, in the future it should be possible to import FeatureZNodeStatus._ within other files when referring to the enum value.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499810462", "createdAt": "2020-10-05T19:05:24Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,161 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      val newFeatures = existingFeatureZNode.status match {\n+        case FeatureZNodeStatus.Enabled => existingFeatureZNode.features\n+        case FeatureZNodeStatus.Disabled =>\n+          if (!existingFeatureZNode.features.empty()) {\n+            warn(s\"FeatureZNode at path: ${FeatureZNode.path} with disabled status\" +\n+              \" contains non-empty features.\")\n+          }\n+          Features.emptyFinalizedFeatures\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTczOTg4OQ=="}, "originalCommit": null, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyODk4NjA3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNjo1NToyOFrOHclvSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTowNjo0NVrOHcqDwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc0MDQ4OQ==", "bodyText": "Should we log the non-empty features too?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499740489", "createdAt": "2020-10-05T16:55:28Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,161 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      val newFeatures = existingFeatureZNode.status match {\n+        case FeatureZNodeStatus.Enabled => existingFeatureZNode.features\n+        case FeatureZNodeStatus.Disabled =>\n+          if (!existingFeatureZNode.features.empty()) {\n+            warn(s\"FeatureZNode at path: ${FeatureZNode.path} with disabled status\" +\n+              \" contains non-empty features.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMTI2NQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499811265", "createdAt": "2020-10-05T19:06:45Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,161 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises support for.\n+   * Each broker advertises the version ranges of its own supported features in its own\n+   * BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has now been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). But the IBP config is still set to lower than KAFKA_2_7_IV0, and may be\n+   *    set to a higher value later. In this case, we want to start with no finalized features and\n+   *    allow the user to finalize them whenever they are ready i.e. in the future whenever the\n+   *    user sets IBP config to be greater than or equal to KAFKA_2_7_IV0, then the user could start\n+   *    finalizing the features. This process ensures we do not enable all the possible features\n+   *    immediately after an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent.\n+   *        - If the node is absent, it will react by creating a FeatureZNode with disabled status\n+   *          and empty finalized features.\n+   *        - Otherwise, if a node already exists in enabled status then the controller will just\n+   *          flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled.\n+   *         - If the node is in disabled status, the controller won\u2019t upgrade all features immediately.\n+   *           Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *           user finalize the features later.\n+   *         - Otherwise, if a node already exists in enabled status then the controller will leave\n+   *           the node umodified.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine there was an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker\n+   *    binary has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and\n+   *    higher). The controller will start up and find that a FeatureZNode is already present with\n+   *    enabled status and existing finalized features. In such a case, the controller leaves the node\n+   *    unmodified.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled,\n+                                          brokerFeatures.defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      val newFeatures = existingFeatureZNode.status match {\n+        case FeatureZNodeStatus.Enabled => existingFeatureZNode.features\n+        case FeatureZNodeStatus.Disabled =>\n+          if (!existingFeatureZNode.features.empty()) {\n+            warn(s\"FeatureZNode at path: ${FeatureZNode.path} with disabled status\" +\n+              \" contains non-empty features.\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc0MDQ4OQ=="}, "originalCommit": null, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTA2NzIzOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoxODo1MlrOHcmhzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTowNzo0MlrOHcqFqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1MzQyMA==", "bodyText": "Should we revert the changes here?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499753420", "createdAt": "2020-10-05T17:18:52Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala", "diffHunk": "@@ -185,7 +185,7 @@ class BrokerEndPointTest {\n       \"endpoints\":[\"CLIENT://host1:9092\", \"REPLICATION://host1:9093\"],\n       \"listener_security_protocol_map\":{\"CLIENT\":\"SSL\", \"REPLICATION\":\"PLAINTEXT\"},\n       \"rack\":\"dc1\",\n-      \"features\": {\"feature1\": {\"min_version\": 1, \"max_version\": 2}, \"feature2\": {\"min_version\": 2, \"max_version\": 4}}\n+      \"features\": {\"feature1\": {\"min_version\": 1, \"first_active_version\": 1, \"max_version\": 2}, \"feature2\": {\"min_version\": 2, \"first_active_version\": 2, \"max_version\": 4}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMTc1Mg==", "bodyText": "Done. Nice catch!", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499811752", "createdAt": "2020-10-05T19:07:42Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala", "diffHunk": "@@ -185,7 +185,7 @@ class BrokerEndPointTest {\n       \"endpoints\":[\"CLIENT://host1:9092\", \"REPLICATION://host1:9093\"],\n       \"listener_security_protocol_map\":{\"CLIENT\":\"SSL\", \"REPLICATION\":\"PLAINTEXT\"},\n       \"rack\":\"dc1\",\n-      \"features\": {\"feature1\": {\"min_version\": 1, \"max_version\": 2}, \"feature2\": {\"min_version\": 2, \"max_version\": 4}}\n+      \"features\": {\"feature1\": {\"min_version\": 1, \"first_active_version\": 1, \"max_version\": 2}, \"feature2\": {\"min_version\": 2, \"first_active_version\": 2, \"max_version\": 4}}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1MzQyMA=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTExOTg1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzozNDoyMFrOHcnDKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDoxNjozNVrOHcsPbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc2MTk2Mw==", "bodyText": "This is probably not enough since it only waits for the controller path to be created in ZK, which happens before the processing of the finalized features.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499761963", "createdAt": "2020-10-05T17:34:20Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "diffHunk": "@@ -715,7 +747,58 @@ class ControllerIntegrationTest extends ZooKeeperTestHarness {\n     doAnswer((_: InvocationOnMock) => {\n       latch.countDown()\n     }).doCallRealMethod().when(spyThread).awaitShutdown()\n-    controller.shutdown() \n+    controller.shutdown()\n+  }\n+\n+  private def testControllerFeatureZNodeSetup(initialZNode: Option[FeatureZNode],\n+                                              interBrokerProtocolVersion: ApiVersion): Unit = {\n+    val versionBeforeOpt = initialZNode match {\n+      case Some(node) =>\n+        zkClient.createFeatureZNode(node)\n+        Some(zkClient.getDataAndVersion(FeatureZNode.path)._2)\n+      case None =>\n+        Option.empty\n+    }\n+    servers = makeServers(1, interBrokerProtocolVersion = Some(interBrokerProtocolVersion))\n+    TestUtils.waitUntilControllerElected(zkClient)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg0NzAyMQ==", "bodyText": "Done. Please take a look at the fix. I've added logic to wait for processing on a dummy event just after waiting for controller election. I'm hoping this will make sure the controller failover logic is completed before the test proceeds further to make assertions.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499847021", "createdAt": "2020-10-05T20:16:35Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "diffHunk": "@@ -715,7 +747,58 @@ class ControllerIntegrationTest extends ZooKeeperTestHarness {\n     doAnswer((_: InvocationOnMock) => {\n       latch.countDown()\n     }).doCallRealMethod().when(spyThread).awaitShutdown()\n-    controller.shutdown() \n+    controller.shutdown()\n+  }\n+\n+  private def testControllerFeatureZNodeSetup(initialZNode: Option[FeatureZNode],\n+                                              interBrokerProtocolVersion: ApiVersion): Unit = {\n+    val versionBeforeOpt = initialZNode match {\n+      case Some(node) =>\n+        zkClient.createFeatureZNode(node)\n+        Some(zkClient.getDataAndVersion(FeatureZNode.path)._2)\n+      case None =>\n+        Option.empty\n+    }\n+    servers = makeServers(1, interBrokerProtocolVersion = Some(interBrokerProtocolVersion))\n+    TestUtils.waitUntilControllerElected(zkClient)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc2MTk2Mw=="}, "originalCommit": null, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTIwMzkzOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzo1OToyMlrOHcn3hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTowODoyMVrOHcqG7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc3NTM2Nw==", "bodyText": "Could we add feature to the javadoc above?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499775367", "createdAt": "2020-10-05T17:59:22Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,580 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.{Optional, Properties}\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.intercept\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def finalizedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new FinalizedVersionRange(versionRange.minVersionLevel(), versionRange.maxVersionLevel()))\n+    }.asJava)\n+  }\n+\n+  private def supportedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.SupportedVersionRange]): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new SupportedVersionRange(versionRange.minVersion(), versionRange.maxVersion()))\n+    }.asJava)\n+  }\n+\n+  private def checkFeatures(client: Admin,\n+                            expectedNode: FeatureZNode,\n+                            expectedFinalizedFeatures: Features[FinalizedVersionRange],\n+                            expectedFinalizedFeaturesEpoch: Long,\n+                            expectedSupportedFeatures: Features[SupportedVersionRange]): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata.get\n+    assertEquals(expectedFinalizedFeatures, finalizedFeatures(featureMetadata.finalizedFeatures))\n+    assertEquals(expectedSupportedFeatures, supportedFeatures(featureMetadata.supportedFeatures))\n+    assertEquals(Optional.of(expectedFinalizedFeaturesEpoch), featureMetadata.finalizedFeaturesEpoch)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(s\"Received unexpected error message: ${cause.getMessage}\",\n+                   exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMjA3Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499812076", "createdAt": "2020-10-05T19:08:21Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,580 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.{Optional, Properties}\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.intercept\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def finalizedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new FinalizedVersionRange(versionRange.minVersionLevel(), versionRange.maxVersionLevel()))\n+    }.asJava)\n+  }\n+\n+  private def supportedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.SupportedVersionRange]): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new SupportedVersionRange(versionRange.minVersion(), versionRange.maxVersion()))\n+    }.asJava)\n+  }\n+\n+  private def checkFeatures(client: Admin,\n+                            expectedNode: FeatureZNode,\n+                            expectedFinalizedFeatures: Features[FinalizedVersionRange],\n+                            expectedFinalizedFeaturesEpoch: Long,\n+                            expectedSupportedFeatures: Features[SupportedVersionRange]): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata.get\n+    assertEquals(expectedFinalizedFeatures, finalizedFeatures(featureMetadata.finalizedFeatures))\n+    assertEquals(expectedSupportedFeatures, supportedFeatures(featureMetadata.supportedFeatures))\n+    assertEquals(Optional.of(expectedFinalizedFeaturesEpoch), featureMetadata.finalizedFeaturesEpoch)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(s\"Received unexpected error message: ${cause.getMessage}\",\n+                   exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc3NTM2Nw=="}, "originalCommit": null, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTIxMTg0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODowMTo0N1rOHcn8ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOToxNjozNFrOHcqXtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc3NjY3NQ==", "bodyText": "Should we use a version > 0?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499776675", "createdAt": "2020-10-05T18:01:47Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,580 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.{Optional, Properties}\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.intercept\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def finalizedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new FinalizedVersionRange(versionRange.minVersionLevel(), versionRange.maxVersionLevel()))\n+    }.asJava)\n+  }\n+\n+  private def supportedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.SupportedVersionRange]): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new SupportedVersionRange(versionRange.minVersion(), versionRange.maxVersion()))\n+    }.asJava)\n+  }\n+\n+  private def checkFeatures(client: Admin,\n+                            expectedNode: FeatureZNode,\n+                            expectedFinalizedFeatures: Features[FinalizedVersionRange],\n+                            expectedFinalizedFeaturesEpoch: Long,\n+                            expectedSupportedFeatures: Features[SupportedVersionRange]): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata.get\n+    assertEquals(expectedFinalizedFeatures, finalizedFeatures(featureMetadata.finalizedFeatures))\n+    assertEquals(expectedSupportedFeatures, supportedFeatures(featureMetadata.supportedFeatures))\n+    assertEquals(Optional.of(expectedFinalizedFeaturesEpoch), featureMetadata.finalizedFeaturesEpoch)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(s\"Received unexpected error message: ${cause.getMessage}\",\n+                   exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,\n+                                                                       invalidUpdate: FeatureUpdate,\n+                                                                       exceptionMsgPattern: Regex)\n+                                                                      (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(Utils.mkMap(Utils.mkEntry(feature, invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, Map(feature -> exceptionMsgPattern))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request sent to a non-Controller node fails as expected.\n+   */\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val validUpdates = new FeatureUpdateKeyCollection()\n+    val validUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    validUpdate.setFeature(\"feature_1\");\n+    validUpdate.setMaxVersionLevel(defaultSupportedFeatures().get(\"feature_1\").max())\n+    validUpdate.setAllowDowngrade(false)\n+    validUpdates.add(validUpdate)\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(validUpdates)).build(),\n+      notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, Errors.forCode(response.data.errorCode()))\n+    assertNotNull(response.data.errorMessage())\n+    assertEquals(0, response.data.results.size)\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the\n+   * allowDowngrade flag is not set during a downgrade request.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel,false),\n+      \".*Can not downgrade finalized feature.*allowDowngrade.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the downgrade\n+   * is attempted to a max version level thats higher than the existing max version level.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() + 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel, true),\n+      \".*When the allowDowngrade flag set in the request, the provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature deletion is\n+   * attempted without setting the allowDowngrade flag.\n+   */\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val invalidUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val invalidUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    invalidUpdate.setFeature(\"feature_1\")\n+    invalidUpdate.setMaxVersionLevel(0)\n+    invalidUpdate.setAllowDowngrade(false)\n+    invalidUpdates.add(invalidUpdate);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(invalidUpdates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(invalidUpdates)).build(),\n+      controllerSocketServer)\n+\n+    assertEquals(1, response.data().results().size())\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.INVALID_REQUEST, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    val exceptionMsgPattern = \".*Can not provide maxVersionLevel: 0 less than 1.*allowDowngrade.*\".r\n+    assertTrue(result.errorMessage, exceptionMsgPattern.findFirstIn(result.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature version level\n+   * upgrade is attempted for a non-existing feature.\n+   */\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_non_existing\",\n+      new FeatureUpdate(0, true),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 288}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxNjM3Mw==", "bodyText": "Done. Good point.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499816373", "createdAt": "2020-10-05T19:16:34Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,580 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.{Optional, Properties}\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.intercept\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def finalizedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new FinalizedVersionRange(versionRange.minVersionLevel(), versionRange.maxVersionLevel()))\n+    }.asJava)\n+  }\n+\n+  private def supportedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.SupportedVersionRange]): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new SupportedVersionRange(versionRange.minVersion(), versionRange.maxVersion()))\n+    }.asJava)\n+  }\n+\n+  private def checkFeatures(client: Admin,\n+                            expectedNode: FeatureZNode,\n+                            expectedFinalizedFeatures: Features[FinalizedVersionRange],\n+                            expectedFinalizedFeaturesEpoch: Long,\n+                            expectedSupportedFeatures: Features[SupportedVersionRange]): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata.get\n+    assertEquals(expectedFinalizedFeatures, finalizedFeatures(featureMetadata.finalizedFeatures))\n+    assertEquals(expectedSupportedFeatures, supportedFeatures(featureMetadata.supportedFeatures))\n+    assertEquals(Optional.of(expectedFinalizedFeaturesEpoch), featureMetadata.finalizedFeaturesEpoch)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(s\"Received unexpected error message: ${cause.getMessage}\",\n+                   exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,\n+                                                                       invalidUpdate: FeatureUpdate,\n+                                                                       exceptionMsgPattern: Regex)\n+                                                                      (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(Utils.mkMap(Utils.mkEntry(feature, invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, Map(feature -> exceptionMsgPattern))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request sent to a non-Controller node fails as expected.\n+   */\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val validUpdates = new FeatureUpdateKeyCollection()\n+    val validUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    validUpdate.setFeature(\"feature_1\");\n+    validUpdate.setMaxVersionLevel(defaultSupportedFeatures().get(\"feature_1\").max())\n+    validUpdate.setAllowDowngrade(false)\n+    validUpdates.add(validUpdate)\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(validUpdates)).build(),\n+      notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, Errors.forCode(response.data.errorCode()))\n+    assertNotNull(response.data.errorMessage())\n+    assertEquals(0, response.data.results.size)\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the\n+   * allowDowngrade flag is not set during a downgrade request.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel,false),\n+      \".*Can not downgrade finalized feature.*allowDowngrade.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the downgrade\n+   * is attempted to a max version level thats higher than the existing max version level.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() + 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel, true),\n+      \".*When the allowDowngrade flag set in the request, the provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature deletion is\n+   * attempted without setting the allowDowngrade flag.\n+   */\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val invalidUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val invalidUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    invalidUpdate.setFeature(\"feature_1\")\n+    invalidUpdate.setMaxVersionLevel(0)\n+    invalidUpdate.setAllowDowngrade(false)\n+    invalidUpdates.add(invalidUpdate);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(invalidUpdates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(invalidUpdates)).build(),\n+      controllerSocketServer)\n+\n+    assertEquals(1, response.data().results().size())\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.INVALID_REQUEST, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    val exceptionMsgPattern = \".*Can not provide maxVersionLevel: 0 less than 1.*allowDowngrade.*\".r\n+    assertTrue(result.errorMessage, exceptionMsgPattern.findFirstIn(result.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature version level\n+   * upgrade is attempted for a non-existing feature.\n+   */\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_non_existing\",\n+      new FeatureUpdate(0, true),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc3NjY3NQ=="}, "originalCommit": null, "originalPosition": 288}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTIxMzE4OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODowMjoxNFrOHcn9fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOToxNzowM1rOHcqYqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc3Njg5NA==", "bodyText": "typo thats", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499776894", "createdAt": "2020-10-05T18:02:14Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,580 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.{Optional, Properties}\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.intercept\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def finalizedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new FinalizedVersionRange(versionRange.minVersionLevel(), versionRange.maxVersionLevel()))\n+    }.asJava)\n+  }\n+\n+  private def supportedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.SupportedVersionRange]): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new SupportedVersionRange(versionRange.minVersion(), versionRange.maxVersion()))\n+    }.asJava)\n+  }\n+\n+  private def checkFeatures(client: Admin,\n+                            expectedNode: FeatureZNode,\n+                            expectedFinalizedFeatures: Features[FinalizedVersionRange],\n+                            expectedFinalizedFeaturesEpoch: Long,\n+                            expectedSupportedFeatures: Features[SupportedVersionRange]): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata.get\n+    assertEquals(expectedFinalizedFeatures, finalizedFeatures(featureMetadata.finalizedFeatures))\n+    assertEquals(expectedSupportedFeatures, supportedFeatures(featureMetadata.supportedFeatures))\n+    assertEquals(Optional.of(expectedFinalizedFeaturesEpoch), featureMetadata.finalizedFeaturesEpoch)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(s\"Received unexpected error message: ${cause.getMessage}\",\n+                   exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,\n+                                                                       invalidUpdate: FeatureUpdate,\n+                                                                       exceptionMsgPattern: Regex)\n+                                                                      (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(Utils.mkMap(Utils.mkEntry(feature, invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, Map(feature -> exceptionMsgPattern))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request sent to a non-Controller node fails as expected.\n+   */\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val validUpdates = new FeatureUpdateKeyCollection()\n+    val validUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    validUpdate.setFeature(\"feature_1\");\n+    validUpdate.setMaxVersionLevel(defaultSupportedFeatures().get(\"feature_1\").max())\n+    validUpdate.setAllowDowngrade(false)\n+    validUpdates.add(validUpdate)\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(validUpdates)).build(),\n+      notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, Errors.forCode(response.data.errorCode()))\n+    assertNotNull(response.data.errorMessage())\n+    assertEquals(0, response.data.results.size)\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the\n+   * allowDowngrade flag is not set during a downgrade request.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel,false),\n+      \".*Can not downgrade finalized feature.*allowDowngrade.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the downgrade\n+   * is attempted to a max version level thats higher than the existing max version level.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() + 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel, true),\n+      \".*When the allowDowngrade flag set in the request, the provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature deletion is\n+   * attempted without setting the allowDowngrade flag.\n+   */\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val invalidUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val invalidUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    invalidUpdate.setFeature(\"feature_1\")\n+    invalidUpdate.setMaxVersionLevel(0)\n+    invalidUpdate.setAllowDowngrade(false)\n+    invalidUpdates.add(invalidUpdate);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(invalidUpdates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(invalidUpdates)).build(),\n+      controllerSocketServer)\n+\n+    assertEquals(1, response.data().results().size())\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.INVALID_REQUEST, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    val exceptionMsgPattern = \".*Can not provide maxVersionLevel: 0 less than 1.*allowDowngrade.*\".r\n+    assertTrue(result.errorMessage, exceptionMsgPattern.findFirstIn(result.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature version level\n+   * upgrade is attempted for a non-existing feature.\n+   */\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_non_existing\",\n+      new FeatureUpdate(0, true),\n+      \".*Can not delete non-existing finalized feature.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature version level\n+   * upgrade is attempted to a version level thats the same as the existing max version level.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 294}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxNjYxOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r499816619", "createdAt": "2020-10-05T19:17:03Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,580 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.{Optional, Properties}\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.intercept\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def finalizedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new FinalizedVersionRange(versionRange.minVersionLevel(), versionRange.maxVersionLevel()))\n+    }.asJava)\n+  }\n+\n+  private def supportedFeatures(features: java.util.Map[String, org.apache.kafka.clients.admin.SupportedVersionRange]): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(features.asScala.map {\n+      case(name, versionRange) =>\n+        (name, new SupportedVersionRange(versionRange.minVersion(), versionRange.maxVersion()))\n+    }.asJava)\n+  }\n+\n+  private def checkFeatures(client: Admin,\n+                            expectedNode: FeatureZNode,\n+                            expectedFinalizedFeatures: Features[FinalizedVersionRange],\n+                            expectedFinalizedFeaturesEpoch: Long,\n+                            expectedSupportedFeatures: Features[SupportedVersionRange]): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata.get\n+    assertEquals(expectedFinalizedFeatures, finalizedFeatures(featureMetadata.finalizedFeatures))\n+    assertEquals(expectedSupportedFeatures, supportedFeatures(featureMetadata.supportedFeatures))\n+    assertEquals(Optional.of(expectedFinalizedFeaturesEpoch), featureMetadata.finalizedFeaturesEpoch)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(s\"Received unexpected error message: ${cause.getMessage}\",\n+                   exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,\n+                                                                       invalidUpdate: FeatureUpdate,\n+                                                                       exceptionMsgPattern: Regex)\n+                                                                      (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(Utils.mkMap(Utils.mkEntry(feature, invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, Map(feature -> exceptionMsgPattern))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request sent to a non-Controller node fails as expected.\n+   */\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val validUpdates = new FeatureUpdateKeyCollection()\n+    val validUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    validUpdate.setFeature(\"feature_1\");\n+    validUpdate.setMaxVersionLevel(defaultSupportedFeatures().get(\"feature_1\").max())\n+    validUpdate.setAllowDowngrade(false)\n+    validUpdates.add(validUpdate)\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(validUpdates)).build(),\n+      notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, Errors.forCode(response.data.errorCode()))\n+    assertNotNull(response.data.errorMessage())\n+    assertEquals(0, response.data.results.size)\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the\n+   * allowDowngrade flag is not set during a downgrade request.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel,false),\n+      \".*Can not downgrade finalized feature.*allowDowngrade.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, for a feature the downgrade\n+   * is attempted to a max version level thats higher than the existing max version level.\n+   */\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    val targetMaxVersionLevel = (defaultFinalizedFeatures().get(\"feature_1\").max() + 1).asInstanceOf[Short]\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(targetMaxVersionLevel, true),\n+      \".*When the allowDowngrade flag set in the request, the provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature deletion is\n+   * attempted without setting the allowDowngrade flag.\n+   */\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val invalidUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val invalidUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    invalidUpdate.setFeature(\"feature_1\")\n+    invalidUpdate.setMaxVersionLevel(0)\n+    invalidUpdate.setAllowDowngrade(false)\n+    invalidUpdates.add(invalidUpdate);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(invalidUpdates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(invalidUpdates)).build(),\n+      controllerSocketServer)\n+\n+    assertEquals(1, response.data().results().size())\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.INVALID_REQUEST, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    val exceptionMsgPattern = \".*Can not provide maxVersionLevel: 0 less than 1.*allowDowngrade.*\".r\n+    assertTrue(result.errorMessage, exceptionMsgPattern.findFirstIn(result.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      defaultFinalizedFeatures(),\n+      versionBefore,\n+      defaultSupportedFeatures())\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature version level\n+   * upgrade is attempted for a non-existing feature.\n+   */\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_non_existing\",\n+      new FeatureUpdate(0, true),\n+      \".*Can not delete non-existing finalized feature.*\".r)\n+  }\n+\n+  /**\n+   * Tests that an UpdateFeatures request fails in the Controller, when, a feature version level\n+   * upgrade is attempted to a version level thats the same as the existing max version level.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc3Njg5NA=="}, "originalCommit": null, "originalPosition": 294}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTcyMDUyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwMjo1MzowOFrOHeMKAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwOToyNjowNlrOHeVvCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTQxODQ5Ng==", "bodyText": "the error message says it can't be null but there is no null check.\nfor another, this check can happen early (when creating updateFutures)", "url": "https://github.com/apache/kafka/pull/9001#discussion_r501418496", "createdAt": "2020-10-08T02:53:08Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4335,6 +4343,150 @@ void handleFailure(Throwable throwable) {\n                 .hi(password, salt, iterations);\n     }\n \n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        final Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            private FeatureMetadata createFeatureMetadata(final ApiVersionsResponse response) {\n+                final Map<String, FinalizedVersionRange> finalizedFeatures = new HashMap<>();\n+                for (final FinalizedFeatureKey key : response.data().finalizedFeatures().valuesSet()) {\n+                    finalizedFeatures.put(key.name(), new FinalizedVersionRange(key.minVersionLevel(), key.maxVersionLevel()));\n+                }\n+\n+                Optional<Long> finalizedFeaturesEpoch;\n+                if (response.data().finalizedFeaturesEpoch() >= 0L) {\n+                    finalizedFeaturesEpoch = Optional.of(response.data().finalizedFeaturesEpoch());\n+                } else {\n+                    finalizedFeaturesEpoch = Optional.empty();\n+                }\n+\n+                final Map<String, SupportedVersionRange> supportedFeatures = new HashMap<>();\n+                for (final SupportedFeatureKey key : response.data().supportedFeatures().valuesSet()) {\n+                    supportedFeatures.put(key.name(), new SupportedVersionRange(key.minVersion(), key.maxVersion()));\n+                }\n+\n+                return new FeatureMetadata(finalizedFeatures, finalizedFeaturesEpoch, supportedFeatures);\n+            }\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(createFeatureMetadata(apiVersionsResponse));\n+                } else if (options.sendRequestToController() &&\n+                           apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(final Map<String, FeatureUpdate> featureUpdates,\n+                                               final UpdateFeaturesOptions options) {\n+        if (featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        for (final Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            updateFutures.put(entry.getKey(), new KafkaFutureImpl<>());\n+        }\n+\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                final UpdateFeaturesRequestData.FeatureUpdateKeyCollection featureUpdatesRequestData\n+                    = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+                for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+                    final String feature = entry.getKey();\n+                    final FeatureUpdate update = entry.getValue();\n+                    if (feature.trim().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1c79cee2ab243d95647935d2b3e7abe371bf6ea"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTU3NTQzMg==", "bodyText": "Done. Addressed in #9393.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r501575432", "createdAt": "2020-10-08T09:26:06Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4335,6 +4343,150 @@ void handleFailure(Throwable throwable) {\n                 .hi(password, salt, iterations);\n     }\n \n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        final Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            private FeatureMetadata createFeatureMetadata(final ApiVersionsResponse response) {\n+                final Map<String, FinalizedVersionRange> finalizedFeatures = new HashMap<>();\n+                for (final FinalizedFeatureKey key : response.data().finalizedFeatures().valuesSet()) {\n+                    finalizedFeatures.put(key.name(), new FinalizedVersionRange(key.minVersionLevel(), key.maxVersionLevel()));\n+                }\n+\n+                Optional<Long> finalizedFeaturesEpoch;\n+                if (response.data().finalizedFeaturesEpoch() >= 0L) {\n+                    finalizedFeaturesEpoch = Optional.of(response.data().finalizedFeaturesEpoch());\n+                } else {\n+                    finalizedFeaturesEpoch = Optional.empty();\n+                }\n+\n+                final Map<String, SupportedVersionRange> supportedFeatures = new HashMap<>();\n+                for (final SupportedFeatureKey key : response.data().supportedFeatures().valuesSet()) {\n+                    supportedFeatures.put(key.name(), new SupportedVersionRange(key.minVersion(), key.maxVersion()));\n+                }\n+\n+                return new FeatureMetadata(finalizedFeatures, finalizedFeaturesEpoch, supportedFeatures);\n+            }\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(createFeatureMetadata(apiVersionsResponse));\n+                } else if (options.sendRequestToController() &&\n+                           apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(final Map<String, FeatureUpdate> featureUpdates,\n+                                               final UpdateFeaturesOptions options) {\n+        if (featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        for (final Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            updateFutures.put(entry.getKey(), new KafkaFutureImpl<>());\n+        }\n+\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                final UpdateFeaturesRequestData.FeatureUpdateKeyCollection featureUpdatesRequestData\n+                    = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+                for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+                    final String feature = entry.getKey();\n+                    final FeatureUpdate update = entry.getValue();\n+                    if (feature.trim().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTQxODQ5Ng=="}, "originalCommit": {"oid": "e1c79cee2ab243d95647935d2b3e7abe371bf6ea"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTgxNzkyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwMzo1MTozN1rOHeNBwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwOToyNjoxMVrOHeVvQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTQzMjc3MA==", "bodyText": "Should we add an empty-parameter variety for describeFeatures? that is similar to other methods, like DescribeUserScramCredentialsResult and describeDelegationToken.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r501432770", "createdAt": "2020-10-08T03:51:37Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1306,6 +1307,73 @@ default AlterUserScramCredentialsResult alterUserScramCredentials(List<UserScram\n     AlterUserScramCredentialsResult alterUserScramCredentials(List<UserScramCredentialAlteration> alterations,\n                                                               AlterUserScramCredentialsOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1c79cee2ab243d95647935d2b3e7abe371bf6ea"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTU3NTQ4OQ==", "bodyText": "Done. Addressed in #9393.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r501575489", "createdAt": "2020-10-08T09:26:11Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1306,6 +1307,73 @@ default AlterUserScramCredentialsResult alterUserScramCredentials(List<UserScram\n     AlterUserScramCredentialsResult alterUserScramCredentials(List<UserScramCredentialAlteration> alterations,\n                                                               AlterUserScramCredentialsOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTQzMjc3MA=="}, "originalCommit": {"oid": "e1c79cee2ab243d95647935d2b3e7abe371bf6ea"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MDIwMDE3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwNjo1ODo0M1rOHeQdpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwOToyNjoxM1rOHeVvXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTQ4OTA2MA==", "bodyText": "the top-level error message is not propagated.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r501489060", "createdAt": "2020-10-08T06:58:43Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4335,6 +4343,150 @@ void handleFailure(Throwable throwable) {\n                 .hi(password, salt, iterations);\n     }\n \n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        final Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            private FeatureMetadata createFeatureMetadata(final ApiVersionsResponse response) {\n+                final Map<String, FinalizedVersionRange> finalizedFeatures = new HashMap<>();\n+                for (final FinalizedFeatureKey key : response.data().finalizedFeatures().valuesSet()) {\n+                    finalizedFeatures.put(key.name(), new FinalizedVersionRange(key.minVersionLevel(), key.maxVersionLevel()));\n+                }\n+\n+                Optional<Long> finalizedFeaturesEpoch;\n+                if (response.data().finalizedFeaturesEpoch() >= 0L) {\n+                    finalizedFeaturesEpoch = Optional.of(response.data().finalizedFeaturesEpoch());\n+                } else {\n+                    finalizedFeaturesEpoch = Optional.empty();\n+                }\n+\n+                final Map<String, SupportedVersionRange> supportedFeatures = new HashMap<>();\n+                for (final SupportedFeatureKey key : response.data().supportedFeatures().valuesSet()) {\n+                    supportedFeatures.put(key.name(), new SupportedVersionRange(key.minVersion(), key.maxVersion()));\n+                }\n+\n+                return new FeatureMetadata(finalizedFeatures, finalizedFeaturesEpoch, supportedFeatures);\n+            }\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(createFeatureMetadata(apiVersionsResponse));\n+                } else if (options.sendRequestToController() &&\n+                           apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(final Map<String, FeatureUpdate> featureUpdates,\n+                                               final UpdateFeaturesOptions options) {\n+        if (featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        for (final Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            updateFutures.put(entry.getKey(), new KafkaFutureImpl<>());\n+        }\n+\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                final UpdateFeaturesRequestData.FeatureUpdateKeyCollection featureUpdatesRequestData\n+                    = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+                for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+                    final String feature = entry.getKey();\n+                    final FeatureUpdate update = entry.getValue();\n+                    if (feature.trim().isEmpty()) {\n+                        throw new IllegalArgumentException(\"Provided feature can not be null or empty.\");\n+                    }\n+\n+                    final UpdateFeaturesRequestData.FeatureUpdateKey requestItem =\n+                        new UpdateFeaturesRequestData.FeatureUpdateKey();\n+                    requestItem.setFeature(feature);\n+                    requestItem.setMaxVersionLevel(update.maxVersionLevel());\n+                    requestItem.setAllowDowngrade(update.allowDowngrade());\n+                    featureUpdatesRequestData.add(requestItem);\n+                }\n+                return new UpdateFeaturesRequest.Builder(\n+                    new UpdateFeaturesRequestData()\n+                        .setTimeoutMs(timeoutMs)\n+                        .setFeatureUpdates(featureUpdatesRequestData));\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse abstractResponse) {\n+                final UpdateFeaturesResponse response =\n+                    (UpdateFeaturesResponse) abstractResponse;\n+\n+                Errors topLevelError = Errors.forCode(response.data().errorCode());\n+                switch (topLevelError) {\n+                    case NONE:\n+                        for (final UpdatableFeatureResult result : response.data().results()) {\n+                            final KafkaFutureImpl<Void> future = updateFutures.get(result.feature());\n+                            if (future == null) {\n+                                log.warn(\"Server response mentioned unknown feature {}\", result.feature());\n+                            } else {\n+                                final Errors error = Errors.forCode(result.errorCode());\n+                                if (error == Errors.NONE) {\n+                                    future.complete(null);\n+                                } else {\n+                                    future.completeExceptionally(error.exception(result.errorMessage()));\n+                                }\n+                            }\n+                        }\n+                        // The server should send back a response for every feature, but we do a sanity check anyway.\n+                        completeUnrealizedFutures(updateFutures.entrySet().stream(),\n+                            feature -> \"The controller response did not contain a result for feature \" + feature);\n+                        break;\n+                    case NOT_CONTROLLER:\n+                        handleNotControllerError(topLevelError);\n+                        break;\n+                    default:\n+                        for (final Map.Entry<String, KafkaFutureImpl<Void>> entry : updateFutures.entrySet()) {\n+                            entry.getValue().completeExceptionally(topLevelError.exception());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1c79cee2ab243d95647935d2b3e7abe371bf6ea"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTU3NTUxOQ==", "bodyText": "Done. Addressed in #9393.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r501575519", "createdAt": "2020-10-08T09:26:13Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4335,6 +4343,150 @@ void handleFailure(Throwable throwable) {\n                 .hi(password, salt, iterations);\n     }\n \n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        final Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            private FeatureMetadata createFeatureMetadata(final ApiVersionsResponse response) {\n+                final Map<String, FinalizedVersionRange> finalizedFeatures = new HashMap<>();\n+                for (final FinalizedFeatureKey key : response.data().finalizedFeatures().valuesSet()) {\n+                    finalizedFeatures.put(key.name(), new FinalizedVersionRange(key.minVersionLevel(), key.maxVersionLevel()));\n+                }\n+\n+                Optional<Long> finalizedFeaturesEpoch;\n+                if (response.data().finalizedFeaturesEpoch() >= 0L) {\n+                    finalizedFeaturesEpoch = Optional.of(response.data().finalizedFeaturesEpoch());\n+                } else {\n+                    finalizedFeaturesEpoch = Optional.empty();\n+                }\n+\n+                final Map<String, SupportedVersionRange> supportedFeatures = new HashMap<>();\n+                for (final SupportedFeatureKey key : response.data().supportedFeatures().valuesSet()) {\n+                    supportedFeatures.put(key.name(), new SupportedVersionRange(key.minVersion(), key.maxVersion()));\n+                }\n+\n+                return new FeatureMetadata(finalizedFeatures, finalizedFeaturesEpoch, supportedFeatures);\n+            }\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(createFeatureMetadata(apiVersionsResponse));\n+                } else if (options.sendRequestToController() &&\n+                           apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(final Map<String, FeatureUpdate> featureUpdates,\n+                                               final UpdateFeaturesOptions options) {\n+        if (featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        for (final Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            updateFutures.put(entry.getKey(), new KafkaFutureImpl<>());\n+        }\n+\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                final UpdateFeaturesRequestData.FeatureUpdateKeyCollection featureUpdatesRequestData\n+                    = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+                for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+                    final String feature = entry.getKey();\n+                    final FeatureUpdate update = entry.getValue();\n+                    if (feature.trim().isEmpty()) {\n+                        throw new IllegalArgumentException(\"Provided feature can not be null or empty.\");\n+                    }\n+\n+                    final UpdateFeaturesRequestData.FeatureUpdateKey requestItem =\n+                        new UpdateFeaturesRequestData.FeatureUpdateKey();\n+                    requestItem.setFeature(feature);\n+                    requestItem.setMaxVersionLevel(update.maxVersionLevel());\n+                    requestItem.setAllowDowngrade(update.allowDowngrade());\n+                    featureUpdatesRequestData.add(requestItem);\n+                }\n+                return new UpdateFeaturesRequest.Builder(\n+                    new UpdateFeaturesRequestData()\n+                        .setTimeoutMs(timeoutMs)\n+                        .setFeatureUpdates(featureUpdatesRequestData));\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse abstractResponse) {\n+                final UpdateFeaturesResponse response =\n+                    (UpdateFeaturesResponse) abstractResponse;\n+\n+                Errors topLevelError = Errors.forCode(response.data().errorCode());\n+                switch (topLevelError) {\n+                    case NONE:\n+                        for (final UpdatableFeatureResult result : response.data().results()) {\n+                            final KafkaFutureImpl<Void> future = updateFutures.get(result.feature());\n+                            if (future == null) {\n+                                log.warn(\"Server response mentioned unknown feature {}\", result.feature());\n+                            } else {\n+                                final Errors error = Errors.forCode(result.errorCode());\n+                                if (error == Errors.NONE) {\n+                                    future.complete(null);\n+                                } else {\n+                                    future.completeExceptionally(error.exception(result.errorMessage()));\n+                                }\n+                            }\n+                        }\n+                        // The server should send back a response for every feature, but we do a sanity check anyway.\n+                        completeUnrealizedFutures(updateFutures.entrySet().stream(),\n+                            feature -> \"The controller response did not contain a result for feature \" + feature);\n+                        break;\n+                    case NOT_CONTROLLER:\n+                        handleNotControllerError(topLevelError);\n+                        break;\n+                    default:\n+                        for (final Map.Entry<String, KafkaFutureImpl<Void>> entry : updateFutures.entrySet()) {\n+                            entry.getValue().completeExceptionally(topLevelError.exception());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTQ4OTA2MA=="}, "originalCommit": {"oid": "e1c79cee2ab243d95647935d2b3e7abe371bf6ea"}, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxOTIwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1NTo1NVrOG16uXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0MzozNlrOG3C-LQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTg1NA==", "bodyText": "Could we just remove  the feature versioning system (KIP-584) is enabled, and? It does not provide any useful information.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459189854", "createdAt": "2020-07-23T02:55:55Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzU0OQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373549", "createdAt": "2020-07-25T06:43:36Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTg1NA=="}, "originalCommit": null, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxOTk3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1NjoyM1rOG16u0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0Mzo1MlrOG3C-OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTk3MA==", "bodyText": "s/This status/The enabled status", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459189970", "createdAt": "2020-07-23T02:56:23Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzU2MQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373561", "createdAt": "2020-07-25T06:43:52Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTk3MA=="}, "originalCommit": null, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYyMjA2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1Nzo0OVrOG16v5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0NDowOFrOG3C-Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDI0Nw==", "bodyText": "We don't need to capitalize Broker here", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459190247", "createdAt": "2020-07-23T02:57:49Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzU5MA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373590", "createdAt": "2020-07-25T06:44:08Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDI0Nw=="}, "originalCommit": null, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYyMjY3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1ODoxNVrOG16wOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0NDoyMFrOG3C-cA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDMyOQ==", "bodyText": "same here", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459190329", "createdAt": "2020-07-23T02:58:15Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzYxNg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373616", "createdAt": "2020-07-25T06:44:20Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDMyOQ=="}, "originalCommit": null, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYyMzI4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1ODo0NFrOG16wjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0NToxNVrOG3C-zA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDQxMg==", "bodyText": "s/The reason to do this is that.../This process ensures we do not enable all the possible features immediately after an upgrade, which could be harmful to the application.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459190412", "createdAt": "2020-07-23T02:58:44Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzcwOA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373708", "createdAt": "2020-07-25T06:45:15Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDQxMg=="}, "originalCommit": null, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYyNjcwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzowMTowM1rOG16yVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0NTozN1rOG3C-4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDg3MA==", "bodyText": "remove then", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459190870", "createdAt": "2020-07-23T03:01:03Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzcyOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373729", "createdAt": "2020-07-25T06:45:37Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MDg3MA=="}, "originalCommit": null, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYzMjQ0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzowNTowNVrOG161cQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0Njo0NVrOG3C_MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MTY2NQ==", "bodyText": "{} could be removed.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459191665", "createdAt": "2020-07-23T03:05:05Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzgwOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373809", "createdAt": "2020-07-25T06:46:45Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MTY2NQ=="}, "originalCommit": null, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYzNTc0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzowNzoxNlrOG163Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo0MjowMFrOG4CowQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MjExNQ==", "bodyText": "nit: would be easier to read if we always compare existingVersionRange towards brokerDefaultVersionRange instead of flipping in this statement.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459192115", "createdAt": "2020-07-23T03:07:16Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxNjY0MQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461416641", "createdAt": "2020-07-28T08:42:00Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MjExNQ=="}, "originalCommit": null, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTY0MTUxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzoxMTowNVrOG166RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwNzozMTowN1rOG6cU9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MjkwMQ==", "bodyText": "I think we need to override equals here.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459192901", "createdAt": "2020-07-23T03:11:05Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&\n+                       brokerDefaultVersionRange.max() >= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NDExNw==", "bodyText": "FeatureZNode is a case class, and therefore the equals method is auto generated. Let me know if I'm missing something. Here is the doc: https://docs.scala-lang.org/overviews/scala-book/case-classes.html.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460374117", "createdAt": "2020-07-25T06:50:59Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&\n+                       brokerDefaultVersionRange.max() >= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MjkwMQ=="}, "originalCommit": null, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ5ODk2MQ==", "bodyText": "I see, still wondering if we could just check whether newFeatures is equal to existingFeatureZNode.features", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462498961", "createdAt": "2020-07-29T18:23:15Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&\n+                       brokerDefaultVersionRange.max() >= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MjkwMQ=="}, "originalCommit": null, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNDcxMA==", "bodyText": "We should keep the existing check as it is. The reason is that if the existing node is (disabled, {}) then here we would like to change it to (enabled, features). Therefore, we have to check the features as well as the FeatureZNodeStatus.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463934710", "createdAt": "2020-08-01T07:31:07Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&\n+                       brokerDefaultVersionRange.max() >= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MjkwMQ=="}, "originalCommit": null, "originalPosition": 191}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTY0MjAwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzoxMToyNlrOG166kA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo1MToyMlrOG3DAeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Mjk3Ng==", "bodyText": "cache", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459192976", "createdAt": "2020-07-23T03:11:26Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&\n+                       brokerDefaultVersionRange.max() >= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the caceh (via FinalizedFeatureChangeListener)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NDEzNg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460374136", "createdAt": "2020-07-25T06:51:22Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= brokerDefaultVersionRange.min() &&\n+                       brokerDefaultVersionRange.max() >= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the caceh (via FinalizedFeatureChangeListener)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Mjk3Ng=="}, "originalCommit": null, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTY0NTUzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzoxMzoyMlrOG168bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo1ODo0M1rOG3DCfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MzQ1Mw==", "bodyText": "No UpdateMetadataRequest will be sent to broker...", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459193453", "createdAt": "2020-07-23T03:13:22Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -977,14 +1159,33 @@ class KafkaController(val config: KafkaConfig,\n \n   /**\n    * Send the leader information for selected partitions to selected brokers so that they can correctly respond to\n-   * metadata requests\n+   * metadata requests. Particularly, when feature versioning is enabled, we filter out brokers with incompatible\n+   * features from receiving the metadata requests. This is because we do not want to activate incompatible brokers,\n+   * as these may have harmful consequences to the cluster.\n    *\n    * @param brokers The brokers that the update metadata request should be sent to\n    */\n   private[controller] def sendUpdateMetadataRequest(brokers: Seq[Int], partitions: Set[TopicPartition]): Unit = {\n     try {\n+      val filteredBrokers = scala.collection.mutable.Set[Int]() ++ brokers\n+      if (config.isFeatureVersioningEnabled) {\n+        def hasIncompatibleFeatures(broker: Broker): Boolean = {\n+          val latestFinalizedFeatures = featureCache.get\n+          if (latestFinalizedFeatures.isDefined) {\n+            BrokerFeatures.hasIncompatibleFeatures(broker.features, latestFinalizedFeatures.get.features)\n+          } else {\n+            false\n+          }\n+        }\n+        controllerContext.liveOrShuttingDownBrokers.foreach(broker => {\n+          if (filteredBrokers.contains(broker.id) && hasIncompatibleFeatures(broker)) {\n+            warn(s\"Ignoring UpdateMetadataRequest to broker: ${broker.id} due to incompatible features\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NDY1Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460374653", "createdAt": "2020-07-25T06:58:43Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -977,14 +1159,33 @@ class KafkaController(val config: KafkaConfig,\n \n   /**\n    * Send the leader information for selected partitions to selected brokers so that they can correctly respond to\n-   * metadata requests\n+   * metadata requests. Particularly, when feature versioning is enabled, we filter out brokers with incompatible\n+   * features from receiving the metadata requests. This is because we do not want to activate incompatible brokers,\n+   * as these may have harmful consequences to the cluster.\n    *\n    * @param brokers The brokers that the update metadata request should be sent to\n    */\n   private[controller] def sendUpdateMetadataRequest(brokers: Seq[Int], partitions: Set[TopicPartition]): Unit = {\n     try {\n+      val filteredBrokers = scala.collection.mutable.Set[Int]() ++ brokers\n+      if (config.isFeatureVersioningEnabled) {\n+        def hasIncompatibleFeatures(broker: Broker): Boolean = {\n+          val latestFinalizedFeatures = featureCache.get\n+          if (latestFinalizedFeatures.isDefined) {\n+            BrokerFeatures.hasIncompatibleFeatures(broker.features, latestFinalizedFeatures.get.features)\n+          } else {\n+            false\n+          }\n+        }\n+        controllerContext.liveOrShuttingDownBrokers.foreach(broker => {\n+          if (filteredBrokers.contains(broker.id) && hasIncompatibleFeatures(broker)) {\n+            warn(s\"Ignoring UpdateMetadataRequest to broker: ${broker.id} due to incompatible features\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MzQ1Mw=="}, "originalCommit": null, "originalPosition": 262}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTY0NzMyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzoxNDo0NVrOG169bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwNzozNDo0NlrOG6cV5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MzcxMQ==", "bodyText": "does features guarantee to be non-null?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459193711", "createdAt": "2020-07-23T03:14:45Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1395,7 +1596,7 @@ class KafkaController(val config: KafkaConfig,\n     if (newMetadataOpt.nonEmpty && oldMetadataOpt.nonEmpty) {\n       val oldMetadata = oldMetadataOpt.get\n       val newMetadata = newMetadataOpt.get\n-      if (newMetadata.endPoints != oldMetadata.endPoints) {\n+      if (newMetadata.endPoints != oldMetadata.endPoints || !oldMetadata.features.equals(newMetadata.features)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTAwNA==", "bodyText": "Yes, Broker.features is just empty when there are no features set or none decoded from the BrokerIdZNode.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375004", "createdAt": "2020-07-25T07:03:27Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1395,7 +1596,7 @@ class KafkaController(val config: KafkaConfig,\n     if (newMetadataOpt.nonEmpty && oldMetadataOpt.nonEmpty) {\n       val oldMetadata = oldMetadataOpt.get\n       val newMetadata = newMetadataOpt.get\n-      if (newMetadata.endPoints != oldMetadata.endPoints) {\n+      if (newMetadata.endPoints != oldMetadata.endPoints || !oldMetadata.features.equals(newMetadata.features)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MzcxMQ=="}, "originalCommit": null, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUwMjc4MA==", "bodyText": "I see, still I'm a bit worried future changes could break this assumption. Not a bad idea to check features != null?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462502780", "createdAt": "2020-07-29T18:29:57Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1395,7 +1596,7 @@ class KafkaController(val config: KafkaConfig,\n     if (newMetadataOpt.nonEmpty && oldMetadataOpt.nonEmpty) {\n       val oldMetadata = oldMetadataOpt.get\n       val newMetadata = newMetadataOpt.get\n-      if (newMetadata.endPoints != oldMetadata.endPoints) {\n+      if (newMetadata.endPoints != oldMetadata.endPoints || !oldMetadata.features.equals(newMetadata.features)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MzcxMQ=="}, "originalCommit": null, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNDk0OA==", "bodyText": "I do not understand the concern.\nWhich code path can possibly introduce null features attribute in Broker object? It is impossible....", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463934948", "createdAt": "2020-08-01T07:34:46Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1395,7 +1596,7 @@ class KafkaController(val config: KafkaConfig,\n     if (newMetadataOpt.nonEmpty && oldMetadataOpt.nonEmpty) {\n       val oldMetadata = oldMetadataOpt.get\n       val newMetadata = newMetadataOpt.get\n-      if (newMetadata.endPoints != oldMetadata.endPoints) {\n+      if (newMetadata.endPoints != oldMetadata.endPoints || !oldMetadata.features.equals(newMetadata.features)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5MzcxMQ=="}, "originalCommit": null, "originalPosition": 278}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTY0Nzc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzoxNTowMlrOG169pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzowNDowMVrOG3DD_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Mzc2Nw==", "bodyText": "format", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459193767", "createdAt": "2020-07-23T03:15:02Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1848,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFeaturesCallback): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 287}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTAzOA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375038", "createdAt": "2020-07-25T07:04:01Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1848,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFeaturesCallback): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Mzc2Nw=="}, "originalCommit": null, "originalPosition": 287}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTY1ODQ2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzoyMjowOFrOG17DiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzowNjoxNVrOG3DE0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NTI3Mg==", "bodyText": "what clients are we referring to here?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459195272", "createdAt": "2020-07-23T03:22:08Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean clients have to stop", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTI1MQ==", "bodyText": "Done. I was referring to external clients of Kafka. Have updated the doc now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375251", "createdAt": "2020-07-25T07:06:15Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean clients have to stop", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NTI3Mg=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTY1ODkzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMzoyMjozNVrOG17Dyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzowNjozMlrOG3DE5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NTMzOA==", "bodyText": "The class is immutable in production", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459195338", "createdAt": "2020-07-23T03:22:35Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean clients have to stop\n+ *    using the finalized min version levels that have been deprecated.\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features. The class is generally immutable. It provides few APIs to", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTI3MA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375270", "createdAt": "2020-07-25T07:06:32Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean clients have to stop\n+ *    using the finalized min version levels that have been deprecated.\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features. The class is generally immutable. It provides few APIs to", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NTMzOA=="}, "originalCommit": null, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NzgxOTY4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNToxNjozNFrOG2PXxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzowODoxMVrOG3DFUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUyODEzMg==", "bodyText": "Is it necessary to quote incompatible?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459528132", "createdAt": "2020-07-23T15:16:34Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean clients have to stop\n+ *    using the finalized min version levels that have been deprecated.\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features. The class is generally immutable. It provides few APIs to\n+ * mutate state only for the purpose of testing.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTM3OA==", "bodyText": "Done. Removed quotes.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375378", "createdAt": "2020-07-25T07:08:11Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean clients have to stop\n+ *    using the finalized min version levels that have been deprecated.\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features. The class is generally immutable. It provides few APIs to\n+ * mutate state only for the purpose of testing.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUyODEzMg=="}, "originalCommit": null, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2Nzg4NDU0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNTozMDo1N1rOG2QANg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoxMzoxMVrOG3DGxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUzODQ4Ng==", "bodyText": "Could you explain a bit why we no longer use singletons for feature cache?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459538486", "createdAt": "2020-07-23T15:30:57Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -85,25 +85,25 @@ class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n       //                                           a case.\n       if (version == ZkVersion.UnknownVersion) {\n         info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n-        FinalizedFeatureCache.clear()\n+        featureCache.clear()\n       } else {\n         var maybeFeatureZNode: Option[FeatureZNode] = Option.empty\n         try {\n           maybeFeatureZNode = Some(FeatureZNode.decode(mayBeFeatureZNodeBytes.get))\n         } catch {\n           case e: IllegalArgumentException => {\n             error(s\"Unable to deserialize feature ZK node at path: $featureZkNodePath\", e)\n-            FinalizedFeatureCache.clear()\n+            featureCache.clear()\n           }\n         }\n-        maybeFeatureZNode.map(featureZNode => {\n+        maybeFeatureZNode.foreach(featureZNode => {\n           featureZNode.status match {\n             case FeatureZNodeStatus.Disabled => {\n               info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status.\")\n-              FinalizedFeatureCache.clear()\n+              featureCache.clear()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTc1MA==", "bodyText": "It became painful to write tests using singletons. Particularly in kafka.server.UpdateFeaturesTest we would like to simulate presence of multiple brokers and a controller within the same test process. Then we would like to set incompatible features for some brokers, and compatible features for some others. Using a singleton for feature cache made it impossible to set up such an environment for testing. That is why we no longer use a singleton, instead we instantiate the class once in KafkaServer and we use the object wherever needed.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375750", "createdAt": "2020-07-25T07:13:11Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -85,25 +85,25 @@ class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n       //                                           a case.\n       if (version == ZkVersion.UnknownVersion) {\n         info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n-        FinalizedFeatureCache.clear()\n+        featureCache.clear()\n       } else {\n         var maybeFeatureZNode: Option[FeatureZNode] = Option.empty\n         try {\n           maybeFeatureZNode = Some(FeatureZNode.decode(mayBeFeatureZNodeBytes.get))\n         } catch {\n           case e: IllegalArgumentException => {\n             error(s\"Unable to deserialize feature ZK node at path: $featureZkNodePath\", e)\n-            FinalizedFeatureCache.clear()\n+            featureCache.clear()\n           }\n         }\n-        maybeFeatureZNode.map(featureZNode => {\n+        maybeFeatureZNode.foreach(featureZNode => {\n           featureZNode.status match {\n             case FeatureZNodeStatus.Disabled => {\n               info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status.\")\n-              FinalizedFeatureCache.clear()\n+              featureCache.clear()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUzODQ4Ng=="}, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2Nzk4NjY1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNTo1NDo1MVrOG2RAMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo0NTowM1rOG4CwIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU1NDg2Ng==", "bodyText": "We could explicitly mention this is either or result.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459554866", "createdAt": "2020-07-23T15:54:51Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      val data = new UpdateFeaturesResponseData().setErrorCode(error.code())\n+      msgOverride.map(msg => data.setErrorMessage(msg))\n+      sendResponseExemptThrottle(request, new UpdateFeaturesResponse(data))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else if (updateFeaturesRequest.data.featureUpdates.isEmpty) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxODUzMA==", "bodyText": "This method has changed greatly and it has been moved to KafkaController.scala.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461418530", "createdAt": "2020-07-28T08:45:03Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      val data = new UpdateFeaturesResponseData().setErrorCode(error.code())\n+      msgOverride.map(msg => data.setErrorMessage(msg))\n+      sendResponseExemptThrottle(request, new UpdateFeaturesResponse(data))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else if (updateFeaturesRequest.data.featureUpdates.isEmpty) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU1NDg2Ng=="}, "originalCommit": null, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2Nzk5NTk1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNTo1Njo0NVrOG2RF0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo0NToxN1rOG4CwuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU1NjMwNw==", "bodyText": "Could be initialized closer to L3005", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459556307", "createdAt": "2020-07-23T15:56:45Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      val data = new UpdateFeaturesResponseData().setErrorCode(error.code())\n+      msgOverride.map(msg => data.setErrorMessage(msg))\n+      sendResponseExemptThrottle(request, new UpdateFeaturesResponse(data))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else if (updateFeaturesRequest.data.featureUpdates.isEmpty) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  private def getTargetFinalizedFeaturesOrError(request: UpdateFeaturesRequest): Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.featureUpdates", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxODY4MQ==", "bodyText": "This method has changed greatly and it has been moved to KafkaController.scala.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461418681", "createdAt": "2020-07-28T08:45:17Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      val data = new UpdateFeaturesResponseData().setErrorCode(error.code())\n+      msgOverride.map(msg => data.setErrorMessage(msg))\n+      sendResponseExemptThrottle(request, new UpdateFeaturesResponse(data))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else if (updateFeaturesRequest.data.featureUpdates.isEmpty) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  private def getTargetFinalizedFeaturesOrError(request: UpdateFeaturesRequest): Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.featureUpdates", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU1NjMwNw=="}, "originalCommit": null, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODAzNTY1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjowNjozN1rOG2Re-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoxNTowN1rOG3DHXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2Mjc0Nw==", "bodyText": "Could we assert the expected version here?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459562747", "createdAt": "2020-07-23T16:06:37Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "diffHunk": "@@ -598,6 +608,21 @@ class ControllerIntegrationTest extends ZooKeeperTestHarness {\n     }, \"Broker fail to initialize after restart\")\n   }\n \n+  private def testControllerFeatureZNodeSetup(interBrokerProtocolVersion: ApiVersion): Unit = {\n+    servers = makeServers(1, interBrokerProtocolVersion = Some(interBrokerProtocolVersion))\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTkwMQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375901", "createdAt": "2020-07-25T07:15:07Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "diffHunk": "@@ -598,6 +608,21 @@ class ControllerIntegrationTest extends ZooKeeperTestHarness {\n     }, \"Broker fail to initialize after restart\")\n   }\n \n+  private def testControllerFeatureZNodeSetup(interBrokerProtocolVersion: ApiVersion): Unit = {\n+    servers = makeServers(1, interBrokerProtocolVersion = Some(interBrokerProtocolVersion))\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2Mjc0Nw=="}, "originalCommit": null, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODAzODgwOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjowNzoyNFrOG2Rg9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoxNToyNVrOG3DHfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2MzI1NQ==", "bodyText": "nit: new line", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459563255", "createdAt": "2020-07-23T16:07:24Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class BrokerFeaturesTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NTkzNQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460375935", "createdAt": "2020-07-25T07:15:25Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class BrokerFeaturesTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2MzI1NQ=="}, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODA0NjY0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjowOTozNlrOG2Rl7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoxNjo0MlrOG3DH4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NDUyNg==", "bodyText": "...WithInvalidSmallValue", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459564526", "createdAt": "2020-07-23T16:09:36Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class BrokerFeaturesTest {\n+  @Test\n+  def testEmpty(): Unit = {\n+    assertTrue(BrokerFeatures.createDefault().supportedFeatures.empty)\n+  }\n+\n+  @Test\n+  def testIncompatibleFeatures(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3))\n+    brokerFeatures.setSupportedFeatures(Features.supportedFeatures(supportedFeatures.asJava))\n+\n+    val compatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val inCompatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_2\" -> new FinalizedVersionRange(1, 4),\n+      \"test_feature_3\" -> new FinalizedVersionRange(3, 4))\n+    val features = compatibleFeatures++inCompatibleFeatures\n+    val finalizedFeatures = Features.finalizedFeatures(features.asJava)\n+\n+    assertEquals(\n+      Features.finalizedFeatures(inCompatibleFeatures.asJava),\n+      brokerFeatures.incompatibleFeatures(finalizedFeatures))\n+  }\n+\n+  @Test\n+  def testFeatureVersionAssertions(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Features.supportedFeatures(Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3)).asJava)\n+    brokerFeatures.setSupportedFeatures(supportedFeatures)\n+\n+    val defaultMinVersionLevelsWithNonExistingFeature = Map[String, Short](\n+      \"test_feature_1\" -> 2,\n+      \"test_feature_2\" -> 2,\n+      \"test_feature_non_existing\" -> 5)\n+    assertThrows(\n+      classOf[IllegalArgumentException],\n+      () => brokerFeatures.setDefaultMinVersionLevels(defaultMinVersionLevelsWithNonExistingFeature))\n+\n+    val defaultMinVersionLevelsWithInvalidValue1 = Map[String, Short](", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NjAzNQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460376035", "createdAt": "2020-07-25T07:16:42Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class BrokerFeaturesTest {\n+  @Test\n+  def testEmpty(): Unit = {\n+    assertTrue(BrokerFeatures.createDefault().supportedFeatures.empty)\n+  }\n+\n+  @Test\n+  def testIncompatibleFeatures(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3))\n+    brokerFeatures.setSupportedFeatures(Features.supportedFeatures(supportedFeatures.asJava))\n+\n+    val compatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val inCompatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_2\" -> new FinalizedVersionRange(1, 4),\n+      \"test_feature_3\" -> new FinalizedVersionRange(3, 4))\n+    val features = compatibleFeatures++inCompatibleFeatures\n+    val finalizedFeatures = Features.finalizedFeatures(features.asJava)\n+\n+    assertEquals(\n+      Features.finalizedFeatures(inCompatibleFeatures.asJava),\n+      brokerFeatures.incompatibleFeatures(finalizedFeatures))\n+  }\n+\n+  @Test\n+  def testFeatureVersionAssertions(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Features.supportedFeatures(Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3)).asJava)\n+    brokerFeatures.setSupportedFeatures(supportedFeatures)\n+\n+    val defaultMinVersionLevelsWithNonExistingFeature = Map[String, Short](\n+      \"test_feature_1\" -> 2,\n+      \"test_feature_2\" -> 2,\n+      \"test_feature_non_existing\" -> 5)\n+    assertThrows(\n+      classOf[IllegalArgumentException],\n+      () => brokerFeatures.setDefaultMinVersionLevels(defaultMinVersionLevelsWithNonExistingFeature))\n+\n+    val defaultMinVersionLevelsWithInvalidValue1 = Map[String, Short](", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NDUyNg=="}, "originalCommit": null, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODA0NzUzOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjowOTo0OFrOG2RmeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoxNjozN1rOG3DHyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NDY2NA==", "bodyText": "...WithInvalidLargeValue", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459564664", "createdAt": "2020-07-23T16:09:48Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class BrokerFeaturesTest {\n+  @Test\n+  def testEmpty(): Unit = {\n+    assertTrue(BrokerFeatures.createDefault().supportedFeatures.empty)\n+  }\n+\n+  @Test\n+  def testIncompatibleFeatures(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3))\n+    brokerFeatures.setSupportedFeatures(Features.supportedFeatures(supportedFeatures.asJava))\n+\n+    val compatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val inCompatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_2\" -> new FinalizedVersionRange(1, 4),\n+      \"test_feature_3\" -> new FinalizedVersionRange(3, 4))\n+    val features = compatibleFeatures++inCompatibleFeatures\n+    val finalizedFeatures = Features.finalizedFeatures(features.asJava)\n+\n+    assertEquals(\n+      Features.finalizedFeatures(inCompatibleFeatures.asJava),\n+      brokerFeatures.incompatibleFeatures(finalizedFeatures))\n+  }\n+\n+  @Test\n+  def testFeatureVersionAssertions(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Features.supportedFeatures(Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3)).asJava)\n+    brokerFeatures.setSupportedFeatures(supportedFeatures)\n+\n+    val defaultMinVersionLevelsWithNonExistingFeature = Map[String, Short](\n+      \"test_feature_1\" -> 2,\n+      \"test_feature_2\" -> 2,\n+      \"test_feature_non_existing\" -> 5)\n+    assertThrows(\n+      classOf[IllegalArgumentException],\n+      () => brokerFeatures.setDefaultMinVersionLevels(defaultMinVersionLevelsWithNonExistingFeature))\n+\n+    val defaultMinVersionLevelsWithInvalidValue1 = Map[String, Short](\n+      \"test_feature_1\" -> 2,\n+      \"test_feature_2\" -> (supportedFeatures.get(\"test_feature_2\").min() - 1).asInstanceOf[Short])\n+    assertThrows(\n+      classOf[IllegalArgumentException],\n+      () => brokerFeatures.setDefaultMinVersionLevels(defaultMinVersionLevelsWithInvalidValue1))\n+\n+    val defaultMinVersionLevelsWithInvalidValue2 = Map[String, Short](", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NjAwOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460376009", "createdAt": "2020-07-25T07:16:37Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class BrokerFeaturesTest {\n+  @Test\n+  def testEmpty(): Unit = {\n+    assertTrue(BrokerFeatures.createDefault().supportedFeatures.empty)\n+  }\n+\n+  @Test\n+  def testIncompatibleFeatures(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3))\n+    brokerFeatures.setSupportedFeatures(Features.supportedFeatures(supportedFeatures.asJava))\n+\n+    val compatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val inCompatibleFeatures = Map[String, FinalizedVersionRange](\n+      \"test_feature_2\" -> new FinalizedVersionRange(1, 4),\n+      \"test_feature_3\" -> new FinalizedVersionRange(3, 4))\n+    val features = compatibleFeatures++inCompatibleFeatures\n+    val finalizedFeatures = Features.finalizedFeatures(features.asJava)\n+\n+    assertEquals(\n+      Features.finalizedFeatures(inCompatibleFeatures.asJava),\n+      brokerFeatures.incompatibleFeatures(finalizedFeatures))\n+  }\n+\n+  @Test\n+  def testFeatureVersionAssertions(): Unit = {\n+    val brokerFeatures = BrokerFeatures.createDefault()\n+    val supportedFeatures = Features.supportedFeatures(Map[String, SupportedVersionRange](\n+      \"test_feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"test_feature_2\" -> new SupportedVersionRange(1, 3)).asJava)\n+    brokerFeatures.setSupportedFeatures(supportedFeatures)\n+\n+    val defaultMinVersionLevelsWithNonExistingFeature = Map[String, Short](\n+      \"test_feature_1\" -> 2,\n+      \"test_feature_2\" -> 2,\n+      \"test_feature_non_existing\" -> 5)\n+    assertThrows(\n+      classOf[IllegalArgumentException],\n+      () => brokerFeatures.setDefaultMinVersionLevels(defaultMinVersionLevelsWithNonExistingFeature))\n+\n+    val defaultMinVersionLevelsWithInvalidValue1 = Map[String, Short](\n+      \"test_feature_1\" -> 2,\n+      \"test_feature_2\" -> (supportedFeatures.get(\"test_feature_2\").min() - 1).asInstanceOf[Short])\n+    assertThrows(\n+      classOf[IllegalArgumentException],\n+      () => brokerFeatures.setDefaultMinVersionLevels(defaultMinVersionLevelsWithInvalidValue1))\n+\n+    val defaultMinVersionLevelsWithInvalidValue2 = Map[String, Short](", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NDY2NA=="}, "originalCommit": null, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODA1NDgxOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjoxMTo0MVrOG2RrGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoyMDozNVrOG3DJAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NTg1MA==", "bodyText": "What's the purpose of this second test?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459565850", "createdAt": "2020-07-23T16:11:41Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -78,25 +76,37 @@ class FinalizedFeatureChangeListenerTest extends ZooKeeperTestHarness {\n   /**\n    * Tests that the listener can be initialized, and that it can listen to ZK notifications\n    * successfully from an \"Enabled\" FeatureZNode (the ZK data has no feature incompatibilities).\n+   * Particularly the test checks if multiple notifications can be processed in ZK\n+   * (i.e. whether the FeatureZNode watch can be re-established).\n    */\n   @Test\n   def testInitSuccessAndNotificationSuccess(): Unit = {\n-    createSupportedFeatures()\n     val initialFinalizedFeatures = createFinalizedFeatures()\n-    val listener = createListener(Some(initialFinalizedFeatures))\n+    val brokerFeatures = createBrokerFeatures()\n+    val cache = new FinalizedFeatureCache(brokerFeatures)\n+    val listener = createListener(cache, Some(initialFinalizedFeatures))\n \n-    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n-      \"feature_1\" -> new FinalizedVersionRange(2, 4))\n-    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n-    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, updatedFinalizedFeatures))\n-    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n-    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n-    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n-    assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n-    TestUtils.waitUntilTrue(() => {\n-      FinalizedFeatureCache.get.get.equals(FinalizedFeaturesAndEpoch(updatedFinalizedFeatures, updatedVersion))\n-    }, \"Timed out waiting for FinalizedFeatureCache to be updated with new features\")\n-    assertTrue(listener.isListenerInitiated)\n+    def updateAndCheckCache(finalizedFeatures: Features[FinalizedVersionRange]): Unit = {\n+      zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, finalizedFeatures))\n+      val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+      assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+      assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+      assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n+\n+      cache.waitUntilEpochOrThrow(updatedVersion, JTestUtils.DEFAULT_MAX_WAIT_MS)\n+      assertEquals(FinalizedFeaturesAndEpoch(finalizedFeatures, updatedVersion), cache.get.get)\n+      assertTrue(listener.isListenerInitiated)\n+    }\n+\n+    updateAndCheckCache(\n+      Features.finalizedFeatures(\n+        Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 4)).asJava))\n+    updateAndCheckCache(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NjMyMg==", "bodyText": "It is explained in the test doc above, and, I have also added comments now. The purpose is to check that the ZK watch on the FeatureZNode was re-established by the broker, after the first update triggers a ZK notification that populates the cache. The best way to check it is to update the node again and see if the notification is received by the broker again.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460376322", "createdAt": "2020-07-25T07:20:35Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -78,25 +76,37 @@ class FinalizedFeatureChangeListenerTest extends ZooKeeperTestHarness {\n   /**\n    * Tests that the listener can be initialized, and that it can listen to ZK notifications\n    * successfully from an \"Enabled\" FeatureZNode (the ZK data has no feature incompatibilities).\n+   * Particularly the test checks if multiple notifications can be processed in ZK\n+   * (i.e. whether the FeatureZNode watch can be re-established).\n    */\n   @Test\n   def testInitSuccessAndNotificationSuccess(): Unit = {\n-    createSupportedFeatures()\n     val initialFinalizedFeatures = createFinalizedFeatures()\n-    val listener = createListener(Some(initialFinalizedFeatures))\n+    val brokerFeatures = createBrokerFeatures()\n+    val cache = new FinalizedFeatureCache(brokerFeatures)\n+    val listener = createListener(cache, Some(initialFinalizedFeatures))\n \n-    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n-      \"feature_1\" -> new FinalizedVersionRange(2, 4))\n-    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n-    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, updatedFinalizedFeatures))\n-    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n-    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n-    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n-    assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n-    TestUtils.waitUntilTrue(() => {\n-      FinalizedFeatureCache.get.get.equals(FinalizedFeaturesAndEpoch(updatedFinalizedFeatures, updatedVersion))\n-    }, \"Timed out waiting for FinalizedFeatureCache to be updated with new features\")\n-    assertTrue(listener.isListenerInitiated)\n+    def updateAndCheckCache(finalizedFeatures: Features[FinalizedVersionRange]): Unit = {\n+      zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, finalizedFeatures))\n+      val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+      assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+      assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+      assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n+\n+      cache.waitUntilEpochOrThrow(updatedVersion, JTestUtils.DEFAULT_MAX_WAIT_MS)\n+      assertEquals(FinalizedFeaturesAndEpoch(finalizedFeatures, updatedVersion), cache.get.get)\n+      assertTrue(listener.isListenerInitiated)\n+    }\n+\n+    updateAndCheckCache(\n+      Features.finalizedFeatures(\n+        Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 4)).asJava))\n+    updateAndCheckCache(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NTg1MA=="}, "originalCommit": null, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODA2MDk3OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjoxMzoxNFrOG2Ru6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoyMzo1NFrOG3DKAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NjgyNg==", "bodyText": "nit: replace with nonControllerServers.head", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459566826", "createdAt": "2020-07-23T16:13:14Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](\n+                                                          result: UpdateFeaturesResult,\n+                                                          exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    val exception = intercept[ExecutionException] {\n+      result.result().get()\n+    }\n+    assertNotNull(exception.getCause)\n+    assertEquals(exception.getCause.getClass, tag.runtimeClass)\n+    assertTrue(exceptionMsgPattern.findFirstIn(exception.getCause.getMessage).isDefined)\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](\n+                                                                        invalidUpdate: FeatureUpdate,\n+                                                                        exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      new HashSet[FeatureUpdate](Collections.singletonList(invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, exceptionMsgPattern)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val requestData = FeatureUpdate.createRequest(\n+      new util.HashSet[FeatureUpdate](\n+        Collections.singletonList(new FeatureUpdate(\"feature_1\",\n+          defaultSupportedFeatures().get(\"feature_1\").max(),\n+        false))))\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(requestData).build(), notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, response.error())\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForInvalidFeatureName(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"\",\n+      defaultSupportedFeatures().get(\"feature_1\").max(),\n+      false),\n+      \".*empty feature name.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"feature_1\",\n+      (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short],\n+      false),\n+      \".*Can not downgrade finalized feature: 'feature_1'.*allowDowngrade.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\n+        \"feature_1\",\n+        defaultSupportedFeatures().get(\"feature_1\").max(),\n+        true),\n+      \".*finalized feature: 'feature_1'.*allowDowngrade.* provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInClientWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    assertThrows[IllegalArgumentException] {\n+      new FeatureUpdate(\"feature_1\", 0, false)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val featureUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val featureUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    featureUpdate.setName(\"feature_1\")\n+    featureUpdate.setMaxVersionLevel(0)\n+    featureUpdate.setAllowDowngrade(false)\n+    featureUpdates.add(featureUpdate);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(featureUpdates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(requestData).build(), controllerSocketServer)\n+\n+    assertEquals(Errors.INVALID_REQUEST, response.error)\n+    val exceptionMsgPattern = \".*Can not delete feature: 'feature_1'.*allowDowngrade.*\".r\n+    assertTrue(exceptionMsgPattern.findFirstIn(response.data.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\"feature_non_existing\", 0, true),\n+      \".*Can not delete non-existing finalized feature: 'feature_non_existing'.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenUpgradingToSameVersionLevel(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\n+        \"feature_1\", defaultFinalizedFeatures().get(\"feature_1\").max(), false),\n+      \".*Can not upgrade a finalized feature: 'feature_1'.*to the same value.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradingBelowMinVersionLevel(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val minVersionLevel = 2.asInstanceOf[Short]\n+    updateDefaultMinVersionLevels(Map[String, Short](\"feature_1\" -> minVersionLevel))\n+    val initialFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(minVersionLevel, 2))))\n+    val versionBefore = updateFeatureZNode(initialFinalizedFeatures)\n+\n+    val update = new FeatureUpdate(\n+      \"feature_1\", (minVersionLevel - 1).asInstanceOf[Short], true)\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      new HashSet[FeatureUpdate](Collections.singletonList(update)), new UpdateFeaturesOptions())\n+\n+    checkException[InvalidRequestException](\n+      result, \".*Can not downgrade finalized feature: 'feature_1' to maxVersionLevel:1.*existing minVersionLevel:2.*\".r)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(initialFinalizedFeatures, versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringBrokerMaxVersionLevelIncompatibility(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val controller = servers.filter { server => server.kafkaController.isActive}.head\n+    val nonControllerServers = servers.filter { server => !server.kafkaController.isActive}\n+    val unsupportedBrokers = Set[KafkaServer](nonControllerServers(0))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NjU3OA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460376578", "createdAt": "2020-07-25T07:23:54Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](\n+                                                          result: UpdateFeaturesResult,\n+                                                          exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    val exception = intercept[ExecutionException] {\n+      result.result().get()\n+    }\n+    assertNotNull(exception.getCause)\n+    assertEquals(exception.getCause.getClass, tag.runtimeClass)\n+    assertTrue(exceptionMsgPattern.findFirstIn(exception.getCause.getMessage).isDefined)\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](\n+                                                                        invalidUpdate: FeatureUpdate,\n+                                                                        exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      new HashSet[FeatureUpdate](Collections.singletonList(invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, exceptionMsgPattern)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val requestData = FeatureUpdate.createRequest(\n+      new util.HashSet[FeatureUpdate](\n+        Collections.singletonList(new FeatureUpdate(\"feature_1\",\n+          defaultSupportedFeatures().get(\"feature_1\").max(),\n+        false))))\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(requestData).build(), notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, response.error())\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForInvalidFeatureName(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"\",\n+      defaultSupportedFeatures().get(\"feature_1\").max(),\n+      false),\n+      \".*empty feature name.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"feature_1\",\n+      (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short],\n+      false),\n+      \".*Can not downgrade finalized feature: 'feature_1'.*allowDowngrade.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\n+        \"feature_1\",\n+        defaultSupportedFeatures().get(\"feature_1\").max(),\n+        true),\n+      \".*finalized feature: 'feature_1'.*allowDowngrade.* provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInClientWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    assertThrows[IllegalArgumentException] {\n+      new FeatureUpdate(\"feature_1\", 0, false)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val featureUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val featureUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    featureUpdate.setName(\"feature_1\")\n+    featureUpdate.setMaxVersionLevel(0)\n+    featureUpdate.setAllowDowngrade(false)\n+    featureUpdates.add(featureUpdate);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(featureUpdates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(requestData).build(), controllerSocketServer)\n+\n+    assertEquals(Errors.INVALID_REQUEST, response.error)\n+    val exceptionMsgPattern = \".*Can not delete feature: 'feature_1'.*allowDowngrade.*\".r\n+    assertTrue(exceptionMsgPattern.findFirstIn(response.data.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\"feature_non_existing\", 0, true),\n+      \".*Can not delete non-existing finalized feature: 'feature_non_existing'.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenUpgradingToSameVersionLevel(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\n+        \"feature_1\", defaultFinalizedFeatures().get(\"feature_1\").max(), false),\n+      \".*Can not upgrade a finalized feature: 'feature_1'.*to the same value.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradingBelowMinVersionLevel(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val minVersionLevel = 2.asInstanceOf[Short]\n+    updateDefaultMinVersionLevels(Map[String, Short](\"feature_1\" -> minVersionLevel))\n+    val initialFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(minVersionLevel, 2))))\n+    val versionBefore = updateFeatureZNode(initialFinalizedFeatures)\n+\n+    val update = new FeatureUpdate(\n+      \"feature_1\", (minVersionLevel - 1).asInstanceOf[Short], true)\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      new HashSet[FeatureUpdate](Collections.singletonList(update)), new UpdateFeaturesOptions())\n+\n+    checkException[InvalidRequestException](\n+      result, \".*Can not downgrade finalized feature: 'feature_1' to maxVersionLevel:1.*existing minVersionLevel:2.*\".r)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(initialFinalizedFeatures, versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringBrokerMaxVersionLevelIncompatibility(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val controller = servers.filter { server => server.kafkaController.isActive}.head\n+    val nonControllerServers = servers.filter { server => !server.kafkaController.isActive}\n+    val unsupportedBrokers = Set[KafkaServer](nonControllerServers(0))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2NjgyNg=="}, "originalCommit": null, "originalPosition": 300}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODA3OTY2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjoxNzo1NlrOG2R6mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo1NDo1OVrOG4DITQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2OTgxOA==", "bodyText": "This case seems not to be tested yet.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459569818", "createdAt": "2020-07-23T16:17:56Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      val data = new UpdateFeaturesResponseData().setErrorCode(error.code())\n+      msgOverride.map(msg => data.setErrorMessage(msg))\n+      sendResponseExemptThrottle(request, new UpdateFeaturesResponse(data))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else if (updateFeaturesRequest.data.featureUpdates.isEmpty) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  private def getTargetFinalizedFeaturesOrError(request: UpdateFeaturesRequest): Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.featureUpdates\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+\n+    def addFeature(update: UpdateFeaturesRequestData.FeatureUpdateKey): Unit = {\n+      // NOTE: Below we set the finalized min version level to be the default minimum version\n+      // level. If the finalized feature already exists, then, this can cause deprecation of all\n+      // version levels in the closed range:\n+      // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+      val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.name)\n+      newFeatures += (\n+        update.name -> new FinalizedVersionRange(\n+          defaultMinVersionLevel,\n+          update.maxVersionLevel))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    updates.asScala.iterator.map(\n+      update => {\n+        if (update.name.isEmpty) {\n+          // Rule #1) Check that the feature name is not empty.\n+          Some(new ApiError(Errors.INVALID_REQUEST,\n+                   \"Can not contain empty feature name in the request.\"))\n+        } else {\n+          val cacheEntry = latestFeatures.map(lf => lf.features.get(update.name)).orNull\n+\n+          // We handle deletion requests separately from non-deletion requests.\n+          if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+            if (!update.allowDowngrade) {\n+              // Rule #2) Disallow deletion of a finalized feature without allowDowngrade flag set.\n+              Some(new ApiError(Errors.INVALID_REQUEST,\n+                                s\"Can not delete feature: '${update.name}' without setting the\" +\n+                                \" allowDowngrade flag to true in the request.\"))\n+            } else if (cacheEntry == null) {\n+              // Rule #3) Disallow deletion of a non-existing finalized feature.\n+              Some(new ApiError(Errors.INVALID_REQUEST,\n+                       s\"Can not delete non-existing finalized feature: '${update.name}'\"))\n+            }\n+          } else {\n+            if (cacheEntry == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQyNDcxNw==", "bodyText": "Added a test now in UpdateFeaturesTest.scala. Look for testSuccessfulFeatureUpgradeAndWithNoExistingFinalizedFeatures.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461424717", "createdAt": "2020-07-28T08:54:59Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      val data = new UpdateFeaturesResponseData().setErrorCode(error.code())\n+      msgOverride.map(msg => data.setErrorMessage(msg))\n+      sendResponseExemptThrottle(request, new UpdateFeaturesResponse(data))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else if (updateFeaturesRequest.data.featureUpdates.isEmpty) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  private def getTargetFinalizedFeaturesOrError(request: UpdateFeaturesRequest): Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.featureUpdates\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+\n+    def addFeature(update: UpdateFeaturesRequestData.FeatureUpdateKey): Unit = {\n+      // NOTE: Below we set the finalized min version level to be the default minimum version\n+      // level. If the finalized feature already exists, then, this can cause deprecation of all\n+      // version levels in the closed range:\n+      // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+      val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.name)\n+      newFeatures += (\n+        update.name -> new FinalizedVersionRange(\n+          defaultMinVersionLevel,\n+          update.maxVersionLevel))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    updates.asScala.iterator.map(\n+      update => {\n+        if (update.name.isEmpty) {\n+          // Rule #1) Check that the feature name is not empty.\n+          Some(new ApiError(Errors.INVALID_REQUEST,\n+                   \"Can not contain empty feature name in the request.\"))\n+        } else {\n+          val cacheEntry = latestFeatures.map(lf => lf.features.get(update.name)).orNull\n+\n+          // We handle deletion requests separately from non-deletion requests.\n+          if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+            if (!update.allowDowngrade) {\n+              // Rule #2) Disallow deletion of a finalized feature without allowDowngrade flag set.\n+              Some(new ApiError(Errors.INVALID_REQUEST,\n+                                s\"Can not delete feature: '${update.name}' without setting the\" +\n+                                \" allowDowngrade flag to true in the request.\"))\n+            } else if (cacheEntry == null) {\n+              // Rule #3) Disallow deletion of a non-existing finalized feature.\n+              Some(new ApiError(Errors.INVALID_REQUEST,\n+                       s\"Can not delete non-existing finalized feature: '${update.name}'\"))\n+            }\n+          } else {\n+            if (cacheEntry == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2OTgxOA=="}, "originalCommit": null, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODA4MzEwOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNjoxODo1NFrOG2R83A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoyMjoyN1rOG3DJnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU3MDM5Ng==", "bodyText": "Format", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459570396", "createdAt": "2020-07-23T16:18:54Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NjQ3Nw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460376477", "createdAt": "2020-07-25T07:22:27Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU3MDM5Ng=="}, "originalCommit": null, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU5OTQ2OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo0MTo1OVrOG2XBlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNzoyMjo1NlrOG3DJyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzUyNQ==", "bodyText": "format", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459653525", "createdAt": "2020-07-23T18:41:59Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](\n+                                                          result: UpdateFeaturesResult,\n+                                                          exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    val exception = intercept[ExecutionException] {\n+      result.result().get()\n+    }\n+    assertNotNull(exception.getCause)\n+    assertEquals(exception.getCause.getClass, tag.runtimeClass)\n+    assertTrue(exceptionMsgPattern.findFirstIn(exception.getCause.getMessage).isDefined)\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NjUyMQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460376521", "createdAt": "2020-07-25T07:22:56Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](\n+                                                          result: UpdateFeaturesResult,\n+                                                          exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    val exception = intercept[ExecutionException] {\n+      result.result().get()\n+    }\n+    assertNotNull(exception.getCause)\n+    assertEquals(exception.getCause.getClass, tag.runtimeClass)\n+    assertTrue(exceptionMsgPattern.findFirstIn(exception.getCause.getMessage).isDefined)\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzUyNQ=="}, "originalCommit": null, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODYxMzQxOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo0NTo0NFrOG2XKTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo0OTo1MVrOG4C7-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NTc1OA==", "bodyText": "We could refactor out a helper in UpdateFeaturesRequest to create FeatureUpdateKey", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459655758", "createdAt": "2020-07-23T18:45:44Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](\n+                                                          result: UpdateFeaturesResult,\n+                                                          exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    val exception = intercept[ExecutionException] {\n+      result.result().get()\n+    }\n+    assertNotNull(exception.getCause)\n+    assertEquals(exception.getCause.getClass, tag.runtimeClass)\n+    assertTrue(exceptionMsgPattern.findFirstIn(exception.getCause.getMessage).isDefined)\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](\n+                                                                        invalidUpdate: FeatureUpdate,\n+                                                                        exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      new HashSet[FeatureUpdate](Collections.singletonList(invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, exceptionMsgPattern)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val requestData = FeatureUpdate.createRequest(\n+      new util.HashSet[FeatureUpdate](\n+        Collections.singletonList(new FeatureUpdate(\"feature_1\",\n+          defaultSupportedFeatures().get(\"feature_1\").max(),\n+        false))))\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(requestData).build(), notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, response.error())\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForInvalidFeatureName(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"\",\n+      defaultSupportedFeatures().get(\"feature_1\").max(),\n+      false),\n+      \".*empty feature name.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"feature_1\",\n+      (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short],\n+      false),\n+      \".*Can not downgrade finalized feature: 'feature_1'.*allowDowngrade.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\n+        \"feature_1\",\n+        defaultSupportedFeatures().get(\"feature_1\").max(),\n+        true),\n+      \".*finalized feature: 'feature_1'.*allowDowngrade.* provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInClientWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    assertThrows[IllegalArgumentException] {\n+      new FeatureUpdate(\"feature_1\", 0, false)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val featureUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val featureUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQyMTU2Mw==", "bodyText": "Hmm, there seem to be very few call sites and therefore seems ok to inline it. Let me know!", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461421563", "createdAt": "2020-07-28T08:49:51Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,467 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Arrays\n+import java.util.Collections\n+import java.util.HashSet\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeatures(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](\n+                                                          result: UpdateFeaturesResult,\n+                                                          exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    val exception = intercept[ExecutionException] {\n+      result.result().get()\n+    }\n+    assertNotNull(exception.getCause)\n+    assertEquals(exception.getCause.getClass, tag.runtimeClass)\n+    assertTrue(exceptionMsgPattern.findFirstIn(exception.getCause.getMessage).isDefined)\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](\n+                                                                        invalidUpdate: FeatureUpdate,\n+                                                                        exceptionMsgPattern: Regex\n+  )(implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      new HashSet[FeatureUpdate](Collections.singletonList(invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, exceptionMsgPattern)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val requestData = FeatureUpdate.createRequest(\n+      new util.HashSet[FeatureUpdate](\n+        Collections.singletonList(new FeatureUpdate(\"feature_1\",\n+          defaultSupportedFeatures().get(\"feature_1\").max(),\n+        false))))\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(requestData).build(), notControllerSocketServer)\n+\n+    assertEquals(Errors.NOT_CONTROLLER, response.error())\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForInvalidFeatureName(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"\",\n+      defaultSupportedFeatures().get(\"feature_1\").max(),\n+      false),\n+      \".*empty feature name.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](new FeatureUpdate(\n+      \"feature_1\",\n+      (defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short],\n+      false),\n+      \".*Can not downgrade finalized feature: 'feature_1'.*allowDowngrade.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      new FeatureUpdate(\n+        \"feature_1\",\n+        defaultSupportedFeatures().get(\"feature_1\").max(),\n+        true),\n+      \".*finalized feature: 'feature_1'.*allowDowngrade.* provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInClientWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    assertThrows[IllegalArgumentException] {\n+      new FeatureUpdate(\"feature_1\", 0, false)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val featureUpdates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val featureUpdate = new UpdateFeaturesRequestData.FeatureUpdateKey();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NTc1OA=="}, "originalCommit": null, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODYzMTAzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo1MDo0NVrOG2XVOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwODo0NDoyMVrOG8AKNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODU1NA==", "bodyText": "Could we add some unit tests in KafkaApisTest.scala, once the refactoring is finished?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459658554", "createdAt": "2020-07-23T18:50:45Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxODM4NA==", "bodyText": "Will take a look.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461418384", "createdAt": "2020-07-28T08:44:51Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODU1NA=="}, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNTYwMw==", "bodyText": "Seems not covered yet", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462715603", "createdAt": "2020-07-30T03:39:57Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODU1NA=="}, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTU3MDM1OQ==", "bodyText": "This does not seem to be required, since it is already achieved via UpdateFeaturesTest. Infact there we test using admin client, which is even better as it tests e2e client to server functionality.\nWhat do we gain by adding the additional tests in KafkaApisTest ?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r465570359", "createdAt": "2020-08-05T08:44:21Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2948,130 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODU1NA=="}, "originalCommit": null, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MjgxODkyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxODowMzozNVrOG4YU2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxODowMzozNVrOG4YU2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc3MTk5Mg==", "bodyText": "nit: s/name/names", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461771992", "createdAt": "2020-07-28T18:03:35Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. This operation is not transactional so it\n+     * may succeed for some features while fail for others.\n+     * <p>\n+     * The API takes in a map of finalized feature name to {@link FeatureUpdate} that need to be", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzIxNjg2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzowODoxNVrOG5B81w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToyMzowNFrOHZVDIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1Mzk3NQ==", "bodyText": "Note in the post-KIP-500 world, this feature could still work, but the request must be redirected to the controller inherently on the broker side, instead of sending it directly. So in the comment, we may try to phrase it to convey the principal is that the request must be handled by the controller instead of the admin client must send this request to the controller.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462453975", "createdAt": "2020-07-29T17:08:15Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg4MDA3Ng==", "bodyText": "Sorry, I do not understand why should describeFeatures (in post KIP-500) be handled only by controller?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463880076", "createdAt": "2020-07-31T23:01:47Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1Mzk3NQ=="}, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTMxMg==", "bodyText": "Yea, you are right, I think this comment belongs to updateFeatures", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496321312", "createdAt": "2020-09-29T01:23:04Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1Mzk3NQ=="}, "originalCommit": null, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzIzNTI2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxMzowM1rOG5CIbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOToyODoyOVrOHXRzlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1Njk0Mg==", "bodyText": "should this a per feature error or a top level error?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462456942", "createdAt": "2020-07-29T17:13:03Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. This operation is not transactional so it\n+     * may succeed for some features while fail for others.\n+     * <p>\n+     * The API takes in a map of finalized feature name to {@link FeatureUpdate} that needs to be\n+     * applied. Each entry in the map specifies the finalized feature to be added or updated or\n+     * deleted, along with the new max feature version level value. This request is issued only to\n+     * the controller since the API is only served by the controller. The return value contains an\n+     * error code for each supplied {@link FeatureUpdate}, and the code indicates if the update\n+     * succeeded or failed in the controller.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the {@link FeatureUpdate} has the allowDowngrade flag set - setting this\n+     * flag conveys user intent to attempt downgrade of a feature max version level. Note that\n+     * despite the allowDowngrade flag being set, certain downgrades may be rejected by the\n+     * controller if it is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It could be\n+     * done by setting the allowDowngrade flag to true in the {@link FeatureUpdate}, and, setting\n+     * the max version level to be less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTU3MjA1Ng==", "bodyText": "Answered below.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r465572056", "createdAt": "2020-08-05T08:47:12Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. This operation is not transactional so it\n+     * may succeed for some features while fail for others.\n+     * <p>\n+     * The API takes in a map of finalized feature name to {@link FeatureUpdate} that needs to be\n+     * applied. Each entry in the map specifies the finalized feature to be added or updated or\n+     * deleted, along with the new max feature version level value. This request is issued only to\n+     * the controller since the API is only served by the controller. The return value contains an\n+     * error code for each supplied {@link FeatureUpdate}, and the code indicates if the update\n+     * succeeded or failed in the controller.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the {@link FeatureUpdate} has the allowDowngrade flag set - setting this\n+     * flag conveys user intent to attempt downgrade of a feature max version level. Note that\n+     * despite the allowDowngrade flag being set, certain downgrades may be rejected by the\n+     * controller if it is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It could be\n+     * done by setting the allowDowngrade flag to true in the {@link FeatureUpdate}, and, setting\n+     * the max version level to be less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1Njk0Mg=="}, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE3MTAzMA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494171030", "createdAt": "2020-09-24T09:28:29Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. This operation is not transactional so it\n+     * may succeed for some features while fail for others.\n+     * <p>\n+     * The API takes in a map of finalized feature name to {@link FeatureUpdate} that needs to be\n+     * applied. Each entry in the map specifies the finalized feature to be added or updated or\n+     * deleted, along with the new max feature version level value. This request is issued only to\n+     * the controller since the API is only served by the controller. The return value contains an\n+     * error code for each supplied {@link FeatureUpdate}, and the code indicates if the update\n+     * succeeded or failed in the controller.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the {@link FeatureUpdate} has the allowDowngrade flag set - setting this\n+     * flag conveys user intent to attempt downgrade of a feature max version level. Note that\n+     * despite the allowDowngrade flag being set, certain downgrades may be rejected by the\n+     * controller if it is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It could be\n+     * done by setting the allowDowngrade flag to true in the {@link FeatureUpdate}, and, setting\n+     * the max version level to be less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1Njk0Mg=="}, "originalCommit": null, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI0NjQ1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxNTo1NlrOG5CPiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOToyODoyNVrOHXRzcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1ODc2MQ==", "bodyText": "For top level exception such as cluster authorization exception, we could just define a top level error code instead of check-marking every feature with the redundant error code. I know we have been a bit inconsistent in such a case, but personally feel having layered error codes could make the response handling clear of whether it is per feature issue, or a high level issue.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462458761", "createdAt": "2020-07-29T17:15:56Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTU3MjAxMQ==", "bodyText": "I don't see that we consistently use a top level error code across other Kafka apis, so I will leave it as it is. It feels OK for this api to not use it, as it does not make a significant difference.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r465572011", "createdAt": "2020-08-05T08:47:06Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1ODc2MQ=="}, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE3MDk5NQ==", "bodyText": "Done. I have added a top-level error code now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494170995", "createdAt": "2020-09-24T09:28:25Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1ODc2MQ=="}, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI0NzU0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxNjoxNVrOG5CQRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxNjoxNVrOG5CQRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1ODk0OA==", "bodyText": "Space", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462458948", "createdAt": "2020-07-29T17:16:15Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI0ODAxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxNjoyMVrOG5CQhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxNjoyMVrOG5CQhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1OTAxNQ==", "bodyText": "Same here", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462459015", "createdAt": "2020-07-29T17:16:21Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI0ODg3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxNjozNFrOG5CREA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxNjozNFrOG5CREA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1OTE1Mg==", "bodyText": "Same here", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462459152", "createdAt": "2020-07-29T17:16:34Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",\n+        \"about\": \"The new maximum version level for the finalized feature. A value >= 1 is valid. A value < 1, is special, and can be used to request the deletion of the finalized feature.\"},\n+      {\"name\": \"AllowDowngrade\", \"type\": \"bool\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI2NzQzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyMTozMFrOG5CcnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyMTozMFrOG5CcnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2MjEwOQ==", "bodyText": "can be issued only to the controller./ must be processed by the controller", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462462109", "createdAt": "2020-07-29T17:21:30Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+/**\n+ * Options for {@link AdminClient#describeFeatures(DescribeFeaturesOptions)}\n+ *\n+ * The API of this class is evolving. See {@link Admin} for details.\n+ */\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+\n+    /**\n+     * - True means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI2ODQzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyMTo0N1rOG5CdPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyMTo0N1rOG5CdPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2MjI2OA==", "bodyText": "could be processed by any random broker", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462462268", "createdAt": "2020-07-29T17:21:47Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+/**\n+ * Options for {@link AdminClient#describeFeatures(DescribeFeaturesOptions)}\n+ *\n+ * The API of this class is evolving. See {@link Admin} for details.\n+ */\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+\n+    /**\n+     * - True means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued only to the controller.\n+     * - False means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI3MTQ5OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyMjo0MVrOG5CfUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyMjo0MVrOG5CfUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2MjgwMQ==", "bodyText": "Same here", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462462801", "createdAt": "2020-07-29T17:22:41Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+/**\n+ * Options for {@link AdminClient#describeFeatures(DescribeFeaturesOptions)}\n+ *\n+ * The API of this class is evolving. See {@link Admin} for details.\n+ */\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+\n+    /**\n+     * - True means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued only to the controller.\n+     * - False means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued to any random broker.\n+     */\n+    private boolean sendRequestToController = false;\n+\n+    /**\n+     * Sets a flag indicating that the describe features request should be issued to the controller.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI3ODY1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyNDozM1rOG5Cj6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyNDozM1rOG5Cj6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2Mzk3Nw==", "bodyText": "Try to put first parameter on the same line as the constructor, and align the rest parameters.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462463977", "createdAt": "2020-07-29T17:24:33Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Integer> finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI4NjQ1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyNjoyOFrOG5CopA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyNjoyOFrOG5CopA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2NTE4OA==", "bodyText": "This won't work well with string format, consider doing orElse", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462465188", "createdAt": "2020-07-29T17:26:28Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Integer> finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(\n+        final Features<FinalizedVersionRange> finalizedFeatures,\n+        final int finalizedFeaturesEpoch,\n+        final Features<SupportedVersionRange> supportedFeatures) {\n+        Objects.requireNonNull(finalizedFeatures, \"Provided finalizedFeatures can not be null.\");\n+        Objects.requireNonNull(supportedFeatures, \"Provided supportedFeatures can not be null.\");\n+        this.finalizedFeatures = finalizedFeatures;\n+        if (finalizedFeaturesEpoch >= 0) {\n+            this.finalizedFeaturesEpoch = Optional.of(finalizedFeaturesEpoch);\n+        } else {\n+            this.finalizedFeaturesEpoch = Optional.empty();\n+        }\n+        this.supportedFeatures = supportedFeatures;\n+    }\n+\n+    /**\n+     * A map of finalized feature versions, with key being finalized feature name and value\n+     * containing the min/max version levels for the finalized feature.\n+     */\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        return finalizedFeatures;\n+    }\n+\n+    /**\n+     * The epoch for the finalized features.\n+     * If the returned value is empty, it means the finalized features are absent/unavailable.\n+     */\n+    public Optional<Integer> finalizedFeaturesEpoch() {\n+        return finalizedFeaturesEpoch;\n+    }\n+\n+    /**\n+     * A map of supported feature versions, with key being supported feature name and value\n+     * containing the min/max version for the supported feature.\n+     */\n+    public Features<SupportedVersionRange> supportedFeatures() {\n+        return supportedFeatures;\n+    }\n+\n+    @Override\n+    public boolean equals(Object other) {\n+        if (this == other) {\n+            return true;\n+        }\n+        if (!(other instanceof FeatureMetadata)) {\n+            return false;\n+        }\n+\n+        final FeatureMetadata that = (FeatureMetadata) other;\n+        return Objects.equals(this.finalizedFeatures, that.finalizedFeatures) &&\n+            Objects.equals(this.finalizedFeaturesEpoch, that.finalizedFeaturesEpoch) &&\n+            Objects.equals(this.supportedFeatures, that.supportedFeatures);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(finalizedFeatures, finalizedFeaturesEpoch, supportedFeatures);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return String.format(\n+            \"FeatureMetadata{finalized:%s, finalizedFeaturesEpoch:%d, supported:%s}\",\n+            finalizedFeatures,\n+            finalizedFeaturesEpoch,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI4NzczOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyNjo1MFrOG5Cpaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoyNjo1MFrOG5Cpaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2NTM4Nw==", "bodyText": "new line", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462465387", "createdAt": "2020-07-29T17:26:50Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the {@link Admin#updateFeatures(Map, UpdateFeaturesOptions)} API.\n+ */\n+public class FeatureUpdate {\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FeatureUpdate(final short maxVersionLevel, final boolean allowDowngrade) {\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(String.format(\n+                \"The allowDowngrade flag should be set when the provided maxVersionLevel:%d is < 1.\",\n+                maxVersionLevel));\n+        }\n+        this.maxVersionLevel = maxVersionLevel;\n+        this.allowDowngrade = allowDowngrade;\n+    }\n+\n+    public short maxVersionLevel() {\n+        return maxVersionLevel;\n+    }\n+\n+    public boolean allowDowngrade() {\n+        return allowDowngrade;\n+    }\n+\n+    @Override\n+    public boolean equals(Object other) {\n+        if (this == other) {\n+            return true;\n+        }\n+\n+        if (!(other instanceof FeatureUpdate)) {\n+            return false;\n+        }\n+\n+        final FeatureUpdate that = (FeatureUpdate) other;\n+        return this.maxVersionLevel == that.maxVersionLevel && this.allowDowngrade == that.allowDowngrade;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(maxVersionLevel, allowDowngrade);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return String.format(\"FeatureUpdate{maxVersionLevel:%d, allowDowngrade:%s}\", maxVersionLevel, allowDowngrade);\n+    }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzMyMzMwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzozNjoyOFrOG5C_fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzozNjoyOFrOG5C_fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ3MTAzOA==", "bodyText": "I suggest we build a static method in the UpdateFeaturesRequest class to avoid exposing the sub modules of feature data, such like:\npublic static UpdateFeaturesRequestData getFeatureRequest(final Map<String, FeatureUpdate> featureUpdate);", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462471038", "createdAt": "2020-07-29T17:36:28Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4052,6 +4058,128 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else if (options.sendRequestToController() && apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(\n+        final Map<String, FeatureUpdate> featureUpdates, final UpdateFeaturesOptions options) {\n+        if (featureUpdates == null || featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+        Objects.requireNonNull(options, \"UpdateFeaturesOptions can not be null\");\n+\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        final UpdateFeaturesRequestData.FeatureUpdateKeyCollection featureUpdatesRequestData", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzMzNTU0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzozOTo0M1rOG5DG7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMjo1NDowN1rOG6a83Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ3Mjk0MA==", "bodyText": "Does this overlap with completeUnrealizedFutures check? We could just keep one to reduce the checking complexity.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462472940", "createdAt": "2020-07-29T17:39:43Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4052,6 +4058,128 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else if (options.sendRequestToController() && apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(\n+        final Map<String, FeatureUpdate> featureUpdates, final UpdateFeaturesOptions options) {\n+        if (featureUpdates == null || featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+        Objects.requireNonNull(options, \"UpdateFeaturesOptions can not be null\");\n+\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        final UpdateFeaturesRequestData.FeatureUpdateKeyCollection featureUpdatesRequestData\n+            = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+        for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            final String feature = entry.getKey();\n+            final FeatureUpdate update = entry.getValue();\n+            if (feature.trim().isEmpty()) {\n+                throw new IllegalArgumentException(\"Provided feature can not be null or empty.\");\n+            }\n+\n+            updateFutures.put(feature, new KafkaFutureImpl<>());\n+            final UpdateFeaturesRequestData.FeatureUpdateKey requestItem =\n+                new UpdateFeaturesRequestData.FeatureUpdateKey();\n+            requestItem.setFeature(feature);\n+            requestItem.setMaxVersionLevel(update.maxVersionLevel());\n+            requestItem.setAllowDowngrade(update.allowDowngrade());\n+            featureUpdatesRequestData.add(requestItem);\n+        }\n+        final UpdateFeaturesRequestData request = new UpdateFeaturesRequestData().setFeatureUpdates(featureUpdatesRequestData);\n+\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                return new UpdateFeaturesRequest.Builder(request);\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse abstractResponse) {\n+                final UpdateFeaturesResponse response =\n+                    (UpdateFeaturesResponse) abstractResponse;\n+\n+                // Check for controller change.\n+                for (UpdatableFeatureResult result : response.data().results()) {\n+                    final Errors error = Errors.forCode(result.errorCode());\n+                    if (error == Errors.NOT_CONTROLLER) {\n+                        handleNotControllerError(error);\n+                        throw error.exception();\n+                    }\n+                }\n+\n+                for (UpdatableFeatureResult result : response.data().results()) {\n+                    final KafkaFutureImpl<Void> future = updateFutures.get(result.feature());\n+                    if (future == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxMjE1Nw==", "bodyText": "It does not overlap. This checks for unexpected responses for features that we never intended to update. completeUnrealizedFutures is for futures that we never got a response for from the server -- we need to complete such futures exceptionally.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463912157", "createdAt": "2020-08-01T02:54:07Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4052,6 +4058,128 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else if (options.sendRequestToController() && apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(\n+        final Map<String, FeatureUpdate> featureUpdates, final UpdateFeaturesOptions options) {\n+        if (featureUpdates == null || featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+        Objects.requireNonNull(options, \"UpdateFeaturesOptions can not be null\");\n+\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        final UpdateFeaturesRequestData.FeatureUpdateKeyCollection featureUpdatesRequestData\n+            = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+        for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            final String feature = entry.getKey();\n+            final FeatureUpdate update = entry.getValue();\n+            if (feature.trim().isEmpty()) {\n+                throw new IllegalArgumentException(\"Provided feature can not be null or empty.\");\n+            }\n+\n+            updateFutures.put(feature, new KafkaFutureImpl<>());\n+            final UpdateFeaturesRequestData.FeatureUpdateKey requestItem =\n+                new UpdateFeaturesRequestData.FeatureUpdateKey();\n+            requestItem.setFeature(feature);\n+            requestItem.setMaxVersionLevel(update.maxVersionLevel());\n+            requestItem.setAllowDowngrade(update.allowDowngrade());\n+            featureUpdatesRequestData.add(requestItem);\n+        }\n+        final UpdateFeaturesRequestData request = new UpdateFeaturesRequestData().setFeatureUpdates(featureUpdatesRequestData);\n+\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                return new UpdateFeaturesRequest.Builder(request);\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse abstractResponse) {\n+                final UpdateFeaturesResponse response =\n+                    (UpdateFeaturesResponse) abstractResponse;\n+\n+                // Check for controller change.\n+                for (UpdatableFeatureResult result : response.data().results()) {\n+                    final Errors error = Errors.forCode(result.errorCode());\n+                    if (error == Errors.NOT_CONTROLLER) {\n+                        handleNotControllerError(error);\n+                        throw error.exception();\n+                    }\n+                }\n+\n+                for (UpdatableFeatureResult result : response.data().results()) {\n+                    final KafkaFutureImpl<Void> future = updateFutures.get(result.feature());\n+                    if (future == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ3Mjk0MA=="}, "originalCommit": null, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzM2MzAxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/errors/FeatureUpdateFailedException.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzo0NjozMlrOG5DX9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMjo1Nzo0NlrOG6a-Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ3NzMwMw==", "bodyText": "Do we need to make this a public error? It seems only be used internally, so could be made private if we don't have intention to let user catch.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462477303", "createdAt": "2020-07-29T17:46:32Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/errors/FeatureUpdateFailedException.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.errors;\n+\n+public class FeatureUpdateFailedException extends ApiException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxMjQ5OA==", "bodyText": "This exception corresponds to Errors.FEATURE_UPDATE_FAILED. The caller of AdminClient#updateFeatures can receive this exception whenever a feature update can not be written to ZK (due to a ZK issue). So this has to be a public error.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463912498", "createdAt": "2020-08-01T02:57:46Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/errors/FeatureUpdateFailedException.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.errors;\n+\n+public class FeatureUpdateFailedException extends ApiException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ3NzMwMw=="}, "originalCommit": null, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzM4NTEzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzo1MjoxNlrOG5Dlug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzo1MjoxNlrOG5Dlug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4MDgyNg==", "bodyText": "Comment here since no better place: createApiVersionsResponse on L198 could be made private", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462480826", "createdAt": "2020-07-29T17:52:16Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -143,7 +172,13 @@ public static ApiVersionsResponse apiVersionsResponse(\n         Features<FinalizedVersionRange> finalizedFeatures,\n         int finalizedFeaturesEpoch) {\n         if (maxMagic == RecordBatch.CURRENT_MAGIC_VALUE && throttleTimeMs == DEFAULT_THROTTLE_TIME) {\n-            return DEFAULT_API_VERSIONS_RESPONSE;\n+            return new ApiVersionsResponse(createApiVersionsResponseData(\n+                DEFAULT_API_VERSIONS_RESPONSE.throttleTimeMs(),\n+                Errors.forCode(DEFAULT_API_VERSIONS_RESPONSE.data().errorCode()),\n+                DEFAULT_API_VERSIONS_RESPONSE.data().apiKeys(),\n+                latestSupportedFeatures,\n+                finalizedFeatures,\n+                finalizedFeaturesEpoch));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzQwMDc0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzo1NjozMVrOG5Dvcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMzozNzozNVrOG6bNtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4MzMxNQ==", "bodyText": "nit: could be replaced with lambda", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462483315", "createdAt": "2020-07-29T17:56:31Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+\n+/**\n+ * Possible error codes:\n+ *\n+ *   - {@link Errors#CLUSTER_AUTHORIZATION_FAILED}\n+ *   - {@link Errors#NOT_CONTROLLER}\n+ *   - {@link Errors#INVALID_REQUEST}\n+ *   - {@link Errors#FEATURE_UPDATE_FAILED}\n+ */\n+public class UpdateFeaturesResponse extends AbstractResponse {\n+\n+    private final UpdateFeaturesResponseData data;\n+\n+    public UpdateFeaturesResponse(UpdateFeaturesResponseData data) {\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct) {\n+        final short latestVersion = (short) (UpdateFeaturesResponseData.SCHEMAS.length - 1);\n+        this.data = new UpdateFeaturesResponseData(struct, latestVersion);\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct, short version) {\n+        this.data = new UpdateFeaturesResponseData(struct, version);\n+    }\n+\n+    public Map<String, ApiError> errors() {\n+        return data.results().valuesSet().stream().collect(\n+            Collectors.toMap(\n+                result -> result.feature(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNjQ3MA==", "bodyText": "Like how? I don't understand. Isn't that what I'm doing currently?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463916470", "createdAt": "2020-08-01T03:37:35Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+\n+/**\n+ * Possible error codes:\n+ *\n+ *   - {@link Errors#CLUSTER_AUTHORIZATION_FAILED}\n+ *   - {@link Errors#NOT_CONTROLLER}\n+ *   - {@link Errors#INVALID_REQUEST}\n+ *   - {@link Errors#FEATURE_UPDATE_FAILED}\n+ */\n+public class UpdateFeaturesResponse extends AbstractResponse {\n+\n+    private final UpdateFeaturesResponseData data;\n+\n+    public UpdateFeaturesResponse(UpdateFeaturesResponseData data) {\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct) {\n+        final short latestVersion = (short) (UpdateFeaturesResponseData.SCHEMAS.length - 1);\n+        this.data = new UpdateFeaturesResponseData(struct, latestVersion);\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct, short version) {\n+        this.data = new UpdateFeaturesResponseData(struct, version);\n+    }\n+\n+    public Map<String, ApiError> errors() {\n+        return data.results().valuesSet().stream().collect(\n+            Collectors.toMap(\n+                result -> result.feature(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4MzMxNQ=="}, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzQxNjYxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODowMDo0MVrOG5D5fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMzoyMzo1MFrOG6bJkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4NTg4Ng==", "bodyText": "Should we also mention that this flag would fail the request when we are not actually doing a downgrade?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462485886", "createdAt": "2020-07-29T18:00:41Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",\n+        \"about\": \"The new maximum version level for the finalized feature. A value >= 1 is valid. A value < 1, is special, and can be used to request the deletion of the finalized feature.\"},\n+      {\"name\": \"AllowDowngrade\", \"type\": \"bool\", \"versions\": \"0+\",\n+        \"about\": \"When set to true, the finalized feature version level is allowed to be downgraded/deleted.\"}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNTQwOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463915409", "createdAt": "2020-08-01T03:23:50Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",\n+        \"about\": \"The new maximum version level for the finalized feature. A value >= 1 is valid. A value < 1, is special, and can be used to request the deletion of the finalized feature.\"},\n+      {\"name\": \"AllowDowngrade\", \"type\": \"bool\", \"versions\": \"0+\",\n+        \"about\": \"When set to true, the finalized feature version level is allowed to be downgraded/deleted.\"}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4NTg4Ng=="}, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzQzMDAyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODowNDo0MVrOG5ECDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMzoyMzo0NFrOG6bJjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4ODA3OA==", "bodyText": "I'm actually wondering whether this is too strict in the perspective of a user. If they accidentally set a feature version larger than the cache, what they only care about is to be able to change the version to it. So it's a matter of whether we think this is a user error, or this could happen when user gets stale feature information from a broker while the downgrade already succeed eventually.\nIf we want to keep this check, it makes sense to update the meta comments around allowDowngrade to inform user that the request could fail when the target version is actually higher than the current finalized feature.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462488078", "createdAt": "2020-07-29T18:04:41Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,188 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+    // NOTE: Below we set the finalized min version level to be the default minimum version\n+    // level. If the finalized feature already exists, then, this can cause deprecation of all\n+    // version levels in the closed range:\n+    // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+    val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+    val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)\n+    val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+      val singleFinalizedFeature =\n+        Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+      BrokerFeatures.hasIncompatibleFeatures(broker.features, singleFinalizedFeature)\n+    })\n+    if (numIncompatibleBrokers == 0) {\n+      Left(newVersionRange)\n+    } else {\n+      Right(\n+        new ApiError(Errors.INVALID_REQUEST,\n+                     s\"Could not apply finalized feature update because $numIncompatibleBrokers\" +\n+                     \" brokers were found to have incompatible features.\"))\n+    }\n+  }\n+\n+  /**\n+   * Validate and process a finalized feature update.\n+   *\n+   * If the processing is successful, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the processing failed, then returned value contains a suitable ApiError.\n+   *\n+   * @param update   the feature update to be processed.\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def processFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      val cacheEntry = existingFeatures.get(update.feature).orNull\n+\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (cacheEntry == null) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             s\"Can not delete non-existing finalized feature: '${update.feature}'\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 for feature: '${update.feature}' without setting the\" +\n+                           \" allowDowngrade flag to true in the request.\"))\n+      } else {\n+        if (cacheEntry == null) {\n+          newVersionRangeOrError(update)\n+        } else {\n+          if (update.maxVersionLevel == cacheEntry.max()) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature: '${update.feature}' from existing\" +\n+                               s\" maxVersionLevel:${cacheEntry.max} to the same value.\"))\n+          } else if (update.maxVersionLevel < cacheEntry.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature: '${update.feature}' from\" +\n+                               s\" existing maxVersionLevel:${cacheEntry.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > cacheEntry.max) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNTQwNg==", "bodyText": "Updated the doc. Let's keep the check, if it happens then it's a user error. Especially because this can not happen if the user is using the tooling that we are going to provide in AK.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463915406", "createdAt": "2020-08-01T03:23:44Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,188 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+    // NOTE: Below we set the finalized min version level to be the default minimum version\n+    // level. If the finalized feature already exists, then, this can cause deprecation of all\n+    // version levels in the closed range:\n+    // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+    val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+    val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)\n+    val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+      val singleFinalizedFeature =\n+        Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+      BrokerFeatures.hasIncompatibleFeatures(broker.features, singleFinalizedFeature)\n+    })\n+    if (numIncompatibleBrokers == 0) {\n+      Left(newVersionRange)\n+    } else {\n+      Right(\n+        new ApiError(Errors.INVALID_REQUEST,\n+                     s\"Could not apply finalized feature update because $numIncompatibleBrokers\" +\n+                     \" brokers were found to have incompatible features.\"))\n+    }\n+  }\n+\n+  /**\n+   * Validate and process a finalized feature update.\n+   *\n+   * If the processing is successful, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the processing failed, then returned value contains a suitable ApiError.\n+   *\n+   * @param update   the feature update to be processed.\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def processFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      val cacheEntry = existingFeatures.get(update.feature).orNull\n+\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (cacheEntry == null) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             s\"Can not delete non-existing finalized feature: '${update.feature}'\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 for feature: '${update.feature}' without setting the\" +\n+                           \" allowDowngrade flag to true in the request.\"))\n+      } else {\n+        if (cacheEntry == null) {\n+          newVersionRangeOrError(update)\n+        } else {\n+          if (update.maxVersionLevel == cacheEntry.max()) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature: '${update.feature}' from existing\" +\n+                               s\" maxVersionLevel:${cacheEntry.max} to the same value.\"))\n+          } else if (update.maxVersionLevel < cacheEntry.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature: '${update.feature}' from\" +\n+                               s\" existing maxVersionLevel:${cacheEntry.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > cacheEntry.max) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4ODA3OA=="}, "originalCommit": null, "originalPosition": 381}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzQzODEwOnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODowNzowMFrOG5EHHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODowNzowMFrOG5EHHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ4OTM3NQ==", "bodyText": "Could be moved to the UpdateFeaturesResponse", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462489375", "createdAt": "2020-07-29T18:07:00Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -466,6 +477,42 @@ private static DescribeGroupsResponseData prepareDescribeGroupsResponseData(Stri\n                 Collections.emptySet()));\n         return data;\n     }\n+\n+    private static UpdateFeaturesResponse prepareUpdateFeaturesResponse(Map<String, Errors> featureUpdateErrors) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzQ1NjI4OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODoxMTo1OFrOG5ESfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODoxMTo1OFrOG5ESfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ5MjI4NQ==", "bodyText": "Could we make updates as a pass-in parameter to avoid calling makeTestFeatureUpdates twice?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462492285", "createdAt": "2020-07-29T18:11:58Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3615,6 +3662,137 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.NONE));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.INVALID_REQUEST));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.FEATURE_UPDATE_FAILED));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesPartialSuccess() throws Exception {\n+        final Map<String, Errors> errors = makeTestFeatureUpdateErrors(Errors.NONE);\n+        errors.put(\"test_feature_2\", Errors.INVALID_REQUEST);\n+        testUpdateFeatures(makeTestFeatureUpdates(), errors);\n+    }\n+\n+    private Map<String, FeatureUpdate> makeTestFeatureUpdates() {\n+        return Utils.mkMap(\n+            Utils.mkEntry(\"test_feature_1\", new FeatureUpdate((short) 2, false)),\n+            Utils.mkEntry(\"test_feature_2\", new FeatureUpdate((short) 3, true)));\n+    }\n+\n+    private Map<String, Errors> makeTestFeatureUpdateErrors(final Errors error) {\n+        final Map<String, FeatureUpdate> updates = makeTestFeatureUpdates();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzQ1OTc1OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODoxMjo1OFrOG5EUmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMzozNDoyM1rOG6bMuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ5MjgyNQ==", "bodyText": "nit: could use lambda", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462492825", "createdAt": "2020-07-29T18:12:58Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3615,6 +3662,137 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.NONE));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.INVALID_REQUEST));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.FEATURE_UPDATE_FAILED));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesPartialSuccess() throws Exception {\n+        final Map<String, Errors> errors = makeTestFeatureUpdateErrors(Errors.NONE);\n+        errors.put(\"test_feature_2\", Errors.INVALID_REQUEST);\n+        testUpdateFeatures(makeTestFeatureUpdates(), errors);\n+    }\n+\n+    private Map<String, FeatureUpdate> makeTestFeatureUpdates() {\n+        return Utils.mkMap(\n+            Utils.mkEntry(\"test_feature_1\", new FeatureUpdate((short) 2, false)),\n+            Utils.mkEntry(\"test_feature_2\", new FeatureUpdate((short) 3, true)));\n+    }\n+\n+    private Map<String, Errors> makeTestFeatureUpdateErrors(final Errors error) {\n+        final Map<String, FeatureUpdate> updates = makeTestFeatureUpdates();\n+        final Map<String, Errors> errors = new HashMap<>();\n+        for (Map.Entry<String, FeatureUpdate> entry : updates.entrySet()) {\n+            errors.put(entry.getKey(), error);\n+        }\n+        return errors;\n+    }\n+\n+    private void testUpdateFeatures(Map<String, FeatureUpdate> featureUpdates,\n+                                    Map<String, Errors> featureUpdateErrors) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(featureUpdateErrors));\n+            final Map<String, KafkaFuture<Void>> futures = env.adminClient().updateFeatures(\n+                featureUpdates,\n+                new UpdateFeaturesOptions().timeoutMs(10000)).values();\n+            for (Map.Entry<String, KafkaFuture<Void>> entry : futures.entrySet()) {\n+                final KafkaFuture<Void> future = entry.getValue();\n+                final Errors error = featureUpdateErrors.get(entry.getKey());\n+                if (error == Errors.NONE) {\n+                    future.get();\n+                } else {\n+                    final ExecutionException e = assertThrows(ExecutionException.class,\n+                        () -> future.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNjIxOQ==", "bodyText": "Isn't that what I'm using currently?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463916219", "createdAt": "2020-08-01T03:34:23Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3615,6 +3662,137 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.NONE));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.INVALID_REQUEST));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeatures(\n+            makeTestFeatureUpdates(),\n+            makeTestFeatureUpdateErrors(Errors.FEATURE_UPDATE_FAILED));\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesPartialSuccess() throws Exception {\n+        final Map<String, Errors> errors = makeTestFeatureUpdateErrors(Errors.NONE);\n+        errors.put(\"test_feature_2\", Errors.INVALID_REQUEST);\n+        testUpdateFeatures(makeTestFeatureUpdates(), errors);\n+    }\n+\n+    private Map<String, FeatureUpdate> makeTestFeatureUpdates() {\n+        return Utils.mkMap(\n+            Utils.mkEntry(\"test_feature_1\", new FeatureUpdate((short) 2, false)),\n+            Utils.mkEntry(\"test_feature_2\", new FeatureUpdate((short) 3, true)));\n+    }\n+\n+    private Map<String, Errors> makeTestFeatureUpdateErrors(final Errors error) {\n+        final Map<String, FeatureUpdate> updates = makeTestFeatureUpdates();\n+        final Map<String, Errors> errors = new HashMap<>();\n+        for (Map.Entry<String, FeatureUpdate> entry : updates.entrySet()) {\n+            errors.put(entry.getKey(), error);\n+        }\n+        return errors;\n+    }\n+\n+    private void testUpdateFeatures(Map<String, FeatureUpdate> featureUpdates,\n+                                    Map<String, Errors> featureUpdateErrors) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(featureUpdateErrors));\n+            final Map<String, KafkaFuture<Void>> futures = env.adminClient().updateFeatures(\n+                featureUpdates,\n+                new UpdateFeaturesOptions().timeoutMs(10000)).values();\n+            for (Map.Entry<String, KafkaFuture<Void>> entry : futures.entrySet()) {\n+                final KafkaFuture<Void> future = entry.getValue();\n+                final Errors error = featureUpdateErrors.get(entry.getKey());\n+                if (error == Errors.NONE) {\n+                    future.get();\n+                } else {\n+                    final ExecutionException e = assertThrows(ExecutionException.class,\n+                        () -> future.get());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ5MjgyNQ=="}, "originalCommit": null, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzQ4MTEzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODoxODoxN1rOG5EhWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMzoyNToyMFrOG6bKIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ5NjA5MQ==", "bodyText": "Do we need to call featureCache.waitUntilEpochOrThrow(newNode, config.zkConnectionTimeoutMs) here to ensure the update is successful?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462496091", "createdAt": "2020-07-29T18:18:17Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,178 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the cache (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {\n+    val newNode = FeatureZNode(FeatureZNodeStatus.Disabled, Features.emptyFinalizedFeatures())\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      createFeatureZNode(newNode)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNTU1Mw==", "bodyText": "No, that is not required. Please refer to the documentation above under NOTE for this method where I have explained why.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463915553", "createdAt": "2020-08-01T03:25:20Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,178 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), or, if the\n+              // existing version levels are ineligible for a modification since they are\n+              // incompatible with default finalized version levels, then we skip the update.\n+              warn(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the cache (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {\n+    val newNode = FeatureZNode(FeatureZNodeStatus.Disabled, Features.emptyFinalizedFeatures())\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      createFeatureZNode(newNode)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ5NjA5MQ=="}, "originalCommit": null, "originalPosition": 220}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzUwNzM4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODoyNTozMVrOG5Ex2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwODozNjozNlrOG7_45Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUwMDMxNA==", "bodyText": "Are we good to proceed in this case? When there is no overlapping between broker default features and remote finalized features, is the current controller still eligible?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462500314", "createdAt": "2020-07-29T18:25:31Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,178 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTU2NTkyNQ==", "bodyText": "Actually this is an error case now. Have updated the code with the fix, and with good documentation.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r465565925", "createdAt": "2020-08-05T08:36:36Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,178 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // If the existing version levels fall completely outside the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUwMDMxNA=="}, "originalCommit": null, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzUzMzUzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODozMjo0NVrOG5FCEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODozMjo0NVrOG5FCEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUwNDQ2Nw==", "bodyText": "State the error explicitly here.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462504467", "createdAt": "2020-07-29T18:32:45Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,188 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 289}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODQ2MDY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMzozODoyMVrOG5N6Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwNzo0NDoxNFrOG6cY5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0OTg5NQ==", "bodyText": "Is this case covered by the case on L1931? Could we merge both?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462649895", "createdAt": "2020-07-29T23:38:21Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,185 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+    // NOTE: Below we set the finalized min version level to be the default minimum version\n+    // level. If the finalized feature already exists, then, this can cause deprecation of all\n+    // version levels in the closed range:\n+    // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+    val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+    val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)\n+    val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+      val singleFinalizedFeature =\n+        Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+      BrokerFeatures.hasIncompatibleFeatures(broker.features, singleFinalizedFeature)\n+    })\n+    if (numIncompatibleBrokers == 0) {\n+      Left(newVersionRange)\n+    } else {\n+      Right(\n+        new ApiError(Errors.INVALID_REQUEST,\n+                     s\"Could not apply finalized feature update because $numIncompatibleBrokers\" +\n+                     \" brokers were found to have incompatible features.\"))\n+    }\n+  }\n+\n+  /**\n+   * Validate and process a finalized feature update on an existing FinalizedVersionRange for the\n+   * feature.\n+   *\n+   * If the processing is successful, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the processing failed, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def processFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             s\"Can not delete non-existing finalized feature: '${update.feature}'\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNTcxOA==", "bodyText": "A value < 1 is indicative of a deletion request (a kind of downgrade request).\nIt is for convenience of generating a special error message, that we handle the case here explicitly: ...less than 1 for feature....", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463935718", "createdAt": "2020-08-01T07:44:14Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,185 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+    // NOTE: Below we set the finalized min version level to be the default minimum version\n+    // level. If the finalized feature already exists, then, this can cause deprecation of all\n+    // version levels in the closed range:\n+    // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+    val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+    val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)\n+    val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+      val singleFinalizedFeature =\n+        Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+      BrokerFeatures.hasIncompatibleFeatures(broker.features, singleFinalizedFeature)\n+    })\n+    if (numIncompatibleBrokers == 0) {\n+      Left(newVersionRange)\n+    } else {\n+      Right(\n+        new ApiError(Errors.INVALID_REQUEST,\n+                     s\"Could not apply finalized feature update because $numIncompatibleBrokers\" +\n+                     \" brokers were found to have incompatible features.\"))\n+    }\n+  }\n+\n+  /**\n+   * Validate and process a finalized feature update on an existing FinalizedVersionRange for the\n+   * feature.\n+   *\n+   * If the processing is successful, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the processing failed, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def processFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             s\"Can not delete non-existing finalized feature: '${update.feature}'\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0OTg5NQ=="}, "originalCommit": null, "originalPosition": 356}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODQ2Mzg5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMzozOTo0NFrOG5N75w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMzozOTo0NFrOG5N75w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MDM0Mw==", "bodyText": "We should be consistent and remove () from maxVersionLevel", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462650343", "createdAt": "2020-07-29T23:39:44Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,185 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+    // NOTE: Below we set the finalized min version level to be the default minimum version\n+    // level. If the finalized feature already exists, then, this can cause deprecation of all\n+    // version levels in the closed range:\n+    // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+    val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+    val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)\n+    val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+      val singleFinalizedFeature =\n+        Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+      BrokerFeatures.hasIncompatibleFeatures(broker.features, singleFinalizedFeature)\n+    })\n+    if (numIncompatibleBrokers == 0) {\n+      Left(newVersionRange)\n+    } else {\n+      Right(\n+        new ApiError(Errors.INVALID_REQUEST,\n+                     s\"Could not apply finalized feature update because $numIncompatibleBrokers\" +\n+                     \" brokers were found to have incompatible features.\"))\n+    }\n+  }\n+\n+  /**\n+   * Validate and process a finalized feature update on an existing FinalizedVersionRange for the\n+   * feature.\n+   *\n+   * If the processing is successful, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the processing failed, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def processFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             s\"Can not delete non-existing finalized feature: '${update.feature}'\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 for feature: '${update.feature}' without setting the\" +\n+                           \" allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                s\" a finalized feature: '${update.feature}' from existing\" +\n+                s\" maxVersionLevel:${existing.max} to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not downgrade finalized feature: '${update.feature}' from\" +\n+                s\" existing maxVersionLevel:${existing.max} to provided\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"When finalized feature: '${update.feature}' has the allowDowngrade\" +\n+                \" flag set in the request, the provided\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel() < existing.min) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 385}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODQ3MDIxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMzo0Mjo1MFrOG5N_aQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMzo0Mjo1MFrOG5N_aQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MTI0MQ==", "bodyText": "nit: new line", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462651241", "createdAt": "2020-07-29T23:42:50Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,185 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 298}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODUxNTg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMDowNTozNlrOG5Oaag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwNzo0ODo0NFrOG6caMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1ODE1NA==", "bodyText": "Could you clarify the reasoning here? If structs are not the same, are we going to do a partial update?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462658154", "createdAt": "2020-07-30T00:05:36Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,185 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+    // NOTE: Below we set the finalized min version level to be the default minimum version\n+    // level. If the finalized feature already exists, then, this can cause deprecation of all\n+    // version levels in the closed range:\n+    // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+    val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+    val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)\n+    val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+      val singleFinalizedFeature =\n+        Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+      BrokerFeatures.hasIncompatibleFeatures(broker.features, singleFinalizedFeature)\n+    })\n+    if (numIncompatibleBrokers == 0) {\n+      Left(newVersionRange)\n+    } else {\n+      Right(\n+        new ApiError(Errors.INVALID_REQUEST,\n+                     s\"Could not apply finalized feature update because $numIncompatibleBrokers\" +\n+                     \" brokers were found to have incompatible features.\"))\n+    }\n+  }\n+\n+  /**\n+   * Validate and process a finalized feature update on an existing FinalizedVersionRange for the\n+   * feature.\n+   *\n+   * If the processing is successful, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the processing failed, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def processFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             s\"Can not delete non-existing finalized feature: '${update.feature}'\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 for feature: '${update.feature}' without setting the\" +\n+                           \" allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                s\" a finalized feature: '${update.feature}' from existing\" +\n+                s\" maxVersionLevel:${existing.max} to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not downgrade finalized feature: '${update.feature}' from\" +\n+                s\" existing maxVersionLevel:${existing.max} to provided\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"When finalized feature: '${update.feature}' has the allowDowngrade\" +\n+                \" flag set in the request, the provided\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel() < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not downgrade finalized feature: '${update.feature}' to\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} because it's lower than\" +\n+                s\" the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      val results = request.data().featureUpdates().asScala.map {\n+        update => update.feature() -> new ApiError(Errors.NOT_CONTROLLER)\n+      }.toMap\n+      callback(results)\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // Map of feature to FinalizedVersionRange. This contains the target features to be eventually\n+    // written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // Map of feature to error.\n+    var errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Process each FeatureUpdate.\n+    // If a FeatureUpdate is found to be valid, then the corresponding entry in errors would contain\n+    // Errors.NONE. Otherwise the entry would contain the appropriate error.\n+    updates.asScala.iterator.foreach { update =>\n+      processFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone\n+            .map(newVersionRange => targetFeatures += (update.feature() -> newVersionRange))\n+            .getOrElse(targetFeatures -= update.feature())\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    if (existingFeatures.equals(targetFeatures)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 439}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNjA0OA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463936048", "createdAt": "2020-08-01T07:48:44Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1844,185 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns a suitable error.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+    // NOTE: Below we set the finalized min version level to be the default minimum version\n+    // level. If the finalized feature already exists, then, this can cause deprecation of all\n+    // version levels in the closed range:\n+    // [existingVersionRange.min(), defaultMinVersionLevel - 1].\n+    val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+    val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)\n+    val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+      val singleFinalizedFeature =\n+        Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+      BrokerFeatures.hasIncompatibleFeatures(broker.features, singleFinalizedFeature)\n+    })\n+    if (numIncompatibleBrokers == 0) {\n+      Left(newVersionRange)\n+    } else {\n+      Right(\n+        new ApiError(Errors.INVALID_REQUEST,\n+                     s\"Could not apply finalized feature update because $numIncompatibleBrokers\" +\n+                     \" brokers were found to have incompatible features.\"))\n+    }\n+  }\n+\n+  /**\n+   * Validate and process a finalized feature update on an existing FinalizedVersionRange for the\n+   * feature.\n+   *\n+   * If the processing is successful, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the processing failed, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def processFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             s\"Can not delete non-existing finalized feature: '${update.feature}'\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 for feature: '${update.feature}' without setting the\" +\n+                           \" allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                s\" a finalized feature: '${update.feature}' from existing\" +\n+                s\" maxVersionLevel:${existing.max} to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not downgrade finalized feature: '${update.feature}' from\" +\n+                s\" existing maxVersionLevel:${existing.max} to provided\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"When finalized feature: '${update.feature}' has the allowDowngrade\" +\n+                \" flag set in the request, the provided\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel() < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+              s\"Can not downgrade finalized feature: '${update.feature}' to\" +\n+                s\" maxVersionLevel:${update.maxVersionLevel} because it's lower than\" +\n+                s\" the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      val results = request.data().featureUpdates().asScala.map {\n+        update => update.feature() -> new ApiError(Errors.NOT_CONTROLLER)\n+      }.toMap\n+      callback(results)\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // Map of feature to FinalizedVersionRange. This contains the target features to be eventually\n+    // written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // Map of feature to error.\n+    var errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Process each FeatureUpdate.\n+    // If a FeatureUpdate is found to be valid, then the corresponding entry in errors would contain\n+    // Errors.NONE. Otherwise the entry would contain the appropriate error.\n+    updates.asScala.iterator.foreach { update =>\n+      processFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone\n+            .map(newVersionRange => targetFeatures += (update.feature() -> newVersionRange))\n+            .getOrElse(targetFeatures -= update.feature())\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    if (existingFeatures.equals(targetFeatures)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1ODE1NA=="}, "originalCommit": null, "originalPosition": 439}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODg5MjEzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzozNDozNFrOG5R1pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwNzo0OToyOFrOG6caYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNDI3OA==", "bodyText": "Could we get a static method instead of initiating a new FinalizedVersionRange for a comparison every time?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462714278", "createdAt": "2020-07-30T03:34:34Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this latest_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean external clients of Kafka\n+ *    would need to stop using the finalized min version levels that have been deprecated.\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features. This class is immutable in production. It provides few APIs to\n+ * mutate state only for the purpose of testing.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be incompatible.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, logIncompatibilities = true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibleFeaturesInfo = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) =>\n+        val supportedVersions = supportedFeatures.get(feature)\n+        if (supportedVersions == null) {\n+          (feature, versionLevels, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, versionLevels, \"{feature=%s, reason='%s is incompatible with %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, versionLevels, null)\n+        }\n+    }.filter{ case(_, _, errorReason) => errorReason != null}.toList\n+\n+    if (logIncompatibilities && incompatibleFeaturesInfo.nonEmpty) {\n+      warn(\n+        \"Feature incompatibilities seen: \" + incompatibleFeaturesInfo.map {\n+          case(_, _, errorReason) => errorReason })\n+    }\n+    Features.finalizedFeatures(incompatibleFeaturesInfo.map {\n+      case(feature, versionLevels, _) => (feature, versionLevels) }.toMap.asJava)\n+  }\n+\n+  /**\n+   * A check that ensures each feature defined with min version level is a supported feature, and\n+   * the min version level value is valid (i.e. it is compatible with the supported version range).\n+   *\n+   * @param supportedFeatures         the supported features\n+   * @param featureMinVersionLevels   the feature minimum version levels\n+   *\n+   * @return                          - true, if the above described check passes.\n+   *                                  - false, otherwise.\n+   */\n+  private def areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures: Features[SupportedVersionRange],\n+    featureMinVersionLevels: Map[String, Short]\n+  ): Boolean = {\n+    featureMinVersionLevels.forall {\n+      case(featureName, minVersionLevel) =>\n+        val supportedFeature = supportedFeatures.get(featureName)\n+        (supportedFeature != null) &&\n+          !new FinalizedVersionRange(minVersionLevel, supportedFeature.max())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNjA5OA==", "bodyText": "Existing approach is equally readable too. I'd rather leave it this way.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463936098", "createdAt": "2020-08-01T07:49:28Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this latest_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    ApiKeys.UPDATE_FINALIZED_FEATURES api). This will automatically mean external clients of Kafka\n+ *    would need to stop using the finalized min version levels that have been deprecated.\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features. This class is immutable in production. It provides few APIs to\n+ * mutate state only for the purpose of testing.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be incompatible.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, logIncompatibilities = true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibleFeaturesInfo = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) =>\n+        val supportedVersions = supportedFeatures.get(feature)\n+        if (supportedVersions == null) {\n+          (feature, versionLevels, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, versionLevels, \"{feature=%s, reason='%s is incompatible with %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, versionLevels, null)\n+        }\n+    }.filter{ case(_, _, errorReason) => errorReason != null}.toList\n+\n+    if (logIncompatibilities && incompatibleFeaturesInfo.nonEmpty) {\n+      warn(\n+        \"Feature incompatibilities seen: \" + incompatibleFeaturesInfo.map {\n+          case(_, _, errorReason) => errorReason })\n+    }\n+    Features.finalizedFeatures(incompatibleFeaturesInfo.map {\n+      case(feature, versionLevels, _) => (feature, versionLevels) }.toMap.asJava)\n+  }\n+\n+  /**\n+   * A check that ensures each feature defined with min version level is a supported feature, and\n+   * the min version level value is valid (i.e. it is compatible with the supported version range).\n+   *\n+   * @param supportedFeatures         the supported features\n+   * @param featureMinVersionLevels   the feature minimum version levels\n+   *\n+   * @return                          - true, if the above described check passes.\n+   *                                  - false, otherwise.\n+   */\n+  private def areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures: Features[SupportedVersionRange],\n+    featureMinVersionLevels: Map[String, Short]\n+  ): Boolean = {\n+    featureMinVersionLevels.forall {\n+      case(featureName, minVersionLevel) =>\n+        val supportedFeature = supportedFeatures.get(featureName)\n+        (supportedFeature != null) &&\n+          !new FinalizedVersionRange(minVersionLevel, supportedFeature.max())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNDI3OA=="}, "originalCommit": null, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODg5OTU4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzozOTowN1rOG5R56A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwNzo1NDowNVrOG6cbnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNTM2OA==", "bodyText": "Why don't we just use System.currentTimeMillis() to avoid conversion between nano time?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462715368", "createdAt": "2020-07-30T03:39:07Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -82,18 +108,54 @@ object FinalizedFeatureCache extends Logging {\n         \" The existing cache contents are %s\").format(latest, oldFeatureAndEpoch)\n       throw new FeatureCacheUpdateException(errorMsg)\n     } else {\n-      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      val incompatibleFeatures = brokerFeatures.incompatibleFeatures(latest.features)\n       if (!incompatibleFeatures.empty) {\n         val errorMsg = (\"FinalizedFeatureCache update failed since feature compatibility\" +\n           \" checks failed! Supported %s has incompatibilities with the latest %s.\"\n-          ).format(SupportedFeatures.get, latest)\n+          ).format(brokerFeatures.supportedFeatures, latest)\n         throw new FeatureCacheUpdateException(errorMsg)\n       } else {\n-        val logMsg = \"Updated cache from existing finalized %s to latest finalized %s\".format(\n+        val logMsg = \"Updated cache from existing %s to latest %s\".format(\n           oldFeatureAndEpoch, latest)\n-        featuresAndEpoch = Some(latest)\n+        synchronized {\n+          featuresAndEpoch = Some(latest)\n+          notifyAll()\n+        }\n         info(logMsg)\n       }\n     }\n   }\n+\n+  /**\n+   * Causes the current thread to wait no more than timeoutMs for the specified condition to be met.\n+   * It is guaranteed that the provided condition will always be invoked only from within a\n+   * synchronized block.\n+   *\n+   * @param waitCondition   the condition to be waited upon:\n+   *                         - if the condition returns true, then, the wait will stop.\n+   *                         - if the condition returns false, it means the wait must continue until\n+   *                           timeout.\n+   *\n+   * @param timeoutMs       the timeout (in milli seconds)\n+   *\n+   * @throws                TimeoutException if the condition is not met within timeoutMs.\n+   */\n+  private def waitUntilConditionOrThrow(waitCondition: () => Boolean, timeoutMs: Long): Unit = {\n+    if(timeoutMs < 0L) {\n+      throw new IllegalArgumentException(s\"Expected timeoutMs >= 0, but $timeoutMs was provided.\")\n+    }\n+    val waitEndTimeNanos = System.nanoTime() + (timeoutMs * 1_000_000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNjQxMg==", "bodyText": "Since the app depends on monotonically increasing elapsed time values, System.nanoTime() is preferred.\nSystem.currentTimeMillis() can change due to daylight saving time, users changing the time settings, leap seconds, and internet time sync etc.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463936412", "createdAt": "2020-08-01T07:54:05Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -82,18 +108,54 @@ object FinalizedFeatureCache extends Logging {\n         \" The existing cache contents are %s\").format(latest, oldFeatureAndEpoch)\n       throw new FeatureCacheUpdateException(errorMsg)\n     } else {\n-      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      val incompatibleFeatures = brokerFeatures.incompatibleFeatures(latest.features)\n       if (!incompatibleFeatures.empty) {\n         val errorMsg = (\"FinalizedFeatureCache update failed since feature compatibility\" +\n           \" checks failed! Supported %s has incompatibilities with the latest %s.\"\n-          ).format(SupportedFeatures.get, latest)\n+          ).format(brokerFeatures.supportedFeatures, latest)\n         throw new FeatureCacheUpdateException(errorMsg)\n       } else {\n-        val logMsg = \"Updated cache from existing finalized %s to latest finalized %s\".format(\n+        val logMsg = \"Updated cache from existing %s to latest %s\".format(\n           oldFeatureAndEpoch, latest)\n-        featuresAndEpoch = Some(latest)\n+        synchronized {\n+          featuresAndEpoch = Some(latest)\n+          notifyAll()\n+        }\n         info(logMsg)\n       }\n     }\n   }\n+\n+  /**\n+   * Causes the current thread to wait no more than timeoutMs for the specified condition to be met.\n+   * It is guaranteed that the provided condition will always be invoked only from within a\n+   * synchronized block.\n+   *\n+   * @param waitCondition   the condition to be waited upon:\n+   *                         - if the condition returns true, then, the wait will stop.\n+   *                         - if the condition returns false, it means the wait must continue until\n+   *                           timeout.\n+   *\n+   * @param timeoutMs       the timeout (in milli seconds)\n+   *\n+   * @throws                TimeoutException if the condition is not met within timeoutMs.\n+   */\n+  private def waitUntilConditionOrThrow(waitCondition: () => Boolean, timeoutMs: Long): Unit = {\n+    if(timeoutMs < 0L) {\n+      throw new IllegalArgumentException(s\"Expected timeoutMs >= 0, but $timeoutMs was provided.\")\n+    }\n+    val waitEndTimeNanos = System.nanoTime() + (timeoutMs * 1_000_000)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNTM2OA=="}, "originalCommit": null, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODkwNDIzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo0MTozNFrOG5R8iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo0MTozNFrOG5R8iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNjA0MA==", "bodyText": "Could be moved to UpdateFeaturesResponse as a utility.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462716040", "createdAt": "2020-07-30T03:41:34Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2956,6 +2959,37 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFeaturesRequest = request.body[UpdateFeaturesRequest]\n+    def featureUpdateErrors(error: Errors, msgOverride: Option[String]): Map[String, ApiError] = {\n+      updateFeaturesRequest.data().featureUpdates().asScala.map(\n+        update => update.feature() -> new ApiError(error, msgOverride.getOrElse(error.message()))\n+      ).toMap\n+    }\n+\n+    def sendResponseCallback(updateErrors: Map[String, ApiError]): Unit = {\n+      val results = new UpdatableFeatureResultCollection()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODkwOTc1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo0NDo1NlrOG5R_yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo0NDo1NlrOG5R_yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNjg3NQ==", "bodyText": "Some methods in the BrokerFeatures are not covered by this suite, such as defaultMinVersionLevel, getDefaultFinalizedFeatures and hasIncompatibleFeatures, you could use code coverage tool to figure out any missing part.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462716875", "createdAt": "2020-07-30T03:44:56Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,84 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class BrokerFeaturesTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODkxMTIyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo0NTo0NVrOG5SAmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo0NTo0NVrOG5SAmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxNzA4Mw==", "bodyText": "Indentation is not right.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462717083", "createdAt": "2020-07-30T03:45:45Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -78,25 +76,42 @@ class FinalizedFeatureChangeListenerTest extends ZooKeeperTestHarness {\n   /**\n    * Tests that the listener can be initialized, and that it can listen to ZK notifications\n    * successfully from an \"Enabled\" FeatureZNode (the ZK data has no feature incompatibilities).\n+   * Particularly the test checks if multiple notifications can be processed in ZK\n+   * (i.e. whether the FeatureZNode watch can be re-established).\n    */\n   @Test\n   def testInitSuccessAndNotificationSuccess(): Unit = {\n-    createSupportedFeatures()\n     val initialFinalizedFeatures = createFinalizedFeatures()\n-    val listener = createListener(Some(initialFinalizedFeatures))\n+    val brokerFeatures = createBrokerFeatures()\n+    val cache = new FinalizedFeatureCache(brokerFeatures)\n+    val listener = createListener(cache, Some(initialFinalizedFeatures))\n \n-    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n-      \"feature_1\" -> new FinalizedVersionRange(2, 4))\n-    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n-    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, updatedFinalizedFeatures))\n-    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n-    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n-    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n-    assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n-    TestUtils.waitUntilTrue(() => {\n-      FinalizedFeatureCache.get.get.equals(FinalizedFeaturesAndEpoch(updatedFinalizedFeatures, updatedVersion))\n-    }, \"Timed out waiting for FinalizedFeatureCache to be updated with new features\")\n-    assertTrue(listener.isListenerInitiated)\n+    def updateAndCheckCache(finalizedFeatures: Features[FinalizedVersionRange]): Unit = {\n+      zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, finalizedFeatures))\n+      val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+      assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+      assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+      assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n+\n+      cache.waitUntilEpochOrThrow(updatedVersion, JTestUtils.DEFAULT_MAX_WAIT_MS)\n+      assertEquals(FinalizedFeaturesAndEpoch(finalizedFeatures, updatedVersion), cache.get.get)\n+      assertTrue(listener.isListenerInitiated)\n+    }\n+\n+    // Check if the write succeeds and a ZK notification is received that causes the feature cache\n+    // to be populated.\n+    updateAndCheckCache(\n+      Features.finalizedFeatures(\n+        Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 4)).asJava))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODkxOTI3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo1MDo0NFrOG5SFMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo1MDo0NFrOG5SFMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxODI1OA==", "bodyText": "The meta comment for FinalizedFeatureCache should be updated as it is now being accessed for both read and write", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462718258", "createdAt": "2020-07-30T03:50:44Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -39,7 +42,7 @@ case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange],\n  *\n  * @see FinalizedFeatureChangeListener\n  */\n-object FinalizedFeatureCache extends Logging {\n+class FinalizedFeatureCache(private val brokerFeatures: BrokerFeatures) extends Logging {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODkyNDQ1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo1NDowOFrOG5SIMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwODowNDozNVrOG6ceoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxOTAyNw==", "bodyText": "nit: this could be extracted as a common struct.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462719027", "createdAt": "2020-07-30T03:54:08Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,550 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevelsInAllBrokers(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(cause.getMessage, exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,\n+                                                                       invalidUpdate: FeatureUpdate,\n+                                                                       exceptionMsgPattern: Regex)\n+                                                                      (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(Utils.mkMap(Utils.mkEntry(feature, invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, Map(feature -> exceptionMsgPattern))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val updates = new FeatureUpdateKeyCollection()\n+    val update = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    update.setFeature(\"feature_1\");\n+    update.setMaxVersionLevel(defaultSupportedFeatures().get(\"feature_1\").max())\n+    update.setAllowDowngrade(false)\n+    updates.add(update)\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(updates)).build(),\n+      notControllerSocketServer)\n+\n+    assertEquals(1, response.data.results.size)\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.NOT_CONTROLLER, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForEmptyUpdates(): Unit = {\n+    val nullMap: util.Map[String, FeatureUpdate] = null\n+    val emptyMap: util.Map[String, FeatureUpdate] = Utils.mkMap()\n+    Set(nullMap, emptyMap).foreach { updates =>\n+      val client = createAdminClient()\n+      val exception = intercept[IllegalArgumentException] {\n+        client.updateFeatures(updates, new UpdateFeaturesOptions())\n+      }\n+      assertNotNull(exception)\n+      assertEquals(\"Feature updates can not be null or empty.\", exception.getMessage)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForNullUpdateFeaturesOptions(): Unit = {\n+    val client = createAdminClient()\n+    val update = new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), false)\n+    val exception = intercept[NullPointerException] {\n+      client.updateFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", update)), null)\n+    }\n+    assertNotNull(exception)\n+    assertEquals(\"UpdateFeaturesOptions can not be null\", exception.getMessage)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForInvalidFeatureName(): Unit = {\n+    val client = createAdminClient()\n+    val update = new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), false)\n+    val exception = intercept[IllegalArgumentException] {\n+      client.updateFeatures(Utils.mkMap(Utils.mkEntry(\"\", update)), new UpdateFeaturesOptions())\n+    }\n+    assertNotNull(exception)\n+    assertTrue((\".*Provided feature can not be null or empty.*\"r).findFirstIn(exception.getMessage).isDefined)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate((defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short],false),\n+      \".*Can not downgrade finalized feature: 'feature_1'.*allowDowngrade.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), true),\n+      \".*finalized feature: 'feature_1'.*allowDowngrade.* provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInClientWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    assertThrows[IllegalArgumentException] {\n+      new FeatureUpdate(0, false)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val updates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val update = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    update.setFeature(\"feature_1\")\n+    update.setMaxVersionLevel(0)\n+    update.setAllowDowngrade(false)\n+    updates.add(update);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(updates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(updates)).build(),\n+      controllerSocketServer)\n+\n+    assertEquals(1, response.data().results().size())\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.INVALID_REQUEST, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    val exceptionMsgPattern = \".*Can not provide maxVersionLevel: 0 less than 1 for feature: 'feature_1'.*allowDowngrade.*\".r\n+    assertTrue(exceptionMsgPattern.findFirstIn(result.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_non_existing\",\n+      new FeatureUpdate(0, true),\n+      \".*Can not delete non-existing finalized feature: 'feature_non_existing'.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenUpgradingToSameVersionLevel(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(defaultFinalizedFeatures().get(\"feature_1\").max(), false),\n+      \".*Can not upgrade a finalized feature: 'feature_1'.*to the same value.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradingBelowMinVersionLevel(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val minVersionLevel = 2.asInstanceOf[Short]\n+    updateDefaultMinVersionLevelsInAllBrokers(Map[String, Short](\"feature_1\" -> minVersionLevel))\n+    val initialFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(minVersionLevel, 2))))\n+    val versionBefore = updateFeatureZNode(initialFinalizedFeatures)\n+\n+    val update = new FeatureUpdate((minVersionLevel - 1).asInstanceOf[Short], true)\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", update)), new UpdateFeaturesOptions())\n+\n+    checkException[InvalidRequestException](\n+      result,\n+      Map(\"feature_1\" -> \".*Can not downgrade finalized feature: 'feature_1' to maxVersionLevel:1.*existing minVersionLevel:2.*\".r))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(initialFinalizedFeatures, versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringBrokerMaxVersionLevelIncompatibility(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val controller = servers.filter { server => server.kafkaController.isActive}.head\n+    val nonControllerServers = servers.filter { server => !server.kafkaController.isActive}\n+    val unsupportedBrokers = Set[KafkaServer](nonControllerServers.head)\n+    val supportedBrokers = Set[KafkaServer](nonControllerServers(1), controller)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures(), supportedBrokers)\n+\n+    val validMinVersion = defaultSupportedFeatures().get(\"feature_1\").min()\n+    val unsupportedMaxVersion =\n+      (defaultSupportedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short]\n+    val badSupportedFeatures = Features.supportedFeatures(\n+      Utils.mkMap(\n+        Utils.mkEntry(\"feature_1\",\n+          new SupportedVersionRange(\n+            validMinVersion,\n+            unsupportedMaxVersion))))\n+    updateSupportedFeatures(badSupportedFeatures, unsupportedBrokers)\n+\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val invalidUpdate = new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), false)\n+    val nodeBefore = getFeatureZNode()\n+    val adminClient = createAdminClient()\n+    val result = adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", invalidUpdate)),\n+      new UpdateFeaturesOptions())\n+\n+    checkException[InvalidRequestException](result, Map(\"feature_1\" -> \".*1 broker.*incompatible.*\".r))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testSuccessfulFeatureUpgradeAndWithNoExistingFinalizedFeatures(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+    updateDefaultMinVersionLevelsInAllBrokers(Map[String, Short](\"feature_1\" -> 1, \"feature_2\" -> 2))\n+    val versionBefore = updateFeatureZNode(Features.emptyFinalizedFeatures())\n+\n+    val targetFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(\n+        Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 3)),\n+        Utils.mkEntry(\"feature_2\", new FinalizedVersionRange(2, 3))))\n+    val update1 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_1\").max(), false)\n+    val update2 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_2\").max(), false)\n+\n+    val expected = new FeatureMetadata(\n+      targetFinalizedFeatures,\n+      versionBefore + 1,\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+\n+    val adminClient = createAdminClient()\n+    adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", update1), Utils.mkEntry(\"feature_2\", update2)),\n+      new UpdateFeaturesOptions()\n+    ).all().get()\n+\n+    checkFeatures(\n+      adminClient,\n+      new FeatureZNode(FeatureZNodeStatus.Enabled, targetFinalizedFeatures),\n+      expected)\n+  }\n+\n+  @Test\n+  def testSuccessfulFeatureUpgradeAndDowngrade(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+    updateDefaultMinVersionLevelsInAllBrokers(Map[String, Short](\"feature_1\" -> 1, \"feature_2\" -> 2))\n+    val versionBefore = updateFeatureZNode(\n+      Features.finalizedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2)),\n+          Utils.mkEntry(\"feature_2\", new FinalizedVersionRange(2, 4)))))\n+\n+    val targetFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(\n+        Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 3)),\n+        Utils.mkEntry(\"feature_2\", new FinalizedVersionRange(2, 3))))\n+    val update1 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_1\").max(), false)\n+    val update2 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_2\").max(), true)\n+\n+    val expected = new FeatureMetadata(\n+      targetFinalizedFeatures,\n+      versionBefore + 1,\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+\n+    val adminClient = createAdminClient()\n+    adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", update1), Utils.mkEntry(\"feature_2\", update2)),\n+      new UpdateFeaturesOptions()\n+    ).all().get()\n+\n+    checkFeatures(\n+      adminClient,\n+      new FeatureZNode(FeatureZNodeStatus.Enabled, targetFinalizedFeatures),\n+      expected)\n+  }\n+\n+  @Test\n+  def testPartialSuccessDuringValidFeatureUpgradeAndInvalidDowngrade(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val initialSupportedFeatures = Features.supportedFeatures(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 457}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNzE4NA==", "bodyText": "Just 2 occurrences (one in this test and other in the next test). I'd leave it the way it is as the test is readable with values inlined in the test body.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463937184", "createdAt": "2020-08-01T08:04:35Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,550 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.ExecutionException\n+\n+import kafka.api.KAFKA_2_7_IV0\n+import kafka.utils.TestUtils\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion}\n+import kafka.utils.TestUtils.waitUntilTrue\n+import org.apache.kafka.clients.admin.{Admin, DescribeFeaturesOptions, FeatureMetadata, FeatureUpdate, UpdateFeaturesOptions, UpdateFeaturesResult}\n+import org.apache.kafka.common.errors.InvalidRequestException\n+import org.apache.kafka.common.feature.FinalizedVersionRange\n+import org.apache.kafka.common.feature.{Features, SupportedVersionRange}\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData.FeatureUpdateKeyCollection\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{UpdateFeaturesRequest, UpdateFeaturesResponse}\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.Test\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertNotNull, assertTrue}\n+import org.scalatest.Assertions.{assertThrows, intercept}\n+\n+import scala.jdk.CollectionConverters._\n+import scala.reflect.ClassTag\n+import scala.util.matching.Regex\n+\n+class UpdateFeaturesTest extends BaseRequestTest {\n+\n+  override def brokerCount = 3\n+\n+  override def brokerPropertyOverrides(props: Properties): Unit = {\n+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_2_7_IV0.toString)\n+  }\n+\n+  private def defaultSupportedFeatures(): Features[SupportedVersionRange] = {\n+    Features.supportedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3))))\n+  }\n+\n+  private def defaultFinalizedFeatures(): Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2))))\n+  }\n+\n+  private def updateSupportedFeatures(\n+    features: Features[SupportedVersionRange], targetServers: Set[KafkaServer]): Unit = {\n+    targetServers.foreach(s => {\n+      s.brokerFeatures.setSupportedFeatures(features)\n+      s.zkClient.updateBrokerInfo(s.createBrokerInfo)\n+    })\n+\n+    // Wait until updates to all BrokerZNode supported features propagate to the controller.\n+    val brokerIds = targetServers.map(s => s.config.brokerId)\n+    waitUntilTrue(\n+      () => servers.exists(s => {\n+        if (s.kafkaController.isActive) {\n+          s.kafkaController.controllerContext.liveOrShuttingDownBrokers\n+            .filter(b => brokerIds.contains(b.id))\n+            .forall(b => {\n+              b.features.equals(features)\n+            })\n+        } else {\n+          false\n+        }\n+      }),\n+      \"Controller did not get broker updates\")\n+  }\n+\n+  private def updateSupportedFeaturesInAllBrokers(features: Features[SupportedVersionRange]): Unit = {\n+    updateSupportedFeatures(features, Set[KafkaServer]() ++ servers)\n+  }\n+\n+  private def updateDefaultMinVersionLevelsInAllBrokers(newMinVersionLevels: Map[String, Short]): Unit = {\n+    servers.foreach(s => {\n+      s.brokerFeatures.setDefaultMinVersionLevels(newMinVersionLevels)\n+    })\n+  }\n+\n+  private def updateFeatureZNode(features: Features[FinalizedVersionRange]): Int = {\n+    val server = serverForId(0).get\n+    val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, features)\n+    val newVersion = server.zkClient.updateFeatureZNode(newNode)\n+    servers.foreach(s => {\n+      s.featureCache.waitUntilEpochOrThrow(newVersion, s.config.zkConnectionTimeoutMs)\n+    })\n+    newVersion\n+  }\n+\n+  private def getFeatureZNode(): FeatureZNode = {\n+    val (mayBeFeatureZNodeBytes, version) = serverForId(0).get.zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+  }\n+\n+  private def checkFeatures(client: Admin, expectedNode: FeatureZNode, expectedMetadata: FeatureMetadata): Unit = {\n+    assertEquals(expectedNode, getFeatureZNode())\n+    val featureMetadata = client.describeFeatures(\n+      new DescribeFeaturesOptions().sendRequestToController(true)).featureMetadata().get()\n+    assertEquals(expectedMetadata, featureMetadata)\n+  }\n+\n+  private def checkException[ExceptionType <: Throwable](result: UpdateFeaturesResult,\n+                                                         featureExceptionMsgPatterns: Map[String, Regex])\n+                                                        (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    featureExceptionMsgPatterns.foreach {\n+      case (feature, exceptionMsgPattern) =>\n+        val exception = intercept[ExecutionException] {\n+          result.values().get(feature).get()\n+        }\n+        val cause = exception.getCause\n+        assertNotNull(cause)\n+        assertEquals(cause.getClass, tag.runtimeClass)\n+        assertTrue(cause.getMessage, exceptionMsgPattern.findFirstIn(cause.getMessage).isDefined)\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests whether an invalid feature update does not get processed on the server as expected,\n+   * and raises the ExceptionType on the client side as expected.\n+   *\n+   * @param invalidUpdate         the invalid feature update to be sent in the\n+   *                              updateFeatures request to the server\n+   * @param exceptionMsgPattern   a pattern for the expected exception message\n+   */\n+  private def testWithInvalidFeatureUpdate[ExceptionType <: Throwable](feature: String,\n+                                                                       invalidUpdate: FeatureUpdate,\n+                                                                       exceptionMsgPattern: Regex)\n+                                                                      (implicit tag: ClassTag[ExceptionType]): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(Utils.mkMap(Utils.mkEntry(feature, invalidUpdate)), new UpdateFeaturesOptions())\n+\n+    checkException[ExceptionType](result, Map(feature -> exceptionMsgPattern))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestIfNotController(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val nodeBefore = getFeatureZNode()\n+    val updates = new FeatureUpdateKeyCollection()\n+    val update = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    update.setFeature(\"feature_1\");\n+    update.setMaxVersionLevel(defaultSupportedFeatures().get(\"feature_1\").max())\n+    update.setAllowDowngrade(false)\n+    updates.add(update)\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(updates)).build(),\n+      notControllerSocketServer)\n+\n+    assertEquals(1, response.data.results.size)\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.NOT_CONTROLLER, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    checkFeatures(\n+      createAdminClient(),\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForEmptyUpdates(): Unit = {\n+    val nullMap: util.Map[String, FeatureUpdate] = null\n+    val emptyMap: util.Map[String, FeatureUpdate] = Utils.mkMap()\n+    Set(nullMap, emptyMap).foreach { updates =>\n+      val client = createAdminClient()\n+      val exception = intercept[IllegalArgumentException] {\n+        client.updateFeatures(updates, new UpdateFeaturesOptions())\n+      }\n+      assertNotNull(exception)\n+      assertEquals(\"Feature updates can not be null or empty.\", exception.getMessage)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForNullUpdateFeaturesOptions(): Unit = {\n+    val client = createAdminClient()\n+    val update = new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), false)\n+    val exception = intercept[NullPointerException] {\n+      client.updateFeatures(Utils.mkMap(Utils.mkEntry(\"feature_1\", update)), null)\n+    }\n+    assertNotNull(exception)\n+    assertEquals(\"UpdateFeaturesOptions can not be null\", exception.getMessage)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestForInvalidFeatureName(): Unit = {\n+    val client = createAdminClient()\n+    val update = new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), false)\n+    val exception = intercept[IllegalArgumentException] {\n+      client.updateFeatures(Utils.mkMap(Utils.mkEntry(\"\", update)), new UpdateFeaturesOptions())\n+    }\n+    assertNotNull(exception)\n+    assertTrue((\".*Provided feature can not be null or empty.*\"r).findFirstIn(exception.getMessage).isDefined)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeFlagIsNotSetDuringDowngrade(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate((defaultFinalizedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short],false),\n+      \".*Can not downgrade finalized feature: 'feature_1'.*allowDowngrade.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradeToHigherVersionLevelIsAttempted(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), true),\n+      \".*finalized feature: 'feature_1'.*allowDowngrade.* provided maxVersionLevel:3.*existing maxVersionLevel:2.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInClientWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    assertThrows[IllegalArgumentException] {\n+      new FeatureUpdate(0, false)\n+    }\n+  }\n+\n+  @Test\n+  def testShouldFailRequestInServerWhenDowngradeFlagIsNotSetDuringDeletion(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val updates\n+      = new UpdateFeaturesRequestData.FeatureUpdateKeyCollection();\n+    val update = new UpdateFeaturesRequestData.FeatureUpdateKey();\n+    update.setFeature(\"feature_1\")\n+    update.setMaxVersionLevel(0)\n+    update.setAllowDowngrade(false)\n+    updates.add(update);\n+    val requestData = new UpdateFeaturesRequestData()\n+    requestData.setFeatureUpdates(updates);\n+\n+    val response = connectAndReceive[UpdateFeaturesResponse](\n+      new UpdateFeaturesRequest.Builder(new UpdateFeaturesRequestData().setFeatureUpdates(updates)).build(),\n+      controllerSocketServer)\n+\n+    assertEquals(1, response.data().results().size())\n+    val result = response.data.results.asScala.head\n+    assertEquals(\"feature_1\", result.feature)\n+    assertEquals(Errors.INVALID_REQUEST, Errors.forCode(result.errorCode))\n+    assertNotNull(result.errorMessage)\n+    assertFalse(result.errorMessage.isEmpty)\n+    val exceptionMsgPattern = \".*Can not provide maxVersionLevel: 0 less than 1 for feature: 'feature_1'.*allowDowngrade.*\".r\n+    assertTrue(exceptionMsgPattern.findFirstIn(result.errorMessage).isDefined)\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringDeletionOfNonExistingFeature(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_non_existing\",\n+      new FeatureUpdate(0, true),\n+      \".*Can not delete non-existing finalized feature: 'feature_non_existing'.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenUpgradingToSameVersionLevel(): Unit = {\n+    testWithInvalidFeatureUpdate[InvalidRequestException](\n+      \"feature_1\",\n+      new FeatureUpdate(defaultFinalizedFeatures().get(\"feature_1\").max(), false),\n+      \".*Can not upgrade a finalized feature: 'feature_1'.*to the same value.*\".r)\n+  }\n+\n+  @Test\n+  def testShouldFailRequestWhenDowngradingBelowMinVersionLevel(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(defaultSupportedFeatures())\n+    val minVersionLevel = 2.asInstanceOf[Short]\n+    updateDefaultMinVersionLevelsInAllBrokers(Map[String, Short](\"feature_1\" -> minVersionLevel))\n+    val initialFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(minVersionLevel, 2))))\n+    val versionBefore = updateFeatureZNode(initialFinalizedFeatures)\n+\n+    val update = new FeatureUpdate((minVersionLevel - 1).asInstanceOf[Short], true)\n+    val adminClient = createAdminClient()\n+    val nodeBefore = getFeatureZNode()\n+\n+    val result = adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", update)), new UpdateFeaturesOptions())\n+\n+    checkException[InvalidRequestException](\n+      result,\n+      Map(\"feature_1\" -> \".*Can not downgrade finalized feature: 'feature_1' to maxVersionLevel:1.*existing minVersionLevel:2.*\".r))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(initialFinalizedFeatures, versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testShouldFailRequestDuringBrokerMaxVersionLevelIncompatibility(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val controller = servers.filter { server => server.kafkaController.isActive}.head\n+    val nonControllerServers = servers.filter { server => !server.kafkaController.isActive}\n+    val unsupportedBrokers = Set[KafkaServer](nonControllerServers.head)\n+    val supportedBrokers = Set[KafkaServer](nonControllerServers(1), controller)\n+\n+    updateSupportedFeatures(defaultSupportedFeatures(), supportedBrokers)\n+\n+    val validMinVersion = defaultSupportedFeatures().get(\"feature_1\").min()\n+    val unsupportedMaxVersion =\n+      (defaultSupportedFeatures().get(\"feature_1\").max() - 1).asInstanceOf[Short]\n+    val badSupportedFeatures = Features.supportedFeatures(\n+      Utils.mkMap(\n+        Utils.mkEntry(\"feature_1\",\n+          new SupportedVersionRange(\n+            validMinVersion,\n+            unsupportedMaxVersion))))\n+    updateSupportedFeatures(badSupportedFeatures, unsupportedBrokers)\n+\n+    val versionBefore = updateFeatureZNode(defaultFinalizedFeatures())\n+\n+    val invalidUpdate = new FeatureUpdate(defaultSupportedFeatures().get(\"feature_1\").max(), false)\n+    val nodeBefore = getFeatureZNode()\n+    val adminClient = createAdminClient()\n+    val result = adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", invalidUpdate)),\n+      new UpdateFeaturesOptions())\n+\n+    checkException[InvalidRequestException](result, Map(\"feature_1\" -> \".*1 broker.*incompatible.*\".r))\n+    checkFeatures(\n+      adminClient,\n+      nodeBefore,\n+      new FeatureMetadata(defaultFinalizedFeatures(), versionBefore, defaultSupportedFeatures()))\n+  }\n+\n+  @Test\n+  def testSuccessfulFeatureUpgradeAndWithNoExistingFinalizedFeatures(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+    updateDefaultMinVersionLevelsInAllBrokers(Map[String, Short](\"feature_1\" -> 1, \"feature_2\" -> 2))\n+    val versionBefore = updateFeatureZNode(Features.emptyFinalizedFeatures())\n+\n+    val targetFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(\n+        Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 3)),\n+        Utils.mkEntry(\"feature_2\", new FinalizedVersionRange(2, 3))))\n+    val update1 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_1\").max(), false)\n+    val update2 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_2\").max(), false)\n+\n+    val expected = new FeatureMetadata(\n+      targetFinalizedFeatures,\n+      versionBefore + 1,\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+\n+    val adminClient = createAdminClient()\n+    adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", update1), Utils.mkEntry(\"feature_2\", update2)),\n+      new UpdateFeaturesOptions()\n+    ).all().get()\n+\n+    checkFeatures(\n+      adminClient,\n+      new FeatureZNode(FeatureZNodeStatus.Enabled, targetFinalizedFeatures),\n+      expected)\n+  }\n+\n+  @Test\n+  def testSuccessfulFeatureUpgradeAndDowngrade(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    updateSupportedFeaturesInAllBrokers(\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+    updateDefaultMinVersionLevelsInAllBrokers(Map[String, Short](\"feature_1\" -> 1, \"feature_2\" -> 2))\n+    val versionBefore = updateFeatureZNode(\n+      Features.finalizedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 2)),\n+          Utils.mkEntry(\"feature_2\", new FinalizedVersionRange(2, 4)))))\n+\n+    val targetFinalizedFeatures = Features.finalizedFeatures(\n+      Utils.mkMap(\n+        Utils.mkEntry(\"feature_1\", new FinalizedVersionRange(1, 3)),\n+        Utils.mkEntry(\"feature_2\", new FinalizedVersionRange(2, 3))))\n+    val update1 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_1\").max(), false)\n+    val update2 = new FeatureUpdate(targetFinalizedFeatures.get(\"feature_2\").max(), true)\n+\n+    val expected = new FeatureMetadata(\n+      targetFinalizedFeatures,\n+      versionBefore + 1,\n+      Features.supportedFeatures(\n+        Utils.mkMap(\n+          Utils.mkEntry(\"feature_1\", new SupportedVersionRange(1, 3)),\n+          Utils.mkEntry(\"feature_2\", new SupportedVersionRange(2, 5)))))\n+\n+    val adminClient = createAdminClient()\n+    adminClient.updateFeatures(\n+      Utils.mkMap(Utils.mkEntry(\"feature_1\", update1), Utils.mkEntry(\"feature_2\", update2)),\n+      new UpdateFeaturesOptions()\n+    ).all().get()\n+\n+    checkFeatures(\n+      adminClient,\n+      new FeatureZNode(FeatureZNodeStatus.Enabled, targetFinalizedFeatures),\n+      expected)\n+  }\n+\n+  @Test\n+  def testPartialSuccessDuringValidFeatureUpgradeAndInvalidDowngrade(): Unit = {\n+    TestUtils.waitUntilControllerElected(zkClient)\n+\n+    val initialSupportedFeatures = Features.supportedFeatures(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxOTAyNw=="}, "originalCommit": null, "originalPosition": 457}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4ODkzMDk2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMzo1Nzo1MFrOG5SL6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwODowMjoyNVrOG6ceCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxOTk3Nw==", "bodyText": "Could we only pass in featureCache to reduce the class coupling here? As we already have brokerFeatures as a private parameter, it shouldn't be too hard to set a helper to get supported features.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462719977", "createdAt": "2020-07-30T03:57:50Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -109,7 +109,9 @@ class KafkaApis(val requestChannel: RequestChannel,\n                 brokerTopicStats: BrokerTopicStats,\n                 val clusterId: String,\n                 time: Time,\n-                val tokenManager: DelegationTokenManager) extends Logging {\n+                val tokenManager: DelegationTokenManager,\n+                val brokerFeatures: BrokerFeatures,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkzNzAzMg==", "bodyText": "The FinalizedFeatureCache.getSupportedFeatures API is not the right fit for the cache's public interface (it is quite unrelated to the other public APIs of the cache). I'd rather not pollute the public API there, just for the sake of convenience.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463937032", "createdAt": "2020-08-01T08:02:25Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -109,7 +109,9 @@ class KafkaApis(val requestChannel: RequestChannel,\n                 brokerTopicStats: BrokerTopicStats,\n                 val clusterId: String,\n                 time: Time,\n-                val tokenManager: DelegationTokenManager) extends Logging {\n+                val tokenManager: DelegationTokenManager,\n+                val brokerFeatures: BrokerFeatures,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxOTk3Nw=="}, "originalCommit": null, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODgyMTM2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMToyOToyN1rOG9ofrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOToyOTo0OVrOHXR2pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI3OTc5MA==", "bodyText": "The KIP wiki has AllowDowngrade at the topic level. Could we update that?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467279790", "createdAt": "2020-08-07T21:29:27Z", "author": {"login": "junrao"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",\n+        \"about\": \"The new maximum version level for the finalized feature. A value >= 1 is valid. A value < 1, is special, and can be used to request the deletion of the finalized feature.\"},\n+      {\"name\": \"AllowDowngrade\", \"type\": \"bool\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NDI2OQ==", "bodyText": "I'm missing something. Which lines on the KIP-584 were you referring to? I didn't find any mention of the flag being at the topic level.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468084269", "createdAt": "2020-08-10T18:04:56Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",\n+        \"about\": \"The new maximum version level for the finalized feature. A value >= 1 is valid. A value < 1, is special, and can be used to request the deletion of the finalized feature.\"},\n+      {\"name\": \"AllowDowngrade\", \"type\": \"bool\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI3OTc5MA=="}, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTYyNzgzNw==", "bodyText": "OK. There are a couple of places that this PR is inconsistent with the KIP.\n\nThe KIP has 2 levels of arrays: []FeatureUpdateKey and []FeatureKey. This PR only has one array.\nThe KIP has a timeoutMs field and this PR doesn't.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r471627837", "createdAt": "2020-08-17T17:13:02Z", "author": {"login": "junrao"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",\n+        \"about\": \"The new maximum version level for the finalized feature. A value >= 1 is valid. A value < 1, is special, and can be used to request the deletion of the finalized feature.\"},\n+      {\"name\": \"AllowDowngrade\", \"type\": \"bool\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI3OTc5MA=="}, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE3MTgxNA==", "bodyText": "Done. Fixed the KIP and the code, so that they align with each other now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494171814", "createdAt": "2020-09-24T09:29:49Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\": \"MaxVersionLevel\", \"type\": \"int16\", \"versions\": \"0+\",\n+        \"about\": \"The new maximum version level for the finalized feature. A value >= 1 is valid. A value < 1, is special, and can be used to request the deletion of the finalized feature.\"},\n+      {\"name\": \"AllowDowngrade\", \"type\": \"bool\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI3OTc5MA=="}, "originalCommit": null, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODgzNTkyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMTozMjoyOFrOG9oo4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTozMjoyMVrOHXR81A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4MjE0Nw==", "bodyText": "The KIP wiki doesn't include this field.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467282147", "createdAt": "2020-08-07T21:32:28Z", "author": {"login": "junrao"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"Results\", \"type\": \"[]UpdatableFeatureResult\", \"versions\": \"0+\",\n+      \"about\": \"Results for each feature update.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NTQ0Mw==", "bodyText": "Yes, we changed to have an error code per feature update. I'll update the KIP-584 write up.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468085443", "createdAt": "2020-08-10T18:07:00Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"Results\", \"type\": \"[]UpdatableFeatureResult\", \"versions\": \"0+\",\n+      \"about\": \"Results for each feature update.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4MjE0Nw=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE3MzM5Ng==", "bodyText": "Done. I've updated the KIP-584 write up, please refer to this section in the KIP.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494173396", "createdAt": "2020-09-24T09:32:21Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"Results\", \"type\": \"[]UpdatableFeatureResult\", \"versions\": \"0+\",\n+      \"about\": \"Results for each feature update.\", \"fields\": [\n+      {\"name\": \"Feature\", \"type\": \"string\", \"versions\": \"0+\", \"mapKey\": true,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4MjE0Nw=="}, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODk5OTQ2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMjoxNTo0M1rOG9qScQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxNDozMlrOG-Z6DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwOTE2OQ==", "bodyText": "When we roll the cluster to bump up IBP, it seems that it's possible for status to be enabled and then disabled repeatedly? This can be a bit weird.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467309169", "createdAt": "2020-08-07T22:15:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4OTM1Nw==", "bodyText": "To be sure we are on same page, is this because of a controller failover during an IBP bump?\nIt seems to me that this can happen mainly when IBP is being bumped from a value less than KAFKA_2_7_IV0 to a value greater than or equal to KAFKA_2_7_IV0 (assuming subsequent IBP bumps will be from KAFKA_2_7_IV0 to a higher value, so the node status will remain enabled).\nIn general, I'm not sure how to avoid this node status flip until IBP bump has been completed cluster-wide.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468089357", "createdAt": "2020-08-10T18:14:32Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwOTE2OQ=="}, "originalCommit": null, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxOTAzOTI5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMjo0MDowNVrOG9qq2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoyOTozMlrOG-aZUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxNTQxNw==", "bodyText": "When we roll the cluster to bump up IBP, it seems that it's possible for the min of finalized version to flip repeatedly? This can be a bit weird.\nAlso, it seems that we should set min version based on the largest min version across all brokers?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467315417", "createdAt": "2020-08-07T22:40:05Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Using the change below, we deprecate all version levels in the range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1].\n+              //\n+              // NOTE: if existingVersionRange.min() equals brokerDefaultVersionRange.min(), then\n+              // we do not deprecate any version levels (since there is none to be deprecated).\n+              //\n+              // Examples:\n+              // 1. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [1, 5].\n+              //    In this case, we deprecate all version levels in the range: [1, 3].\n+              // 2. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [4, 5].\n+              //    In this case, we do not deprecate any version levels since\n+              //    brokerDefaultVersionRange.min() equals existingVersionRange.min().\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5NzM2MA==", "bodyText": "When we roll the cluster to bump up IBP, it seems that it's possible for the min of finalized version to flip repeatedly? This can be a bit weird.\n\nTrue, this is possible. Good point. To be sure I understood, are you referring broadly to any future IBP bump? Or specifically are you referring to the IBP bump from a value less than KAFKA_2_7_IV0 to a value greater than or equal to KAFKA_2_7_IV0? (since KAFKA_2_7_IV0 is the IBP where the feature versioning system gets activated)\nTo answer your question, I'm not sure how to avoid the flip. It is to be noted that min version level changes are used only for feature version deprecation. Due to the flipping values, it merely means some version levels would go a few times from deprecated -> available -> deprecated -> available...., until the IBP bump has been completed cluster-wide. I can't (yet) think of a case where the flip is dangerous, since:\n\nWe have this check: https://github.com/apache/kafka/blob/89a3ba69e03acbe9635ee1039abb567bf0c6631b/core/src/main/scala/kafka/server/BrokerFeatures.scala#L47-L48  and\nAs best practice, we can recommend to not change a) minVersion of SupportedFeature as well as b) default minVersionLevel within the same release. The reason being that we typically first deprecate a feature version level before we remove the code to drop support for it i.e. (b) usually has to happen before (a).", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468097360", "createdAt": "2020-08-10T18:29:32Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Using the change below, we deprecate all version levels in the range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1].\n+              //\n+              // NOTE: if existingVersionRange.min() equals brokerDefaultVersionRange.min(), then\n+              // we do not deprecate any version levels (since there is none to be deprecated).\n+              //\n+              // Examples:\n+              // 1. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [1, 5].\n+              //    In this case, we deprecate all version levels in the range: [1, 3].\n+              // 2. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [4, 5].\n+              //    In this case, we do not deprecate any version levels since\n+              //    brokerDefaultVersionRange.min() equals existingVersionRange.min().\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxNTQxNw=="}, "originalCommit": null, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxOTA2NzUwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMjo1NzowOVrOG9q66g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODoyMDozN1rOHXnFTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxOTUzMA==", "bodyText": "Hmm, do we need to do this? If there is an incompatible feature, the broker will realize that and can just shut itself down.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467319530", "createdAt": "2020-08-07T22:57:09Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -977,14 +1179,30 @@ class KafkaController(val config: KafkaConfig,\n \n   /**\n    * Send the leader information for selected partitions to selected brokers so that they can correctly respond to\n-   * metadata requests\n+   * metadata requests. Particularly, when feature versioning is enabled, we filter out brokers with incompatible\n+   * features from receiving the metadata requests. This is because we do not want to activate incompatible brokers,\n+   * as these may have harmful consequences to the cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODEwMTk4Mg==", "bodyText": "Good question. Yes, the broker will shut itself down. But still there is a possible race condition that needs to be handled to prevent an incompatible broker from causing damage to cluster. The race condition is described in the KIP-584 in this section. Please let me know your thoughts.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468101982", "createdAt": "2020-08-10T18:38:18Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -977,14 +1179,30 @@ class KafkaController(val config: KafkaConfig,\n \n   /**\n    * Send the leader information for selected partitions to selected brokers so that they can correctly respond to\n-   * metadata requests\n+   * metadata requests. Particularly, when feature versioning is enabled, we filter out brokers with incompatible\n+   * features from receiving the metadata requests. This is because we do not want to activate incompatible brokers,\n+   * as these may have harmful consequences to the cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxOTUzMA=="}, "originalCommit": null, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwNjM3OA==", "bodyText": "My understanding of the race condition is that the controller finalizes a feature while there is a pending broker registration in the controller event queue. When the controller starts to process the new broker registration, it will realize that its supported feature is not compatible. Here, it's seems that we will still process this new broker registration and only avoid sending UpdatateMetadataRequest to it. I am not sure if this helps since we already acted on this incompatible broker registration and some damage may already be done. The same UpdatateMetadataRequest will still be sent to other brokers and its metadata will be available to the clients.\nAn alternative way is to just skip the handling of new broker registration if it's detected as incompatible.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r471806378", "createdAt": "2020-08-17T22:20:32Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -977,14 +1179,30 @@ class KafkaController(val config: KafkaConfig,\n \n   /**\n    * Send the leader information for selected partitions to selected brokers so that they can correctly respond to\n-   * metadata requests\n+   * metadata requests. Particularly, when feature versioning is enabled, we filter out brokers with incompatible\n+   * features from receiving the metadata requests. This is because we do not want to activate incompatible brokers,\n+   * as these may have harmful consequences to the cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxOTUzMA=="}, "originalCommit": null, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUxOTYzMQ==", "bodyText": "Done. I've changed the code such that we skip the broker registration if it's detected as incompatible.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494519631", "createdAt": "2020-09-24T18:20:37Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -977,14 +1179,30 @@ class KafkaController(val config: KafkaConfig,\n \n   /**\n    * Send the leader information for selected partitions to selected brokers so that they can correctly respond to\n-   * metadata requests\n+   * metadata requests. Particularly, when feature versioning is enabled, we filter out brokers with incompatible\n+   * features from receiving the metadata requests. This is because we do not want to activate incompatible brokers,\n+   * as these may have harmful consequences to the cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxOTUzMA=="}, "originalCommit": null, "originalPosition": 268}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxOTExNjgyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMzoyOToxMVrOG9rWdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODo1NDoxNFrOHXodTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMyNjU4MA==", "bodyText": "If update.maxVersionLevel < defaultMinVersionLevel, we throw an IllegalStateException. Should we catch it and convert it to an error code?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467326580", "createdAt": "2020-08-07T23:29:11Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1865,192 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors#INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val incompatibilityError = \"Could not apply finalized feature update because\" +\n+      \" brokers were found to have incompatible versions for the feature.\"\n+\n+    if (brokerFeatures.supportedFeatures.get(update.feature()) == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST, incompatibilityError))\n+    } else {\n+      val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+      val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 328}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODEwMzIyNA==", "bodyText": "Yes, excellent point. I'll fix this.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468103224", "createdAt": "2020-08-10T18:40:39Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1865,192 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors#INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val incompatibilityError = \"Could not apply finalized feature update because\" +\n+      \" brokers were found to have incompatible versions for the feature.\"\n+\n+    if (brokerFeatures.supportedFeatures.get(update.feature()) == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST, incompatibilityError))\n+    } else {\n+      val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+      val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMyNjU4MA=="}, "originalCommit": null, "originalPosition": 328}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU0MjE1Ng==", "bodyText": "Done. This is fixed now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494542156", "createdAt": "2020-09-24T18:54:14Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1865,192 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors#INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val incompatibilityError = \"Could not apply finalized feature update because\" +\n+      \" brokers were found to have incompatible versions for the feature.\"\n+\n+    if (brokerFeatures.supportedFeatures.get(update.feature()) == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST, incompatibilityError))\n+    } else {\n+      val defaultMinVersionLevel = brokerFeatures.defaultMinVersionLevel(update.feature)\n+      val newVersionRange = new FinalizedVersionRange(defaultMinVersionLevel, update.maxVersionLevel)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMyNjU4MA=="}, "originalCommit": null, "originalPosition": 328}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxOTE0MjU4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMzo0ODowMlrOG9rklQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMjo1MjoxNFrOHXvPQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMzMDE5Nw==", "bodyText": "Since we are doing the compatibility check for every broker, do we need to special case here just for the broker feature on the controller?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467330197", "createdAt": "2020-08-07T23:48:02Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1865,192 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors#INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val incompatibilityError = \"Could not apply finalized feature update because\" +\n+      \" brokers were found to have incompatible versions for the feature.\"\n+\n+    if (brokerFeatures.supportedFeatures.get(update.feature()) == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 324}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODEwOTc5MQ==", "bodyText": "It's required because defaultMinVersionLevel does not exist for a feature that's not in the supported list. However, I'll change the code to make the check more obvious to the reader (currently it's not).", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468109791", "createdAt": "2020-08-10T18:52:51Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1865,192 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors#INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val incompatibilityError = \"Could not apply finalized feature update because\" +\n+      \" brokers were found to have incompatible versions for the feature.\"\n+\n+    if (brokerFeatures.supportedFeatures.get(update.feature()) == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMzMDE5Nw=="}, "originalCommit": null, "originalPosition": 324}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY1MzI1MQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494653251", "createdAt": "2020-09-24T22:52:14Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1865,192 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors#INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val incompatibilityError = \"Could not apply finalized feature update because\" +\n+      \" brokers were found to have incompatible versions for the feature.\"\n+\n+    if (brokerFeatures.supportedFeatures.get(update.feature()) == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMzMDE5Nw=="}, "originalCommit": null, "originalPosition": 324}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxOTE2OTQwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOFQwMDowODozM1rOG9rzBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODo1NTozMFrOG-bPxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMzMzg5NQ==", "bodyText": "If the broker discovers that it's incompatible, should it just shut itself down?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467333895", "createdAt": "2020-08-08T00:08:33Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -82,18 +110,54 @@ object FinalizedFeatureCache extends Logging {\n         \" The existing cache contents are %s\").format(latest, oldFeatureAndEpoch)\n       throw new FeatureCacheUpdateException(errorMsg)\n     } else {\n-      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      val incompatibleFeatures = brokerFeatures.incompatibleFeatures(latest.features)\n       if (!incompatibleFeatures.empty) {\n         val errorMsg = (\"FinalizedFeatureCache update failed since feature compatibility\" +\n           \" checks failed! Supported %s has incompatibilities with the latest %s.\"\n-          ).format(SupportedFeatures.get, latest)\n+          ).format(brokerFeatures.supportedFeatures, latest)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODExMTMwMA==", "bodyText": "Good question. The existing behavior is that it shuts itself down, as triggered by this LOC. The reason to do it is that an incompatible broker can potentially do harmful things to a cluster (because max version level upgrades are used for breaking changes): https://github.com/apache/kafka/blob/89a3ba69e03acbe9635ee1039abb567bf0c6631b/core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala#L154-L156.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468111300", "createdAt": "2020-08-10T18:55:30Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -82,18 +110,54 @@ object FinalizedFeatureCache extends Logging {\n         \" The existing cache contents are %s\").format(latest, oldFeatureAndEpoch)\n       throw new FeatureCacheUpdateException(errorMsg)\n     } else {\n-      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      val incompatibleFeatures = brokerFeatures.incompatibleFeatures(latest.features)\n       if (!incompatibleFeatures.empty) {\n         val errorMsg = (\"FinalizedFeatureCache update failed since feature compatibility\" +\n           \" checks failed! Supported %s has incompatibilities with the latest %s.\"\n-          ).format(SupportedFeatures.get, latest)\n+          ).format(brokerFeatures.supportedFeatures, latest)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMzMzg5NQ=="}, "originalCommit": null, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxOTIyODQ5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOFQwMTowNDoxMVrOG9sR8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxOTo0NjoyMFrOHXqI_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0MTgxMQ==", "bodyText": "Could you explain how the default min version is different from the min in supportedFeatures?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r467341811", "createdAt": "2020-08-08T01:04:11Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODExMTc4NQ==", "bodyText": "Sure, I'll update the PR documenting it.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468111785", "createdAt": "2020-08-10T18:56:28Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0MTgxMQ=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU2OTcyNg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494569726", "createdAt": "2020-09-24T19:46:20Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0MTgxMQ=="}, "originalCommit": null, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMzk3MTA1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNTo0MjoxOFrOG-UgAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTozNDowOVrOHXSA7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwMDc2OA==", "bodyText": "The return type is different from the KIP. Which one is correct? Since this is a public interface, in general, we don't want to expose anything other than truly necessary. This PR seems to expose a lot more public methods to the user.\nFinalizedVersionRange is in org.apache.kafka.common.feature. Currently, all public interfaces are specified under javadoc in build.gradle. So, we need to either include that package in javadoc or move it to a public package.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468000768", "createdAt": "2020-08-10T15:42:18Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Integer> finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Features<FinalizedVersionRange> finalizedFeatures,\n+                           final int finalizedFeaturesEpoch,\n+                           final Features<SupportedVersionRange> supportedFeatures) {\n+        Objects.requireNonNull(finalizedFeatures, \"Provided finalizedFeatures can not be null.\");\n+        Objects.requireNonNull(supportedFeatures, \"Provided supportedFeatures can not be null.\");\n+        this.finalizedFeatures = finalizedFeatures;\n+        if (finalizedFeaturesEpoch >= 0) {\n+            this.finalizedFeaturesEpoch = Optional.of(finalizedFeaturesEpoch);\n+        } else {\n+            this.finalizedFeaturesEpoch = Optional.empty();\n+        }\n+        this.supportedFeatures = supportedFeatures;\n+    }\n+\n+    /**\n+     * A map of finalized feature versions, with key being finalized feature name and value\n+     * containing the min/max version levels for the finalized feature.\n+     */\n+    public Features<FinalizedVersionRange> finalizedFeatures() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE3NDQ0Ng==", "bodyText": "Done. I've fixed this now to align with the KIP.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494174446", "createdAt": "2020-09-24T09:34:09Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Integer> finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Features<FinalizedVersionRange> finalizedFeatures,\n+                           final int finalizedFeaturesEpoch,\n+                           final Features<SupportedVersionRange> supportedFeatures) {\n+        Objects.requireNonNull(finalizedFeatures, \"Provided finalizedFeatures can not be null.\");\n+        Objects.requireNonNull(supportedFeatures, \"Provided supportedFeatures can not be null.\");\n+        this.finalizedFeatures = finalizedFeatures;\n+        if (finalizedFeaturesEpoch >= 0) {\n+            this.finalizedFeaturesEpoch = Optional.of(finalizedFeaturesEpoch);\n+        } else {\n+            this.finalizedFeaturesEpoch = Optional.empty();\n+        }\n+        this.supportedFeatures = supportedFeatures;\n+    }\n+\n+    /**\n+     * A map of finalized feature versions, with key being finalized feature name and value\n+     * containing the min/max version levels for the finalized feature.\n+     */\n+    public Features<FinalizedVersionRange> finalizedFeatures() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwMDc2OA=="}, "originalCommit": null, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMzk3MTc1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNTo0MjoyOFrOG-UgaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0Mjo1NVrOHXSWRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwMDg3Mg==", "bodyText": "The return type is different from the KIP. Which one is correct?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468000872", "createdAt": "2020-08-10T15:42:28Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Integer> finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Features<FinalizedVersionRange> finalizedFeatures,\n+                           final int finalizedFeaturesEpoch,\n+                           final Features<SupportedVersionRange> supportedFeatures) {\n+        Objects.requireNonNull(finalizedFeatures, \"Provided finalizedFeatures can not be null.\");\n+        Objects.requireNonNull(supportedFeatures, \"Provided supportedFeatures can not be null.\");\n+        this.finalizedFeatures = finalizedFeatures;\n+        if (finalizedFeaturesEpoch >= 0) {\n+            this.finalizedFeaturesEpoch = Optional.of(finalizedFeaturesEpoch);\n+        } else {\n+            this.finalizedFeaturesEpoch = Optional.empty();\n+        }\n+        this.supportedFeatures = supportedFeatures;\n+    }\n+\n+    /**\n+     * A map of finalized feature versions, with key being finalized feature name and value\n+     * containing the min/max version levels for the finalized feature.\n+     */\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        return finalizedFeatures;\n+    }\n+\n+    /**\n+     * The epoch for the finalized features.\n+     * If the returned value is empty, it means the finalized features are absent/unavailable.\n+     */\n+    public Optional<Integer> finalizedFeaturesEpoch() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE3OTkxMQ==", "bodyText": "Done. I've updated the KIP to use Optional<Integer> as well.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494179911", "createdAt": "2020-09-24T09:42:55Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Integer> finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Features<FinalizedVersionRange> finalizedFeatures,\n+                           final int finalizedFeaturesEpoch,\n+                           final Features<SupportedVersionRange> supportedFeatures) {\n+        Objects.requireNonNull(finalizedFeatures, \"Provided finalizedFeatures can not be null.\");\n+        Objects.requireNonNull(supportedFeatures, \"Provided supportedFeatures can not be null.\");\n+        this.finalizedFeatures = finalizedFeatures;\n+        if (finalizedFeaturesEpoch >= 0) {\n+            this.finalizedFeaturesEpoch = Optional.of(finalizedFeaturesEpoch);\n+        } else {\n+            this.finalizedFeaturesEpoch = Optional.empty();\n+        }\n+        this.supportedFeatures = supportedFeatures;\n+    }\n+\n+    /**\n+     * A map of finalized feature versions, with key being finalized feature name and value\n+     * containing the min/max version levels for the finalized feature.\n+     */\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        return finalizedFeatures;\n+    }\n+\n+    /**\n+     * The epoch for the finalized features.\n+     * If the returned value is empty, it means the finalized features are absent/unavailable.\n+     */\n+    public Optional<Integer> finalizedFeaturesEpoch() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwMDg3Mg=="}, "originalCommit": null, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMzk3ODkyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNTo0NDoyMFrOG-UlDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0MzowNlrOHXSWtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwMjA2MA==", "bodyText": "The KIP also exposes host() and port(). Are they still needed?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468002060", "createdAt": "2020-08-10T15:44:20Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDAyMQ==", "bodyText": "Done. I've removed those methods from the KIP.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494180021", "createdAt": "2020-09-24T09:43:06Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwMjA2MA=="}, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDAwMDQ3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNTo0OTozMFrOG-UyLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0MzoyMlrOHXSXRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwNTQyMA==", "bodyText": "The KIP doesn't have DescribeFeaturesOptions. If we are changing the KIP, could we summarize the list of the things that are changed?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468005420", "createdAt": "2020-08-10T15:49:30Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1216,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDE2Nw==", "bodyText": "Done. I've updated the KIP to mention DescribeFeaturesOptions.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494180167", "createdAt": "2020-09-24T09:43:22Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1216,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwNTQyMA=="}, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDAxOTA0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNTo1Mzo1M1rOG-U9Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0MzozNlrOHXSX0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwODI5NA==", "bodyText": "Again, this method has a different signature from the KIP.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468008294", "createdAt": "2020-08-10T15:53:53Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1216,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. This operation is not transactional so it\n+     * may succeed for some features while fail for others.\n+     * <p>\n+     * The API takes in a map of finalized feature names to {@link FeatureUpdate} that needs to be\n+     * applied. Each entry in the map specifies the finalized feature to be added or updated or\n+     * deleted, along with the new max feature version level value. This request is issued only to\n+     * the controller since the API is only served by the controller. The return value contains an\n+     * error code for each supplied {@link FeatureUpdate}, and the code indicates if the update\n+     * succeeded or failed in the controller.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the {@link FeatureUpdate} has the allowDowngrade flag set - setting this\n+     * flag conveys user intent to attempt downgrade of a feature max version level. Note that\n+     * despite the allowDowngrade flag being set, certain downgrades may be rejected by the\n+     * controller if it is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It could be\n+     * done by setting the allowDowngrade flag to true in the {@link FeatureUpdate}, and, setting\n+     * the max version level to be less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link FeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the map of finalized feature name to {@link FeatureUpdate}\n+     * @param options          the options to use\n+     *\n+     * @return                 the {@link UpdateFeaturesResult} containing the result\n+     */\n+    UpdateFeaturesResult updateFeatures(Map<String, FeatureUpdate> featureUpdates, UpdateFeaturesOptions options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDMwNQ==", "bodyText": "Done. I've updated the KIP to align with whats used here, so both are the same now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494180305", "createdAt": "2020-09-24T09:43:36Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1216,71 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the {@link DescribeFeaturesResult} containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. This operation is not transactional so it\n+     * may succeed for some features while fail for others.\n+     * <p>\n+     * The API takes in a map of finalized feature names to {@link FeatureUpdate} that needs to be\n+     * applied. Each entry in the map specifies the finalized feature to be added or updated or\n+     * deleted, along with the new max feature version level value. This request is issued only to\n+     * the controller since the API is only served by the controller. The return value contains an\n+     * error code for each supplied {@link FeatureUpdate}, and the code indicates if the update\n+     * succeeded or failed in the controller.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the {@link FeatureUpdate} has the allowDowngrade flag set - setting this\n+     * flag conveys user intent to attempt downgrade of a feature max version level. Note that\n+     * despite the allowDowngrade flag being set, certain downgrades may be rejected by the\n+     * controller if it is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It could be\n+     * done by setting the allowDowngrade flag to true in the {@link FeatureUpdate}, and, setting\n+     * the max version level to be less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link FeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the map of finalized feature name to {@link FeatureUpdate}\n+     * @param options          the options to use\n+     *\n+     * @return                 the {@link UpdateFeaturesResult} containing the result\n+     */\n+    UpdateFeaturesResult updateFeatures(Map<String, FeatureUpdate> featureUpdates, UpdateFeaturesOptions options);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwODI5NA=="}, "originalCommit": null, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDAyOTE0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNTo1NjoxM1rOG-VDkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0NDowM1rOHXSY-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwOTg3NA==", "bodyText": "The KIP doesn't have this method.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468009874", "createdAt": "2020-08-10T15:56:13Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Map;\n+import org.apache.kafka.common.KafkaFuture;\n+\n+/**\n+ * The result of the {@link Admin#updateFeatures(Map, UpdateFeaturesOptions)} call.\n+ *\n+ * The API of this class is evolving, see {@link Admin} for details.\n+ */\n+public class UpdateFeaturesResult {\n+    private final Map<String, KafkaFuture<Void>> futures;\n+\n+    /**\n+     * @param futures   a map from feature names to future, which can be used to check the status of\n+     *                  individual feature updates.\n+     */\n+    public UpdateFeaturesResult(final Map<String, KafkaFuture<Void>> futures) {\n+        this.futures = futures;\n+    }\n+\n+    public Map<String, KafkaFuture<Void>> values() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDYwMQ==", "bodyText": "Done. The KIP has been updated to have this method now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494180601", "createdAt": "2020-09-24T09:44:03Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Map;\n+import org.apache.kafka.common.KafkaFuture;\n+\n+/**\n+ * The result of the {@link Admin#updateFeatures(Map, UpdateFeaturesOptions)} call.\n+ *\n+ * The API of this class is evolving, see {@link Admin} for details.\n+ */\n+public class UpdateFeaturesResult {\n+    private final Map<String, KafkaFuture<Void>> futures;\n+\n+    /**\n+     * @param futures   a map from feature names to future, which can be used to check the status of\n+     *                  individual feature updates.\n+     */\n+    public UpdateFeaturesResult(final Map<String, KafkaFuture<Void>> futures) {\n+        this.futures = futures;\n+    }\n+\n+    public Map<String, KafkaFuture<Void>> values() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAwOTg3NA=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDA5OTQ4OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNjoxNDowMFrOG-VuQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0ODozN1rOHXSkIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAyMDgwMA==", "bodyText": "handleNotControllerError() already throws an exception.\nShould other errors like CLUSTER_AUTHORIZATION_FAILED be treated in the same way?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r468020800", "createdAt": "2020-08-10T16:14:00Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4071,6 +4078,113 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else if (options.sendRequestToController() && apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(\n+        final Map<String, FeatureUpdate> featureUpdates, final UpdateFeaturesOptions options) {\n+        if (featureUpdates == null || featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+        Objects.requireNonNull(options, \"UpdateFeaturesOptions can not be null\");\n+\n+        final UpdateFeaturesRequestData request = UpdateFeaturesRequest.create(featureUpdates);\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            updateFutures.put(entry.getKey(), new KafkaFutureImpl<>());\n+        }\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                return new UpdateFeaturesRequest.Builder(request);\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse abstractResponse) {\n+                final UpdateFeaturesResponse response =\n+                    (UpdateFeaturesResponse) abstractResponse;\n+\n+                // Check for controller change.\n+                for (UpdatableFeatureResult result : response.data().results()) {\n+                    final Errors error = Errors.forCode(result.errorCode());\n+                    if (error == Errors.NOT_CONTROLLER) {\n+                        handleNotControllerError(error);\n+                        throw error.exception();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MzQ1Ng==", "bodyText": "handleNotControllerError() already throws an exception.\n\nDone. Fixed the code to not throw exception again when handling NOT_CONTROLLER error.\n\nShould other errors like CLUSTER_AUTHORIZATION_FAILED be treated in the same way?\n\nI'm not sure how could we treat it the same way. In the case of the NOT_CONTROLLER error, the admin client code would retry the request once again when the exception is raised. But when cluster authorization fails, would a retry help?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r494183456", "createdAt": "2020-09-24T09:48:37Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -4071,6 +4078,113 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        final NodeProvider provider =\n+            options.sendRequestToController() ? new ControllerNodeProvider() : new LeastLoadedNodeProvider();\n+\n+        Call call = new Call(\n+            \"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), provider) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else if (options.sendRequestToController() && apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                    handleNotControllerError(Errors.NOT_CONTROLLER);\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFeaturesResult updateFeatures(\n+        final Map<String, FeatureUpdate> featureUpdates, final UpdateFeaturesOptions options) {\n+        if (featureUpdates == null || featureUpdates.isEmpty()) {\n+            throw new IllegalArgumentException(\"Feature updates can not be null or empty.\");\n+        }\n+        Objects.requireNonNull(options, \"UpdateFeaturesOptions can not be null\");\n+\n+        final UpdateFeaturesRequestData request = UpdateFeaturesRequest.create(featureUpdates);\n+        final Map<String, KafkaFutureImpl<Void>> updateFutures = new HashMap<>();\n+        for (Map.Entry<String, FeatureUpdate> entry : featureUpdates.entrySet()) {\n+            updateFutures.put(entry.getKey(), new KafkaFutureImpl<>());\n+        }\n+        final long now = time.milliseconds();\n+        final Call call = new Call(\"updateFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new ControllerNodeProvider()) {\n+\n+            @Override\n+            UpdateFeaturesRequest.Builder createRequest(int timeoutMs) {\n+                return new UpdateFeaturesRequest.Builder(request);\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse abstractResponse) {\n+                final UpdateFeaturesResponse response =\n+                    (UpdateFeaturesResponse) abstractResponse;\n+\n+                // Check for controller change.\n+                for (UpdatableFeatureResult result : response.data().results()) {\n+                    final Errors error = Errors.forCode(result.errorCode());\n+                    if (error == Errors.NOT_CONTROLLER) {\n+                        handleNotControllerError(error);\n+                        throw error.exception();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAyMDgwMA=="}, "originalCommit": null, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTYxMDE4OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/ApiVersionsResponse.json", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjo0ODo1NVrOHZHIIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODowOTo1N1rOHZgWLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA5MzIxNg==", "bodyText": "Space before \"name\".", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496093216", "createdAt": "2020-09-28T16:48:55Z", "author": {"login": "junrao"}, "path": "clients/src/main/resources/common/message/ApiVersionsResponse.json", "diffHunk": "@@ -55,8 +55,8 @@\n           \"about\": \"The maximum supported version for the feature.\" }\n       ]\n     },\n-    {\"name\": \"FinalizedFeaturesEpoch\", \"type\": \"int32\", \"versions\": \"3+\",\n-      \"tag\": 1, \"taggedVersions\": \"3+\", \"default\": \"-1\",\n+    {\"name\": \"FinalizedFeaturesEpoch\", \"type\": \"int64\", \"versions\": \"3+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwNjQxMw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496506413", "createdAt": "2020-09-29T08:09:57Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/ApiVersionsResponse.json", "diffHunk": "@@ -55,8 +55,8 @@\n           \"about\": \"The maximum supported version for the feature.\" }\n       ]\n     },\n-    {\"name\": \"FinalizedFeaturesEpoch\", \"type\": \"int32\", \"versions\": \"3+\",\n-      \"tag\": 1, \"taggedVersions\": \"3+\", \"default\": \"-1\",\n+    {\"name\": \"FinalizedFeaturesEpoch\", \"type\": \"int64\", \"versions\": \"3+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA5MzIxNg=="}, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTY0OTQxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjo1OTowMlrOHZHgmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoxMjoyNFrOHZggqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA5OTQ4Mg==", "bodyText": "This is not included in the KIP. Should we update the KIP?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496099482", "createdAt": "2020-09-28T16:59:02Z", "author": {"login": "junrao"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,35 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 57,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"timeoutMs\", \"type\": \"int32\", \"versions\": \"0+\", \"default\": \"60000\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwOTA5Nw==", "bodyText": "Done. Updated the KIP. Please refer to this section.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496509097", "createdAt": "2020-09-29T08:12:24Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,35 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 57,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"timeoutMs\", \"type\": \"int32\", \"versions\": \"0+\", \"default\": \"60000\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA5OTQ4Mg=="}, "originalCommit": null, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTY4NDQyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzowNTo0NlrOHZH0iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoxMjozNFrOHZghYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEwNDU4NA==", "bodyText": "Since this is public facing, could we include the description in the KIP?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496104584", "createdAt": "2020-09-28T17:05:46Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+/**\n+ * Options for {@link AdminClient#describeFeatures(DescribeFeaturesOptions)}.\n+ *\n+ * The API of this class is evolving. See {@link Admin} for details.\n+ */\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwOTI4Mg==", "bodyText": "Done. Updated the KIP. Please refer to this section.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496509282", "createdAt": "2020-09-29T08:12:34Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+/**\n+ * Options for {@link AdminClient#describeFeatures(DescribeFeaturesOptions)}.\n+ *\n+ * The API of this class is evolving. See {@link Admin} for details.\n+ */\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEwNDU4NA=="}, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTc3NDY5OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzozMToyN1rOHZIs0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoxNDo0MlrOHZgqdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjExODk5Mw==", "bodyText": "Since the user is not expected to instantiate this, should we make the constructor non-public?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496118993", "createdAt": "2020-09-28T17:31:27Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Represents a range of version levels supported by every broker in a cluster for some feature.\n+ */\n+public class FinalizedVersionRange {\n+    private final short minVersionLevel;\n+\n+    private final short maxVersionLevel;\n+\n+    /**\n+     * Raises an exception unless the following condition is met:\n+     * minVersionLevel >= 1 and maxVersionLevel >= 1 and maxVersionLevel >= minVersionLevel.\n+     *\n+     * @param minVersionLevel   The minimum version level value.\n+     * @param maxVersionLevel   The maximum version level value.\n+     *\n+     * @throws IllegalArgumentException   Raised when the condition described above is not met.\n+     */\n+    public FinalizedVersionRange(final short minVersionLevel, final short maxVersionLevel) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUxMTYwNA==", "bodyText": "It is instantiated from kafka.server.UpdateFeaturesTest, so have to keep the c'tor public.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496511604", "createdAt": "2020-09-29T08:14:42Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Represents a range of version levels supported by every broker in a cluster for some feature.\n+ */\n+public class FinalizedVersionRange {\n+    private final short minVersionLevel;\n+\n+    private final short maxVersionLevel;\n+\n+    /**\n+     * Raises an exception unless the following condition is met:\n+     * minVersionLevel >= 1 and maxVersionLevel >= 1 and maxVersionLevel >= minVersionLevel.\n+     *\n+     * @param minVersionLevel   The minimum version level value.\n+     * @param maxVersionLevel   The maximum version level value.\n+     *\n+     * @throws IllegalArgumentException   Raised when the condition described above is not met.\n+     */\n+    public FinalizedVersionRange(final short minVersionLevel, final short maxVersionLevel) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjExODk5Mw=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTc4MzgyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzozNDowM1rOHZIyVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoxNDo1NVrOHZgreA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyMDQwNg==", "bodyText": "Since the user is not expected to instantiate this, should we make the constructor non-public?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496120406", "createdAt": "2020-09-28T17:34:03Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Map<String, FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Long> finalizedFeaturesEpoch;\n+\n+    private final Map<String, SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Map<String, FinalizedVersionRange> finalizedFeatures,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUxMTg2NA==", "bodyText": "It is instantiated from kafka.server.UpdateFeaturesTest, so have to keep the c'tor public.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496511864", "createdAt": "2020-09-29T08:14:55Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Map<String, FinalizedVersionRange> finalizedFeatures;\n+\n+    private final Optional<Long> finalizedFeaturesEpoch;\n+\n+    private final Map<String, SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(final Map<String, FinalizedVersionRange> finalizedFeatures,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyMDQwNg=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTc5NDYzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzozNzowOVrOHZI5Dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyNToyMVrOHZhYMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyMjEyNw==", "bodyText": "This seems identical to SupportedVersionRange. Should we just have one, sth like VersionRange?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496122127", "createdAt": "2020-09-28T17:37:09Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Represents a range of version levels supported by every broker in a cluster for some feature.\n+ */\n+public class FinalizedVersionRange {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyMzMxNQ==", "bodyText": "I considered this, however if we plan to expose firstActiveVersion to the client, then, it is better to have 2 separate classes like we do now. This is because firstActiveVersion will become an attribute only in SupportedVersionRange class.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496523315", "createdAt": "2020-09-29T08:25:21Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Represents a range of version levels supported by every broker in a cluster for some feature.\n+ */\n+public class FinalizedVersionRange {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyMjEyNw=="}, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTgxMTY2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzo0MTo1MlrOHZJDKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyNTo1MlrOHZhadg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyNDcxMw==", "bodyText": "Are we adding the timeout option based on the KIP discussion?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496124713", "createdAt": "2020-09-28T17:41:52Z", "author": {"login": "junrao"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Map;\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+/**\n+ * Options for {@link AdminClient#updateFeatures(Map, UpdateFeaturesOptions)}.\n+ *\n+ * The API of this class is evolving. See {@link Admin} for details.\n+ */\n+@InterfaceStability.Evolving\n+public class UpdateFeaturesOptions extends AbstractOptions<UpdateFeaturesOptions> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyMzg5NA==", "bodyText": "Yes, it is already added. The base class: AbstractOptions contains a timeoutMs attribute and the value is set in the UpdateFeaturesRequest.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496523894", "createdAt": "2020-09-29T08:25:52Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Map;\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+/**\n+ * Options for {@link AdminClient#updateFeatures(Map, UpdateFeaturesOptions)}.\n+ *\n+ * The API of this class is evolving. See {@link Admin} for details.\n+ */\n+@InterfaceStability.Evolving\n+public class UpdateFeaturesOptions extends AbstractOptions<UpdateFeaturesOptions> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyNDcxMw=="}, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTg1MzIwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzo1Mjo1N1rOHZJccg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyNjowM1rOHZhbKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEzMTE4Ng==", "bodyText": "\"at a those \" typo?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496131186", "createdAt": "2020-09-28T17:52:57Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at a those version levels, across the entire Kafka cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyNDA3Mg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496524072", "createdAt": "2020-09-29T08:26:03Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at a those version levels, across the entire Kafka cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEzMTE4Ng=="}, "originalCommit": null, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjM1NTgzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMDoyNDo0NlrOHZOUQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyNjo0NFrOHZhd-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIxMTAxMQ==", "bodyText": "I guess after the first step, deprecated finalized versions are no longer advertised to the client, but they can still be used by existing connections?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496211011", "createdAt": "2020-09-28T20:24:46Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at a those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyNDc5Mw==", "bodyText": "Yes, correct. I have updated the doc mentioning the same.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496524793", "createdAt": "2020-09-29T08:26:44Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at a those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIxMTAxMQ=="}, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjM3MDIxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMDoyOTowNFrOHZOc8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyNjo1MFrOHZhebg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIxMzIzMg==", "bodyText": "Perhaps isFeatureVersioningSupported is a better name?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496213232", "createdAt": "2020-09-28T20:29:04Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.defaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Using the change below, we deprecate all version levels in the range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1].\n+              //\n+              // NOTE: if existingVersionRange.min() equals brokerDefaultVersionRange.min(), then\n+              // we do not deprecate any version levels (since there is none to be deprecated).\n+              //\n+              // Examples:\n+              // 1. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [1, 5].\n+              //    In this case, we deprecate all version levels in the range: [1, 3].\n+              // 2. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [4, 5].\n+              //    In this case, we do not deprecate any version levels since\n+              //    brokerDefaultVersionRange.min() equals existingVersionRange.min().\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a serious error. We should never be reaching here, since we already\n+              // verify once during KafkaServer startup that existing finalized feature versions in\n+              // the FeatureZNode contained no incompatibilities. If we are here, it means that one\n+              // of the following is true:\n+              // 1. The existing version levels fall completely outside the range of the default\n+              // finalized version levels (i.e. no intersection), or\n+              // 2. The existing version levels are incompatible with default finalized version\n+              // levels.\n+              //\n+              // Examples of invalid cases that can cause this exception to be triggered:\n+              // 1. No intersection      : brokerDefaultVersionRange = [4, 7] and existingVersionRange = [2, 3].\n+              // 2. No intersection      : brokerDefaultVersionRange = [2, 3] and existingVersionRange = [4, 7].\n+              // 3. Incompatible versions: brokerDefaultVersionRange = [2, 3] and existingVersionRange = [1, 7].\n+              throw new IllegalStateException(\n+                s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange. This should never happen\"\n+                + s\" since feature version incompatibilities are already checked during\"\n+                + s\" Kafka server startup.\")\n+            }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the cache (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {\n+    val newNode = FeatureZNode(FeatureZNodeStatus.Disabled, Features.emptyFinalizedFeatures())\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      createFeatureZNode(newNode)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Disabled)) {\n+        updateFeatureZNode(newNode)\n+      }\n+    }\n+  }\n+\n+  private def setupFeatureVersioning(): Unit = {\n+    if (config.isFeatureVersioningEnabled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyNDkxMA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496524910", "createdAt": "2020-09-29T08:26:50Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.defaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Using the change below, we deprecate all version levels in the range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1].\n+              //\n+              // NOTE: if existingVersionRange.min() equals brokerDefaultVersionRange.min(), then\n+              // we do not deprecate any version levels (since there is none to be deprecated).\n+              //\n+              // Examples:\n+              // 1. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [1, 5].\n+              //    In this case, we deprecate all version levels in the range: [1, 3].\n+              // 2. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [4, 5].\n+              //    In this case, we do not deprecate any version levels since\n+              //    brokerDefaultVersionRange.min() equals existingVersionRange.min().\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a serious error. We should never be reaching here, since we already\n+              // verify once during KafkaServer startup that existing finalized feature versions in\n+              // the FeatureZNode contained no incompatibilities. If we are here, it means that one\n+              // of the following is true:\n+              // 1. The existing version levels fall completely outside the range of the default\n+              // finalized version levels (i.e. no intersection), or\n+              // 2. The existing version levels are incompatible with default finalized version\n+              // levels.\n+              //\n+              // Examples of invalid cases that can cause this exception to be triggered:\n+              // 1. No intersection      : brokerDefaultVersionRange = [4, 7] and existingVersionRange = [2, 3].\n+              // 2. No intersection      : brokerDefaultVersionRange = [2, 3] and existingVersionRange = [4, 7].\n+              // 3. Incompatible versions: brokerDefaultVersionRange = [2, 3] and existingVersionRange = [1, 7].\n+              throw new IllegalStateException(\n+                s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange. This should never happen\"\n+                + s\" since feature version incompatibilities are already checked during\"\n+                + s\" Kafka server startup.\")\n+            }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the cache (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {\n+    val newNode = FeatureZNode(FeatureZNodeStatus.Disabled, Features.emptyFinalizedFeatures())\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      createFeatureZNode(newNode)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Disabled)) {\n+        updateFeatureZNode(newNode)\n+      }\n+    }\n+  }\n+\n+  private def setupFeatureVersioning(): Unit = {\n+    if (config.isFeatureVersioningEnabled) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIxMzIzMg=="}, "originalCommit": null, "originalPosition": 248}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjQ2MTk5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMDo1Njo0NFrOHZPUoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxMDoxNzoxNlrOHaW2gQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNzQ4OA==", "bodyText": "Is it useful to expose firstActiveVersion to the client?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496227488", "createdAt": "2020-09-28T20:56:44Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at a those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:\n+ * In the first step, a major Kafka release is made with a Broker code change (explained later\n+ * below) that establishes the intent to deprecate certain versions of one or more features\n+ * cluster-wide. When this new Kafka release is deployed to the cluster, the feature versioning\n+ * system (via the controller) will automatically persist the new minVersionLevel for the feature in\n+ * Zk to propagate the deprecation of certain versions. After this happens, any external client that\n+ * queries the Broker to learn the feature versions will at some point start to see the new value\n+ * for the finalized minVersionLevel for the feature. This makes the version deprecation permanent.\n+ *\n+ * Here is how the above code change needs to be done:\n+ * In order to deprecate feature version levels, in the supportedFeatures map you need to supply a\n+ * specific firstActiveVersion value that's higher than the minVersion for the feature. The\n+ * value for firstActiveVersion should be 1 beyond the highest version that you intend to deprecate\n+ * for that feature. When features are finalized via the ApiKeys.UPDATE_FEATURES api, the feature\n+ * version levels in the closed range: [minVersion, firstActiveVersion - 1] are automatically\n+ * deprecated in ZK by the controller logic.\n+ * Example:\n+ * - Let us assume the existing finalized feature in ZK:\n+ *   {\n+ *      \"feature_1\" -> FinalizedVersionRange(minVersionLevel=1, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to deprecate feature version levels: [1, 2].\n+ *   Then, in the supportedFeatures map you should supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=3, maxVersion=5)\n+ *   }\n+ * - If you do NOT want to deprecate a version level for a feature, then in the supportedFeatures\n+ *   map you should supply the firstActiveVersion to be the same as the minVersion supplied for that\n+ *   feature.\n+ *   Example:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=1, maxVersion=5)\n+ *   }\n+ *   This indicates no intent to deprecate any version levels for the feature.\n+ *\n+ * STEP 2:\n+ * After the first step is over, you may (at some point) want to permanently remove the code/logic\n+ * for the functionality offered by the deprecated feature versions. This is the second step. Here a\n+ * subsequent major Kafka release is made with another Broker code change that removes the code for\n+ * the functionality offered by the deprecated feature versions. This would completely drop support\n+ * for the deprecated versions. Such a code change needs to be supplemented by supplying a\n+ * suitable higher minVersion value for the feature in the supportedFeatures map.\n+ * Example:\n+ * - In the example above in step 1, we showed how to deprecate version levels [1, 2] for\n+ *   \"feature_1\". Now let us assume the following finalized feature in ZK (after the deprecation\n+ *   has been carried out):\n+ *   {\n+ *     \"feature_1\" -> FinalizedVersionRange(minVersionLevel=3, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to permanently remove support for feature versions: [1, 2].\n+ *   Then, in the supportedFeatures map you should now supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=3, firstActiveVersion=3, maxVersion=5)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyNTk0OQ==", "bodyText": "This is a really good point. Yes, I feel it is useful to expose it to the client via ApiVersionsResponse. I can change the KIP suitably and then update the PR.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496525949", "createdAt": "2020-09-29T08:27:48Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at a those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:\n+ * In the first step, a major Kafka release is made with a Broker code change (explained later\n+ * below) that establishes the intent to deprecate certain versions of one or more features\n+ * cluster-wide. When this new Kafka release is deployed to the cluster, the feature versioning\n+ * system (via the controller) will automatically persist the new minVersionLevel for the feature in\n+ * Zk to propagate the deprecation of certain versions. After this happens, any external client that\n+ * queries the Broker to learn the feature versions will at some point start to see the new value\n+ * for the finalized minVersionLevel for the feature. This makes the version deprecation permanent.\n+ *\n+ * Here is how the above code change needs to be done:\n+ * In order to deprecate feature version levels, in the supportedFeatures map you need to supply a\n+ * specific firstActiveVersion value that's higher than the minVersion for the feature. The\n+ * value for firstActiveVersion should be 1 beyond the highest version that you intend to deprecate\n+ * for that feature. When features are finalized via the ApiKeys.UPDATE_FEATURES api, the feature\n+ * version levels in the closed range: [minVersion, firstActiveVersion - 1] are automatically\n+ * deprecated in ZK by the controller logic.\n+ * Example:\n+ * - Let us assume the existing finalized feature in ZK:\n+ *   {\n+ *      \"feature_1\" -> FinalizedVersionRange(minVersionLevel=1, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to deprecate feature version levels: [1, 2].\n+ *   Then, in the supportedFeatures map you should supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=3, maxVersion=5)\n+ *   }\n+ * - If you do NOT want to deprecate a version level for a feature, then in the supportedFeatures\n+ *   map you should supply the firstActiveVersion to be the same as the minVersion supplied for that\n+ *   feature.\n+ *   Example:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=1, maxVersion=5)\n+ *   }\n+ *   This indicates no intent to deprecate any version levels for the feature.\n+ *\n+ * STEP 2:\n+ * After the first step is over, you may (at some point) want to permanently remove the code/logic\n+ * for the functionality offered by the deprecated feature versions. This is the second step. Here a\n+ * subsequent major Kafka release is made with another Broker code change that removes the code for\n+ * the functionality offered by the deprecated feature versions. This would completely drop support\n+ * for the deprecated versions. Such a code change needs to be supplemented by supplying a\n+ * suitable higher minVersion value for the feature in the supportedFeatures map.\n+ * Example:\n+ * - In the example above in step 1, we showed how to deprecate version levels [1, 2] for\n+ *   \"feature_1\". Now let us assume the following finalized feature in ZK (after the deprecation\n+ *   has been carried out):\n+ *   {\n+ *     \"feature_1\" -> FinalizedVersionRange(minVersionLevel=3, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to permanently remove support for feature versions: [1, 2].\n+ *   Then, in the supportedFeatures map you should now supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=3, firstActiveVersion=3, maxVersion=5)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNzQ4OA=="}, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzM5OTQyNQ==", "bodyText": "Done. The firstActiveVersion is now part of ApiVersionsResponse. I added it in the recent commit: a7f4860f5f8bb87cfb01452e208ff8f4e45bcd8b.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r497399425", "createdAt": "2020-09-30T10:17:16Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the latest features supported by the Broker and also provides APIs to\n+ * check for incompatibilities between the features supported by the Broker and finalized features.\n+ * The class also enables feature version level deprecation, as explained below. This class is\n+ * immutable in production. It provides few APIs to mutate state only for the purpose of testing.\n+ *\n+ * Feature version level deprecation:\n+ * ==================================\n+ *\n+ * Deprecation of certain version levels of a feature is a process to stop supporting the\n+ * functionality offered by the feature at a those version levels, across the entire Kafka cluster.\n+ * Feature version deprecation is a simple 2-step process explained below. In each step below, an\n+ * example is provided to help understand the process better:\n+ *\n+ * STEP 1:\n+ * In the first step, a major Kafka release is made with a Broker code change (explained later\n+ * below) that establishes the intent to deprecate certain versions of one or more features\n+ * cluster-wide. When this new Kafka release is deployed to the cluster, the feature versioning\n+ * system (via the controller) will automatically persist the new minVersionLevel for the feature in\n+ * Zk to propagate the deprecation of certain versions. After this happens, any external client that\n+ * queries the Broker to learn the feature versions will at some point start to see the new value\n+ * for the finalized minVersionLevel for the feature. This makes the version deprecation permanent.\n+ *\n+ * Here is how the above code change needs to be done:\n+ * In order to deprecate feature version levels, in the supportedFeatures map you need to supply a\n+ * specific firstActiveVersion value that's higher than the minVersion for the feature. The\n+ * value for firstActiveVersion should be 1 beyond the highest version that you intend to deprecate\n+ * for that feature. When features are finalized via the ApiKeys.UPDATE_FEATURES api, the feature\n+ * version levels in the closed range: [minVersion, firstActiveVersion - 1] are automatically\n+ * deprecated in ZK by the controller logic.\n+ * Example:\n+ * - Let us assume the existing finalized feature in ZK:\n+ *   {\n+ *      \"feature_1\" -> FinalizedVersionRange(minVersionLevel=1, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to deprecate feature version levels: [1, 2].\n+ *   Then, in the supportedFeatures map you should supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=3, maxVersion=5)\n+ *   }\n+ * - If you do NOT want to deprecate a version level for a feature, then in the supportedFeatures\n+ *   map you should supply the firstActiveVersion to be the same as the minVersion supplied for that\n+ *   feature.\n+ *   Example:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=1, firstActiveVersion=1, maxVersion=5)\n+ *   }\n+ *   This indicates no intent to deprecate any version levels for the feature.\n+ *\n+ * STEP 2:\n+ * After the first step is over, you may (at some point) want to permanently remove the code/logic\n+ * for the functionality offered by the deprecated feature versions. This is the second step. Here a\n+ * subsequent major Kafka release is made with another Broker code change that removes the code for\n+ * the functionality offered by the deprecated feature versions. This would completely drop support\n+ * for the deprecated versions. Such a code change needs to be supplemented by supplying a\n+ * suitable higher minVersion value for the feature in the supportedFeatures map.\n+ * Example:\n+ * - In the example above in step 1, we showed how to deprecate version levels [1, 2] for\n+ *   \"feature_1\". Now let us assume the following finalized feature in ZK (after the deprecation\n+ *   has been carried out):\n+ *   {\n+ *     \"feature_1\" -> FinalizedVersionRange(minVersionLevel=3, maxVersionLevel=5)\n+ *   }\n+ *   Now, supposing you would like to permanently remove support for feature versions: [1, 2].\n+ *   Then, in the supportedFeatures map you should now supply the following:\n+ *   supportedFeatures = {\n+ *     \"feature1\" -> SupportedVersionRange(minVersion=3, firstActiveVersion=3, maxVersion=5)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNzQ4OA=="}, "originalCommit": null, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjYyNzY3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo0NzowMFrOHZQ4mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyODowNFrOHZhjrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1MzA4Mw==", "bodyText": "Could you define the default finalized features? Also, default minimum version seems outdated now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496253083", "createdAt": "2020-09-28T21:47:00Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyNjI1NA==", "bodyText": "Done. I reworded a bit and I'm now no longer using \"default finalized features\" and \"default minimum version\" in the wordings.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496526254", "createdAt": "2020-09-29T08:28:04Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1MzA4Mw=="}, "originalCommit": null, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjcyODEzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMjoyMjoyMVrOHZR1pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyOToxNFrOHZhosw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODcxMQ==", "bodyText": "Perhaps it's better for the following code to use match instead if/else.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496268711", "createdAt": "2020-09-28T22:22:21Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -1733,8 +1734,8 @@ class KafkaApis(val requestChannel: RequestChannel,\n       else if (!apiVersionRequest.isValid)\n         apiVersionRequest.getErrorResponse(requestThrottleMs, Errors.INVALID_REQUEST.exception)\n       else {\n-        val supportedFeatures = SupportedFeatures.get\n-        val finalizedFeatures = FinalizedFeatureCache.get\n+        val supportedFeatures = brokerFeatures.supportedFeatures\n+        val finalizedFeatures = featureCache.get", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyNzUzOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496527539", "createdAt": "2020-09-29T08:29:14Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -1733,8 +1734,8 @@ class KafkaApis(val requestChannel: RequestChannel,\n       else if (!apiVersionRequest.isValid)\n         apiVersionRequest.getErrorResponse(requestThrottleMs, Errors.INVALID_REQUEST.exception)\n       else {\n-        val supportedFeatures = SupportedFeatures.get\n-        val finalizedFeatures = FinalizedFeatureCache.get\n+        val supportedFeatures = brokerFeatures.supportedFeatures\n+        val finalizedFeatures = featureCache.get", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODcxMQ=="}, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjc0NDE4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMjoyOToxN1rOHZR_Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODoyOTo0N1rOHZhrKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI3MTE0Mw==", "bodyText": "setupFeatureVersioning => mayBeSetupFeatureVersioning ?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496271143", "createdAt": "2020-09-28T22:29:17Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.defaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Using the change below, we deprecate all version levels in the range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1].\n+              //\n+              // NOTE: if existingVersionRange.min() equals brokerDefaultVersionRange.min(), then\n+              // we do not deprecate any version levels (since there is none to be deprecated).\n+              //\n+              // Examples:\n+              // 1. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [1, 5].\n+              //    In this case, we deprecate all version levels in the range: [1, 3].\n+              // 2. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [4, 5].\n+              //    In this case, we do not deprecate any version levels since\n+              //    brokerDefaultVersionRange.min() equals existingVersionRange.min().\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a serious error. We should never be reaching here, since we already\n+              // verify once during KafkaServer startup that existing finalized feature versions in\n+              // the FeatureZNode contained no incompatibilities. If we are here, it means that one\n+              // of the following is true:\n+              // 1. The existing version levels fall completely outside the range of the default\n+              // finalized version levels (i.e. no intersection), or\n+              // 2. The existing version levels are incompatible with default finalized version\n+              // levels.\n+              //\n+              // Examples of invalid cases that can cause this exception to be triggered:\n+              // 1. No intersection      : brokerDefaultVersionRange = [4, 7] and existingVersionRange = [2, 3].\n+              // 2. No intersection      : brokerDefaultVersionRange = [2, 3] and existingVersionRange = [4, 7].\n+              // 3. Incompatible versions: brokerDefaultVersionRange = [2, 3] and existingVersionRange = [1, 7].\n+              throw new IllegalStateException(\n+                s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange. This should never happen\"\n+                + s\" since feature version incompatibilities are already checked during\"\n+                + s\" Kafka server startup.\")\n+            }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the cache (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {\n+    val newNode = FeatureZNode(FeatureZNodeStatus.Disabled, Features.emptyFinalizedFeatures())\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      createFeatureZNode(newNode)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Disabled)) {\n+        updateFeatureZNode(newNode)\n+      }\n+    }\n+  }\n+\n+  private def setupFeatureVersioning(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyODE2OQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496528169", "createdAt": "2020-09-29T08:29:47Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -272,6 +281,199 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (string) and a range of versions (defined by a\n+   * SupportedVersionRange). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of its own supported features in its\n+   * own BrokerIdZNode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is represented by a name (string) and a range of version levels (defined\n+   * by a FinalizedVersionRange). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the only entity modifying the\n+   * information about finalized features.\n+   *\n+   * This method sets up the FeatureZNode with enabled status, which means that the finalized\n+   * features stored in the FeatureZNode are active. The enabled status should be written by the\n+   * controller to the FeatureZNode only when the broker IBP config is greater than or equal to\n+   * KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. This process ensures we do not enable all the possible features immediately after\n+   *    an upgrade, which could be harmful to Kafka.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, it will\n+   *        react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, its existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.defaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) =>\n+            val brokerDefaultVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (brokerDefaultVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (brokerDefaultVersionRange.max() >= existingVersionRange.max() &&\n+                       brokerDefaultVersionRange.min() <= existingVersionRange.max()) {\n+              // Using the change below, we deprecate all version levels in the range:\n+              // [existingVersionRange.min(), brokerDefaultVersionRange.min() - 1].\n+              //\n+              // NOTE: if existingVersionRange.min() equals brokerDefaultVersionRange.min(), then\n+              // we do not deprecate any version levels (since there is none to be deprecated).\n+              //\n+              // Examples:\n+              // 1. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [1, 5].\n+              //    In this case, we deprecate all version levels in the range: [1, 3].\n+              // 2. brokerDefaultVersionRange = [4, 7] and existingVersionRange = [4, 5].\n+              //    In this case, we do not deprecate any version levels since\n+              //    brokerDefaultVersionRange.min() equals existingVersionRange.min().\n+              (featureName, new FinalizedVersionRange(brokerDefaultVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a serious error. We should never be reaching here, since we already\n+              // verify once during KafkaServer startup that existing finalized feature versions in\n+              // the FeatureZNode contained no incompatibilities. If we are here, it means that one\n+              // of the following is true:\n+              // 1. The existing version levels fall completely outside the range of the default\n+              // finalized version levels (i.e. no intersection), or\n+              // 2. The existing version levels are incompatible with default finalized version\n+              // levels.\n+              //\n+              // Examples of invalid cases that can cause this exception to be triggered:\n+              // 1. No intersection      : brokerDefaultVersionRange = [4, 7] and existingVersionRange = [2, 3].\n+              // 2. No intersection      : brokerDefaultVersionRange = [2, 3] and existingVersionRange = [4, 7].\n+              // 3. Incompatible versions: brokerDefaultVersionRange = [2, 3] and existingVersionRange = [1, 7].\n+              throw new IllegalStateException(\n+                s\"Can not update minimum version level in finalized feature: $featureName,\"\n+                + s\" since the existing $existingVersionRange is not eligible for a change\"\n+                + s\" based on the default $brokerDefaultVersionRange. This should never happen\"\n+                + s\" since feature version incompatibilities are already checked during\"\n+                + s\" Kafka server startup.\")\n+            }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the cache (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {\n+    val newNode = FeatureZNode(FeatureZNodeStatus.Disabled, Features.emptyFinalizedFeatures())\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      createFeatureZNode(newNode)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      if (!existingFeatureZNode.status.equals(FeatureZNodeStatus.Disabled)) {\n+        updateFeatureZNode(newNode)\n+      }\n+    }\n+  }\n+\n+  private def setupFeatureVersioning(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI3MTE0Mw=="}, "originalCommit": null, "originalPosition": 247}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjk3NTQ2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMDoyMDo1NlrOHZUEiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODozMDowMFrOHZhsPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNTI5MQ==", "bodyText": "map() is supposed to be used with no side effect. Perhaps we could use match here.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496305291", "createdAt": "2020-09-29T00:20:56Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1893,203 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain ApiError(Errors.NONE).\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone\n+            .map(newVersionRange => targetFeatures += (update.feature() -> newVersionRange))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 493}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUyODQ0NQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496528445", "createdAt": "2020-09-29T08:30:00Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1893,203 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain ApiError(Errors.NONE).\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone\n+            .map(newVersionRange => targetFeatures += (update.feature() -> newVersionRange))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNTI5MQ=="}, "originalCommit": null, "originalPosition": 493}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjk4MDE2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMDoyMzo0OFrOHZUHVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODozMjoyM1rOHZh1eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNjAwNQ==", "bodyText": "Do we need to return the stacktrace to the caller? Since this is unexpected, perhaps we can log a warn?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496306005", "createdAt": "2020-09-29T00:23:48Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1893,203 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain ApiError(Errors.NONE).\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone\n+            .map(newVersionRange => targetFeatures += (update.feature() -> newVersionRange))\n+            .getOrElse(targetFeatures -= update.feature())\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    } catch {\n+      // For all features that correspond to valid FeatureUpdate (i.e. error is Errors.NONE),\n+      // we set the error as Errors.FEATURE_UPDATE_FAILED since the FeatureZNode update has failed\n+      // for these. For the rest, the existing error is left untouched.\n+      case e: Exception =>\n+        errors.foreach { case (feature, apiError) =>\n+          if (apiError.error() == Errors.NONE) {\n+            errors(feature) = new ApiError(Errors.FEATURE_UPDATE_FAILED,\n+                                           Errors.FEATURE_UPDATE_FAILED.message() + \" Error: \" + e)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 519}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzMDgxMA==", "bodyText": "Done. Good point. I'm now logging just a warning and I've removed the stacktrace from the return value.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r496530810", "createdAt": "2020-09-29T08:32:23Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1656,6 +1893,203 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  /**\n+   * Returns the new FinalizedVersionRange for the feature, if there are no feature\n+   * incompatibilities seen with all known brokers for the provided feature update.\n+   * Otherwise returns an ApiError object containing Errors.INVALID_REQUEST.\n+   *\n+   * @param update   the feature update to be processed (this can not be meant to delete the feature)\n+   *\n+   * @return         the new FinalizedVersionRange or error, as described above.\n+   */\n+  private def newFinalizedVersionRangeOrIncompatibilityError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[FinalizedVersionRange, ApiError] = {\n+    if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+      throw new IllegalArgumentException(s\"Provided feature update can not be meant to delete the feature: $update\")\n+    }\n+\n+    val supportedVersionRange = brokerFeatures.supportedFeatures.get(update.feature)\n+    if (supportedVersionRange == null) {\n+      Right(new ApiError(Errors.INVALID_REQUEST,\n+                         \"Could not apply finalized feature update because the provided feature\" +\n+                         \" is not supported.\"))\n+    } else {\n+      var newVersionRange: FinalizedVersionRange = null\n+      try {\n+        newVersionRange = new FinalizedVersionRange(supportedVersionRange.firstActiveVersion, update.maxVersionLevel)\n+      } catch {\n+        case _: IllegalArgumentException => {\n+          // This exception means the provided maxVersionLevel is invalid. It is handled below\n+          // outside of this catch clause.\n+        }\n+      }\n+      if (newVersionRange == null) {\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+          \"Could not apply finalized feature update because the provided\" +\n+          s\" maxVersionLevel:${update.maxVersionLevel} is lower than the\" +\n+          s\" first active version:${supportedVersionRange.firstActiveVersion}.\"))\n+      } else {\n+        val newFinalizedFeature =\n+          Features.finalizedFeatures(Utils.mkMap(Utils.mkEntry(update.feature, newVersionRange)))\n+        val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+          BrokerFeatures.hasIncompatibleFeatures(broker.features, newFinalizedFeature)\n+        })\n+        if (numIncompatibleBrokers == 0) {\n+          Left(newVersionRange)\n+        } else {\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Could not apply finalized feature update because\" +\n+                             \" brokers were found to have incompatible versions for the feature.\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates a feature update on an existing FinalizedVersionRange.\n+   * If the validation succeeds, then, the return value contains:\n+   * 1. the new FinalizedVersionRange for the feature, if the feature update was not meant to delete the feature.\n+   * 2. Option.empty, if the feature update was meant to delete the feature.\n+   *\n+   * If the validation fails, then returned value contains a suitable ApiError.\n+   *\n+   * @param update                 the feature update to be processed.\n+   * @param existingVersionRange   the existing FinalizedVersionRange which can be empty when no\n+   *                               FinalizedVersionRange exists for the associated feature\n+   *\n+   * @return                       the new FinalizedVersionRange to be updated into ZK or error\n+   *                               as described above.\n+   */\n+  private def validateFeatureUpdate(update: UpdateFeaturesRequestData.FeatureUpdateKey,\n+                                   existingVersionRange: Option[FinalizedVersionRange]): Either[Option[FinalizedVersionRange], ApiError] = {\n+    def newVersionRangeOrError(update: UpdateFeaturesRequestData.FeatureUpdateKey): Either[Option[FinalizedVersionRange], ApiError] = {\n+      newFinalizedVersionRangeOrIncompatibilityError(update)\n+        .fold(versionRange => Left(Some(versionRange)), error => Right(error))\n+    }\n+\n+    if (update.feature.isEmpty) {\n+      // Check that the feature name is not empty.\n+      Right(new ApiError(Errors.INVALID_REQUEST, \"Feature name can not be empty.\"))\n+    } else {\n+      // We handle deletion requests separately from non-deletion requests.\n+      if (UpdateFeaturesRequest.isDeleteRequest(update)) {\n+        if (existingVersionRange.isEmpty) {\n+          // Disallow deletion of a non-existing finalized feature.\n+          Right(new ApiError(Errors.INVALID_REQUEST,\n+                             \"Can not delete non-existing finalized feature.\"))\n+        } else {\n+          Left(Option.empty)\n+        }\n+      } else if (update.maxVersionLevel() < 1) {\n+        // Disallow deletion of a finalized feature without allowDowngrade flag set.\n+        Right(new ApiError(Errors.INVALID_REQUEST,\n+                           s\"Can not provide maxVersionLevel: ${update.maxVersionLevel} less\" +\n+                           s\" than 1 without setting the allowDowngrade flag to true in the request.\"))\n+      } else {\n+        existingVersionRange.map(existing =>\n+          if (update.maxVersionLevel == existing.max) {\n+            // Disallow a case where target maxVersionLevel matches existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not ${if (update.allowDowngrade) \"downgrade\" else \"upgrade\"}\" +\n+                               s\" a finalized feature from existing maxVersionLevel:${existing.max}\" +\n+                               \" to the same value.\"))\n+          } else if (update.maxVersionLevel < existing.max && !update.allowDowngrade) {\n+            // Disallow downgrade of a finalized feature without the allowDowngrade flag set.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature from existing\" +\n+                               s\" maxVersionLevel:${existing.max} to provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} without setting the\" +\n+                               \" allowDowngrade flag in the request.\"))\n+          } else if (update.allowDowngrade && update.maxVersionLevel > existing.max) {\n+            // Disallow a request that sets allowDowngrade flag without specifying a\n+            // maxVersionLevel that's lower than the existing maxVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"When the allowDowngrade flag set in the request, the provided\" +\n+                               s\" maxVersionLevel:${update.maxVersionLevel} can not be greater than\" +\n+                               s\" existing maxVersionLevel:${existing.max}.\"))\n+          } else if (update.maxVersionLevel < existing.min) {\n+            // Disallow downgrade of a finalized feature below the existing finalized\n+            // minVersionLevel.\n+            Right(new ApiError(Errors.INVALID_REQUEST,\n+                               s\"Can not downgrade finalized feature to maxVersionLevel:${update.maxVersionLevel}\" +\n+                               s\" because it's lower than the existing minVersionLevel:${existing.min}.\"))\n+          } else {\n+            newVersionRangeOrError(update)\n+          }\n+        ).getOrElse(newVersionRangeOrError(update))\n+      }\n+    }\n+  }\n+\n+  private def processFeatureUpdates(request: UpdateFeaturesRequest,\n+                                    callback: UpdateFeaturesCallback): Unit = {\n+    if (isActive) {\n+      processFeatureUpdatesWithActiveController(request, callback)\n+    } else {\n+      callback(Left(new ApiError(Errors.NOT_CONTROLLER)))\n+    }\n+  }\n+\n+  private def processFeatureUpdatesWithActiveController(request: UpdateFeaturesRequest,\n+                                                        callback: UpdateFeaturesCallback): Unit = {\n+    val updates = request.data.featureUpdates\n+    val existingFeatures = featureCache.get\n+      .map(featuresAndEpoch => featuresAndEpoch.features.features().asScala)\n+      .getOrElse(Map[String, FinalizedVersionRange]())\n+    // A map with key being feature name and value being FinalizedVersionRange.\n+    // This contains the target features to be eventually written to FeatureZNode.\n+    val targetFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]() ++ existingFeatures\n+    // A map with key being feature name and value being error encountered when the FeatureUpdate\n+    // was applied.\n+    val errors = scala.collection.mutable.Map[String, ApiError]()\n+\n+    // Below we process each FeatureUpdate using the following logic:\n+    //  - If a FeatureUpdate is found to be valid, then:\n+    //    - The corresponding entry in errors map would be updated to contain ApiError(Errors.NONE).\n+    //    - If the FeatureUpdate is an add or update request, then the targetFeatures map is updated\n+    //      to contain the new FinalizedVersionRange for the feature.\n+    //    - Otherwise if the FeatureUpdate is a delete request, then the feature is removed from the\n+    //      targetFeatures map.\n+    //  - Otherwise if a FeatureUpdate is found to be invalid, then:\n+    //    - The corresponding entry in errors map would be updated with the appropriate ApiError.\n+    //    - The entry in targetFeatures map is left untouched.\n+    updates.asScala.iterator.foreach { update =>\n+      validateFeatureUpdate(update, existingFeatures.get(update.feature())) match {\n+        case Left(newVersionRangeOrNone) =>\n+          newVersionRangeOrNone\n+            .map(newVersionRange => targetFeatures += (update.feature() -> newVersionRange))\n+            .getOrElse(targetFeatures -= update.feature())\n+          errors += (update.feature() -> new ApiError(Errors.NONE))\n+        case Right(featureUpdateFailureReason) =>\n+          errors += (update.feature() -> featureUpdateFailureReason)\n+      }\n+    }\n+\n+    // If the existing and target features are the same, then, we skip the update to the\n+    // FeatureZNode as no changes to the node are required. Otherwise, we replace the contents\n+    // of the FeatureZNode with the new features. This may result in partial or full modification\n+    // of the existing finalized features in ZK.\n+    try {\n+      if (!existingFeatures.equals(targetFeatures)) {\n+        val newNode = new FeatureZNode(FeatureZNodeStatus.Enabled, Features.finalizedFeatures(targetFeatures.asJava))\n+        val newVersion = zkClient.updateFeatureZNode(newNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    } catch {\n+      // For all features that correspond to valid FeatureUpdate (i.e. error is Errors.NONE),\n+      // we set the error as Errors.FEATURE_UPDATE_FAILED since the FeatureZNode update has failed\n+      // for these. For the rest, the existing error is left untouched.\n+      case e: Exception =>\n+        errors.foreach { case (feature, apiError) =>\n+          if (apiError.error() == Errors.NONE) {\n+            errors(feature) = new ApiError(Errors.FEATURE_UPDATE_FAILED,\n+                                           Errors.FEATURE_UPDATE_FAILED.message() + \" Error: \" + e)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNjAwNQ=="}, "originalCommit": null, "originalPosition": 519}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDY4MDk2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNTo0NFrOGvcLSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNTo0NFrOGvcLSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM5Nzg5OA==", "bodyText": "add doc", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452397898", "createdAt": "2020-07-09T18:05:44Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,10 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDY4MTM2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNTo1MFrOGvcLjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNTo1MFrOGvcLjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM5Nzk2Ng==", "bodyText": "add doc", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452397966", "createdAt": "2020-07-09T18:05:50Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,10 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    UpdateFinalizedFeaturesResult updateFinalizedFeatures(Set<FeatureUpdate> featureUpdates, UpdateFinalizedFeaturesOptions options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDY4MjQ3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNjoxNFrOGvcMSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNjoxNFrOGvcMSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM5ODE1NA==", "bodyText": "add doc", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452398154", "createdAt": "2020-07-09T18:06:14Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,26 @@\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+    private boolean shouldUseControllerAsDestination = false;\n+\n+    public DescribeFeaturesOptions shouldUseControllerAsDestination(boolean shouldUse) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDY4NDI1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNjo0OVrOGvcNgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowNjo0OVrOGvcNgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM5ODQ2NA==", "bodyText": "remove word 'should'", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452398464", "createdAt": "2020-07-09T18:06:49Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,26 @@\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+    private boolean shouldUseControllerAsDestination = false;\n+\n+    public DescribeFeaturesOptions shouldUseControllerAsDestination(boolean shouldUse) {\n+        shouldUseControllerAsDestination = shouldUse;\n+        return this;\n+    }\n+\n+    public boolean shouldUseControllerAsDestination() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDY5MjIwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowOToxN1rOGvcSgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODowOToxN1rOGvcSgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM5OTc0NA==", "bodyText": "add doc to entire class", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452399744", "createdAt": "2020-07-09T18:09:17Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,70 @@\n+package org.apache.kafka.clients.admin;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+import java.util.Objects;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+public class FeatureMetadata {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDY5NTEwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMDoxMFrOGvcUcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMDoxMFrOGvcUcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwMDI0MA==", "bodyText": "add doc to entire class", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452400240", "createdAt": "2020-07-09T18:10:10Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+public class FeatureUpdate {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDY5NTk3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMDoyN1rOGvcU-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMDoyN1rOGvcU-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwMDM3Ng==", "bodyText": "attributes can be final", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452400376", "createdAt": "2020-07-09T18:10:27Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+public class FeatureUpdate {\n+    private String name;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDcwNTc4OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMzoyMFrOGvcbQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMzoyMFrOGvcbQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwMTk4Nw==", "bodyText": "add test code in KafkaAdminClientTest\nfinal variable names", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452401987", "createdAt": "2020-07-09T18:13:20Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3979,6 +3986,98 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDcwNjcyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMzozNlrOGvcbzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxMzozNlrOGvcbzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwMjEyNw==", "bodyText": "add test code in KafkaAdminClientTest", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452402127", "createdAt": "2020-07-09T18:13:36Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3979,6 +3986,98 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        Call callViaLeastLoadedNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new LeastLoadedNodeProvider()) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        Call call = callViaLeastLoadedNode;\n+        if (options.shouldUseControllerAsDestination()) {\n+            call = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+                new ControllerNodeProvider()) {\n+\n+                @Override\n+                ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                    return (ApiVersionsRequest.Builder) callViaLeastLoadedNode.createRequest(timeoutMs);\n+                }\n+\n+                @Override\n+                void handleResponse(AbstractResponse response) {\n+                    callViaLeastLoadedNode.handleResponse(response);\n+                }\n+\n+                @Override\n+                void handleFailure(Throwable throwable) {\n+                    callViaLeastLoadedNode.handleFailure(throwable);\n+                }\n+            };\n+        }\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFinalizedFeaturesResult updateFinalizedFeatures(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDcxNDk1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxNjoxM1rOGvchaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoxNjoxM1rOGvchaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwMzU2MQ==", "bodyText": "1 line gap before cal", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452403561", "createdAt": "2020-07-09T18:16:13Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3979,6 +3986,98 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        Call callViaLeastLoadedNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new LeastLoadedNodeProvider()) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        Call call = callViaLeastLoadedNode;\n+        if (options.shouldUseControllerAsDestination()) {\n+            call = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+                new ControllerNodeProvider()) {\n+\n+                @Override\n+                ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                    return (ApiVersionsRequest.Builder) callViaLeastLoadedNode.createRequest(timeoutMs);\n+                }\n+\n+                @Override\n+                void handleResponse(AbstractResponse response) {\n+                    callViaLeastLoadedNode.handleResponse(response);\n+                }\n+\n+                @Override\n+                void handleFailure(Throwable throwable) {\n+                    callViaLeastLoadedNode.handleFailure(throwable);\n+                }\n+            };\n+        }\n+        runnable.call(call, now);\n+        return new DescribeFeaturesResult(future);\n+    }\n+\n+    @Override\n+    public UpdateFinalizedFeaturesResult updateFinalizedFeatures(\n+        Set<FeatureUpdate> featureUpdates, UpdateFinalizedFeaturesOptions options) {\n+        final KafkaFutureImpl<Void> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        Call call = new Call(\"updateFinalizedFeatures\", calcDeadlineMs(now, options.timeoutMs()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDczMjcwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMToxOFrOGvcseQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMToxOFrOGvcseQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwNjM5Mw==", "bodyText": "Eliminate and use INVALID_REQUEST", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452406393", "createdAt": "2020-07-09T18:21:18Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "diffHunk": "@@ -319,7 +322,10 @@\n     GROUP_SUBSCRIBED_TO_TOPIC(86, \"Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it.\",\n         GroupSubscribedToTopicException::new),\n     INVALID_RECORD(87, \"This record has failed the validation on broker and hence will be rejected.\", InvalidRecordException::new),\n-    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new);\n+    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new),\n+    INCOMPATIBLE_FEATURES(89, \"Could not apply finalized feature updates due to incompatible features.\", IncompatibleFeaturesException::new),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDczNDkyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMTo0OVrOGvctwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMTo0OVrOGvctwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwNjcyMQ==", "bodyText": "Eliminate and use INVALID_REQUEST", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452406721", "createdAt": "2020-07-09T18:21:49Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "diffHunk": "@@ -319,7 +322,10 @@\n     GROUP_SUBSCRIBED_TO_TOPIC(86, \"Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it.\",\n         GroupSubscribedToTopicException::new),\n     INVALID_RECORD(87, \"This record has failed the validation on broker and hence will be rejected.\", InvalidRecordException::new),\n-    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new);\n+    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new),\n+    INCOMPATIBLE_FEATURES(89, \"Could not apply finalized feature updates due to incompatible features.\", IncompatibleFeaturesException::new),\n+    FEATURE_VERSIONING_DISABLED(90, \"Feature versioning system is disabled.\", FeatureVersioningDisabledException::new),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDczODAwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMjo0MlrOGvcvsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMjo0MlrOGvcvsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwNzIxOQ==", "bodyText": "space between \",\" and \"key\"", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452407219", "createdAt": "2020-07-09T18:22:42Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -68,6 +69,36 @@ public ApiVersionsResponse(Struct struct, short version) {\n         this(new ApiVersionsResponseData(struct, version));\n     }\n \n+    public ApiVersionsResponseData data() {\n+        return data;\n+    }\n+\n+    public Features<SupportedVersionRange> supportedFeatures() {\n+        Map<String, SupportedVersionRange> features = new HashMap<>();\n+\n+        for (SupportedFeatureKey key : data.supportedFeatures().valuesSet()) {\n+            features.put(key.name(), new SupportedVersionRange(key.minVersion(),key.maxVersion()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDczODIwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMjo0NlrOGvcv1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMjo0NlrOGvcv1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwNzI1NQ==", "bodyText": "final", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452407255", "createdAt": "2020-07-09T18:22:46Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -68,6 +69,36 @@ public ApiVersionsResponse(Struct struct, short version) {\n         this(new ApiVersionsResponseData(struct, version));\n     }\n \n+    public ApiVersionsResponseData data() {\n+        return data;\n+    }\n+\n+    public Features<SupportedVersionRange> supportedFeatures() {\n+        Map<String, SupportedVersionRange> features = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDczODY3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMjo1NlrOGvcwIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyMjo1NlrOGvcwIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwNzMyOQ==", "bodyText": "final", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452407329", "createdAt": "2020-07-09T18:22:56Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -68,6 +69,36 @@ public ApiVersionsResponse(Struct struct, short version) {\n         this(new ApiVersionsResponseData(struct, version));\n     }\n \n+    public ApiVersionsResponseData data() {\n+        return data;\n+    }\n+\n+    public Features<SupportedVersionRange> supportedFeatures() {\n+        Map<String, SupportedVersionRange> features = new HashMap<>();\n+\n+        for (SupportedFeatureKey key : data.supportedFeatures().valuesSet()) {\n+            features.put(key.name(), new SupportedVersionRange(key.minVersion(),key.maxVersion()));\n+        }\n+\n+        return Features.supportedFeatures(features);\n+    }\n+\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        Map<String, FinalizedVersionRange> features = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDc0Njc2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFinalizedFeaturesRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyNToxNVrOGvc1Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyNToxNVrOGvc1Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQwODYxNA==", "bodyText": "make variables final throught class\nadd doc", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452408614", "createdAt": "2020-07-09T18:25:15Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFinalizedFeaturesRequest.java", "diffHunk": "@@ -0,0 +1,72 @@\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesResponseData;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFinalizedFeaturesRequest extends AbstractRequest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDc1NTkwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFinalizedFeaturesResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyNzo0NVrOGvc6nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyNzo0NVrOGvc6nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQxMDAxNQ==", "bodyText": "Fix ApiKeys", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452410015", "createdAt": "2020-07-09T18:27:45Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFinalizedFeaturesResponse.java", "diffHunk": "@@ -0,0 +1,54 @@\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import org.apache.kafka.common.message.AlterPartitionReassignmentsResponseData;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFinalizedFeaturesResponse extends AbstractResponse {\n+\n+    public final UpdateFinalizedFeaturesResponseData data;\n+\n+    public UpdateFinalizedFeaturesResponse(UpdateFinalizedFeaturesResponseData data) {\n+        this.data = data;\n+    }\n+\n+    public UpdateFinalizedFeaturesResponse(Struct struct) {\n+        short latestVersion = (short) (UpdateFinalizedFeaturesResponseData.SCHEMAS.length - 1);\n+        this.data = new UpdateFinalizedFeaturesResponseData(struct, latestVersion);\n+    }\n+\n+    public UpdateFinalizedFeaturesResponse(Struct struct, short version) {\n+        this.data = new UpdateFinalizedFeaturesResponseData(struct, version);\n+    }\n+\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n+\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        return errorCounts(Errors.forCode(data.errorCode()));\n+    }\n+\n+    @Override\n+    protected Struct toStruct(short version) {\n+        return data.toStruct(version);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return data.toString();\n+    }\n+\n+    public UpdateFinalizedFeaturesResponseData data() {\n+        return data;\n+    }\n+\n+    public static UpdateFinalizedFeaturesResponse parse(ByteBuffer buffer, short version) {\n+        return new UpdateFinalizedFeaturesResponse(ApiKeys.SYNC_GROUP.parseResponse(version, buffer), version);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDc1OTUxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyODo1NFrOGvc8_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODoyODo1NFrOGvc8_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQxMDYyMA==", "bodyText": "Eliminate timeout?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452410620", "createdAt": "2020-07-09T18:28:54Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,35 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFinalizedFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"timeoutMs\", \"type\": \"int32\", \"versions\": \"0+\", \"default\": \"60000\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDc3NjgxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODozNDowMlrOGvdHow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODozNDowMlrOGvdHow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQxMzM0Nw==", "bodyText": "add doc and explain various cases", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452413347", "createdAt": "2020-07-09T18:34:02Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +277,24 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def setupFeatureZNode(newNode: FeatureZNode): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDc3OTM5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODozNDo1NFrOGvdJUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODozNDo1NFrOGvdJUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQxMzc3Nw==", "bodyText": "call the variable as nodeContents ?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452413777", "createdAt": "2020-07-09T18:34:54Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +277,24 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def setupFeatureZNode(newNode: FeatureZNode): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDgwODY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODo0NDowMFrOGvdbtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODo0NDowMFrOGvdbtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQxODQ4Nw==", "bodyText": "Perhaps add info about newFeatures and incompatibleBrokers.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452418487", "createdAt": "2020-07-09T18:44:00Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1693,33 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (numIncompatibleBrokers > 0) {\n+        callback(\n+          Errors.INCOMPATIBLE_FEATURES,\n+          Some(\n+            s\"Could not apply finalized feature updates because $numIncompatibleBrokers brokers\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDgwOTk0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODo0NDoyNFrOGvdcdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODo0NDoyNFrOGvdcdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQxODY3OQ==", "bodyText": "can improve by splitting into few lines", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452418679", "createdAt": "2020-07-09T18:44:24Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -24,13 +24,13 @@ import org.apache.kafka.common.feature.Features._\n import scala.jdk.CollectionConverters._\n \n /**\n- * A common immutable object used in the Broker to define the latest features supported by the\n- * Broker. Also provides API to check for incompatibilities between the latest features supported\n- * by the Broker and cluster-wide finalized features.\n+ * A class that defines the latest features supported by the Broker, and the finalized cluster-wide", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDg1OTU4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODo1OTowNVrOGvd7Yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxODo1OTowNVrOGvd7Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQyNjU5NQ==", "bodyText": "check braces ()", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452426595", "createdAt": "2020-07-09T18:59:05Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -39,21 +39,40 @@ object SupportedFeatures extends Logging {\n    */\n   @volatile private var supportedFeatures = emptySupportedFeatures\n \n+  /**\n+   * This is the cluster-wide finalized minimum version levels.\n+   * This is currently empty, but in the future as we define supported features, this map can be\n+   * populated in cases where minimum version level of a finalized feature is advanced beyond 1.\n+   */\n+  @volatile private var finalizedFeatureMinVersionLevels = Map[String, Short]()\n+\n   /**\n    * Returns a reference to the latest features supported by the Broker.\n    */\n-  def get: Features[SupportedVersionRange] = {\n+  def getSupportedFeatures: Features[SupportedVersionRange] = {\n     supportedFeatures\n   }\n \n   // For testing only.\n-  def update(newFeatures: Features[SupportedVersionRange]): Unit = {\n+  def updateSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n     supportedFeatures = newFeatures\n   }\n \n+  def getFinalizedMinVersionLevel(feature: String): Short = {\n+    finalizedFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n   // For testing only.\n-  def clear(): Unit = {\n-    supportedFeatures = emptySupportedFeatures\n+  def updateMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    finalizedFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDg2ODc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOTowMTo0NFrOGveBLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOTowMTo0NFrOGveBLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQyODA3OQ==", "bodyText": "s/supported/supportedFeatures\nsame for other one", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452428079", "createdAt": "2020-07-09T19:01:44Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -71,9 +90,37 @@ object SupportedFeatures extends Logging {\n    *                    is empty, it means there were no feature incompatibilities found.\n    */\n   def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(getSupportedFeatures, finalized, true)\n+  }\n+\n+}\n+\n+object BrokerFeatures extends Logging {\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supported   The supported features to be compared\n+   * @param finalized   The finalized features to be compared\n+   *\n+   * @return            - True if there are any incompatibilities.\n+   *                    - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(\n+    supported: Features[SupportedVersionRange],", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDg2OTkyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOTowMjowN1rOGveCBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOTowMjowN1rOGveCBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQyODI5Mw==", "bodyText": "say \"if there are any feature incompatibilities found.\"", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452428293", "createdAt": "2020-07-09T19:02:07Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -71,9 +90,37 @@ object SupportedFeatures extends Logging {\n    *                    is empty, it means there were no feature incompatibilities found.\n    */\n   def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(getSupportedFeatures, finalized, true)\n+  }\n+\n+}\n+\n+object BrokerFeatures extends Logging {\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supported   The supported features to be compared\n+   * @param finalized   The finalized features to be compared\n+   *\n+   * @return            - True if there are any incompatibilities.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDg3MzM3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOTowMzoyMlrOGveEOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOTowMzoyMlrOGveEOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQyODg1OA==", "bodyText": "Add unit test\nAdd doc", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452428858", "createdAt": "2020-07-09T19:03:22Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -53,6 +56,26 @@ object FinalizedFeatureCache extends Logging {\n     featuresAndEpoch.isEmpty\n   }\n \n+  def waitUntilEpochOrThrow(expectedMinEpoch: Int, timeoutMs: Long): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDg5NjM4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxMDo1NlrOGveTEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxMDo1NlrOGveTEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMjY1OA==", "bodyText": "Shouldn't the code be waiting here?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452432658", "createdAt": "2020-07-09T19:10:56Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1693,33 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val numIncompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.count(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (numIncompatibleBrokers > 0) {\n+        callback(\n+          Errors.INCOMPATIBLE_FEATURES,\n+          Some(\n+            s\"Could not apply finalized feature updates because $numIncompatibleBrokers brokers\" +\n+            \" were found to have incompatible features.\"))\n+      } else {\n+        try {\n+          zkClient.updateFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDkxNjY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxNjo1N1rOGvefPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxNjo1N1rOGvefPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzNTc3Mg==", "bodyText": "add doc", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452435772", "createdAt": "2020-07-09T19:16:57Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,113 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.FEATURE_VERSIONING_DISABLED, Option.empty)\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDkxNzU2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxNzoxNVrOGvefyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxNzoxNVrOGvefyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzNTkxMw==", "bodyText": "remove these 2 lines", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452435913", "createdAt": "2020-07-09T19:17:15Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -1480,6 +1480,8 @@ class KafkaConfig(val props: java.util.Map[_, _], doLog: Boolean, dynamicConfigO\n   def logMessageTimestampDifferenceMaxMs: Long = getLong(KafkaConfig.LogMessageTimestampDifferenceMaxMsProp)\n   def logMessageDownConversionEnable: Boolean = getBoolean(KafkaConfig.LogMessageDownConversionEnableProp)\n \n+  /** Feature  configuration*/", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMDkxOTg3OnYy", "diffSide": "RIGHT", "path": "core/src/test/resources/log4j.properties", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxNzo0MVrOGvehCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxNzo0MVrOGvehCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzNjIzMw==", "bodyText": "revert the file eventually", "url": "https://github.com/apache/kafka/pull/9001#discussion_r452436233", "createdAt": "2020-07-09T19:17:41Z", "author": {"login": "kowshik"}, "path": "core/src/test/resources/log4j.properties", "diffHunk": "@@ -12,14 +12,80 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMDQ3OTQzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxODoyMTo0MFrOGw0YBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzowNjoxNFrOG0mapA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0Mjk0OQ==", "bodyText": "nit: get a  {@link UpdateFinalizedFeaturesResult} as well", "url": "https://github.com/apache/kafka/pull/9001#discussion_r453842949", "createdAt": "2020-07-13T18:21:40Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.FinalizedFeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the set of finalized feature updates\n+     * @param options          the options to use\n+     *\n+     * @return                 the UpdateFinalizedFeaturesResult containing the result", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwODU0OA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457808548", "createdAt": "2020-07-21T03:06:14Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.FinalizedFeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the set of finalized feature updates\n+     * @param options          the options to use\n+     *\n+     * @return                 the UpdateFinalizedFeaturesResult containing the result", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0Mjk0OQ=="}, "originalCommit": null, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTE0MTkwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMjo1NzoxMFrOGy_kSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMjo1NjowNlrOG0mRYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyMzQ2Nw==", "bodyText": "s/as input a set of FinalizedFeatureUpdate/in a set of feature updates", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456123467", "createdAt": "2020-07-16T22:57:10Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwNjE3Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457806176", "createdAt": "2020-07-21T02:56:06Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyMzQ2Nw=="}, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTE1MDI1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMzowMDo1MFrOGy_pJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMjo1NzowMFrOG0mSPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyNDcwOA==", "bodyText": "For the entire sentence, I assume you want to say something like\nIt could be done by turning on the allowDowngrade flag and setting the max version level to be less than 1", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456124708", "createdAt": "2020-07-16T23:00:50Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwNjM5OQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457806399", "createdAt": "2020-07-21T02:57:00Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyNDcwOA=="}, "originalCommit": null, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTE1MzY1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMzowMjozMVrOGy_rHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODozNToxNFrOG4CZaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyNTIxMg==", "bodyText": "Looking at UpdateFinalizedFeaturesResult, we don't have a per feature based error code returned. If this is the case, how could we know which feature is missing?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456125212", "createdAt": "2020-07-16T23:02:31Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwNjU0MQ==", "bodyText": "To your point, this information is available in the error message returned in the response.\nThe feature updates are atomically applied to ZK by the controller i.e it is all or none. We don't have a use case (yet) where we have to programmatically learn which feature updates are incorrect. Instead an error message with details seems sufficient to us. Please let me know how you feel about it, and if you feel that we are better off in returning per-feature-update error code. This was discussed in the KIP-584 thread, search for the word \"transaction\".", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457806541", "createdAt": "2020-07-21T02:57:43Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyNTIxMg=="}, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxMjcxMw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461412713", "createdAt": "2020-07-28T08:35:14Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyNTIxMg=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTE2MjAyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMzowNjowNVrOGy_v8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMjo0MDo0OVrOG0mB7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyNjQ1MA==", "bodyText": "We should suggest in what circumstances a user may require sending the request directly to the controller, to me if there is a case where user wants stronger consistency.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456126450", "createdAt": "2020-07-16T23:06:05Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMjIyMA==", "bodyText": "Done. Updated the doc now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457802220", "createdAt": "2020-07-21T02:40:49Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyNjQ1MA=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTMyNjUyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMDoyNTo1NlrOGzBOiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzoxOTo1M1rOG0mpdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MDY2NA==", "bodyText": "We should consider using Optional for finalizedFeaturesEpoch to indicate absence.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456150664", "createdAt": "2020-07-17T00:25:56Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final int finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(\n+        final Features<FinalizedVersionRange> finalizedFeatures,\n+        final int finalizedFeaturesEpoch,\n+        final Features<SupportedVersionRange> supportedFeatures\n+    ) {\n+        Objects.requireNonNull(finalizedFeatures, \"Provided finalizedFeatures can not be null.\");\n+        Objects.requireNonNull(supportedFeatures, \"Provided supportedFeatures can not be null.\");\n+        this.finalizedFeatures = finalizedFeatures;\n+        this.finalizedFeaturesEpoch = finalizedFeaturesEpoch;\n+        this.supportedFeatures = supportedFeatures;\n+    }\n+\n+    /**\n+     * A map of finalized feature versions, with key being finalized feature name and value\n+     * containing the min/max version levels for the finalized feature.\n+     */\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        return finalizedFeatures;\n+    }\n+\n+    /**\n+     * The epoch for the finalized features.\n+     * Valid values are >= 0. A value < 0 means the finalized features are absent/unavailable.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxMjM0MA==", "bodyText": "Done. Good point.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457812340", "createdAt": "2020-07-21T03:19:53Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import org.apache.kafka.common.feature.Features;\n+import org.apache.kafka.common.feature.FinalizedVersionRange;\n+import org.apache.kafka.common.feature.SupportedVersionRange;\n+\n+/**\n+ * Encapsulates details about finalized as well as supported features. This is particularly useful\n+ * to hold the result returned by the {@link Admin#describeFeatures(DescribeFeaturesOptions)} API.\n+ */\n+public class FeatureMetadata {\n+\n+    private final Features<FinalizedVersionRange> finalizedFeatures;\n+\n+    private final int finalizedFeaturesEpoch;\n+\n+    private final Features<SupportedVersionRange> supportedFeatures;\n+\n+    public FeatureMetadata(\n+        final Features<FinalizedVersionRange> finalizedFeatures,\n+        final int finalizedFeaturesEpoch,\n+        final Features<SupportedVersionRange> supportedFeatures\n+    ) {\n+        Objects.requireNonNull(finalizedFeatures, \"Provided finalizedFeatures can not be null.\");\n+        Objects.requireNonNull(supportedFeatures, \"Provided supportedFeatures can not be null.\");\n+        this.finalizedFeatures = finalizedFeatures;\n+        this.finalizedFeaturesEpoch = finalizedFeaturesEpoch;\n+        this.supportedFeatures = supportedFeatures;\n+    }\n+\n+    /**\n+     * A map of finalized feature versions, with key being finalized feature name and value\n+     * containing the min/max version levels for the finalized feature.\n+     */\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        return finalizedFeatures;\n+    }\n+\n+    /**\n+     * The epoch for the finalized features.\n+     * Valid values are >= 0. A value < 0 means the finalized features are absent/unavailable.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MDY2NA=="}, "originalCommit": null, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTMzMDkyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMDoyNzo1N1rOGzBQ9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzoyMTowMFrOG0mq0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MTI4Nw==", "bodyText": "nit: one parameter each line, with the first parameter on the same line as constructor name.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456151287", "createdAt": "2020-07-17T00:27:57Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxMjY5MA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457812690", "createdAt": "2020-07-21T03:21:00Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MTI4Nw=="}, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTMzMjE2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMDoyODo0MVrOGzBRtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzoyMTo0NFrOG0mrgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MTQ3Ng==", "bodyText": "nit: seems not necessary", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456151476", "createdAt": "2020-07-17T00:28:41Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(\n+        final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +\n+                    \" provided maxVersionLevel:%d is < 1.\", featureName, maxVersionLevel));\n+        }\n+        this.featureName = featureName;\n+        this.maxVersionLevel = maxVersionLevel;\n+        this.allowDowngrade = allowDowngrade;\n+    }\n+\n+    /**\n+     * @return   the name of the finalized feature to be updated.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxMjg2Nw==", "bodyText": "Done. Removed.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457812867", "createdAt": "2020-07-21T03:21:44Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(\n+        final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +\n+                    \" provided maxVersionLevel:%d is < 1.\", featureName, maxVersionLevel));\n+        }\n+        this.featureName = featureName;\n+        this.maxVersionLevel = maxVersionLevel;\n+        this.allowDowngrade = allowDowngrade;\n+    }\n+\n+    /**\n+     * @return   the name of the finalized feature to be updated.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MTQ3Ng=="}, "originalCommit": null, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTMzMzIzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMDoyOToxNlrOGzBSUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzoyMjoxN1rOG0msHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MTYzNQ==", "bodyText": "false otherwise doesn't provide too much useful info.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456151635", "createdAt": "2020-07-17T00:29:16Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(\n+        final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +\n+                    \" provided maxVersionLevel:%d is < 1.\", featureName, maxVersionLevel));\n+        }\n+        this.featureName = featureName;\n+        this.maxVersionLevel = maxVersionLevel;\n+        this.allowDowngrade = allowDowngrade;\n+    }\n+\n+    /**\n+     * @return   the name of the finalized feature to be updated.\n+     */\n+    public String featureName() {\n+        return featureName;\n+    }\n+\n+    /**\n+     * @return   the new maximum version level for the finalized feature.\n+     */\n+    public short maxVersionLevel() {\n+        return maxVersionLevel;\n+    }\n+\n+    /**\n+     * @return   - true, if this feature update was meant to downgrade the maximum version level of\n+     *             the finalized feature.\n+     *           - false, otherwise.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxMzAyMQ==", "bodyText": "Done. Removed.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457813021", "createdAt": "2020-07-21T03:22:17Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(\n+        final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +\n+                    \" provided maxVersionLevel:%d is < 1.\", featureName, maxVersionLevel));\n+        }\n+        this.featureName = featureName;\n+        this.maxVersionLevel = maxVersionLevel;\n+        this.allowDowngrade = allowDowngrade;\n+    }\n+\n+    /**\n+     * @return   the name of the finalized feature to be updated.\n+     */\n+    public String featureName() {\n+        return featureName;\n+    }\n+\n+    /**\n+     * @return   the new maximum version level for the finalized feature.\n+     */\n+    public short maxVersionLevel() {\n+        return maxVersionLevel;\n+    }\n+\n+    /**\n+     * @return   - true, if this feature update was meant to downgrade the maximum version level of\n+     *             the finalized feature.\n+     *           - false, otherwise.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE1MTYzNQ=="}, "originalCommit": null, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI3MTc5OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyMDo0MVrOGzc1gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzoyMjo1M1rOG0msoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMzAxMA==", "bodyText": "nit: we could just return new UpdateFinalizedFeaturesRequestData().setFinalizedFeatureUpdates(items)", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456603010", "createdAt": "2020-07-17T18:20:41Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(\n+        final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +\n+                    \" provided maxVersionLevel:%d is < 1.\", featureName, maxVersionLevel));\n+        }\n+        this.featureName = featureName;\n+        this.maxVersionLevel = maxVersionLevel;\n+        this.allowDowngrade = allowDowngrade;\n+    }\n+\n+    /**\n+     * @return   the name of the finalized feature to be updated.\n+     */\n+    public String featureName() {\n+        return featureName;\n+    }\n+\n+    /**\n+     * @return   the new maximum version level for the finalized feature.\n+     */\n+    public short maxVersionLevel() {\n+        return maxVersionLevel;\n+    }\n+\n+    /**\n+     * @return   - true, if this feature update was meant to downgrade the maximum version level of\n+     *             the finalized feature.\n+     *           - false, otherwise.\n+     */\n+    public boolean allowDowngrade() {\n+        return allowDowngrade;\n+    }\n+\n+    /**\n+     * Helper function that creates {@link UpdateFinalizedFeaturesRequestData} from a set of\n+     * {@link FinalizedFeatureUpdate}.\n+     *\n+     * @param updates   the set of {@link FinalizedFeatureUpdate}\n+     *\n+     * @return          a newly constructed UpdateFinalizedFeaturesRequestData object\n+     */\n+    public static UpdateFinalizedFeaturesRequestData createRequest(Set<FinalizedFeatureUpdate> updates) {\n+        final UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKeyCollection items\n+            = new UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKeyCollection();\n+        for (FinalizedFeatureUpdate update : updates) {\n+            final UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKey item =\n+                new UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKey();\n+            item.setName(update.featureName());\n+            item.setMaxVersionLevel(update.maxVersionLevel());\n+            item.setAllowDowngrade(update.allowDowngrade());\n+            items.add(item);\n+        }\n+        final UpdateFinalizedFeaturesRequestData data = new UpdateFinalizedFeaturesRequestData();\n+        data.setFinalizedFeatureUpdates(items);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxMzE1Mg==", "bodyText": "Done. Good point.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457813152", "createdAt": "2020-07-21T03:22:53Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FinalizedFeatureUpdate.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the\n+ * {@link Admin#updateFinalizedFeatures(Set, UpdateFinalizedFeaturesOptions)} API request.\n+ */\n+public class FinalizedFeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FinalizedFeatureUpdate(\n+        final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +\n+                    \" provided maxVersionLevel:%d is < 1.\", featureName, maxVersionLevel));\n+        }\n+        this.featureName = featureName;\n+        this.maxVersionLevel = maxVersionLevel;\n+        this.allowDowngrade = allowDowngrade;\n+    }\n+\n+    /**\n+     * @return   the name of the finalized feature to be updated.\n+     */\n+    public String featureName() {\n+        return featureName;\n+    }\n+\n+    /**\n+     * @return   the new maximum version level for the finalized feature.\n+     */\n+    public short maxVersionLevel() {\n+        return maxVersionLevel;\n+    }\n+\n+    /**\n+     * @return   - true, if this feature update was meant to downgrade the maximum version level of\n+     *             the finalized feature.\n+     *           - false, otherwise.\n+     */\n+    public boolean allowDowngrade() {\n+        return allowDowngrade;\n+    }\n+\n+    /**\n+     * Helper function that creates {@link UpdateFinalizedFeaturesRequestData} from a set of\n+     * {@link FinalizedFeatureUpdate}.\n+     *\n+     * @param updates   the set of {@link FinalizedFeatureUpdate}\n+     *\n+     * @return          a newly constructed UpdateFinalizedFeaturesRequestData object\n+     */\n+    public static UpdateFinalizedFeaturesRequestData createRequest(Set<FinalizedFeatureUpdate> updates) {\n+        final UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKeyCollection items\n+            = new UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKeyCollection();\n+        for (FinalizedFeatureUpdate update : updates) {\n+            final UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKey item =\n+                new UpdateFinalizedFeaturesRequestData.FinalizedFeatureUpdateKey();\n+            item.setName(update.featureName());\n+            item.setMaxVersionLevel(update.maxVersionLevel());\n+            item.setAllowDowngrade(update.allowDowngrade());\n+            items.add(item);\n+        }\n+        final UpdateFinalizedFeaturesRequestData data = new UpdateFinalizedFeaturesRequestData();\n+        data.setFinalizedFeatureUpdates(items);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMzAxMA=="}, "originalCommit": null, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDExNjExOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/UpdateFinalizedFeaturesTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwMjoxOToyMVrOGzr3Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDoxMToyNVrOG0xTFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg0OTE4Ng==", "bodyText": "missing header", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456849186", "createdAt": "2020-07-19T02:19:21Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFinalizedFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,450 @@\n+package kafka.server", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk4NjgzNw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457986837", "createdAt": "2020-07-21T10:11:25Z", "author": {"login": "kowshik"}, "path": "core/src/test/scala/unit/kafka/server/UpdateFinalizedFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,450 @@\n+package kafka.server", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg0OTE4Ng=="}, "originalCommit": null, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDE3NDYwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDowMDo1NFrOGzsRBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzoxMDowNVrOG0mehw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NTgxNQ==", "bodyText": "nit: put first parameter on this line.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456855815", "createdAt": "2020-07-19T04:00:54Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.FinalizedFeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the set of finalized feature updates\n+     * @param options          the options to use\n+     *\n+     * @return                 the UpdateFinalizedFeaturesResult containing the result\n+     */\n+    UpdateFinalizedFeaturesResult updateFinalizedFeatures(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwOTU0Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457809543", "createdAt": "2020-07-21T03:10:05Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.FinalizedFeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the set of finalized feature updates\n+     * @param options          the options to use\n+     *\n+     * @return                 the UpdateFinalizedFeaturesResult containing the result\n+     */\n+    UpdateFinalizedFeaturesResult updateFinalizedFeatures(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NTgxNQ=="}, "originalCommit": null, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDE3NTcwOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDowMjo1MlrOGzsRhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDowODozOVrOG0xNRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NTk0MQ==", "bodyText": "The definition seems not aligned with the KIP which states updateFeatures, do you think it's necessary to mention finalized in all the function signatures?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456855941", "createdAt": "2020-07-19T04:02:52Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.FinalizedFeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the set of finalized feature updates\n+     * @param options          the options to use\n+     *\n+     * @return                 the UpdateFinalizedFeaturesResult containing the result\n+     */\n+    UpdateFinalizedFeaturesResult updateFinalizedFeatures(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk4NTM0OA==", "bodyText": "Done. Removed the word \"finalized\"in the context of this API.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457985348", "createdAt": "2020-07-21T10:08:39Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1214,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker, but it can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result\n+     */\n+    DescribeFeaturesResult describeFeatures(DescribeFeaturesOptions options);\n+\n+    /**\n+     * Applies specified updates to finalized features. The API is atomic, meaning that if a single\n+     * feature update in the request can't succeed on the controller, then none of the feature\n+     * updates are carried out. This request is issued only to the controller since the API is\n+     * only served by the controller.\n+     * <p>\n+     * The API takes as input a set of FinalizedFeatureUpdate that need to be applied. Each such\n+     * update specifies the finalized feature to be added or updated or deleted, along with the new\n+     * max feature version level value.\n+     * <ul>\n+     * <li>Downgrade of feature version level is not a regular operation/intent. It is only allowed\n+     * in the controller if the feature update has the allowDowngrade flag set - setting this flag\n+     * conveys user intent to attempt downgrade of a feature max version level. Note that despite\n+     * the allowDowngrade flag being set, certain downgrades may be rejected by the controller if it\n+     * is deemed impossible.</li>\n+     * <li>Deletion of a finalized feature version is not a regular operation/intent. It is allowed\n+     * only if the allowDowngrade flag is set in the feature update, and, if the max version level\n+     * is set to a value less than 1.</li>\n+     * </ul>\n+     *<p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the futures\n+     * obtained from the returned {@link UpdateFinalizedFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.ClusterAuthorizationException}\n+     *   If the authenticated user didn't have alter access to the cluster.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.InvalidRequestException}\n+     *   If the request details are invalid. e.g., a non-existing finalized feature is attempted\n+     *   to be deleted or downgraded.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the updates could finish. It cannot be guaranteed whether\n+     *   the updates succeeded or not.</li>\n+     *   <li>{@link org.apache.kafka.common.errors.FinalizedFeatureUpdateFailedException}\n+     *   If the updates could not be applied on the controller, despite the request being valid.\n+     *   This may be a temporary problem.</li>\n+     * </ul>\n+     * <p>\n+     * This operation is supported by brokers with version 2.7.0 or higher.\n+\n+     * @param featureUpdates   the set of finalized feature updates\n+     * @param options          the options to use\n+     *\n+     * @return                 the UpdateFinalizedFeaturesResult containing the result\n+     */\n+    UpdateFinalizedFeaturesResult updateFinalizedFeatures(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NTk0MQ=="}, "originalCommit": null, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDE4NTQ2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDoxOToyNlrOGzsV-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzozMjowN1rOG0m00w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NzA4MA==", "bodyText": "It looks weird to complete callViaLeastLoadedNode in a controller response handler. I'm inclined to increase a bit on the code duplication, based on if (options.sendRequestToController()) to have two separate request traces like:\nif (options.sendRequestToController()) {\n            ...\n            runnable.call(callControllerNode, now);\n        } else {\n            ...\n            runnable.call(callViaLeastLoadedNode, now);\n        }\n\nand try to complete the same future.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456857080", "createdAt": "2020-07-19T04:19:26Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3984,6 +3988,108 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        Call callViaLeastLoadedNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new LeastLoadedNodeProvider()) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        Call call = callViaLeastLoadedNode;\n+        if (options.sendRequestToController()) {\n+            call = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+                new ControllerNodeProvider()) {\n+\n+                @Override\n+                ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                    return (ApiVersionsRequest.Builder) callViaLeastLoadedNode.createRequest(timeoutMs);\n+                }\n+\n+                @Override\n+                void handleResponse(AbstractResponse response) {\n+                    final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                    if (apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                        handleNotControllerError(Errors.NOT_CONTROLLER);\n+                    } else {\n+                        callViaLeastLoadedNode.handleResponse(response);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxNTI1MQ==", "bodyText": "Done. Good point.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457815251", "createdAt": "2020-07-21T03:32:07Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3984,6 +3988,108 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+        Call callViaLeastLoadedNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new LeastLoadedNodeProvider()) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        Call call = callViaLeastLoadedNode;\n+        if (options.sendRequestToController()) {\n+            call = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+                new ControllerNodeProvider()) {\n+\n+                @Override\n+                ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                    return (ApiVersionsRequest.Builder) callViaLeastLoadedNode.createRequest(timeoutMs);\n+                }\n+\n+                @Override\n+                void handleResponse(AbstractResponse response) {\n+                    final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                    if (apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code()) {\n+                        handleNotControllerError(Errors.NOT_CONTROLLER);\n+                    } else {\n+                        callViaLeastLoadedNode.handleResponse(response);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NzA4MA=="}, "originalCommit": null, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDE5MTA2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFinalizedFeaturesResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDoyOTo1MFrOGzsYgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzo1ODo1NlrOG0nOXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NzczMA==", "bodyText": "Would be good to redundantly copy over the expected error codes from Admin.java definition, similar to other response class such as OffsetCommitResponse", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456857730", "createdAt": "2020-07-19T04:29:50Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFinalizedFeaturesResponse.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFinalizedFeaturesResponse extends AbstractResponse {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgyMTc5MQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457821791", "createdAt": "2020-07-21T03:58:56Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFinalizedFeaturesResponse.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import org.apache.kafka.common.message.UpdateFinalizedFeaturesResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFinalizedFeaturesResponse extends AbstractResponse {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1NzczMA=="}, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDE5NzI1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDozOToyMVrOGzsbRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo0MzozNFrOG1vhOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1ODQzOQ==", "bodyText": "Seems not necessary to have this helper as it doesn't reduce the code length.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456858439", "createdAt": "2020-07-19T04:39:21Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -177,15 +214,33 @@ public static ApiVersionsResponse createApiVersionsResponse(\n             }\n         }\n \n+        return new ApiVersionsResponse(\n+            createApiVersionsResponseData(\n+                throttleTimeMs,\n+                Errors.NONE,\n+                apiKeys,\n+                latestSupportedFeatures,\n+                finalizedFeatures,\n+                finalizedFeaturesEpoch));\n+    }\n+\n+    public static ApiVersionsResponseData createApiVersionsResponseData(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgyMTE5NQ==", "bodyText": "It calls into couple other helper functions. Let us keep it.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457821195", "createdAt": "2020-07-21T03:56:30Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -177,15 +214,33 @@ public static ApiVersionsResponse createApiVersionsResponse(\n             }\n         }\n \n+        return new ApiVersionsResponse(\n+            createApiVersionsResponseData(\n+                throttleTimeMs,\n+                Errors.NONE,\n+                apiKeys,\n+                latestSupportedFeatures,\n+                finalizedFeatures,\n+                finalizedFeaturesEpoch));\n+    }\n+\n+    public static ApiVersionsResponseData createApiVersionsResponseData(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1ODQzOQ=="}, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNjI2NQ==", "bodyText": "Make sense, after looking further I realized that we also did some data format conversion.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459006265", "createdAt": "2020-07-22T18:43:34Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -177,15 +214,33 @@ public static ApiVersionsResponse createApiVersionsResponse(\n             }\n         }\n \n+        return new ApiVersionsResponse(\n+            createApiVersionsResponseData(\n+                throttleTimeMs,\n+                Errors.NONE,\n+                apiKeys,\n+                latestSupportedFeatures,\n+                finalizedFeatures,\n+                finalizedFeaturesEpoch));\n+    }\n+\n+    public static ApiVersionsResponseData createApiVersionsResponseData(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1ODQzOQ=="}, "originalCommit": null, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDE5OTkxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDo0Mjo0MlrOGzscdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzo1OToyNVrOG0nO5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1ODc0MQ==", "bodyText": "space", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456858741", "createdAt": "2020-07-19T04:42:42Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFinalizedFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FinalizedFeatureUpdates\", \"type\": \"[]FinalizedFeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Name\", \"type\":  \"string\", \"versions\":  \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\":  \"MaxVersionLevel\", \"type\":  \"int16\", \"versions\":  \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgyMTkyNQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457821925", "createdAt": "2020-07-21T03:59:25Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFinalizedFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FinalizedFeatureUpdates\", \"type\": \"[]FinalizedFeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Name\", \"type\":  \"string\", \"versions\":  \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\":  \"MaxVersionLevel\", \"type\":  \"int16\", \"versions\":  \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1ODc0MQ=="}, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIwMjIyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesResponse.json", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDo0NTo0NVrOGzsdeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMzo0MDoyNVrOG6bOkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTAwMQ==", "bodyText": "This is a bit unique, since we should commonly rely on the error code to propagate information instead of a message which has unbounded size. Could you explain why we couldn't simply re-invent a new error code if existing ones are not sufficient?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456859001", "createdAt": "2020-07-19T04:45:45Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,28 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFinalizedFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+      \"about\": \"The error code or `0` if there was no error.\" },\n+    { \"name\": \"ErrorMessage\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgyMzEzOQ==", "bodyText": "The purpose of the error message is to sometimes describe with finer details on what is the error (such as which feature update is incorrect). To your point, it seems there are existing response types that do allow for an error message, examples are: CreateTopicsResponse, CreatePartitionsResponse, DeleteAclsResponse etc.\nThere is ongoing related discussion under another PR review comment and we can continue the discussion there: https://github.com/apache/kafka/pull/9001/files#r456125212 .", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457823139", "createdAt": "2020-07-21T04:04:34Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,28 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFinalizedFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+      \"about\": \"The error code or `0` if there was no error.\" },\n+    { \"name\": \"ErrorMessage\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTAwMQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNjY4OA==", "bodyText": "Done now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463916688", "createdAt": "2020-08-01T03:40:25Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFinalizedFeaturesResponse.json", "diffHunk": "@@ -0,0 +1,28 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"UpdateFinalizedFeaturesResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+      \"about\": \"The error code or `0` if there was no error.\" },\n+    { \"name\": \"ErrorMessage\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTAwMQ=="}, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIwNTA2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDo1MDo1MlrOGzsezg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzo0ODo1MVrOG0sNdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTM0Mg==", "bodyText": "We don't need to include the same error information twice, as the client side will recognize anyway.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456859342", "createdAt": "2020-07-19T04:50:52Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzkwMzQ3Nw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457903477", "createdAt": "2020-07-21T07:48:51Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTM0Mg=="}, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIwODYwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDo1Njo1NFrOGzsgcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzo0OTowNFrOG0sN3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTc2MQ==", "bodyText": "access should be private", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456859761", "createdAt": "2020-07-19T04:56:54Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzkwMzU4Mg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457903582", "createdAt": "2020-07-21T07:49:04Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTc2MQ=="}, "originalCommit": null, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIwOTU1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDo1ODowNVrOGzsg2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzo1MToxMVrOG0sSjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTg2NA==", "bodyText": "nit: the comment seems unnecessary on L3011", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456859864", "createdAt": "2020-07-19T04:58:05Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):\n+    Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.finalizedFeatureUpdates\n+    if (updates.isEmpty) {\n+      return Right(new ApiError(Errors.INVALID_REQUEST,\n+        \"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+    updates.asScala.foreach(\n+      update => {\n+        // Rule #1) Check that the feature name is not empty.\n+        if (update.name.isEmpty) {\n+          return Right(\n+            new ApiError(Errors.INVALID_REQUEST,\n+              \"Can not contain empty feature name in the request.\"))\n+        }\n+\n+        val cacheEntry = latestFeatures.map(lf => lf.features.get(update.name)).orNull\n+\n+        // We handle deletion requests separately from non-deletion requests.\n+        if (UpdateFinalizedFeaturesRequest.isDeleteRequest(update)) { // Deletion request", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzkwNDc4Mg==", "bodyText": "Done. Removed.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457904782", "createdAt": "2020-07-21T07:51:11Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):\n+    Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.finalizedFeatureUpdates\n+    if (updates.isEmpty) {\n+      return Right(new ApiError(Errors.INVALID_REQUEST,\n+        \"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+    updates.asScala.foreach(\n+      update => {\n+        // Rule #1) Check that the feature name is not empty.\n+        if (update.name.isEmpty) {\n+          return Right(\n+            new ApiError(Errors.INVALID_REQUEST,\n+              \"Can not contain empty feature name in the request.\"))\n+        }\n+\n+        val cacheEntry = latestFeatures.map(lf => lf.features.get(update.name)).orNull\n+\n+        // We handle deletion requests separately from non-deletion requests.\n+        if (UpdateFinalizedFeaturesRequest.isDeleteRequest(update)) { // Deletion request", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTg2NA=="}, "originalCommit": null, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIxMDQ1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNDo1ODo1MVrOGzshOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwODo1MDowN1rOG0uZKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTk2MA==", "bodyText": "Commonly in scala we try to avoid using return, consider using if-else instead.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456859960", "createdAt": "2020-07-19T04:58:51Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):\n+    Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.finalizedFeatureUpdates\n+    if (updates.isEmpty) {\n+      return Right(new ApiError(Errors.INVALID_REQUEST,\n+        \"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+    updates.asScala.foreach(\n+      update => {\n+        // Rule #1) Check that the feature name is not empty.\n+        if (update.name.isEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzkzOTI0MQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457939241", "createdAt": "2020-07-21T08:50:07Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):\n+    Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.finalizedFeatureUpdates\n+    if (updates.isEmpty) {\n+      return Right(new ApiError(Errors.INVALID_REQUEST,\n+        \"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+    updates.asScala.foreach(\n+      update => {\n+        // Rule #1) Check that the feature name is not empty.\n+        if (update.name.isEmpty) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg1OTk2MA=="}, "originalCommit": null, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIxMjM3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNTowMTozMVrOGzsiGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzo1MTozNFrOG0sTcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MDE4Nw==", "bodyText": "setting the allowDowngrade flag to true in the request", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456860187", "createdAt": "2020-07-19T05:01:31Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):\n+    Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.finalizedFeatureUpdates\n+    if (updates.isEmpty) {\n+      return Right(new ApiError(Errors.INVALID_REQUEST,\n+        \"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+    updates.asScala.foreach(\n+      update => {\n+        // Rule #1) Check that the feature name is not empty.\n+        if (update.name.isEmpty) {\n+          return Right(\n+            new ApiError(Errors.INVALID_REQUEST,\n+              \"Can not contain empty feature name in the request.\"))\n+        }\n+\n+        val cacheEntry = latestFeatures.map(lf => lf.features.get(update.name)).orNull\n+\n+        // We handle deletion requests separately from non-deletion requests.\n+        if (UpdateFinalizedFeaturesRequest.isDeleteRequest(update)) { // Deletion request\n+          // Rule #2) Disallow deletion of a finalized feature without allowDowngrade flag set.\n+          if (!update.allowDowngrade) {\n+            return Right(\n+              new ApiError(Errors.INVALID_REQUEST,\n+                s\"Can not delete feature: '${update.name}' without setting the\" +\n+                  \" allowDowngrade flag in the request.\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzkwNTAxMA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457905010", "createdAt": "2020-07-21T07:51:34Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2945,6 +2950,137 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleUpdateFinalizedFeatures(request: RequestChannel.Request): Unit = {\n+    val updateFinalizedFeaturesRequest = request.body[UpdateFinalizedFeaturesRequest]\n+    def sendResponseCallback(error: Errors, msgOverride: Option[String]): Unit = {\n+      sendResponseExemptThrottle(request, new UpdateFinalizedFeaturesResponse(\n+        new UpdateFinalizedFeaturesResponseData()\n+          .setErrorCode(error.code())\n+          .setErrorMessage(msgOverride.getOrElse(error.message()))))\n+    }\n+\n+    if (!authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n+      sendResponseCallback(Errors.CLUSTER_AUTHORIZATION_FAILED, Option.empty)\n+    } else if (!controller.isActive) {\n+      sendResponseCallback(Errors.NOT_CONTROLLER, Option.empty)\n+    } else if (!config.isFeatureVersioningEnabled) {\n+      sendResponseCallback(Errors.INVALID_REQUEST, Some(\"Feature versioning system is disabled.\"))\n+    } else {\n+      val targetFeaturesOrError = getTargetFinalizedFeaturesOrError(updateFinalizedFeaturesRequest)\n+      targetFeaturesOrError match {\n+        case Left(targetFeatures) =>\n+          controller.updateFinalizedFeatures(targetFeatures, sendResponseCallback)\n+        case Right(error) =>\n+          sendResponseCallback(error.error, Some(error.message))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the provided UpdateFinalizedFeaturesRequest, checking for various error cases.\n+   * If the validation is successful, returns the target finalized features constructed from the\n+   * request.\n+   *\n+   * @param request   the request to be validated\n+   *\n+   * @return          - the target finalized features, if request validation is successful\n+   *                  - an ApiError if request validation fails\n+   */\n+  def getTargetFinalizedFeaturesOrError(request: UpdateFinalizedFeaturesRequest):\n+    Either[Features[FinalizedVersionRange], ApiError] = {\n+    val updates = request.data.finalizedFeatureUpdates\n+    if (updates.isEmpty) {\n+      return Right(new ApiError(Errors.INVALID_REQUEST,\n+        \"Can not provide empty FinalizedFeatureUpdates in the request.\"))\n+    }\n+\n+    val latestFeatures = featureCache.get\n+    val newFeatures = scala.collection.mutable.Map[String, FinalizedVersionRange]()\n+    updates.asScala.foreach(\n+      update => {\n+        // Rule #1) Check that the feature name is not empty.\n+        if (update.name.isEmpty) {\n+          return Right(\n+            new ApiError(Errors.INVALID_REQUEST,\n+              \"Can not contain empty feature name in the request.\"))\n+        }\n+\n+        val cacheEntry = latestFeatures.map(lf => lf.features.get(update.name)).orNull\n+\n+        // We handle deletion requests separately from non-deletion requests.\n+        if (UpdateFinalizedFeaturesRequest.isDeleteRequest(update)) { // Deletion request\n+          // Rule #2) Disallow deletion of a finalized feature without allowDowngrade flag set.\n+          if (!update.allowDowngrade) {\n+            return Right(\n+              new ApiError(Errors.INVALID_REQUEST,\n+                s\"Can not delete feature: '${update.name}' without setting the\" +\n+                  \" allowDowngrade flag in the request.\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MDE4Nw=="}, "originalCommit": null, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIzMzU0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNTozMTo0M1rOGzsrtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoxNDoyOFrOG0rG5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MjY0NQ==", "bodyText": "ApiKeys.UPDATE_FINALIZED_FEATURES API", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456862645", "createdAt": "2020-07-19T05:31:43Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4NTQxNQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457885415", "createdAt": "2020-07-21T07:14:28Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MjY0NQ=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIzNDM2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNTozMzoxMVrOGzssGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoxNDozOFrOG0rHGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2Mjc0NQ==", "bodyText": "We only need to mark testing only comment on the functions", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456862745", "createdAt": "2020-07-19T05:33:11Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4NTQ2Nw==", "bodyText": "Done. Made the comment better. Pls take a look.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457885467", "createdAt": "2020-07-21T07:14:38Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2Mjc0NQ=="}, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIzNzkxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNTozODoxNVrOGzstsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoyNTo0NFrOG0rc2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzE1Mw==", "bodyText": "nit: mark the parameter as logIncompatibilities = true)", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456863153", "createdAt": "2020-07-19T05:38:15Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg5MTAzNA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457891034", "createdAt": "2020-07-21T07:25:44Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzE1Mw=="}, "originalCommit": null, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDIzOTA4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNTozOTo1OFrOGzsuNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoyODoyOVrOG0riFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzI4Nw==", "bodyText": "redundant {}", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456863287", "createdAt": "2020-07-19T05:39:58Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg5MjM3NA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457892374", "createdAt": "2020-07-21T07:28:29Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) => {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzI4Nw=="}, "originalCommit": null, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDI0MTM5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNTo0Mjo1MFrOGzsvQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoyOToyNVrOG0rj-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzU1NA==", "bodyText": "redundant {}", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456863554", "createdAt": "2020-07-19T05:42:50Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supportedFeatures.get(feature)\n+        if (supportedVersions == null) {\n+          (feature, versionLevels, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, versionLevels, \"{feature=%s, reason='%s is incompatible with %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, versionLevels, null)\n+        }\n+      }\n+    }.filter{ case(_, _, errorReason) => errorReason != null}.toList\n+\n+    if (logIncompatibilities && incompatibilities.nonEmpty) {\n+      warn(\n+        \"Feature incompatibilities seen: \" + incompatibilities.map{\n+          case(_, _, errorReason) => errorReason })\n+    }\n+    Features.finalizedFeatures(incompatibilities.map{\n+      case(feature, versionLevels, _) => (feature, versionLevels) }.toMap.asJava)\n+  }\n+\n+  /**\n+   * A check that ensures each feature defined with min version level is a supported feature, and\n+   * the min version level value is valid (i.e. it is compatible with the supported version range).\n+   *\n+   * @param supportedFeatures         the supported features\n+   * @param featureMinVersionLevels   the feature minimum version levels\n+   *\n+   * @return                          - true, if the above described check passes.\n+   *                                  - false, otherwise.\n+   */\n+  private def areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures: Features[SupportedVersionRange],\n+    featureMinVersionLevels: Map[String, Short]\n+  ): Boolean = {\n+    featureMinVersionLevels.forall {\n+      case(featureName, minVersionLevel) => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg5Mjg1OA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457892858", "createdAt": "2020-07-21T07:29:25Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supportedFeatures.get(feature)\n+        if (supportedVersions == null) {\n+          (feature, versionLevels, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, versionLevels, \"{feature=%s, reason='%s is incompatible with %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, versionLevels, null)\n+        }\n+      }\n+    }.filter{ case(_, _, errorReason) => errorReason != null}.toList\n+\n+    if (logIncompatibilities && incompatibilities.nonEmpty) {\n+      warn(\n+        \"Feature incompatibilities seen: \" + incompatibilities.map{\n+          case(_, _, errorReason) => errorReason })\n+    }\n+    Features.finalizedFeatures(incompatibilities.map{\n+      case(feature, versionLevels, _) => (feature, versionLevels) }.toMap.asJava)\n+  }\n+\n+  /**\n+   * A check that ensures each feature defined with min version level is a supported feature, and\n+   * the min version level value is valid (i.e. it is compatible with the supported version range).\n+   *\n+   * @param supportedFeatures         the supported features\n+   * @param featureMinVersionLevels   the feature minimum version levels\n+   *\n+   * @return                          - true, if the above described check passes.\n+   *                                  - false, otherwise.\n+   */\n+  private def areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures: Features[SupportedVersionRange],\n+    featureMinVersionLevels: Map[String, Short]\n+  ): Boolean = {\n+    featureMinVersionLevels.forall {\n+      case(featureName, minVersionLevel) => {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzU1NA=="}, "originalCommit": null, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDI0NDQzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQwNTo0ODoxMlrOGzswqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzozNzo0OVrOG0r1cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzkxNA==", "bodyText": "Do we need this precision of exact wait time? Could we just track the function start time and compare with current system time for expiration?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456863914", "createdAt": "2020-07-19T05:48:12Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -82,18 +108,55 @@ object FinalizedFeatureCache extends Logging {\n         \" The existing cache contents are %s\").format(latest, oldFeatureAndEpoch)\n       throw new FeatureCacheUpdateException(errorMsg)\n     } else {\n-      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      val incompatibleFeatures = brokerFeatures.incompatibleFeatures(latest.features)\n       if (!incompatibleFeatures.empty) {\n         val errorMsg = (\"FinalizedFeatureCache update failed since feature compatibility\" +\n           \" checks failed! Supported %s has incompatibilities with the latest %s.\"\n-          ).format(SupportedFeatures.get, latest)\n+          ).format(brokerFeatures.supportedFeatures, latest)\n         throw new FeatureCacheUpdateException(errorMsg)\n       } else {\n-        val logMsg = \"Updated cache from existing finalized %s to latest finalized %s\".format(\n+        val logMsg = \"Updated cache from existing %s to latest %s\".format(\n           oldFeatureAndEpoch, latest)\n-        featuresAndEpoch = Some(latest)\n+        synchronized {\n+          featuresAndEpoch = Some(latest)\n+          notifyAll()\n+        }\n         info(logMsg)\n       }\n     }\n   }\n+\n+  /**\n+   * Causes the current thread to wait no more than timeoutMs for the specified condition to be met.\n+   * It is guaranteed that the provided condition will always be invoked only from within a\n+   * synchronized block.\n+   *\n+   * @param waitCondition   the condition to be waited upon:\n+   *                         - if the condition returns true, then, the wait will stop.\n+   *                         - if the condition returns false, it means the wait must continue until\n+   *                           timeout.\n+   *\n+   * @param timeoutMs       the timeout (in milli seconds)\n+   *\n+   * @throws                TimeoutException if the condition is not met within timeoutMs.\n+   */\n+  private def waitUntilConditionOrThrow(waitCondition: () => Boolean, timeoutMs: Long): Unit = {\n+    if(timeoutMs < 0L) {\n+      throw new IllegalArgumentException(s\"Expected timeoutMs >= 0, but $timeoutMs was provided.\")\n+    }\n+    synchronized {\n+      var sleptTimeMs = 0L\n+      while (!waitCondition()) {\n+        val timeoutLeftMs = timeoutMs - sleptTimeMs\n+        if (timeoutLeftMs <= 0) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${timeoutMs}ms for required condition to be met.\" +\n+              s\" Current epoch: ${featuresAndEpoch.map(fe => fe.epoch).getOrElse(\"<none>\")}.\")\n+        }\n+        val timeBeforeNanos = System.nanoTime", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg5NzMzMQ==", "bodyText": "Done. Made it the way you suggested, pls take a look.\nOverall either way looked fine to me but the one you suggested is a bit simpler.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457897331", "createdAt": "2020-07-21T07:37:49Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -82,18 +108,55 @@ object FinalizedFeatureCache extends Logging {\n         \" The existing cache contents are %s\").format(latest, oldFeatureAndEpoch)\n       throw new FeatureCacheUpdateException(errorMsg)\n     } else {\n-      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      val incompatibleFeatures = brokerFeatures.incompatibleFeatures(latest.features)\n       if (!incompatibleFeatures.empty) {\n         val errorMsg = (\"FinalizedFeatureCache update failed since feature compatibility\" +\n           \" checks failed! Supported %s has incompatibilities with the latest %s.\"\n-          ).format(SupportedFeatures.get, latest)\n+          ).format(brokerFeatures.supportedFeatures, latest)\n         throw new FeatureCacheUpdateException(errorMsg)\n       } else {\n-        val logMsg = \"Updated cache from existing finalized %s to latest finalized %s\".format(\n+        val logMsg = \"Updated cache from existing %s to latest %s\".format(\n           oldFeatureAndEpoch, latest)\n-        featuresAndEpoch = Some(latest)\n+        synchronized {\n+          featuresAndEpoch = Some(latest)\n+          notifyAll()\n+        }\n         info(logMsg)\n       }\n     }\n   }\n+\n+  /**\n+   * Causes the current thread to wait no more than timeoutMs for the specified condition to be met.\n+   * It is guaranteed that the provided condition will always be invoked only from within a\n+   * synchronized block.\n+   *\n+   * @param waitCondition   the condition to be waited upon:\n+   *                         - if the condition returns true, then, the wait will stop.\n+   *                         - if the condition returns false, it means the wait must continue until\n+   *                           timeout.\n+   *\n+   * @param timeoutMs       the timeout (in milli seconds)\n+   *\n+   * @throws                TimeoutException if the condition is not met within timeoutMs.\n+   */\n+  private def waitUntilConditionOrThrow(waitCondition: () => Boolean, timeoutMs: Long): Unit = {\n+    if(timeoutMs < 0L) {\n+      throw new IllegalArgumentException(s\"Expected timeoutMs >= 0, but $timeoutMs was provided.\")\n+    }\n+    synchronized {\n+      var sleptTimeMs = 0L\n+      while (!waitCondition()) {\n+        val timeoutLeftMs = timeoutMs - sleptTimeMs\n+        if (timeoutLeftMs <= 0) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${timeoutMs}ms for required condition to be met.\" +\n+              s\" Current epoch: ${featuresAndEpoch.map(fe => fe.epoch).getOrElse(\"<none>\")}.\")\n+        }\n+        val timeBeforeNanos = System.nanoTime", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njg2MzkxNA=="}, "originalCommit": null, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDc4Mzg2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQxNzowMzozMFrOGzw4lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNjo0NjoxNFrOG0qVOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzMTQ3Nw==", "bodyText": "For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster with all the possible supported features finalized immediately.\nI think this comment is hard to understand if reader has zero context on the feature versioning. It would be good to include a short explanation on what does a supported feature mean, and what it means to be finalized.\nThe new cluster will almost never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0.\nThis sentence is positioned awkwardly. I would suggest we just propose As a new cluster starting with IBP setting equal to or greater than KAFKA_2_7_IV0", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456931477", "createdAt": "2020-07-19T17:03:30Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MjY5OQ==", "bodyText": "Done. Updated the doc.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457872699", "createdAt": "2020-07-21T06:46:14Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzMTQ3Nw=="}, "originalCommit": null, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDc4NDUwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQxNzowNDoyM1rOGzw45w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNjo0Nzo1NVrOG0qX4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzMTU1OQ==", "bodyText": "then here is how we it could be removed:\nAssuming this is the case, then the controller...", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456931559", "createdAt": "2020-07-19T17:04:23Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MzM3Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457873376", "createdAt": "2020-07-21T06:47:55Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzMTU1OQ=="}, "originalCommit": null, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDc5NDkwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQxNzoxNzo0MFrOGzw90Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoyMzo1NVrOG0rY_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzMjgxNw==", "bodyText": "Maybe a newbie question here: since the supportedFeatures could be mutated, why couldn't we just assume its min level marks the defaultFeatureMinVersionLevels? Trying to understand the necessity for secondary bookkeeping. Might be good to also put reasonings in the meta comment as well to clear confusion.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456932817", "createdAt": "2020-07-19T17:17:40Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg5MDA0Ng==", "bodyText": "It is already explained in the class level doc. This is also explained in the KIP-584 in this section.\nThis is needed because defaultFeatureMinVersionLevels is mainly for feature version deprecation. When we deprecate feature version levels, we first bump the defaultFeatureMinVersionLevels in a broker release (after making an announcement to community). This will automatically mean clients have to stop using the finalized min version levels that have been deprecated (because upon startup the controller will write the defaultFeatureMinVersionLevels to ZK from within KafkaController#setupFeatureVersioning method). Once the write to ZK happens, clients that are using the finalized features are forced to stop using the deprecated version levels.\nThen, finally in the future when we remove the code for the deprecated version levels, that is when we will bump the min version for the supported feature in the broker. Thereby we will completely drop support for a feature version altogether.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457890046", "createdAt": "2020-07-21T07:23:55Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzMjgxNw=="}, "originalCommit": null, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDgxMTYxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQxNzozODoyN1rOGzxFrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDowMTo0M1rOG0w_CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzNDgzMQ==", "bodyText": "If this is broker required feature set, I feel we could name it something like brokerRequiredVersionRange. Updated sounds a bit blur for reader, as it couldn't infer the subject.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456934831", "createdAt": "2020-07-19T17:38:27Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk4MTcwNQ==", "bodyText": "Done. I'm calling it brokerDefaultVersionRange now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457981705", "createdAt": "2020-07-21T10:01:43Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzNDgzMQ=="}, "originalCommit": null, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MDgxMzYyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQxNzo0MDozN1rOGzxGoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNjo1NDoxN1rOG0qimg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzNTA3Mw==", "bodyText": "What about the case where existingVersionRange.min() > updatedVersionRange.max() is true? For example:\nexisting: [4, 5]\nupdated: [1, 2]\n\nAre we enabling version 3 as well?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r456935073", "createdAt": "2020-07-19T17:40:37Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (updatedVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= updatedVersionRange.min()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NjEyMg==", "bodyText": "Done. This case is also handled now.\nTo your point, the case  where updated.max < existing.min can never happen unless brokers get downgraded (after finalizing features at higher levels), and especially if the downgrade was done improperly (without applying feature tooling commands). It's a rare case. But even in that case, the broker will start crashing because of incompatibility in supported feature version max level, so the problem is found before it reaches this point.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457876122", "createdAt": "2020-07-21T06:54:17Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (updatedVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= updatedVersionRange.min()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkzNTA3Mw=="}, "originalCommit": null, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTg1NDU1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNTo0NjowMlrOGz5fAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo1NzoxNVrOG3DCHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3MjM4Nw==", "bodyText": "Is it ok for us to always do updateFeatureZNode, since this call is idempotent?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457072387", "createdAt": "2020-07-20T05:46:02Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (updatedVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= updatedVersionRange.min()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), updatedVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(updatedVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a special case: If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), then, this\n+              // case is not eligible for deprecation. This requires that the max version level be\n+              // upgraded first to a value that's equal to the the default minimum version level.\n+              info(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+              + s\" since the existing $existingVersionRange does not intersect with the default\"\n+              + s\" $updatedVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the caceh (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3OTU2Nw==", "bodyText": "Not sure I understood. We will only update the FeatureZNode if the status is not disabled currently (see the implementation below). What am I missing?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457879567", "createdAt": "2020-07-21T07:02:03Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (updatedVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= updatedVersionRange.min()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), updatedVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(updatedVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a special case: If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), then, this\n+              // case is not eligible for deprecation. This requires that the max version level be\n+              // upgraded first to a value that's equal to the the default minimum version level.\n+              info(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+              + s\" since the existing $existingVersionRange does not intersect with the default\"\n+              + s\" $updatedVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the caceh (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3MjM4Nw=="}, "originalCommit": null, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxNDg4Mw==", "bodyText": "My pt is that since we know the outcome (feature versioning will be disabled), we don't need to do one more lookup but just try to push the update. Anyway, I think this is a nit.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459014883", "createdAt": "2020-07-22T18:58:04Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (updatedVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= updatedVersionRange.min()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), updatedVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(updatedVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a special case: If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), then, this\n+              // case is not eligible for deprecation. This requires that the max version level be\n+              // upgraded first to a value that's equal to the the default minimum version level.\n+              info(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+              + s\" since the existing $existingVersionRange does not intersect with the default\"\n+              + s\" $updatedVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the caceh (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3MjM4Nw=="}, "originalCommit": null, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3NDU1OA==", "bodyText": "We can not just push the update, because, we have to decide if the node needs to be created or existing node should be updated. That is why we read the node first to understand if it exists or not, then we update the existing node only if the status does not match (this avoids a ZK write in the most common cases).", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460374558", "createdAt": "2020-07-25T06:57:15Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,158 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * Enables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with enabled status. This status means the feature versioning system\n+   * (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode are active. This\n+   * status should be written by the controller to the FeatureZNode only when the broker IBP config\n+   * is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+   *    with all the possible supported features finalized immediately. The new cluster will almost\n+   *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. Assuming this is the\n+   *    case, then here is how we it: the controller will start up and notice that the FeatureZNode\n+   *    is absent in the new cluster, it will then create a FeatureZNode (with enabled status)\n+   *    containing the entire list of default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default\n+   *    minimum version level maintained in the BrokerFeatures object. The goal of this mutation is\n+   *    to permanently deprecate one or more feature version levels. The range of feature version\n+   *    levels deprecated are from the closed range: [existing_min_version_level, default_min_version_level].\n+   *    NOTE: Deprecating a feature version level is an incompatible change, which requires a major\n+   *    release of Kafka. In such a release, the minimum version level maintained within the\n+   *    BrokerFeatures class is updated suitably to record the deprecation of the feature.\n+   *\n+   * 4. Broker downgrade:\n+   *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+   *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+   *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+   *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+   *    will switch the FeatureZNode status to disabled with empty features.\n+   */\n+  private def enableFeatureVersioning(): Unit = {\n+    val defaultFinalizedFeatures = brokerFeatures.getDefaultFinalizedFeatures\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    if (version == ZkVersion.UnknownVersion) {\n+      val newVersion = createFeatureZNode(new FeatureZNode(FeatureZNodeStatus.Enabled, defaultFinalizedFeatures))\n+      featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+    } else {\n+      val existingFeatureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+      var newFeatures: Features[FinalizedVersionRange] = Features.emptyFinalizedFeatures()\n+      if (existingFeatureZNode.status.equals(FeatureZNodeStatus.Enabled)) {\n+        newFeatures = Features.finalizedFeatures(existingFeatureZNode.features.features().asScala.map {\n+          case (featureName, existingVersionRange) => {\n+            val updatedVersionRange = defaultFinalizedFeatures.get(featureName)\n+            if (updatedVersionRange == null) {\n+              warn(s\"Existing finalized feature: $featureName with $existingVersionRange\"\n+                + s\" is absent in default finalized $defaultFinalizedFeatures\")\n+              (featureName, existingVersionRange)\n+            } else if (existingVersionRange.max() >= updatedVersionRange.min()) {\n+              // Through this change, we deprecate all version levels in the closed range:\n+              // [existingVersionRange.min(), updatedVersionRange.min() - 1]\n+              (featureName, new FinalizedVersionRange(updatedVersionRange.min(), existingVersionRange.max()))\n+            } else {\n+              // This is a special case: If the existing version levels fall completely outside the\n+              // range of the default finalized version levels (i.e. no intersection), then, this\n+              // case is not eligible for deprecation. This requires that the max version level be\n+              // upgraded first to a value that's equal to the the default minimum version level.\n+              info(s\"Can not update minimum version level in finalized feature: $featureName,\"\n+              + s\" since the existing $existingVersionRange does not intersect with the default\"\n+              + s\" $updatedVersionRange.\")\n+              (featureName, existingVersionRange)\n+            }\n+          }\n+        }.asJava)\n+      }\n+      val newFeatureZNode = new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures)\n+      if (!newFeatureZNode.equals(existingFeatureZNode)) {\n+        val newVersion = updateFeatureZNode(newFeatureZNode)\n+        featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Disables the feature versioning system (KIP-584).\n+   *\n+   * Sets up the FeatureZNode with disabled status. This status means the feature versioning system\n+   * (KIP-584) is disabled, and, the finalized features stored in the FeatureZNode are not relevant.\n+   * This status should be written by the controller to the FeatureZNode only when the broker\n+   * IBP config is less than KAFKA_2_7_IV0.\n+   *\n+   * NOTE:\n+   * 1. When this method returns, existing finalized features (if any) will be cleared from the\n+   *    FeatureZNode.\n+   * 2. This method, unlike enableFeatureVersioning() need not wait for the FinalizedFeatureCache\n+   *    to be updated, because, such updates to the caceh (via FinalizedFeatureChangeListener)\n+   *    are disabled when IBP config is < than KAFKA_2_7_IV0.\n+   */\n+  private def disableFeatureVersioning(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3MjM4Nw=="}, "originalCommit": null, "originalPosition": 192}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTg3NzgwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNTo1MjoxMVrOGz5rHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQwMzozOTo0OVrOG6bOQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NTQ4NA==", "bodyText": "Could the receiving broker analyze the request and decide to shut down itself? What's the gain we have by avoiding sending update metadata to incompatible brokers?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457075484", "createdAt": "2020-07-20T05:52:11Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -983,8 +1144,25 @@ class KafkaController(val config: KafkaConfig,\n    */\n   private[controller] def sendUpdateMetadataRequest(brokers: Seq[Int], partitions: Set[TopicPartition]): Unit = {\n     try {\n+      val filteredBrokers = scala.collection.mutable.Set[Int]() ++ brokers\n+      if (config.isFeatureVersioningEnabled) {\n+        def hasIncompatibleFeatures(broker: Broker): Boolean = {\n+          val latestFinalizedFeatures = featureCache.get\n+          if (latestFinalizedFeatures.isDefined) {\n+            BrokerFeatures.hasIncompatibleFeatures(broker.features, latestFinalizedFeatures.get.features)\n+          } else {\n+            false\n+          }\n+        }\n+        controllerContext.liveOrShuttingDownBrokers.foreach(broker => {\n+          if (filteredBrokers.contains(broker.id) && hasIncompatibleFeatures(broker)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4MDM4Mw==", "bodyText": "This handles the race condition described in the KIP-584 in this section. Please refer to the KIP for details. I have also added doc to this method.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457880383", "createdAt": "2020-07-21T07:03:54Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -983,8 +1144,25 @@ class KafkaController(val config: KafkaConfig,\n    */\n   private[controller] def sendUpdateMetadataRequest(brokers: Seq[Int], partitions: Set[TopicPartition]): Unit = {\n     try {\n+      val filteredBrokers = scala.collection.mutable.Set[Int]() ++ brokers\n+      if (config.isFeatureVersioningEnabled) {\n+        def hasIncompatibleFeatures(broker: Broker): Boolean = {\n+          val latestFinalizedFeatures = featureCache.get\n+          if (latestFinalizedFeatures.isDefined) {\n+            BrokerFeatures.hasIncompatibleFeatures(broker.features, latestFinalizedFeatures.get.features)\n+          } else {\n+            false\n+          }\n+        }\n+        controllerContext.liveOrShuttingDownBrokers.foreach(broker => {\n+          if (filteredBrokers.contains(broker.id) && hasIncompatibleFeatures(broker)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NTQ4NA=="}, "originalCommit": null, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUwMTMwMQ==", "bodyText": "I see, what would happen to a currently live broker if it couldn't get any metadata update for a while, will it shut down itself?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462501301", "createdAt": "2020-07-29T18:27:19Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -983,8 +1144,25 @@ class KafkaController(val config: KafkaConfig,\n    */\n   private[controller] def sendUpdateMetadataRequest(brokers: Seq[Int], partitions: Set[TopicPartition]): Unit = {\n     try {\n+      val filteredBrokers = scala.collection.mutable.Set[Int]() ++ brokers\n+      if (config.isFeatureVersioningEnabled) {\n+        def hasIncompatibleFeatures(broker: Broker): Boolean = {\n+          val latestFinalizedFeatures = featureCache.get\n+          if (latestFinalizedFeatures.isDefined) {\n+            BrokerFeatures.hasIncompatibleFeatures(broker.features, latestFinalizedFeatures.get.features)\n+          } else {\n+            false\n+          }\n+        }\n+        controllerContext.liveOrShuttingDownBrokers.foreach(broker => {\n+          if (filteredBrokers.contains(broker.id) && hasIncompatibleFeatures(broker)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NTQ4NA=="}, "originalCommit": null, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNjYxMA==", "bodyText": "If the broker has feature incompatibilities, then it should die as soon as it has received the ZK update (it would die from within FinalizedFeatureChangeListener).", "url": "https://github.com/apache/kafka/pull/9001#discussion_r463916610", "createdAt": "2020-08-01T03:39:49Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -983,8 +1144,25 @@ class KafkaController(val config: KafkaConfig,\n    */\n   private[controller] def sendUpdateMetadataRequest(brokers: Seq[Int], partitions: Set[TopicPartition]): Unit = {\n     try {\n+      val filteredBrokers = scala.collection.mutable.Set[Int]() ++ brokers\n+      if (config.isFeatureVersioningEnabled) {\n+        def hasIncompatibleFeatures(broker: Broker): Boolean = {\n+          val latestFinalizedFeatures = featureCache.get\n+          if (latestFinalizedFeatures.isDefined) {\n+            BrokerFeatures.hasIncompatibleFeatures(broker.features, latestFinalizedFeatures.get.features)\n+          } else {\n+            false\n+          }\n+        }\n+        controllerContext.liveOrShuttingDownBrokers.foreach(broker => {\n+          if (filteredBrokers.contains(broker.id) && hasIncompatibleFeatures(broker)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NTQ4NA=="}, "originalCommit": null, "originalPosition": 231}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTg4MTg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNTo1MzoxOVrOGz5tSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoxMTowM1rOG0rAgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NjA0MA==", "bodyText": "Replace with nonEmpty", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457076040", "createdAt": "2020-07-20T05:53:19Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1825,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val incompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.filter(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (incompatibleBrokers.size > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4Mzc3OQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457883779", "createdAt": "2020-07-21T07:11:03Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1825,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val incompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.filter(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (incompatibleBrokers.size > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NjA0MA=="}, "originalCommit": null, "originalPosition": 262}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTg4NzQyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNTo1NDo0OVrOGz5wMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo0MzoyMlrOG4Cr8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3Njc4Nw==", "bodyText": "Could we avoid blocking controller processing here, by putting the callback into a delayed queue or sth?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457076787", "createdAt": "2020-07-20T05:54:49Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1825,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val incompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.filter(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (incompatibleBrokers.size > 0) {\n+        callback(\n+          Errors.INVALID_REQUEST,\n+          Some(\n+            s\"Could not apply finalized feature updates because ${incompatibleBrokers.size} brokers\"\n+            + s\" were found to have incompatible features. newFeatures: $newFeatures\"\n+           + s\", incompatibleBrokers: $incompatibleBrokers.\"))\n+      } else {\n+        try {\n+          val newVersion = zkClient.updateFeatureZNode(\n+            new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures))\n+          featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4NDkwMw==", "bodyText": "I feel that there isn't a pressing reason to optimize this API path currently, and make it async.\nThe API is not going to be frequently used, and an infrequent write to a ZK node with low write contention feels like a relatively inexpensive case that we could block the controller on.\nPlease let me know how you feel.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457884903", "createdAt": "2020-07-21T07:13:24Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1825,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val incompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.filter(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (incompatibleBrokers.size > 0) {\n+        callback(\n+          Errors.INVALID_REQUEST,\n+          Some(\n+            s\"Could not apply finalized feature updates because ${incompatibleBrokers.size} brokers\"\n+            + s\" were found to have incompatible features. newFeatures: $newFeatures\"\n+           + s\", incompatibleBrokers: $incompatibleBrokers.\"))\n+      } else {\n+        try {\n+          val newVersion = zkClient.updateFeatureZNode(\n+            new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures))\n+          featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3Njc4Nw=="}, "originalCommit": null, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NDU5Mw==", "bodyText": "Yea, I'm a bit worried about such a blocking call here as we don't have a precedence for relying on zk connect timeout (18 seconds), besides the result doesn't matter to the controller (since client will do the retry). cc @cmccabe @junrao to see if they have a different opinion on this.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459194593", "createdAt": "2020-07-23T03:19:03Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1825,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val incompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.filter(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (incompatibleBrokers.size > 0) {\n+        callback(\n+          Errors.INVALID_REQUEST,\n+          Some(\n+            s\"Could not apply finalized feature updates because ${incompatibleBrokers.size} brokers\"\n+            + s\" were found to have incompatible features. newFeatures: $newFeatures\"\n+           + s\", incompatibleBrokers: $incompatibleBrokers.\"))\n+      } else {\n+        try {\n+          val newVersion = zkClient.updateFeatureZNode(\n+            new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures))\n+          featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3Njc4Nw=="}, "originalCommit": null, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxNzQ1OA==", "bodyText": "Sure, we can hear what others say.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461417458", "createdAt": "2020-07-28T08:43:22Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1647,6 +1825,36 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def processUpdateFinalizedFeatures(newFeatures: Features[FinalizedVersionRange],\n+                                             callback: UpdateFinalizedFeaturesCallback): Unit = {\n+    if (isActive) {\n+      val incompatibleBrokers = controllerContext.liveOrShuttingDownBrokers.filter(broker => {\n+        BrokerFeatures.hasIncompatibleFeatures(broker.features, newFeatures)\n+      })\n+      if (incompatibleBrokers.size > 0) {\n+        callback(\n+          Errors.INVALID_REQUEST,\n+          Some(\n+            s\"Could not apply finalized feature updates because ${incompatibleBrokers.size} brokers\"\n+            + s\" were found to have incompatible features. newFeatures: $newFeatures\"\n+           + s\", incompatibleBrokers: $incompatibleBrokers.\"))\n+      } else {\n+        try {\n+          val newVersion = zkClient.updateFeatureZNode(\n+            new FeatureZNode(FeatureZNodeStatus.Enabled, newFeatures))\n+          featureCache.waitUntilEpochOrThrow(newVersion, config.zkConnectionTimeoutMs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3Njc4Nw=="}, "originalCommit": null, "originalPosition": 273}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTg5MDcyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNTo1NTo0NlrOGz5x_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoyNzo0NVrOG0rguA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NzI0Ng==", "bodyText": "incompatibleFeatures?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457077246", "createdAt": "2020-07-20T05:55:46Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg5MjAyNA==", "bodyText": "Done. Calling it incompatibleFeaturesInfo now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457892024", "createdAt": "2020-07-21T07:27:45Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3NzI0Ng=="}, "originalCommit": null, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTg5NTk5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNTo1NzoxOFrOGz50wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzoyODo1NVrOG0rjDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3Nzk1NA==", "bodyText": "nit: I have seen that we use both map{ and map {, could we try using only one format consistently within the current file?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457077954", "createdAt": "2020-07-20T05:57:18Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supportedFeatures.get(feature)\n+        if (supportedVersions == null) {\n+          (feature, versionLevels, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, versionLevels, \"{feature=%s, reason='%s is incompatible with %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, versionLevels, null)\n+        }\n+      }\n+    }.filter{ case(_, _, errorReason) => errorReason != null}.toList\n+\n+    if (logIncompatibilities && incompatibilities.nonEmpty) {\n+      warn(\n+        \"Feature incompatibilities seen: \" + incompatibilities.map{", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg5MjYyMg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r457892622", "createdAt": "2020-07-21T07:28:55Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/server/BrokerFeatures.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A class that encapsulates the following:\n+ *\n+ * 1. The latest features supported by the Broker.\n+ *\n+ * 2. The default minimum version levels for specific features. This map enables feature\n+ *    version level deprecation. This is how it works: in order to deprecate feature version levels,\n+ *    in this map the default minimum version level of a feature can be set to a new value that's\n+ *    higher than 1 (let's call this new_min_version_level). In doing so, the feature version levels\n+ *    in the closed range: [1, latest_min_version_level - 1] get deprecated by the controller logic\n+ *    that applies this map to persistent finalized feature state in ZK (this mutation happens\n+ *    during controller election and during finalized feature updates via the\n+ *    APIKeys.UPDATE_FINALIZED_FEATURES api).\n+ *\n+ * This class also provides APIs to check for incompatibilities between the features supported by\n+ * the Broker and finalized features.\n+ *\n+ * NOTE: the update*() and clear*() APIs of this class should be used only for testing purposes.\n+ */\n+class BrokerFeatures private (@volatile var supportedFeatures: Features[SupportedVersionRange],\n+                              @volatile var defaultFeatureMinVersionLevels: Map[String, Short]) {\n+  require(BrokerFeatures.areFeatureMinVersionLevelsCompatible(\n+    supportedFeatures, defaultFeatureMinVersionLevels))\n+\n+  // For testing only.\n+  def setSupportedFeatures(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(newFeatures, defaultFeatureMinVersionLevels))\n+    supportedFeatures = newFeatures\n+  }\n+\n+  /**\n+   * Returns the default minimum version level for a specific feature.\n+   *\n+   * @param feature   the name of the feature\n+   *\n+   * @return          the default minimum version level for the feature if its defined.\n+   *                  otherwise, returns 1.\n+   */\n+  def defaultMinVersionLevel(feature: String): Short = {\n+    defaultFeatureMinVersionLevels.getOrElse(feature, 1)\n+  }\n+\n+  // For testing only.\n+  def setDefaultMinVersionLevels(newMinVersionLevels: Map[String, Short]): Unit = {\n+    require(\n+      BrokerFeatures.areFeatureMinVersionLevelsCompatible(supportedFeatures, newMinVersionLevels))\n+    defaultFeatureMinVersionLevels = newMinVersionLevels\n+  }\n+\n+  /**\n+   * Returns the default finalized features that a new Kafka cluster with IBP config >= KAFKA_2_7_IV0\n+   * needs to be bootstrapped with.\n+   */\n+  def getDefaultFinalizedFeatures: Features[FinalizedVersionRange] = {\n+    Features.finalizedFeatures(\n+      supportedFeatures.features.asScala.map {\n+        case(name, versionRange) => (\n+          name, new FinalizedVersionRange(defaultMinVersionLevel(name), versionRange.max))\n+      }.asJava)\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the\n+   *     supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    BrokerFeatures.incompatibleFeatures(supportedFeatures, finalized, true)\n+  }\n+}\n+\n+object BrokerFeatures extends Logging {\n+\n+  def createDefault(): BrokerFeatures = {\n+    // The arguments are currently empty, but, in the future as we define features we should\n+    // populate the required values here.\n+    new BrokerFeatures(emptySupportedFeatures, Map[String, Short]())\n+  }\n+\n+  /**\n+   * Returns true if any of the provided finalized features are incompatible with the provided\n+   * supported features.\n+   *\n+   * @param supportedFeatures   The supported features to be compared\n+   * @param finalizedFeatures   The finalized features to be compared\n+   *\n+   * @return                    - True if there are any feature incompatibilities found.\n+   *                            - False otherwise.\n+   */\n+  def hasIncompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                              finalizedFeatures: Features[FinalizedVersionRange]): Boolean = {\n+    !incompatibleFeatures(supportedFeatures, finalizedFeatures, false).empty\n+  }\n+\n+  private def incompatibleFeatures(supportedFeatures: Features[SupportedVersionRange],\n+                                   finalizedFeatures: Features[FinalizedVersionRange],\n+                                   logIncompatibilities: Boolean): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalizedFeatures.features.asScala.map {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supportedFeatures.get(feature)\n+        if (supportedVersions == null) {\n+          (feature, versionLevels, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, versionLevels, \"{feature=%s, reason='%s is incompatible with %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, versionLevels, null)\n+        }\n+      }\n+    }.filter{ case(_, _, errorReason) => errorReason != null}.toList\n+\n+    if (logIncompatibilities && incompatibilities.nonEmpty) {\n+      warn(\n+        \"Feature incompatibilities seen: \" + incompatibilities.map{", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3Nzk1NA=="}, "originalCommit": null, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTIzODMxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMjo1Mjo0OFrOG1Qzuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDo1MTo1MlrOG3CdNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwMzA5OQ==", "bodyText": "nit: do {@link DescribeFeaturesResult}", "url": "https://github.com/apache/kafka/pull/9001#discussion_r458503099", "createdAt": "2020-07-22T02:52:48Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2NTExMA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460365110", "createdAt": "2020-07-25T04:51:52Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java", "diffHunk": "@@ -1214,6 +1215,70 @@ default AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlterati\n      */\n     AlterClientQuotasResult alterClientQuotas(Collection<ClientQuotaAlteration> entries, AlterClientQuotasOptions options);\n \n+    /**\n+     * Describes finalized as well as supported features. By default, the request is issued to any\n+     * broker. It can be optionally directed only to the controller via DescribeFeaturesOptions\n+     * parameter. This is particularly useful if the user requires strongly consistent reads of\n+     * finalized features.\n+     * <p>\n+     * The following exceptions can be anticipated when calling {@code get()} on the future from the\n+     * returned {@link DescribeFeaturesResult}:\n+     * <ul>\n+     *   <li>{@link org.apache.kafka.common.errors.TimeoutException}\n+     *   If the request timed out before the describe operation could finish.</li>\n+     * </ul>\n+     * <p>\n+     * @param options   the options to use\n+     *\n+     * @return          the DescribeFeaturesResult containing the result", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwMzA5OQ=="}, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDI5NzI2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODowNjo1OVrOG1uPWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDo1Mjo1N1rOG3CdgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NTMwNg==", "bodyText": "nit: new line", "url": "https://github.com/apache/kafka/pull/9001#discussion_r458985306", "createdAt": "2020-07-22T18:06:59Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2NTE4NQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460365185", "createdAt": "2020-07-25T04:52:57Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NTMwNg=="}, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDI5OTU1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODowNzo0MFrOG1uQzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDo1Mzo0N1rOG3CdrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NTY3Ng==", "bodyText": "Could be simplified as sendToController", "url": "https://github.com/apache/kafka/pull/9001#discussion_r458985676", "createdAt": "2020-07-22T18:07:40Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+    /**\n+     * - True means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued only to the controller.\n+     * - False means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued to any random broker.\n+     */\n+    private boolean shouldSendRequestToController = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2NTIyOQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460365229", "createdAt": "2020-07-25T04:53:47Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+    /**\n+     * - True means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued only to the controller.\n+     * - False means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued to any random broker.\n+     */\n+    private boolean shouldSendRequestToController = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NTY3Ng=="}, "originalCommit": null, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDMwMzEzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODowODo0NlrOG1uTMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDo1NDozMlrOG3Cd-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NjI4OA==", "bodyText": "Why do we need this override, which seems to be exactly the same with super class?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r458986288", "createdAt": "2020-07-22T18:08:46Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+    /**\n+     * - True means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued only to the controller.\n+     * - False means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued to any random broker.\n+     */\n+    private boolean shouldSendRequestToController = false;\n+\n+    /**\n+     * Sets a flag indicating that the describe features request should be issued to the controller.\n+     */\n+    public DescribeFeaturesOptions sendRequestToController(boolean shouldSendRequestToController) {\n+        this.shouldSendRequestToController = shouldSendRequestToController;\n+        return this;\n+    }\n+\n+    public boolean sendRequestToController() {\n+        return shouldSendRequestToController;\n+    }\n+\n+    /**\n+     * Sets the timeout in milliseconds for this operation or {@code null} if the default API\n+     * timeout for the AdminClient should be used.\n+     */\n+    public DescribeFeaturesOptions timeoutMs(Integer timeoutMs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2NTMwNQ==", "bodyText": "Done. Removed now. Didn't realize it was present in super class too.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460365305", "createdAt": "2020-07-25T04:54:32Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class DescribeFeaturesOptions extends AbstractOptions<DescribeFeaturesOptions> {\n+    /**\n+     * - True means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued only to the controller.\n+     * - False means the {@link Admin#describeFeatures(DescribeFeaturesOptions)} request can be\n+     *   issued to any random broker.\n+     */\n+    private boolean shouldSendRequestToController = false;\n+\n+    /**\n+     * Sets a flag indicating that the describe features request should be issued to the controller.\n+     */\n+    public DescribeFeaturesOptions sendRequestToController(boolean shouldSendRequestToController) {\n+        this.shouldSendRequestToController = shouldSendRequestToController;\n+        return this;\n+    }\n+\n+    public boolean sendRequestToController() {\n+        return shouldSendRequestToController;\n+    }\n+\n+    /**\n+     * Sets the timeout in milliseconds for this operation or {@code null} if the default API\n+     * timeout for the AdminClient should be used.\n+     */\n+    public DescribeFeaturesOptions timeoutMs(Integer timeoutMs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NjI4OA=="}, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDMwODg3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODoxMDoyNFrOG1uW2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDo1NjoyM1rOG3Celg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NzIyNw==", "bodyText": "s/featureName/feature", "url": "https://github.com/apache/kafka/pull/9001#discussion_r458987227", "createdAt": "2020-07-22T18:10:24Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the {@link Admin#updateFeatures(Set, UpdateFeaturesOptions)} API.\n+ */\n+public class FeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FeatureUpdate(final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2NTQ2Mg==", "bodyText": "Done. Actually feature is removed from this class now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460365462", "createdAt": "2020-07-25T04:56:23Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData;\n+\n+/**\n+ * Encapsulates details about an update to a finalized feature. This is particularly useful to\n+ * define each feature update in the {@link Admin#updateFeatures(Set, UpdateFeaturesOptions)} API.\n+ */\n+public class FeatureUpdate {\n+    private final String featureName;\n+    private final short maxVersionLevel;\n+    private final boolean allowDowngrade;\n+\n+    /**\n+     * @param featureName       the name of the finalized feature to be updated.\n+     * @param maxVersionLevel   the new maximum version level for the finalized feature.\n+     *                          a value < 1 is special and indicates that the update is intended to\n+     *                          delete the finalized feature, and should be accompanied by setting\n+     *                          the allowDowngrade flag to true.\n+     * @param allowDowngrade    - true, if this feature update was meant to downgrade the existing\n+     *                            maximum version level of the finalized feature.\n+     *                          - false, otherwise.\n+     */\n+    public FeatureUpdate(final String featureName, final short maxVersionLevel, final boolean allowDowngrade) {\n+        Objects.requireNonNull(featureName, \"Provided feature name can not be null.\");\n+        if (maxVersionLevel < 1 && !allowDowngrade) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"For featureName: %s, the allowDowngrade flag is not set when the\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4NzIyNw=="}, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQwNjUxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODozNzo0MVrOG1vTvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo0NDozNVrOG3CsiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwMjgxMw==", "bodyText": "The two Call structs only have two differences:\n\nUsed different node provider\nOne would handle not controller, one not\n\nSo I would suggest a bit refactoring to reduce the code redundancy, by providing a helper as:\ngetDescribeFeaturesCall(boolean sendToController) {\n  NodeProvider provider = sendToController ? new ControllerNodeProvider()  : new \n  LeastLoadedNodeProvider();\n  return new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()), \nprovider, ...\n  ...\n     void handleResponse(AbstractResponse response) {\n      ....\n      if ( apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code() && sendToController) \n     {\n        handleNotControllerError(Errors.NOT_CONTROLLER);\n     }\n  };\n}", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459002813", "createdAt": "2020-07-22T18:37:41Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3984,6 +3988,120 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+\n+        Call callViaLeastLoadedNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new LeastLoadedNodeProvider()) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        Call callViaControllerNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2OTAzMw==", "bodyText": "Done. Good point.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460369033", "createdAt": "2020-07-25T05:44:35Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3984,6 +3988,120 @@ void handleFailure(Throwable throwable) {\n         return new AlterClientQuotasResult(Collections.unmodifiableMap(futures));\n     }\n \n+    @Override\n+    public DescribeFeaturesResult describeFeatures(final DescribeFeaturesOptions options) {\n+        final KafkaFutureImpl<FeatureMetadata> future = new KafkaFutureImpl<>();\n+        final long now = time.milliseconds();\n+\n+        Call callViaLeastLoadedNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),\n+            new LeastLoadedNodeProvider()) {\n+\n+            @Override\n+            ApiVersionsRequest.Builder createRequest(int timeoutMs) {\n+                return new ApiVersionsRequest.Builder();\n+            }\n+\n+            @Override\n+            void handleResponse(AbstractResponse response) {\n+                final ApiVersionsResponse apiVersionsResponse = (ApiVersionsResponse) response;\n+                if (apiVersionsResponse.data.errorCode() == Errors.NONE.code()) {\n+                    future.complete(\n+                        new FeatureMetadata(\n+                            apiVersionsResponse.finalizedFeatures(),\n+                            apiVersionsResponse.finalizedFeaturesEpoch(),\n+                            apiVersionsResponse.supportedFeatures()));\n+                } else {\n+                    future.completeExceptionally(\n+                        Errors.forCode(apiVersionsResponse.data.errorCode()).exception());\n+                }\n+            }\n+\n+            @Override\n+            void handleFailure(Throwable throwable) {\n+                completeAllExceptionally(Collections.singletonList(future), throwable);\n+            }\n+        };\n+\n+        Call callViaControllerNode = new Call(\"describeFeatures\", calcDeadlineMs(now, options.timeoutMs()),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwMjgxMw=="}, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQxMzExOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODozOToyNVrOG1vXqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo0NToyNVrOG3Cszw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwMzgxOA==", "bodyText": "Same here, why do we need this extension?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459003818", "createdAt": "2020-07-22T18:39:25Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class UpdateFeaturesOptions extends AbstractOptions<UpdateFeaturesOptions> {\n+    /**\n+     * Sets the timeout in milliseconds for this operation or {@code null} if the default API\n+     * timeout for the AdminClient should be used.\n+     */\n+    public UpdateFeaturesOptions timeoutMs(final Integer timeoutMs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2OTEwMw==", "bodyText": "Done. Removed now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460369103", "createdAt": "2020-07-25T05:45:25Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.annotation.InterfaceStability;\n+\n+@InterfaceStability.Evolving\n+public class UpdateFeaturesOptions extends AbstractOptions<UpdateFeaturesOptions> {\n+    /**\n+     * Sets the timeout in milliseconds for this operation or {@code null} if the default API\n+     * timeout for the AdminClient should be used.\n+     */\n+    public UpdateFeaturesOptions timeoutMs(final Integer timeoutMs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwMzgxOA=="}, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQxNDc5OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODozOTo1NFrOG1vYwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODozNjoyMVrOG4CcBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNDA5Nw==", "bodyText": "As discussed offline, we need to extend the result as per feature.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459004097", "createdAt": "2020-07-22T18:39:54Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.KafkaFuture;\n+\n+public class UpdateFeaturesResult {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxMzM4Mg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461413382", "createdAt": "2020-07-28T08:36:21Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.KafkaFuture;\n+\n+public class UpdateFeaturesResult {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNDA5Nw=="}, "originalCommit": null, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQxODM2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo0MDo1NlrOG1vbHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo0OTozMVrOG3CuDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNDcwMQ==", "bodyText": "Couldn't we just use isIncompatibleWith?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459004701", "createdAt": "2020-07-22T18:40:56Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "diffHunk": "@@ -50,4 +50,8 @@ public static FinalizedVersionRange fromMap(Map<String, Short> versionRangeMap)\n     public boolean isIncompatibleWith(SupportedVersionRange supportedVersionRange) {\n         return min() < supportedVersionRange.min() || max() > supportedVersionRange.max();\n     }\n+\n+    public boolean isCompatibleWith(SupportedVersionRange supportedVersionRange) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2OTQyMw==", "bodyText": "Done. Removed this method now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460369423", "createdAt": "2020-07-25T05:49:31Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "diffHunk": "@@ -50,4 +50,8 @@ public static FinalizedVersionRange fromMap(Map<String, Short> versionRangeMap)\n     public boolean isIncompatibleWith(SupportedVersionRange supportedVersionRange) {\n         return min() < supportedVersionRange.min() || max() > supportedVersionRange.max();\n     }\n+\n+    public boolean isCompatibleWith(SupportedVersionRange supportedVersionRange) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNDcwMQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQyMDIzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo0MToyOVrOG1vcYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo1MDoxOVrOG3CuZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNTAyNw==", "bodyText": "Why do we jump from code 88 to 91?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459005027", "createdAt": "2020-07-22T18:41:29Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "diffHunk": "@@ -319,7 +320,8 @@\n     GROUP_SUBSCRIBED_TO_TOPIC(86, \"Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it.\",\n         GroupSubscribedToTopicException::new),\n     INVALID_RECORD(87, \"This record has failed the validation on broker and hence will be rejected.\", InvalidRecordException::new),\n-    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new);\n+    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new),\n+    FEATURE_UPDATE_FAILED(91, \"Unable to update finalized features.\", FeatureUpdateFailedException::new);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2OTUwOQ==", "bodyText": "Done. Not intentional. Changed to 89 now.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460369509", "createdAt": "2020-07-25T05:50:19Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "diffHunk": "@@ -319,7 +320,8 @@\n     GROUP_SUBSCRIBED_TO_TOPIC(86, \"Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it.\",\n         GroupSubscribedToTopicException::new),\n     INVALID_RECORD(87, \"This record has failed the validation on broker and hence will be rejected.\", InvalidRecordException::new),\n-    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new);\n+    UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new),\n+    FEATURE_UPDATE_FAILED(91, \"Unable to update finalized features.\", FeatureUpdateFailedException::new);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNTAyNw=="}, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQ0MzQ5OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo0Nzo0OFrOG1vrBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODozNzoyOVrOG4CeaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwODc3Mg==", "bodyText": "Do we also need to check allowAutoDowngrade here?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459008772", "createdAt": "2020-07-22T18:47:48Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFeaturesRequest extends AbstractRequest {\n+\n+    public static class Builder extends AbstractRequest.Builder<UpdateFeaturesRequest> {\n+\n+        private final UpdateFeaturesRequestData data;\n+\n+        public Builder(UpdateFeaturesRequestData data) {\n+            super(ApiKeys.UPDATE_FEATURES);\n+            this.data = data;\n+        }\n+\n+        @Override\n+        public UpdateFeaturesRequest build(short version) {\n+            return new UpdateFeaturesRequest(data, version);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return data.toString();\n+        }\n+    }\n+\n+    public final UpdateFeaturesRequestData data;\n+\n+    public UpdateFeaturesRequest(UpdateFeaturesRequestData data, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesRequest(Struct struct, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = new UpdateFeaturesRequestData(struct, version);\n+    }\n+\n+    @Override\n+    public AbstractResponse getErrorResponse(int throttleTimeMsIgnored, Throwable e) {\n+        final ApiError apiError = ApiError.fromThrowable(e);\n+        return new UpdateFeaturesResponse(\n+            new UpdateFeaturesResponseData()\n+                .setErrorCode(apiError.error().code())\n+                .setErrorMessage(apiError.message()));\n+    }\n+\n+    @Override\n+    protected Struct toStruct() {\n+        return data.toStruct(version());\n+    }\n+\n+    public UpdateFeaturesRequestData data() {\n+        return data;\n+    }\n+\n+    public static UpdateFeaturesRequest parse(ByteBuffer buffer, short version) {\n+        return new UpdateFeaturesRequest(\n+            ApiKeys.UPDATE_FEATURES.parseRequest(version, buffer), version);\n+    }\n+\n+    public static boolean isDeleteRequest(UpdateFeaturesRequestData.FeatureUpdateKey update) {\n+        return update.maxVersionLevel() < 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxMzk5Mg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r461413992", "createdAt": "2020-07-28T08:37:29Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFeaturesRequest extends AbstractRequest {\n+\n+    public static class Builder extends AbstractRequest.Builder<UpdateFeaturesRequest> {\n+\n+        private final UpdateFeaturesRequestData data;\n+\n+        public Builder(UpdateFeaturesRequestData data) {\n+            super(ApiKeys.UPDATE_FEATURES);\n+            this.data = data;\n+        }\n+\n+        @Override\n+        public UpdateFeaturesRequest build(short version) {\n+            return new UpdateFeaturesRequest(data, version);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return data.toString();\n+        }\n+    }\n+\n+    public final UpdateFeaturesRequestData data;\n+\n+    public UpdateFeaturesRequest(UpdateFeaturesRequestData data, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesRequest(Struct struct, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = new UpdateFeaturesRequestData(struct, version);\n+    }\n+\n+    @Override\n+    public AbstractResponse getErrorResponse(int throttleTimeMsIgnored, Throwable e) {\n+        final ApiError apiError = ApiError.fromThrowable(e);\n+        return new UpdateFeaturesResponse(\n+            new UpdateFeaturesResponseData()\n+                .setErrorCode(apiError.error().code())\n+                .setErrorMessage(apiError.message()));\n+    }\n+\n+    @Override\n+    protected Struct toStruct() {\n+        return data.toStruct(version());\n+    }\n+\n+    public UpdateFeaturesRequestData data() {\n+        return data;\n+    }\n+\n+    public static UpdateFeaturesRequest parse(ByteBuffer buffer, short version) {\n+        return new UpdateFeaturesRequest(\n+            ApiKeys.UPDATE_FEATURES.parseRequest(version, buffer), version);\n+    }\n+\n+    public static boolean isDeleteRequest(UpdateFeaturesRequestData.FeatureUpdateKey update) {\n+        return update.maxVersionLevel() < 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwODc3Mg=="}, "originalCommit": null, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQ0ODI1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo0OTowOVrOG1vuLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo1Nzo1MVrOG3Cwog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwOTU4Mg==", "bodyText": "We could consider either making data to be private or remove this unnecessary accessor. I would prefer making it private.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459009582", "createdAt": "2020-07-22T18:49:09Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFeaturesRequest extends AbstractRequest {\n+\n+    public static class Builder extends AbstractRequest.Builder<UpdateFeaturesRequest> {\n+\n+        private final UpdateFeaturesRequestData data;\n+\n+        public Builder(UpdateFeaturesRequestData data) {\n+            super(ApiKeys.UPDATE_FEATURES);\n+            this.data = data;\n+        }\n+\n+        @Override\n+        public UpdateFeaturesRequest build(short version) {\n+            return new UpdateFeaturesRequest(data, version);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return data.toString();\n+        }\n+    }\n+\n+    public final UpdateFeaturesRequestData data;\n+\n+    public UpdateFeaturesRequest(UpdateFeaturesRequestData data, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesRequest(Struct struct, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = new UpdateFeaturesRequestData(struct, version);\n+    }\n+\n+    @Override\n+    public AbstractResponse getErrorResponse(int throttleTimeMsIgnored, Throwable e) {\n+        final ApiError apiError = ApiError.fromThrowable(e);\n+        return new UpdateFeaturesResponse(\n+            new UpdateFeaturesResponseData()\n+                .setErrorCode(apiError.error().code())\n+                .setErrorMessage(apiError.message()));\n+    }\n+\n+    @Override\n+    protected Struct toStruct() {\n+        return data.toStruct(version());\n+    }\n+\n+    public UpdateFeaturesRequestData data() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MDA4Mg==", "bodyText": "Done. Made the attribute private.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460370082", "createdAt": "2020-07-25T05:57:51Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.message.UpdateFeaturesRequestData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class UpdateFeaturesRequest extends AbstractRequest {\n+\n+    public static class Builder extends AbstractRequest.Builder<UpdateFeaturesRequest> {\n+\n+        private final UpdateFeaturesRequestData data;\n+\n+        public Builder(UpdateFeaturesRequestData data) {\n+            super(ApiKeys.UPDATE_FEATURES);\n+            this.data = data;\n+        }\n+\n+        @Override\n+        public UpdateFeaturesRequest build(short version) {\n+            return new UpdateFeaturesRequest(data, version);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return data.toString();\n+        }\n+    }\n+\n+    public final UpdateFeaturesRequestData data;\n+\n+    public UpdateFeaturesRequest(UpdateFeaturesRequestData data, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesRequest(Struct struct, short version) {\n+        super(ApiKeys.UPDATE_FEATURES, version);\n+        this.data = new UpdateFeaturesRequestData(struct, version);\n+    }\n+\n+    @Override\n+    public AbstractResponse getErrorResponse(int throttleTimeMsIgnored, Throwable e) {\n+        final ApiError apiError = ApiError.fromThrowable(e);\n+        return new UpdateFeaturesResponse(\n+            new UpdateFeaturesResponseData()\n+                .setErrorCode(apiError.error().code())\n+                .setErrorMessage(apiError.message()));\n+    }\n+\n+    @Override\n+    protected Struct toStruct() {\n+        return data.toStruct(version());\n+    }\n+\n+    public UpdateFeaturesRequestData data() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwOTU4Mg=="}, "originalCommit": null, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQ0OTMxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo0OToyNFrOG1vu2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo1ODowM1rOG3CwqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwOTc1Mw==", "bodyText": "Same here for consistency.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459009753", "createdAt": "2020-07-22T18:49:24Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+\n+/**\n+ * Possible error codes:\n+ *\n+ *   - {@link Errors#CLUSTER_AUTHORIZATION_FAILED}\n+ *   - {@link Errors#NOT_CONTROLLER}\n+ *   - {@link Errors#INVALID_REQUEST}\n+ *   - {@link Errors#FEATURE_UPDATE_FAILED}\n+ */\n+public class UpdateFeaturesResponse extends AbstractResponse {\n+\n+    public final UpdateFeaturesResponseData data;\n+\n+    public UpdateFeaturesResponse(UpdateFeaturesResponseData data) {\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct) {\n+        final short latestVersion = (short) (UpdateFeaturesResponseData.SCHEMAS.length - 1);\n+        this.data = new UpdateFeaturesResponseData(struct, latestVersion);\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct, short version) {\n+        this.data = new UpdateFeaturesResponseData(struct, version);\n+    }\n+\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n+\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        return errorCounts(Errors.forCode(data.errorCode()));\n+    }\n+\n+    @Override\n+    protected Struct toStruct(short version) {\n+        return data.toStruct(version);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return data.toString();\n+    }\n+\n+    public UpdateFeaturesResponseData data() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MDA4OQ==", "bodyText": "Done. Made the attribute private.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460370089", "createdAt": "2020-07-25T05:58:03Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import org.apache.kafka.common.message.UpdateFeaturesResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+\n+/**\n+ * Possible error codes:\n+ *\n+ *   - {@link Errors#CLUSTER_AUTHORIZATION_FAILED}\n+ *   - {@link Errors#NOT_CONTROLLER}\n+ *   - {@link Errors#INVALID_REQUEST}\n+ *   - {@link Errors#FEATURE_UPDATE_FAILED}\n+ */\n+public class UpdateFeaturesResponse extends AbstractResponse {\n+\n+    public final UpdateFeaturesResponseData data;\n+\n+    public UpdateFeaturesResponse(UpdateFeaturesResponseData data) {\n+        this.data = data;\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct) {\n+        final short latestVersion = (short) (UpdateFeaturesResponseData.SCHEMAS.length - 1);\n+        this.data = new UpdateFeaturesResponseData(struct, latestVersion);\n+    }\n+\n+    public UpdateFeaturesResponse(Struct struct, short version) {\n+        this.data = new UpdateFeaturesResponseData(struct, version);\n+    }\n+\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n+\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        return errorCounts(Errors.forCode(data.errorCode()));\n+    }\n+\n+    @Override\n+    protected Struct toStruct(short version) {\n+        return data.toStruct(version);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return data.toString();\n+    }\n+\n+    public UpdateFeaturesResponseData data() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwOTc1Mw=="}, "originalCommit": null, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDQ1MTc2OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo1MDowM1rOG1vwZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo1OTowOVrOG3Cw8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxMDE0OQ==", "bodyText": "Spaces look weird, let's try to remove all two space cases in this file.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459010149", "createdAt": "2020-07-22T18:50:03Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Name\", \"type\":  \"string\", \"versions\":  \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\":  \"MaxVersionLevel\", \"type\": \"int16\", \"versions\":  \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MDE2Mg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460370162", "createdAt": "2020-07-25T05:59:09Z", "author": {"login": "kowshik"}, "path": "clients/src/main/resources/common/message/UpdateFeaturesRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"UpdateFeaturesRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"FeatureUpdates\", \"type\": \"[]FeatureUpdateKey\", \"versions\": \"0+\",\n+      \"about\": \"The list of updates to finalized features.\", \"fields\": [\n+      {\"name\": \"Name\", \"type\":  \"string\", \"versions\":  \"0+\", \"mapKey\": true,\n+        \"about\": \"The name of the finalized feature to be updated.\"},\n+      {\"name\":  \"MaxVersionLevel\", \"type\": \"int16\", \"versions\":  \"0+\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxMDE0OQ=="}, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU3NTk3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjoyNjoxMVrOG16WVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNTo1MDo1MVrOG3CuiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzcwMg==", "bodyText": "Parameters could be on the same line to be consistent with L80", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459183702", "createdAt": "2020-07-23T02:26:11Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -68,6 +69,36 @@ public ApiVersionsResponse(Struct struct, short version) {\n         this(new ApiVersionsResponseData(struct, version));\n     }\n \n+    public ApiVersionsResponseData data() {\n+        return data;\n+    }\n+\n+    public Features<SupportedVersionRange> supportedFeatures() {\n+        final Map<String, SupportedVersionRange> features = new HashMap<>();\n+\n+        for (SupportedFeatureKey key : data.supportedFeatures().valuesSet()) {\n+            features.put(key.name(), new SupportedVersionRange(key.minVersion(), key.maxVersion()));\n+        }\n+\n+        return Features.supportedFeatures(features);\n+    }\n+\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        final Map<String, FinalizedVersionRange> features = new HashMap<>();\n+\n+        for (FinalizedFeatureKey key : data.finalizedFeatures().valuesSet()) {\n+            features.put(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2OTU0NQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460369545", "createdAt": "2020-07-25T05:50:51Z", "author": {"login": "kowshik"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -68,6 +69,36 @@ public ApiVersionsResponse(Struct struct, short version) {\n         this(new ApiVersionsResponseData(struct, version));\n     }\n \n+    public ApiVersionsResponseData data() {\n+        return data;\n+    }\n+\n+    public Features<SupportedVersionRange> supportedFeatures() {\n+        final Map<String, SupportedVersionRange> features = new HashMap<>();\n+\n+        for (SupportedFeatureKey key : data.supportedFeatures().valuesSet()) {\n+            features.put(key.name(), new SupportedVersionRange(key.minVersion(), key.maxVersion()));\n+        }\n+\n+        return Features.supportedFeatures(features);\n+    }\n+\n+    public Features<FinalizedVersionRange> finalizedFeatures() {\n+        final Map<String, FinalizedVersionRange> features = new HashMap<>();\n+\n+        for (FinalizedFeatureKey key : data.finalizedFeatures().valuesSet()) {\n+            features.put(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzcwMg=="}, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU4NTI4OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjozMjo1OVrOG16bog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjoxMjoyNFrOG3C0yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NTA1OA==", "bodyText": "testUpdateFeatures should be suffice, as we sometimes are not passing in a real error.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459185058", "createdAt": "2020-07-23T02:32:59Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MTE0Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460371146", "createdAt": "2020-07-25T06:12:24Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NTA1OA=="}, "originalCommit": null, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU4NjA3OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjozMzozMlrOG16cEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjoxNDozMVrOG3C1Uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NTE3MQ==", "bodyText": "nit: prefer using error == Errors.NONE", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459185171", "createdAt": "2020-07-23T02:33:32Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MTI4Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460371283", "createdAt": "2020-07-25T06:14:31Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NTE3MQ=="}, "originalCommit": null, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU5MTk5OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjozNzoyNlrOG16fUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjoxODowNVrOG3C2dA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjAwMA==", "bodyText": "Collections.emptyList() should be suffice.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459186000", "createdAt": "2020-07-23T02:37:26Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {\n+                future.get();\n+            } else {\n+                final ExecutionException e = assertThrows(ExecutionException.class,\n+                    () -> future.get());\n+                assertEquals(e.getCause().getClass(), error.exception().getClass());\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesHandleNotControllerException() throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponseFrom(\n+                prepareUpdateFeaturesResponse(Errors.NOT_CONTROLLER),\n+                env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(),\n+                env.cluster().clusterResource().clusterId(),\n+                1,\n+                Collections.<MetadataResponse.TopicMetadata>emptyList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MTU3Mg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460371572", "createdAt": "2020-07-25T06:18:05Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {\n+                future.get();\n+            } else {\n+                final ExecutionException e = assertThrows(ExecutionException.class,\n+                    () -> future.get());\n+                assertEquals(e.getCause().getClass(), error.exception().getClass());\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesHandleNotControllerException() throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponseFrom(\n+                prepareUpdateFeaturesResponse(Errors.NOT_CONTROLLER),\n+                env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(),\n+                env.cluster().clusterResource().clusterId(),\n+                1,\n+                Collections.<MetadataResponse.TopicMetadata>emptyList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjAwMA=="}, "originalCommit": null, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU5MzgxOnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjozODozOVrOG16gSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzo0NDowMlrOG5DRqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjI0OQ==", "bodyText": "We should have a matcher checking whether the sent request is pointing at the correct controller id.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459186249", "createdAt": "2020-07-23T02:38:39Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {\n+                future.get();\n+            } else {\n+                final ExecutionException e = assertThrows(ExecutionException.class,\n+                    () -> future.get());\n+                assertEquals(e.getCause().getClass(), error.exception().getClass());\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesHandleNotControllerException() throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponseFrom(\n+                prepareUpdateFeaturesResponse(Errors.NOT_CONTROLLER),\n+                env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(),\n+                env.cluster().clusterResource().clusterId(),\n+                1,\n+                Collections.<MetadataResponse.TopicMetadata>emptyList()));\n+            env.kafkaClient().prepareResponseFrom(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzEyMg==", "bodyText": "I have improved the matcher now, but how do I check the correct controller id?", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373122", "createdAt": "2020-07-25T06:38:13Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {\n+                future.get();\n+            } else {\n+                final ExecutionException e = assertThrows(ExecutionException.class,\n+                    () -> future.get());\n+                assertEquals(e.getCause().getClass(), error.exception().getClass());\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesHandleNotControllerException() throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponseFrom(\n+                prepareUpdateFeaturesResponse(Errors.NOT_CONTROLLER),\n+                env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(),\n+                env.cluster().clusterResource().clusterId(),\n+                1,\n+                Collections.<MetadataResponse.TopicMetadata>emptyList()));\n+            env.kafkaClient().prepareResponseFrom(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjI0OQ=="}, "originalCommit": null, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ3NTY4OQ==", "bodyText": "You are right, it seems not necessary.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462475689", "createdAt": "2020-07-29T17:44:02Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {\n+                future.get();\n+            } else {\n+                final ExecutionException e = assertThrows(ExecutionException.class,\n+                    () -> future.get());\n+                assertEquals(e.getCause().getClass(), error.exception().getClass());\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesHandleNotControllerException() throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponseFrom(\n+                prepareUpdateFeaturesResponse(Errors.NOT_CONTROLLER),\n+                env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(),\n+                env.cluster().clusterResource().clusterId(),\n+                1,\n+                Collections.<MetadataResponse.TopicMetadata>emptyList()));\n+            env.kafkaClient().prepareResponseFrom(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjI0OQ=="}, "originalCommit": null, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU5NDcxOnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjozOTowOFrOG16gww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjoxNjoyMlrOG3C18w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjM3MQ==", "bodyText": "make it a variable, as int controllerId = 1", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459186371", "createdAt": "2020-07-23T02:39:08Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {\n+                future.get();\n+            } else {\n+                final ExecutionException e = assertThrows(ExecutionException.class,\n+                    () -> future.get());\n+                assertEquals(e.getCause().getClass(), error.exception().getClass());\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesHandleNotControllerException() throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponseFrom(\n+                prepareUpdateFeaturesResponse(Errors.NOT_CONTROLLER),\n+                env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(),\n+                env.cluster().clusterResource().clusterId(),\n+                1,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MTQ0Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460371443", "createdAt": "2020-07-25T06:16:22Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3193,6 +3238,104 @@ public void testListOffsetsNonRetriableErrors() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testUpdateFeaturesDuringSuccess() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.NONE);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesInvalidRequestError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.INVALID_REQUEST);\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesUpdateFailedError() throws Exception {\n+        testUpdateFeaturesDuringError(Errors.FEATURE_UPDATE_FAILED);\n+    }\n+\n+    private void testUpdateFeaturesDuringError(Errors error) throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponse(\n+                body -> body instanceof UpdateFeaturesRequest,\n+                prepareUpdateFeaturesResponse(error));\n+            final KafkaFuture<Void> future = env.adminClient().updateFeatures(\n+                new HashSet<>(\n+                    Arrays.asList(\n+                        new FeatureUpdate(\n+                            \"test_feature_1\", (short) 2, false),\n+                        new FeatureUpdate(\n+                            \"test_feature_2\", (short) 3, true))),\n+                new UpdateFeaturesOptions().timeoutMs(10000)).result();\n+            if (error.exception() == null) {\n+                future.get();\n+            } else {\n+                final ExecutionException e = assertThrows(ExecutionException.class,\n+                    () -> future.get());\n+                assertEquals(e.getCause().getClass(), error.exception().getClass());\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateFeaturesHandleNotControllerException() throws Exception {\n+        try (final AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().prepareResponseFrom(\n+                prepareUpdateFeaturesResponse(Errors.NOT_CONTROLLER),\n+                env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(),\n+                env.cluster().clusterResource().clusterId(),\n+                1,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjM3MQ=="}, "originalCommit": null, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU5Njg1OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo0MDo0NFrOG16h-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjoxMDoxNFrOG3C0OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjY4MQ==", "bodyText": "defaultFeatureMetadata should be suffice. AK repo normally tries to avoid using get as func prefix.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459186681", "createdAt": "2020-07-23T02:40:44Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -413,6 +422,42 @@ private static DescribeGroupsResponseData prepareDescribeGroupsResponseData(Stri\n                 Collections.emptySet()));\n         return data;\n     }\n+\n+    private static UpdateFeaturesResponse prepareUpdateFeaturesResponse(Errors error) {\n+        final UpdateFeaturesResponseData data = new UpdateFeaturesResponseData();\n+        data.setErrorCode(error.code());\n+        return new UpdateFeaturesResponse(data);\n+    }\n+\n+    private static FeatureMetadata getDefaultFeatureMetadata() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MTAwMQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460371001", "createdAt": "2020-07-25T06:10:14Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -413,6 +422,42 @@ private static DescribeGroupsResponseData prepareDescribeGroupsResponseData(Stri\n                 Collections.emptySet()));\n         return data;\n     }\n+\n+    private static UpdateFeaturesResponse prepareUpdateFeaturesResponse(Errors error) {\n+        final UpdateFeaturesResponseData data = new UpdateFeaturesResponseData();\n+        data.setErrorCode(error.code());\n+        return new UpdateFeaturesResponse(data);\n+    }\n+\n+    private static FeatureMetadata getDefaultFeatureMetadata() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NjY4MQ=="}, "originalCommit": null, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTU5NzM3OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo0MTowOVrOG16iSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjoxMTo0M1rOG3C0oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4Njc2MA==", "bodyText": "nit: could use Utils.mkMap to simplify here.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459186760", "createdAt": "2020-07-23T02:41:09Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -413,6 +422,42 @@ private static DescribeGroupsResponseData prepareDescribeGroupsResponseData(Stri\n                 Collections.emptySet()));\n         return data;\n     }\n+\n+    private static UpdateFeaturesResponse prepareUpdateFeaturesResponse(Errors error) {\n+        final UpdateFeaturesResponseData data = new UpdateFeaturesResponseData();\n+        data.setErrorCode(error.code());\n+        return new UpdateFeaturesResponse(data);\n+    }\n+\n+    private static FeatureMetadata getDefaultFeatureMetadata() {\n+        return new FeatureMetadata(\n+            Features.finalizedFeatures(new HashMap<String, FinalizedVersionRange>() {\n+                {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MTEwNA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460371104", "createdAt": "2020-07-25T06:11:43Z", "author": {"login": "kowshik"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -413,6 +422,42 @@ private static DescribeGroupsResponseData prepareDescribeGroupsResponseData(Stri\n                 Collections.emptySet()));\n         return data;\n     }\n+\n+    private static UpdateFeaturesResponse prepareUpdateFeaturesResponse(Errors error) {\n+        final UpdateFeaturesResponseData data = new UpdateFeaturesResponseData();\n+        data.setErrorCode(error.code());\n+        return new UpdateFeaturesResponse(data);\n+    }\n+\n+    private static FeatureMetadata getDefaultFeatureMetadata() {\n+        return new FeatureMetadata(\n+            Features.finalizedFeatures(new HashMap<String, FinalizedVersionRange>() {\n+                {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4Njc2MA=="}, "originalCommit": null, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxMDQzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1MDoxNlrOG16png==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMjozMDoyNlrOG5MhdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODYzOA==", "bodyText": "nit: zkClient.getDataAndVersion(FeatureZNode.path)._2 should be suffice", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459188638", "createdAt": "2020-07-23T02:50:16Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzIxMQ==", "bodyText": "newVersion is more readable than _2.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373211", "createdAt": "2020-07-25T06:39:17Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODYzOA=="}, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNzE4OQ==", "bodyText": "Yea, I mean you could use val newVersion = zkClient.getDataAndVersion(FeatureZNode.path)._2, but it's up to you.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r462627189", "createdAt": "2020-07-29T22:30:26Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODYzOA=="}, "originalCommit": null, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxMTgyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1MToxOFrOG16qbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0MDoxNVrOG3C9MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODg0NA==", "bodyText": "s/Znode/ZNode", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459188844", "createdAt": "2020-07-23T02:51:18Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzI5Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373296", "createdAt": "2020-07-25T06:40:15Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODg0NA=="}, "originalCommit": null, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxMjQxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1MTo0MVrOG16qww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0MTozMVrOG3C9iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODkzMQ==", "bodyText": "s/ is is / is", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459188931", "createdAt": "2020-07-23T02:51:41Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzM4NA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373384", "createdAt": "2020-07-25T06:41:31Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODkzMQ=="}, "originalCommit": null, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxMzI5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1MjowOVrOG16rQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0MTozN1rOG3C9jQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTA1Nw==", "bodyText": "remove in ZK", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459189057", "createdAt": "2020-07-23T02:52:09Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzM4OQ==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373389", "createdAt": "2020-07-25T06:41:37Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTA1Nw=="}, "originalCommit": null, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxNDIzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1Mjo1NVrOG16rzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjozOTo1MlrOG3C9Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTE5OA==", "bodyText": "nit: s/it's/its", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459189198", "createdAt": "2020-07-23T02:52:55Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzI1NA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373254", "createdAt": "2020-07-25T06:39:52Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTE5OA=="}, "originalCommit": null, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxNDM2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1MzowM1rOG16r5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0NTo1MVrOG3C-8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTIyMA==", "bodyText": "nit: s/it's/its", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459189220", "createdAt": "2020-07-23T02:53:03Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3Mzc0Ng==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373746", "createdAt": "2020-07-25T06:45:51Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.\n+   *\n+   * This method sets up the FeatureZNode with enabled status. This status means the feature\n+   * versioning system (KIP-584) is enabled, and, the finalized features stored in the FeatureZNode\n+   * are active. This status should be written by the controller to the FeatureZNode only when the\n+   * broker IBP config is greater than or equal to KAFKA_2_7_IV0.\n+   *\n+   * There are multiple cases handled here:\n+   *\n+   * 1. New cluster bootstrap:\n+   *    A new Kafka cluster (i.e. it is deployed first time) is almost always started with IBP config\n+   *    setting greater than or equal to KAFKA_2_7_IV0. We would like to start the cluster with all\n+   *    the possible supported features finalized immediately. Assuming this is the case, the\n+   *    controller will start up and notice that the FeatureZNode is absent in the new cluster,\n+   *    it will then create a FeatureZNode (with enabled status) containing the entire list of\n+   *    default supported features as its finalized features.\n+   *\n+   * 2. Broker binary upgraded, but IBP config set to lower than KAFKA_2_7_IV0:\n+   *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, and the\n+   *    Broker binary has been upgraded to a newer version that supports the feature versioning\n+   *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+   *    binary. In this case, we want to start with no finalized features and allow the user to\n+   *    finalize them whenever they are ready i.e. in the future whenever the user sets IBP config\n+   *    to be greater than or equal to KAFKA_2_7_IV0, then the user could start finalizing the\n+   *    features. The reason to do this is that enabling all the possible features immediately after\n+   *    an upgrade could be harmful to the cluster.\n+   *    This is how we handle such a case:\n+   *      - Before the IBP config upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the\n+   *        controller will start up and check if the FeatureZNode is absent. If absent, then it\n+   *        will react by creating a FeatureZNode with disabled status and empty finalized features.\n+   *        Otherwise, if a node already exists in enabled status then the controller will just\n+   *        flip the status to disabled and clear the finalized features.\n+   *      - After the IBP config upgrade (i.e. IBP config set to greater than or equal to\n+   *        KAFKA_2_7_IV0), when the controller starts up it will check if the FeatureZNode exists\n+   *        and whether it is disabled. In such a case, it won\u2019t upgrade all features immediately.\n+   *        Instead it will just switch the FeatureZNode status to enabled status. This lets the\n+   *        user finalize the features later.\n+   *\n+   * 3. Broker binary upgraded, with existing cluster IBP config >= KAFKA_2_7_IV0:\n+   *    Imagine an existing Kafka cluster with IBP config >= KAFKA_2_7_IV0, and the broker binary\n+   *    has just been upgraded to a newer version (that supports IBP config KAFKA_2_7_IV0 and higher).\n+   *    The controller will start up and find that a FeatureZNode is already present with enabled\n+   *    status and existing finalized features. In such a case, the controller needs to scan the\n+   *    existing finalized features and mutate them for the purpose of version level deprecation\n+   *    (if needed).\n+   *    This is how we handle this case: If an existing finalized feature is present in the default\n+   *    finalized features, then, it's existing minimum version level is updated to the default", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTIyMA=="}, "originalCommit": null, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxNTI1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1Mzo0OVrOG16saA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0MjoxNFrOG3C9zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTM1Mg==", "bodyText": "remove one and", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459189352", "createdAt": "2020-07-23T02:53:49Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzQ1NA==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373454", "createdAt": "2020-07-25T06:42:14Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTM1Mg=="}, "originalCommit": null, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTYxNjUwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjo1NDozNlrOG16tDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNjo0MjoyMVrOG3C91w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTUxNw==", "bodyText": "remove and their version levels or restructure as the information about finalized features' version levels", "url": "https://github.com/apache/kafka/pull/9001#discussion_r459189517", "createdAt": "2020-07-23T02:54:36Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM3MzQ2Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9001#discussion_r460373463", "createdAt": "2020-07-25T06:42:21Z", "author": {"login": "kowshik"}, "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -266,6 +275,179 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  private def createFeatureZNode(newNode: FeatureZNode): Int = {\n+    info(s\"Creating FeatureZNode at path: ${FeatureZNode.path} with contents: $newNode\")\n+    zkClient.createFeatureZNode(newNode)\n+    val (_, newVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    newVersion\n+  }\n+\n+  private def updateFeatureZNode(updatedNode: FeatureZNode): Int = {\n+    info(s\"Updating FeatureZNode at path: ${FeatureZNode.path} with contents: $updatedNode\")\n+    zkClient.updateFeatureZNode(updatedNode)\n+  }\n+\n+  /**\n+   * This method enables the feature versioning system (KIP-584).\n+   *\n+   * Development in Kafka (from a high level) is organized into features. Each feature is tracked by\n+   * a name and a range of version numbers. A feature can be of two types:\n+   *\n+   * 1. Supported feature:\n+   * A supported feature is represented by a name (String) and a range of versions (defined by a\n+   * {@link SupportedVersionRange}). It refers to a feature that a particular broker advertises\n+   * support for. Each broker advertises the version ranges of it\u2019s own supported features in its\n+   * own BrokerIdZnode. The contents of the advertisement are specific to the particular broker and\n+   * do not represent any guarantee of a cluster-wide availability of the feature for any particular\n+   * range of versions.\n+   *\n+   * 2. Finalized feature:\n+   * A finalized feature is is represented by a name (String) and a range of version levels (defined\n+   * by a {@link FinalizedVersionRange}). Whenever the feature versioning system (KIP-584) is\n+   * enabled, the finalized features are stored in ZK in the cluster-wide common FeatureZNode.\n+   * In comparison to a supported feature, the key difference is that a finalized feature exists\n+   * in ZK only when it is guaranteed to be supported by any random broker in the cluster for a\n+   * specified range of version levels. Also, the controller is the one and only entity modifying\n+   * the information about finalized features and their version levels.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4OTUxNw=="}, "originalCommit": null, "originalPosition": 94}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2215, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}