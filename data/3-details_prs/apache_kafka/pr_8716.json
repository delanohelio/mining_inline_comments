{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIyMDM3MTU1", "number": 8716, "title": "KAFKA-6145: KIP-441: Fix assignor config passthough", "bodyText": "Also fixes a system test by configuring the HATA to perform a one-shot balanced assignment\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-05-22T16:47:44Z", "url": "https://github.com/apache/kafka/pull/8716", "merged": true, "mergeCommit": {"oid": "2cff1fab3f1c20f464f75f32b6138707050199b5"}, "closed": true, "closedAt": "2020-05-27T18:50:12Z", "author": {"login": "vvcephei"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcj1f4CgFqTQxNzA1MjMwOA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcldIL0AFqTQxOTQ2MzgyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MDUyMzA4", "url": "https://github.com/apache/kafka/pull/8716#pullrequestreview-417052308", "createdAt": "2020-05-22T16:48:10Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNjo0ODoxMFrOGZdlUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNzoxMTo1OFrOGZeNzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg==", "bodyText": "This is where we forgot to copy over the configs.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429352272", "createdAt": "2020-05-22T16:48:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));\n+        consumerProps.put(MAX_WARMUP_REPLICAS_CONFIG, getInt(MAX_WARMUP_REPLICAS_CONFIG));\n+        consumerProps.put(PROBING_REBALANCE_INTERVAL_MS_CONFIG, getLong(PROBING_REBALANCE_INTERVAL_MS_CONFIG));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjY2MQ==", "bodyText": "I added this constraint to mirror the constraint we already apply in StreamConfig. It's not critical, but I was disappointed that I had written a bunch of tests that included a technically invalid configuration.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429352661", "createdAt": "2020-05-22T16:49:03Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -359,6 +359,10 @@ private AssignmentConfigs(final StreamsConfig configs) {\n                           final Integer maxWarmupReplicas,\n                           final Integer numStandbyReplicas,\n                           final Long probingRebalanceIntervalMs) {\n+            if (maxWarmupReplicas < 1) {\n+                throw new IllegalArgumentException(\"must configure at least one warmup replica\");\n+            }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1Mjc1OQ==", "bodyText": "I found this useful while debugging the system test.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429352759", "createdAt": "2020-05-22T16:49:19Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -334,6 +334,7 @@ public String toString() {\n             \") prevStandbyTasks: (\" + prevStandbyTasks +\n             \") prevOwnedPartitionsByConsumerId: (\" + ownedPartitions.keySet() +\n             \") changelogOffsetTotalsByTask: (\" + taskOffsetSums.entrySet() +\n+            \") taskLagTotals: (\" + taskLagTotals.entrySet() +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MzE5Nw==", "bodyText": "Since we can't have zero warmups, we don't need this condition (that I added in #8696)", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429353197", "createdAt": "2020-05-22T16:50:14Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -90,15 +91,12 @@ public boolean assign(final Map<UUID, ClientState> clients,\n \n         assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n-        // We shouldn't plan a probing rebalance if we _needed_ task movements, but couldn't do any\n-        // due to being configured for no warmups.\n-        final boolean probingRebalanceNeeded =\n-            configs.maxWarmupReplicas > 0 && neededActiveTaskMovements + neededStandbyTaskMovements > 0;\n+        final boolean probingRebalanceNeeded = neededActiveTaskMovements + neededStandbyTaskMovements > 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MzcwOQ==", "bodyText": "Fixing a double-space we were printing when there was a followup. (It would say with  followup)", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429353709", "createdAt": "2020-05-22T16:51:18Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -90,15 +91,12 @@ public boolean assign(final Map<UUID, ClientState> clients,\n \n         assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n-        // We shouldn't plan a probing rebalance if we _needed_ task movements, but couldn't do any\n-        // due to being configured for no warmups.\n-        final boolean probingRebalanceNeeded =\n-            configs.maxWarmupReplicas > 0 && neededActiveTaskMovements + neededStandbyTaskMovements > 0;\n+        final boolean probingRebalanceNeeded = neededActiveTaskMovements + neededStandbyTaskMovements > 0;\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n-                     \" with \" +\n-                     (probingRebalanceNeeded ? \"\" : \"no\") +\n+                     \" with\" +\n+                     (probingRebalanceNeeded ? \"\" : \" no\") +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1NDIzMg==", "bodyText": "A short-lived, empty TreeSet costs practically nothing, and I found the other logic (with null meaning empty) a bit confusing during debugging.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429354232", "createdAt": "2020-05-22T16:52:23Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -241,16 +239,29 @@ private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState>\n         final Map<TaskId, SortedSet<UUID>> taskToCaughtUpClients = new HashMap<>();\n \n         for (final TaskId task : statefulTasks) {\n-\n+            final TreeSet<UUID> caughtUpClients = new TreeSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTI1MQ==", "bodyText": "I realized that our condition was actually wrong here. In addition to all the zero-or-greater lags, there are two negative lags, one meaning \"unknown\" (-1), and one meaning \"latest\" (-2). When we said taskLag <= acceptableRecoveryLag, it was unintentionally encompassing the sentinel values as well. Even if we want a sentinel to be considered \"caught up\" (as with \"Latest\"), we should do so explicitly, not by mathematical coincidence.\nI also added a special case when acceptableRecoveryLag is set to MAX_VALUE to indicate that all tasks, regardless of their lag (even if it's a sentinel), are to be considered caught-up.\nI also found the boolean expression with all the conditionals a little hard to read, so I pulled out some semantic methods.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429359251", "createdAt": "2020-05-22T17:03:41Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -241,16 +239,29 @@ private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState>\n         final Map<TaskId, SortedSet<UUID>> taskToCaughtUpClients = new HashMap<>();\n \n         for (final TaskId task : statefulTasks) {\n-\n+            final TreeSet<UUID> caughtUpClients = new TreeSet<>();\n             for (final Map.Entry<UUID, ClientState> clientEntry : clientStates.entrySet()) {\n                 final UUID client = clientEntry.getKey();\n                 final long taskLag = clientEntry.getValue().lagFor(task);\n-                if (taskLag == Task.LATEST_OFFSET || taskLag <= acceptableRecoveryLag) {\n-                    taskToCaughtUpClients.computeIfAbsent(task, ignored -> new TreeSet<>()).add(client);\n+                if (active(taskLag) || unbounded(acceptableRecoveryLag) || acceptable(acceptableRecoveryLag, taskLag)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTk1OQ==", "bodyText": "Expanding DeMorgan's law at @cadonna 's request (which I also appreciated).", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429359959", "createdAt": "2020-05-22T17:05:24Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -56,14 +56,17 @@ private int numCaughtUpClients() {\n         return caughtUpClients.size();\n     }\n \n-    /**\n-     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n-     */\n+    private static boolean taskIsNotCaughtUpOnClientAndOtherCaughtUpClientsExist(final TaskId task,\n+                                                                                 final UUID client,\n+                                                                                 final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients) {\n+        return !taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjA0MA==", "bodyText": "The diff is misaligned. I removed shouldBeStickyForActiveAndStandbyTasksEvenIfNoWarmups and added shouldSkipWarmupsWhenAcceptableLagIsMax.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362040", "createdAt": "2020-05-22T17:10:30Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -81,7 +81,7 @@\n     );\n \n     @Test\n-    public void shouldBeStickyForActiveAndStandbyTasksEvenIfNoWarmups() {\n+    public void shouldBeStickyForActiveAndStandbyTasksWhileWarmingUp() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjE4OA==", "bodyText": "All these tests erroneously set \"max warmups\" to zero.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362188", "createdAt": "2020-05-22T17:10:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -150,7 +147,7 @@ public void shouldAssignActiveStatefulTasksEvenlyOverClientsWhereNumberOfClients\n             clientStates,\n             allTaskIds,\n             allTaskIds,\n-            new AssignmentConfigs(0L, 0, 0, 0L)\n+            new AssignmentConfigs(0L, 1, 0, 0L)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjM5MA==", "bodyText": "It was handy to be able to see the used config file while debugging.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362390", "createdAt": "2020-05-22T17:11:19Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -44,6 +44,9 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):\n     CLEAN_NODE_ENABLED = True\n \n     logs = {\n+        \"streams_config\": {\n+            \"path\": CONFIG_FILE,\n+            \"collect_default\": True},", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjYzOA==", "bodyText": "Added this configuration to fix the flaky StreamsOptimizedTest.test_upgrade_optimized_topology", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362638", "createdAt": "2020-05-22T17:11:58Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -465,6 +468,9 @@ def prop_file(self):\n         properties['reduce.topic'] = self.REDUCE_TOPIC\n         properties['join.topic'] = self.JOIN_TOPIC\n \n+        # Long.MAX_VALUE lets us do the assignment without a warmup\n+        properties['acceptable.recovery.lag'] = \"9223372036854775807\"\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MTQyMzg5", "url": "https://github.com/apache/kafka/pull/8716#pullrequestreview-417142389", "createdAt": "2020-05-22T19:26:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxOToyNjozNVrOGZht8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxOToyNjozNVrOGZht8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQyMDAxNw==", "bodyText": "Added this test, and verified that it does indeed fail on trunk for the expected reason that the new configs were ignored and defaults were substituted instead.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429420017", "createdAt": "2020-05-22T19:26:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.internals.StreamThread;\n+import org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.hamcrest.Matchers;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.lang.reflect.Field;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class TaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, IllegalAccessException {\n+        // This test uses reflection to check and make sure that all the expected configurations really\n+        // make it all the way to configure the task assignor. There's no other use case for being able\n+        // to extract all these fields, so reflection is a good choice until we find that the maintenance\n+        // burden is too high.\n+        //\n+        // Also note that this is an integration test because so many components have to come together to\n+        // ensure these configurations wind up where they belong, and any number of future code changes\n+        // could break this change.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjI2OTQy", "url": "https://github.com/apache/kafka/pull/8716#pullrequestreview-417226942", "createdAt": "2020-05-22T23:47:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMzo0NzozOVrOGZmDrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDowOTowNVrOGZmPxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MTExOA==", "bodyText": "Fair enough. I don't think it was meant as a cost saving thing, just to make it easier to understand when something did or did not have caught-up clients. If you find this logic easier to follow, go for it", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429491118", "createdAt": "2020-05-22T23:47:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -241,16 +239,29 @@ private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState>\n         final Map<TaskId, SortedSet<UUID>> taskToCaughtUpClients = new HashMap<>();\n \n         for (final TaskId task : statefulTasks) {\n-\n+            final TreeSet<UUID> caughtUpClients = new TreeSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1NDIzMg=="}, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MTUxMw==", "bodyText": "Nice catch! One nit is that \"active\" alone is not sufficient for being considered caught-up. Can we rename the active condition to running or activeRunning, etc?", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429491513", "createdAt": "2020-05-22T23:50:32Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -241,16 +239,29 @@ private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState>\n         final Map<TaskId, SortedSet<UUID>> taskToCaughtUpClients = new HashMap<>();\n \n         for (final TaskId task : statefulTasks) {\n-\n+            final TreeSet<UUID> caughtUpClients = new TreeSet<>();\n             for (final Map.Entry<UUID, ClientState> clientEntry : clientStates.entrySet()) {\n                 final UUID client = clientEntry.getKey();\n                 final long taskLag = clientEntry.getValue().lagFor(task);\n-                if (taskLag == Task.LATEST_OFFSET || taskLag <= acceptableRecoveryLag) {\n-                    taskToCaughtUpClients.computeIfAbsent(task, ignored -> new TreeSet<>()).add(client);\n+                if (active(taskLag) || unbounded(acceptableRecoveryLag) || acceptable(acceptableRecoveryLag, taskLag)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTI1MQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MTk4NQ==", "bodyText": "Do we automatically pass through the internal configs? I notice we don't copy over the task assignor class, or the new assignment listener callback I added for the integration tests. But both of them seem to get through", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429491985", "createdAt": "2020-05-22T23:53:56Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));\n+        consumerProps.put(MAX_WARMUP_REPLICAS_CONFIG, getInt(MAX_WARMUP_REPLICAS_CONFIG));\n+        consumerProps.put(PROBING_REBALANCE_INTERVAL_MS_CONFIG, getLong(PROBING_REBALANCE_INTERVAL_MS_CONFIG));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MjU5MQ==", "bodyText": "I know it's deprecated, but I think the PartitionGrouper config is missing as well.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429492591", "createdAt": "2020-05-22T23:58:45Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));\n+        consumerProps.put(MAX_WARMUP_REPLICAS_CONFIG, getInt(MAX_WARMUP_REPLICAS_CONFIG));\n+        consumerProps.put(PROBING_REBALANCE_INTERVAL_MS_CONFIG, getLong(PROBING_REBALANCE_INTERVAL_MS_CONFIG));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NDIxMw==", "bodyText": "Should we move the handful of AssignorConfiguration related tests from StreamsPartitionAssignorTest to here?", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429494213", "createdAt": "2020-05-23T00:09:05Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfigurationTest.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.junit.Test;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.junit.Assert.assertThrows;\n+\n+public class AssignorConfigurationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjMzMTM5", "url": "https://github.com/apache/kafka/pull/8716#pullrequestreview-417233139", "createdAt": "2020-05-23T00:29:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDoyOTo1N1rOGZmZHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDoyOTo1N1rOGZmZHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjYwNQ==", "bodyText": "Trying to avoid piling on even more unrelated questions to the thread below, but there's another config that would need to be passed in, the admin client timeout.\nThat said, can we just remove it? It only gets the timeout the admin is configured with anyway", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429496605", "createdAt": "2020-05-23T00:29:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3Njc4MzYw", "url": "https://github.com/apache/kafka/pull/8716#pullrequestreview-417678360", "createdAt": "2020-05-25T12:29:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMjoyOToyN1rOGZ_mDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMzoxMjowOVrOGaAsQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwOTUxOQ==", "bodyText": "req: Please add verifications to StreamsConfigTest#consumerConfigMustContainStreamPartitionAssignorConfig()", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429909519", "createdAt": "2020-05-25T12:29:27Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));\n+        consumerProps.put(MAX_WARMUP_REPLICAS_CONFIG, getInt(MAX_WARMUP_REPLICAS_CONFIG));\n+        consumerProps.put(PROBING_REBALANCE_INTERVAL_MS_CONFIG, getLong(PROBING_REBALANCE_INTERVAL_MS_CONFIG));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkxMzk3Mw==", "bodyText": "IMO, we should check the limits for all configs. However, I am not sure if we should check probingRebalanceIntervalMs to be >= 60 * 1000L (as we do in StreamsConfig) or just `>= 0.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429913973", "createdAt": "2020-05-25T12:40:36Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -359,6 +359,10 @@ private AssignmentConfigs(final StreamsConfig configs) {\n                           final Integer maxWarmupReplicas,\n                           final Integer numStandbyReplicas,\n                           final Long probingRebalanceIntervalMs) {\n+            if (maxWarmupReplicas < 1) {\n+                throw new IllegalArgumentException(\"must configure at least one warmup replica\");\n+            }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjY2MQ=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkyMzA3MQ==", "bodyText": "Nice catch indeed! I agree with @ableegoldman about the renaming. I am in favour of activeRunning or activeAndRunning.", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429923071", "createdAt": "2020-05-25T13:02:08Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -241,16 +239,29 @@ private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState>\n         final Map<TaskId, SortedSet<UUID>> taskToCaughtUpClients = new HashMap<>();\n \n         for (final TaskId task : statefulTasks) {\n-\n+            final TreeSet<UUID> caughtUpClients = new TreeSet<>();\n             for (final Map.Entry<UUID, ClientState> clientEntry : clientStates.entrySet()) {\n                 final UUID client = clientEntry.getKey();\n                 final long taskLag = clientEntry.getValue().lagFor(task);\n-                if (taskLag == Task.LATEST_OFFSET || taskLag <= acceptableRecoveryLag) {\n-                    taskToCaughtUpClients.computeIfAbsent(task, ignored -> new TreeSet<>()).add(client);\n+                if (active(taskLag) || unbounded(acceptableRecoveryLag) || acceptable(acceptableRecoveryLag, taskLag)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTI1MQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkyNzQ5MA==", "bodyText": "Yes, please!", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429927490", "createdAt": "2020-05-25T13:12:09Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfigurationTest.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.junit.Test;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.junit.Assert.assertThrows;\n+\n+public class AssignorConfigurationTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NDIxMw=="}, "originalCommit": null, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzY1ODI2", "url": "https://github.com/apache/kafka/pull/8716#pullrequestreview-418765826", "createdAt": "2020-05-27T00:28:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoyODo1OVrOGa0r6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoyODo1OVrOGa0r6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3OTM3MA==", "bodyText": "I think this comment got lost in a discussion thread, but can we add a note to AssignorConfiguration pointing out that any Streams configs added here will need to be explicitly passed through? It seems like it's too easy to fall into this same trap again", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430779370", "createdAt": "2020-05-27T00:28:59Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -347,22 +348,28 @@ public AssignmentListener assignmentListener() {\n         public final long probingRebalanceIntervalMs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c8426c3678dfff53a49327bc7a98be5dff90da5", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/3c8426c3678dfff53a49327bc7a98be5dff90da5", "committedDate": "2020-05-27T14:33:35Z", "message": "KAFKA-6145: KIP-441: Fix assignor config passthough"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94194f2f6cc87a202266fbabfe426f8bc4fb09aa", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/94194f2f6cc87a202266fbabfe426f8bc4fb09aa", "committedDate": "2020-05-27T14:44:23Z", "message": "fix test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NDYzODI1", "url": "https://github.com/apache/kafka/pull/8716#pullrequestreview-419463825", "createdAt": "2020-05-27T17:43:43Z", "commit": {"oid": "94194f2f6cc87a202266fbabfe426f8bc4fb09aa"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNzo0Mzo0M1rOGbWEtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNzo1MTo0M1rOGbWcIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyNjM5MA==", "bodyText": "\ud83e\udd26\u200d\u2640\ufe0f", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431326390", "createdAt": "2020-05-27T17:43:43Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -874,7 +874,7 @@\n         public static final String TIME = \"__time__\";\n \n         // This is settable in the main Streams config, but it's a private API for testing\n-        public static final String ASSIGNMENT_LISTENER = \"__asignment.listener__\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94194f2f6cc87a202266fbabfe426f8bc4fb09aa"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyNzg5OA==", "bodyText": "I'll open a separate PR to remove the extra timeout config", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431327898", "createdAt": "2020-05-27T17:46:16Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjYwNQ=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMzMjM4Ng==", "bodyText": "Should we also validate that the task assignor gets passed through? We could even pass in a custom assignor and use that to verify the correct assignor configs got passed through.\nOf course, reflection black magic-ry is just more fun \ud83d\ude42", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431332386", "createdAt": "2020-05-27T17:51:43Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.internals.StreamThread;\n+import org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.lang.reflect.Field;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.sameInstance;\n+\n+@Category(IntegrationTest.class)\n+public class TaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, IllegalAccessException {\n+        // This test uses reflection to check and make sure that all the expected configurations really\n+        // make it all the way to configure the task assignor. There's no other use case for being able\n+        // to extract all these fields, so reflection is a good choice until we find that the maintenance\n+        // burden is too high.\n+        //\n+        // Also note that this is an integration test because so many components have to come together to\n+        // ensure these configurations wind up where they belong, and any number of future code changes\n+        // could break this change.\n+\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + testId;\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, \"input\");\n+\n+        // Maybe I'm paranoid, but I don't want the compiler deciding that my lambdas are equal to the identity\n+        // function and defeating my identity check\n+        final AtomicInteger compilerDefeatingReference = new AtomicInteger(0);\n+\n+        // the implementation doesn't matter, we're just going to verify the reference.\n+        final AssignorConfiguration.AssignmentListener configuredAssignmentListener =\n+            stable -> compilerDefeatingReference.incrementAndGet();\n+\n+        final Properties properties = mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, \"5\"),\n+                mkEntry(StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG, \"6\"),\n+                mkEntry(StreamsConfig.MAX_WARMUP_REPLICAS_CONFIG, \"7\"),\n+                mkEntry(StreamsConfig.PROBING_REBALANCE_INTERVAL_MS_CONFIG, \"480000\"),\n+                mkEntry(StreamsConfig.InternalConfig.ASSIGNMENT_LISTENER, configuredAssignmentListener),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94194f2f6cc87a202266fbabfe426f8bc4fb09aa"}, "originalPosition": 91}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1147, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}