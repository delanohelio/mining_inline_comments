{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA4MjQ4Njcx", "number": 8541, "reviewThreads": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzoyMzo1N1rOD1hNKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxNjozOTo1OFrOD3SLZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ1MTYyOnYy", "diffSide": "RIGHT", "path": "build.gradle", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzoyMzo1N1rOGLADsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNToxMzozMFrOGLceCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODQ2NQ==", "bodyText": "Necessary because the test name that JUnit generates for the parameterized StreamsPartitionAssignorTest is slightly too long. I have no way to shorten it because the thing that pushes it over is the fact that there are two package names in the parameterized method name, and there's no control over the format of the test name itself. So, I decided just to truncate the file name instead, which is almost certainly still unique for pretty much any test.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414188465", "createdAt": "2020-04-23T23:23:57Z", "author": {"login": "vvcephei"}, "path": "build.gradle", "diffHunk": "@@ -236,8 +236,10 @@ subprojects {\n     def logStreams = new HashMap<String, FileOutputStream>()\n     beforeTest { TestDescriptor td ->\n       def tid = testId(td)\n+      // truncate the file name if it's too long\n       def logFile = new File(\n-          \"${projectDir}/build/reports/testOutput/${tid}.test.stdout\")\n+              \"${projectDir}/build/reports/testOutput/${tid.substring(0, Math.min(tid.size(),240))}.test.stdout\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY1Mzk2Mg==", "bodyText": "The only alternative I can think of is to parameterize the \"short name\" of the TaskAssignor, which seems kind of wacky.\nAlso, worth noting the impact of truncation is nothing if the file name is still unique. If the name is shared between two tests, then the impact is still nothing if both tests pass. The only observable effect is that if one or both tests fail, their logs would get combined. It seems like we can afford just to defer this problem until it happens, if ever.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414653962", "createdAt": "2020-04-24T15:13:30Z", "author": {"login": "vvcephei"}, "path": "build.gradle", "diffHunk": "@@ -236,8 +236,10 @@ subprojects {\n     def logStreams = new HashMap<String, FileOutputStream>()\n     beforeTest { TestDescriptor td ->\n       def tid = testId(td)\n+      // truncate the file name if it's too long\n       def logFile = new File(\n-          \"${projectDir}/build/reports/testOutput/${tid}.test.stdout\")\n+              \"${projectDir}/build/reports/testOutput/${tid.substring(0, Math.min(tid.size(),240))}.test.stdout\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODQ2NQ=="}, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ1MjYyOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzoyNDoxN1rOGLAEMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMjo0MzoyMVrOGMhJtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODU5Mg==", "bodyText": "I've been wanting this for a while, so I just decided to add it.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414188592", "createdAt": "2020-04-23T23:24:17Z", "author": {"login": "vvcephei"}, "path": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java", "diffHunk": "@@ -1146,4 +1146,13 @@ private static byte checkRange(final byte i) {\n             }\n         };\n     }\n+\n+    @SafeVarargs\n+    public static <E> Set<E> union(final Supplier<Set<E>> constructor, final Set<E>... set) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc3OTI1Mg==", "bodyText": "req: Please add unit tests for this method", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415779252", "createdAt": "2020-04-27T12:43:21Z", "author": {"login": "cadonna"}, "path": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java", "diffHunk": "@@ -1146,4 +1146,13 @@ private static byte checkRange(final byte i) {\n             }\n         };\n     }\n+\n+    @SafeVarargs\n+    public static <E> Set<E> union(final Supplier<Set<E>> constructor, final Set<E>... set) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODU5Mg=="}, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ1MzQ1OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/test/TestUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzoyNDo0MFrOGLAEsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzoyNDo0MFrOGLAEsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODcyMw==", "bodyText": "This is pointless unless we evaluate it inside the lambda.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414188723", "createdAt": "2020-04-23T23:24:40Z", "author": {"login": "vvcephei"}, "path": "clients/src/test/java/org/apache/kafka/test/TestUtils.java", "diffHunk": "@@ -361,9 +361,9 @@ public static void waitForCondition(final TestCondition testCondition, final lon\n      * avoid transient failures due to slow or overloaded machines.\n      */\n     public static void waitForCondition(final TestCondition testCondition, final long maxWaitMs, Supplier<String> conditionDetailsSupplier) throws InterruptedException {\n-        String conditionDetailsSupplied = conditionDetailsSupplier != null ? conditionDetailsSupplier.get() : null;\n-        String conditionDetails = conditionDetailsSupplied != null ? conditionDetailsSupplied : \"\";\n         retryOnExceptionWithTimeout(maxWaitMs, () -> {\n+            String conditionDetailsSupplied = conditionDetailsSupplier != null ? conditionDetailsSupplier.get() : null;\n+            String conditionDetails = conditionDetailsSupplied != null ? conditionDetailsSupplied : \"\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ3MzM0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": true, "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozMTo0NlrOGLAPtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMjozNzozMVrOGM_67g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw==", "bodyText": "This is a change. I decided that from the perspective of the TaskAssignor API, the lags are one of the inputs, so it doesn't make sense to invoke the assignor if the lags aren't present.\nThis potentially harms assignors that don't care about the lag (like the StickyTaskAssignor), but it also seems like if we can't compute the lags, then we probably can't do lots of other stuff that Streams needs to do anyway, so maybe it's not the worst thing in the world to schedule a \"retry\" on the assignment, even if it's not strictly necessary.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414191543", "createdAt": "2020-04-23T23:31:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIyNzUyNQ==", "bodyText": "Fine with me (although it does slightly detract from the opt-out possibility). WDYT about adding a retry backoff though? I'm a bit concerned we might just end up stuck in a loop of useless rebalancing, and waiting the full probing.rebalance.interval doesn't feel right either", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414227525", "createdAt": "2020-04-24T01:21:18Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY1MTQ4Mw==", "bodyText": "This is a good thought. I think it mitigates the downside that we do still assign all the tasks when we fail to fetch lags, so it's not like we make no progress while waiting for the next rebalance. The \"endless cycle\" is a concern, but I'm not sure how it could happen in practice. I.e., what would make brokers consistently fail to report end offsets, but not fail on any other APIs that Streams needs, especially since Streams needs to query the end-offset API during restoration anyway.\nIt seems like the failure would either be transient or permanent(ish).\nIf transient, then Streams will make progress during the probing.rebalance.interval, and succeed in balancing the assignment later. Even if we get further transient exceptions during the sequence of HATA probing rebalances, the fact that we just return all tasks to their prior owners and that the HATA is stable mean that we just delay convergence by a single probing.rebalance.interval, not start all over again.\nIf permanent, then Streams will fail anyway after the assignment completes, since it also  tends to query the end offsets immediately after getting the assignment. Even if it gets all prior tasks returned, which would make it skip the restoration phase, it seems implausible that we'd see a permanent failure on only the end-offset API and Streams would happily be able to poll, commit, manage transactions, etc.\nOur big alternative is just to immediately raise the exception, and leave it to KIP-572 to deal with the situation holistically. But I'm concerned that the impact of bombing out of assignment is greater than that of handling other failures during processing. It seems like an exception in assignment dooms the current Join/SyncGroup phase for everyone, which means that they have to wait for a timeout and then redo the rebalance. So KIP-572 can still recover gracefully, by reconstructing the consumer, but it can't help the extra downtime of waiting for the failed rebalance to time out and trying again.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414651483", "createdAt": "2020-04-24T15:10:13Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxMTk2Mw==", "bodyText": "I see your point. What I do not like so much is that it is not very intuitive to require successful lag computation for sticky assignor. I understand that if lag computation is not successful other parts of Streams will fail, but it is not the responsibility of this class to avoid that. I think what I am trying to say is that the verifications should be done where they are required to make the code easily comprehensible. I am just imagining me coming back to this code and trying to understand why the lag computation must be successful for the sticky assignor.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415811963", "createdAt": "2020-04-27T13:27:51Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjE4NTkwNw==", "bodyText": "I agree with you on the principle. I think I'm just reaching a different conclusion.\nI think we can leave aside the practical thought about the likelihood of total failure when the end-offset API fails permanently. That's really more of a supporting point that maybe this isn't a terrible idea.\nThe thing that makes me think that this is really preferable is that the task lags are an input to the TaskAssignor interface. It seems unreasonable for the StreamsPartitionAssignor would inspect the configured task assignor class and then decide just to pass in the ClientStates with one of the properties missing because it thinks it knows that particular assignor won't use it. Speaking of responsibility, it seems like it's not the responsibility of this class to reason about the implementation of each TaskAssignor. The purpose of an interface is that we don't have to worry about it, we just have to satisfy the interface. I think it would be just as strange, if not stranger to imagine coming back to the code base and trying to figure out why it's ok to pass a broken ClientState object just into the StickyTaskAssignor.\nI actually can attest that that last point is strange, having just spent a bunch of time in the integration tests, trying to figure out which combination of ClientState arguments are actually preconditions for TaskAssignor#assign, and why it was ok to mock some, but not all, of them, some of the time.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416185907", "createdAt": "2020-04-27T22:14:00Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjE5OTQ5Mg==", "bodyText": "I buy the argument that it's weird (aka bad) to supply an input that may be invalid just because we know that particular implementation doesn't use the input. Of course, we don't actually input the lags directly; we just happen to supplement the ClientState with the computed lags in some cases, and the HATA just happens to use that. It seems only slightly less weird to not invoke something because we haven't added some supplemental information that isn't even required in the first place.\nWhat if we add a marker interface (warning: terrible placeholder name coming) called UsingClientLagsAssignor that only HATA implements to indicate that it needs this extra information on the ClientState to do its job. This will also be useful if we want to eventually make the task assignor fully user customizable; I expect someone who implemented a custom assignor that does not require task lags would be surprised (and annoyed) to find out that their custom assignor was skipped because we couldn't compute superfluous client data.\nOr, we could just move the lag computation into the HATA. But personally I'd rather leave it in the SPA for the above reason", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416199492", "createdAt": "2020-04-27T22:44:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIwMDk3Ng==", "bodyText": "Probably a better alternative to the marker interface is just an abstract assignor class that adds the lag info to the ClientState which the HATA and potentially future custom assignors could implement", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416200976", "createdAt": "2020-04-27T22:47:33Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIyMzM5NQ==", "bodyText": "Thanks @ableegoldman ,\nI thought about the marker interface also, but didn't mention it because it seems like a bad sign if we have two implementations and two interfaces from the very start.\nI think I'm getting a clue as to the contention when you mention \"supplemental information\". From my perspective, the TaskAssignor interface takes as input a \"ClientState\" for each instance in the cluster, which represents the current state of the cluster. One of the things it tells you is the lag for each task on the instance. How is this supplimental? It seems to be just one of the properties of the object. It actually happens to have a JavaDoc:\n    /**\n     * Returns the total lag across all logged stores in the task. Equal to the end offset sum if this client\n     * did not have any state for this task on disk.\n     *\n     * @return end offset sum - offset sum\n     *          Task.LATEST_OFFSET if this was previously an active running task on this client\n     */\n    long lagFor(final TaskId task)\nThis is a private interface, so we can change this definition if it's not accurate. However, I'd be concerned about trying to program against the TaskAssignor interface if the caveat is that some of the arguments might be wrong or missing.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416223395", "createdAt": "2020-04-27T23:42:17Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI0ODE2NQ==", "bodyText": "Well, the FallbackPriorTaskAssignor also gets the same input but can't look at the lags. I figured if we're going to make a distinction between assignors that can check the lags and those that can't, StickyTaskAssignor should fall into the latter category.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416248165", "createdAt": "2020-04-28T00:49:40Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI0OTIwNg==", "bodyText": "Hah, I got the last word!\nJust kidding, fwiw I'm not trying to block this PR on the matter so it's fine by me if you merge as-is. Only wanted to make sure we're being fair to our assignor friends :P", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416249206", "createdAt": "2020-04-28T00:52:41Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI4MzM3NA==", "bodyText": "Hah! You know I can't let that happen :)\nI think the fallback assignor is special in that, if we are unable to satisfy the TaskAssignor interface (because we couldn't compute lags), then at least we'd produce some kind of assignment. I.e., it's really just hitting the \"abort\" button on the whole assignment. In contrast to the internal, emergency-mode panic assignor, the StickyTaskAssignor is a regular, pluggable, assignor that people could use.\nWe'll have to revisit this topic anyway before making TaskAssignor a public API. Since it seems like both you and @cadonna view this change with suspicion, I'll add a special case for the StickyTaskAssignor, preserving the behavior before this PR.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416283374", "createdAt": "2020-04-28T02:37:31Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ3NTIzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozMjo0NFrOGLAQ3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozMjo0NFrOGLAQ3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTgzOA==", "bodyText": "Just to clarify everyone's roles, I added a new assignor whose only behavior is to return all previously owned tasks, and then assign any unowned tasks.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414191838", "createdAt": "2020-04-23T23:32:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {\n+            log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n+                         + \"trigger another rebalance to retry.\");\n+            setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n+            taskAssignor = new PriorTaskAssignor();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ3ODQ0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozMzo1MFrOGLASlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoxOTowNFrOGM9Fyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MjI3Ng==", "bodyText": "This constructor is currently only used in tests, but I'm planning a follow-on refactor that would actually use it from the StickyTaskAssignor as well.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414192276", "createdAt": "2020-04-23T23:33:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -86,6 +90,22 @@ private ClientState(final Set<TaskId> activeTasks,\n         this.capacity = capacity;\n     }\n \n+    public ClientState(final Set<TaskId> previousActiveTasks,\n+                       final Set<TaskId> previousStandbyTasks,\n+                       final Map<TaskId, Long> taskLagTotals,\n+                       final int capacity) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIzNzAwMg==", "bodyText": "I'm having a failure of imagination to see why any task assignor would ever be creating ClientState objects, but I eagerly wait to be enlightened.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416237002", "createdAt": "2020-04-28T00:19:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -86,6 +90,22 @@ private ClientState(final Set<TaskId> activeTasks,\n         this.capacity = capacity;\n     }\n \n+    public ClientState(final Set<TaskId> previousActiveTasks,\n+                       final Set<TaskId> previousStandbyTasks,\n+                       final Map<TaskId, Long> taskLagTotals,\n+                       final int capacity) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MjI3Ng=="}, "originalCommit": null, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ4MTU5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozNDo1OVrOGLAURA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozNDo1OVrOGLAURA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MjcwOA==", "bodyText": "All these fields have to be non-final now, because we're setting them in assign instead of the constructor.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414192708", "createdAt": "2020-04-23T23:34:59Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -16,49 +16,50 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n-import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.UUID;\n import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n-import java.util.Map;\n-import java.util.Set;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n+import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private final Map<UUID, ClientState> clientStates;\n-    private final Map<UUID, Integer> clientsToNumberOfThreads;\n-    private final SortedSet<UUID> sortedClients;\n+    private Map<UUID, ClientState> clientStates;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ4MjUwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozNToyNVrOGLAU1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMTozMzozN1rOGLCp4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5Mjg1NQ==", "bodyText": "I found this log useful while debugging the integration tests. WDYT about keeping it?", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414192855", "createdAt": "2020-04-23T23:35:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -95,6 +94,11 @@ public boolean assign() {\n \n         assignStatelessActiveTasks();\n \n+        log.info(\"Decided on assignment: \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMTAxMQ==", "bodyText": "I'm all for useful logging \ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414231011", "createdAt": "2020-04-24T01:33:37Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -95,6 +94,11 @@ public boolean assign() {\n \n         assignStatelessActiveTasks();\n \n+        log.info(\"Decided on assignment: \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5Mjg1NQ=="}, "originalCommit": null, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ4OTk4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzozODowN1rOGLAY7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMTozNDo1MVrOGLCrVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MzkwMQ==", "bodyText": "The StickyTaskAssignor is capable of satisfying the PriorTaskAssignor's contract, so we can just delegate to it. The important thing is that we now have two separately defined contracts:\n\nreturn all previous tasks and assign the rest (PriorTaskAssignor)\nstrike a balance between stickiness and balance (StickyTaskAssignor)\n\nThe fact that the implementation is shared is an ... implementation detail.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414193901", "createdAt": "2020-04-23T23:38:07Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+public class PriorTaskAssignor implements TaskAssignor {\n+    private final StickyTaskAssignor delegate;\n+\n+    public PriorTaskAssignor() {\n+        delegate = new StickyTaskAssignor(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMTM4Mw==", "bodyText": "Thanks for the improvement, this feels a lot nicer", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414231383", "createdAt": "2020-04-24T01:34:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+public class PriorTaskAssignor implements TaskAssignor {\n+    private final StickyTaskAssignor delegate;\n+\n+    public PriorTaskAssignor() {\n+        delegate = new StickyTaskAssignor(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MzkwMQ=="}, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDQ5NzEwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0MDoxNFrOGLAcpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNToxNDozNFrOGLcg6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5NDg1Mg==", "bodyText": "While debugging this test, I found the \"garbage collection\" nomenclature confusing (because it is possible to invoke the JVM GC, but that's not what we're doing here), so I transitioned to calling it a \"stall\", which was also a term used in the test comments.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414194852", "createdAt": "2020-04-23T23:40:14Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -111,8 +115,9 @@\n     private final String storeName = \"store\";\n \n     private AtomicBoolean errorInjected;\n-    private AtomicBoolean gcInjected;\n-    private volatile boolean doGC = true;\n+    private AtomicBoolean stallInjected;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY1NDY5Ng==", "bodyText": "This is another case where I've gotten tripped up by the same thing twice, and decided to fix it this time.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414654696", "createdAt": "2020-04-24T15:14:34Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -111,8 +115,9 @@\n     private final String storeName = \"store\";\n \n     private AtomicBoolean errorInjected;\n-    private AtomicBoolean gcInjected;\n-    private volatile boolean doGC = true;\n+    private AtomicBoolean stallInjected;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5NDg1Mg=="}, "originalCommit": null, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDUwMzUzOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0MjoyM1rOGLAf4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0MjoyM1rOGLAf4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5NTY4MA==", "bodyText": "I added an argument to the KafkaStreams builder to set the dummy host name. Previously, it was always \"dummy\" even though we had two instances, which resulted in the metadata map only containing one entry, even though there were two nodes in the cluster. I'm not sure if this was a cause of flakiness (since it seems it would be non-deterministic), but it's definitely not right.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414195680", "createdAt": "2020-04-23T23:42:23Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDUxNzM4OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0NzoyMlrOGLAnPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0NzoyMlrOGLAnPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5NzU2NQ==", "bodyText": "The previous test was seemingly dependent on the non-stalling instance being the one to \"win\" and be present in the metadata map, which is why the metadatas for both instances were 1 before.\nNow, we're being a little more explicit, by actually finding out which instance is the stalled one. Then we can assert that the instance that isn't stalled (the only one still in the group) doesn't see the stalled instance anymore (since it has dropped out), and that it is now assigned both partitions.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414197565", "createdAt": "2020-04-23T23:47:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)\n         ) {\n             startKafkaStreamsAndWaitForRunningState(streams1, MAX_WAIT_TIME_MS);\n             startKafkaStreamsAndWaitForRunningState(streams2, MAX_WAIT_TIME_MS);\n \n-            final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);\n-            final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> committedDataBeforeStall = prepareData(0L, 10L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> uncommittedDataBeforeStall = prepareData(10L, 15L, 0L, 1L);\n \n-            final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();\n-            dataBeforeGC.addAll(committedDataBeforeGC);\n-            dataBeforeGC.addAll(uncommittedDataBeforeGC);\n+            final List<KeyValue<Long, Long>> dataBeforeStall = new ArrayList<>();\n+            dataBeforeStall.addAll(committedDataBeforeStall);\n+            dataBeforeStall.addAll(uncommittedDataBeforeStall);\n \n             final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);\n \n             final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);\n \n-            writeInputData(committedDataBeforeGC);\n+            writeInputData(committedDataBeforeStall);\n \n             waitForCondition(\n                 () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n                 \"SteamsTasks did not request commit.\");\n \n-            writeInputData(uncommittedDataBeforeGC);\n+            writeInputData(uncommittedDataBeforeStall);\n \n-            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);\n-            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);\n+            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeStall.size(), null);\n+            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeStall.size(), CONSUMER_GROUP_ID);\n \n-            checkResultPerKey(committedRecords, committedDataBeforeGC);\n-            checkResultPerKey(uncommittedRecords, dataBeforeGC);\n+            checkResultPerKey(committedRecords, committedDataBeforeStall);\n+            checkResultPerKey(uncommittedRecords, dataBeforeStall);\n \n-            gcInjected.set(true);\n+            LOG.info(\"Injecting Stall\");\n+            stallInjected.set(true);\n             writeInputData(dataToTriggerFirstRebalance);\n+            LOG.info(\"Input Data Written\");\n+            waitForCondition(\n+                () -> stallingHost.get() != null,\n+                MAX_WAIT_TIME_MS,\n+                \"Expected a host to start stalling\"\n+            );\n+            final String observedStallingHost = stallingHost.get();\n+            final KafkaStreams stallingInstance;\n+            final KafkaStreams remainingInstance;\n+            if (\"streams1\".equals(observedStallingHost)) {\n+                stallingInstance = streams1;\n+                remainingInstance = streams2;\n+            } else if (\"streams2\".equals(observedStallingHost)) {\n+                stallingInstance = streams2;\n+                remainingInstance = streams1;\n+            } else {\n+                throw new IllegalArgumentException(\"unexpected host name: \" + observedStallingHost);\n+            }\n \n+            // the stalling instance won't have an updated view, and it doesn't matter what it thinks\n+            // the assignment is. We only really care that the remaining instance only sees one host\n+            // that owns both partitions.\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2\n-                        || streams2.allMetadata().iterator().next().topicPartitions().size() == 2),\n-                MAX_WAIT_TIME_MS, \"Should have rebalanced.\");\n+                () -> stallingInstance.allMetadata().size() == 2\n+                    && remainingInstance.allMetadata().size() == 1\n+                    && remainingInstance.allMetadata().iterator().next().topicPartitions().size() == 2,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDUxOTcwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0ODoxN1rOGLAoiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0ODoxN1rOGLAoiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5Nzg5Nw==", "bodyText": "Again, 2 was always the right answer, we were just accidentally overwriting one instance's metadata with the other.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414197897", "createdAt": "2020-04-23T23:48:17Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)\n         ) {\n             startKafkaStreamsAndWaitForRunningState(streams1, MAX_WAIT_TIME_MS);\n             startKafkaStreamsAndWaitForRunningState(streams2, MAX_WAIT_TIME_MS);\n \n-            final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);\n-            final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> committedDataBeforeStall = prepareData(0L, 10L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> uncommittedDataBeforeStall = prepareData(10L, 15L, 0L, 1L);\n \n-            final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();\n-            dataBeforeGC.addAll(committedDataBeforeGC);\n-            dataBeforeGC.addAll(uncommittedDataBeforeGC);\n+            final List<KeyValue<Long, Long>> dataBeforeStall = new ArrayList<>();\n+            dataBeforeStall.addAll(committedDataBeforeStall);\n+            dataBeforeStall.addAll(uncommittedDataBeforeStall);\n \n             final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);\n \n             final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);\n \n-            writeInputData(committedDataBeforeGC);\n+            writeInputData(committedDataBeforeStall);\n \n             waitForCondition(\n                 () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n                 \"SteamsTasks did not request commit.\");\n \n-            writeInputData(uncommittedDataBeforeGC);\n+            writeInputData(uncommittedDataBeforeStall);\n \n-            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);\n-            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);\n+            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeStall.size(), null);\n+            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeStall.size(), CONSUMER_GROUP_ID);\n \n-            checkResultPerKey(committedRecords, committedDataBeforeGC);\n-            checkResultPerKey(uncommittedRecords, dataBeforeGC);\n+            checkResultPerKey(committedRecords, committedDataBeforeStall);\n+            checkResultPerKey(uncommittedRecords, dataBeforeStall);\n \n-            gcInjected.set(true);\n+            LOG.info(\"Injecting Stall\");\n+            stallInjected.set(true);\n             writeInputData(dataToTriggerFirstRebalance);\n+            LOG.info(\"Input Data Written\");\n+            waitForCondition(\n+                () -> stallingHost.get() != null,\n+                MAX_WAIT_TIME_MS,\n+                \"Expected a host to start stalling\"\n+            );\n+            final String observedStallingHost = stallingHost.get();\n+            final KafkaStreams stallingInstance;\n+            final KafkaStreams remainingInstance;\n+            if (\"streams1\".equals(observedStallingHost)) {\n+                stallingInstance = streams1;\n+                remainingInstance = streams2;\n+            } else if (\"streams2\".equals(observedStallingHost)) {\n+                stallingInstance = streams2;\n+                remainingInstance = streams1;\n+            } else {\n+                throw new IllegalArgumentException(\"unexpected host name: \" + observedStallingHost);\n+            }\n \n+            // the stalling instance won't have an updated view, and it doesn't matter what it thinks\n+            // the assignment is. We only really care that the remaining instance only sees one host\n+            // that owns both partitions.\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2\n-                        || streams2.allMetadata().iterator().next().topicPartitions().size() == 2),\n-                MAX_WAIT_TIME_MS, \"Should have rebalanced.\");\n+                () -> stallingInstance.allMetadata().size() == 2\n+                    && remainingInstance.allMetadata().size() == 1\n+                    && remainingInstance.allMetadata().iterator().next().topicPartitions().size() == 2,\n+                MAX_WAIT_TIME_MS,\n+                () -> \"Should have rebalanced.\\n\" +\n+                    \"Streams1[\" + streams1.allMetadata() + \"]\\n\" +\n+                    \"Streams2[\" + streams2.allMetadata() + \"]\");\n \n             final List<KeyValue<Long, Long>> committedRecordsAfterRebalance = readResult(\n-                uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size(),\n+                uncommittedDataBeforeStall.size() + dataToTriggerFirstRebalance.size(),\n                 CONSUMER_GROUP_ID);\n \n             final List<KeyValue<Long, Long>> expectedCommittedRecordsAfterRebalance = new ArrayList<>();\n-            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeGC);\n+            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeStall);\n             expectedCommittedRecordsAfterRebalance.addAll(dataToTriggerFirstRebalance);\n \n             checkResultPerKey(committedRecordsAfterRebalance, expectedCommittedRecordsAfterRebalance);\n \n-            doGC = false;\n+            LOG.info(\"Releasing Stall\");\n+            doStall = false;\n+            // Once the stalling host rejoins the group, we expect both instances to see both instances.\n+            // It doesn't really matter what the assignment is, but we might as well also assert that they\n+            // both see both partitions assigned exactly once\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && streams1.allMetadata().iterator().next().topicPartitions().size() == 1\n-                    && streams2.allMetadata().iterator().next().topicPartitions().size() == 1,\n+                () -> streams1.allMetadata().size() == 2\n+                    && streams2.allMetadata().size() == 2", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDUyMjQ0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0OToxMFrOGLAp_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzo0OToxMFrOGLAp_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5ODI2OQ==", "bodyText": "The prior expectation was dependent on the rebalance algorithm's behavior. Now, we relax it and just ensure that all the partitions are assigned.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414198269", "createdAt": "2020-04-23T23:49:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)\n         ) {\n             startKafkaStreamsAndWaitForRunningState(streams1, MAX_WAIT_TIME_MS);\n             startKafkaStreamsAndWaitForRunningState(streams2, MAX_WAIT_TIME_MS);\n \n-            final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);\n-            final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> committedDataBeforeStall = prepareData(0L, 10L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> uncommittedDataBeforeStall = prepareData(10L, 15L, 0L, 1L);\n \n-            final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();\n-            dataBeforeGC.addAll(committedDataBeforeGC);\n-            dataBeforeGC.addAll(uncommittedDataBeforeGC);\n+            final List<KeyValue<Long, Long>> dataBeforeStall = new ArrayList<>();\n+            dataBeforeStall.addAll(committedDataBeforeStall);\n+            dataBeforeStall.addAll(uncommittedDataBeforeStall);\n \n             final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);\n \n             final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);\n \n-            writeInputData(committedDataBeforeGC);\n+            writeInputData(committedDataBeforeStall);\n \n             waitForCondition(\n                 () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n                 \"SteamsTasks did not request commit.\");\n \n-            writeInputData(uncommittedDataBeforeGC);\n+            writeInputData(uncommittedDataBeforeStall);\n \n-            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);\n-            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);\n+            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeStall.size(), null);\n+            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeStall.size(), CONSUMER_GROUP_ID);\n \n-            checkResultPerKey(committedRecords, committedDataBeforeGC);\n-            checkResultPerKey(uncommittedRecords, dataBeforeGC);\n+            checkResultPerKey(committedRecords, committedDataBeforeStall);\n+            checkResultPerKey(uncommittedRecords, dataBeforeStall);\n \n-            gcInjected.set(true);\n+            LOG.info(\"Injecting Stall\");\n+            stallInjected.set(true);\n             writeInputData(dataToTriggerFirstRebalance);\n+            LOG.info(\"Input Data Written\");\n+            waitForCondition(\n+                () -> stallingHost.get() != null,\n+                MAX_WAIT_TIME_MS,\n+                \"Expected a host to start stalling\"\n+            );\n+            final String observedStallingHost = stallingHost.get();\n+            final KafkaStreams stallingInstance;\n+            final KafkaStreams remainingInstance;\n+            if (\"streams1\".equals(observedStallingHost)) {\n+                stallingInstance = streams1;\n+                remainingInstance = streams2;\n+            } else if (\"streams2\".equals(observedStallingHost)) {\n+                stallingInstance = streams2;\n+                remainingInstance = streams1;\n+            } else {\n+                throw new IllegalArgumentException(\"unexpected host name: \" + observedStallingHost);\n+            }\n \n+            // the stalling instance won't have an updated view, and it doesn't matter what it thinks\n+            // the assignment is. We only really care that the remaining instance only sees one host\n+            // that owns both partitions.\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2\n-                        || streams2.allMetadata().iterator().next().topicPartitions().size() == 2),\n-                MAX_WAIT_TIME_MS, \"Should have rebalanced.\");\n+                () -> stallingInstance.allMetadata().size() == 2\n+                    && remainingInstance.allMetadata().size() == 1\n+                    && remainingInstance.allMetadata().iterator().next().topicPartitions().size() == 2,\n+                MAX_WAIT_TIME_MS,\n+                () -> \"Should have rebalanced.\\n\" +\n+                    \"Streams1[\" + streams1.allMetadata() + \"]\\n\" +\n+                    \"Streams2[\" + streams2.allMetadata() + \"]\");\n \n             final List<KeyValue<Long, Long>> committedRecordsAfterRebalance = readResult(\n-                uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size(),\n+                uncommittedDataBeforeStall.size() + dataToTriggerFirstRebalance.size(),\n                 CONSUMER_GROUP_ID);\n \n             final List<KeyValue<Long, Long>> expectedCommittedRecordsAfterRebalance = new ArrayList<>();\n-            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeGC);\n+            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeStall);\n             expectedCommittedRecordsAfterRebalance.addAll(dataToTriggerFirstRebalance);\n \n             checkResultPerKey(committedRecordsAfterRebalance, expectedCommittedRecordsAfterRebalance);\n \n-            doGC = false;\n+            LOG.info(\"Releasing Stall\");\n+            doStall = false;\n+            // Once the stalling host rejoins the group, we expect both instances to see both instances.\n+            // It doesn't really matter what the assignment is, but we might as well also assert that they\n+            // both see both partitions assigned exactly once\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && streams1.allMetadata().iterator().next().topicPartitions().size() == 1\n-                    && streams2.allMetadata().iterator().next().topicPartitions().size() == 1,\n+                () -> streams1.allMetadata().size() == 2\n+                    && streams2.allMetadata().size() == 2\n+                    && streams1.allMetadata().stream().mapToLong(meta -> meta.topicPartitions().size()).sum() == 2\n+                    && streams2.allMetadata().stream().mapToLong(meta -> meta.topicPartitions().size()).sum() == 2,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDU3Mzc1OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDowODo1M1rOGLBFHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMjo0NjowMVrOGLD7dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNTIxNQ==", "bodyText": "I'm not sure if we should really delete this test. The restore test is fine, but the rebalance test fails 50% of the time. Here's the situation:\n\nThe test relies on the second instance being the one to get the standby replica, which just happens to be the behavior of the StickyTaskAssignor.\nMaking this test agnostic to the assignor's choice is extremely difficult because different barriers and latches need to be passed into the \"standby\" instance than the \"active\" one.\nI looked into the allLocalStorePartitionLags() implementation, and it doesn't look like there's any way in which the rebalance lifecycle could affect users' ability to query the lags. I'm not sure if a prior version of the code made this more plausible.\n\nReally, the rationale to delete the test is (3). I figured that out because I was investigating the possibility of writing a unit test instead, to make sure you could query the lags in each phase of the rebalance, but I came up blank because there seems to be no relationship at all between lag computation and rebalancing.\nIf we don't want to delete the test, then we could consider setting the TaskAssignor to the (new) PriorTaskAssignor, which would actually guarantee in a reliable way that the second instance gets the standby task.\nBut before just \"fixing\" the test, I wanted to double-check that we really need this test at all.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414205215", "createdAt": "2020-04-24T00:08:53Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -1,349 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.integration;\n-\n-import static org.apache.kafka.common.utils.Utils.mkSet;\n-import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.core.IsEqual.equalTo;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.io.File;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.Comparator;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.CyclicBarrier;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicReference;\n-import kafka.utils.MockTime;\n-import org.apache.kafka.clients.consumer.ConsumerConfig;\n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.serialization.LongDeserializer;\n-import org.apache.kafka.common.serialization.LongSerializer;\n-import org.apache.kafka.common.serialization.Serdes;\n-import org.apache.kafka.common.serialization.StringDeserializer;\n-import org.apache.kafka.common.serialization.StringSerializer;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KafkaStreamsWrapper;\n-import org.apache.kafka.streams.KeyValue;\n-import org.apache.kafka.streams.LagInfo;\n-import org.apache.kafka.streams.StreamsBuilder;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n-import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n-import org.apache.kafka.streams.kstream.KTable;\n-import org.apache.kafka.streams.kstream.Materialized;\n-import org.apache.kafka.streams.processor.StateRestoreListener;\n-import org.apache.kafka.streams.processor.internals.StreamThread;\n-import org.apache.kafka.test.IntegrationTest;\n-import org.apache.kafka.test.TestUtils;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.ClassRule;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.experimental.categories.Category;\n-import org.junit.rules.TestName;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-@Category({IntegrationTest.class})\n-public class LagFetchIntegrationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI1MTg5NA==", "bodyText": "Ok, after some reflection, I feel better about my alternative proposal, so I've restored this test and just set the assignor to PriorTaskAssignor.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414251894", "createdAt": "2020-04-24T02:46:01Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -1,349 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.integration;\n-\n-import static org.apache.kafka.common.utils.Utils.mkSet;\n-import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.core.IsEqual.equalTo;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.io.File;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.Comparator;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.CyclicBarrier;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicReference;\n-import kafka.utils.MockTime;\n-import org.apache.kafka.clients.consumer.ConsumerConfig;\n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.serialization.LongDeserializer;\n-import org.apache.kafka.common.serialization.LongSerializer;\n-import org.apache.kafka.common.serialization.Serdes;\n-import org.apache.kafka.common.serialization.StringDeserializer;\n-import org.apache.kafka.common.serialization.StringSerializer;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KafkaStreamsWrapper;\n-import org.apache.kafka.streams.KeyValue;\n-import org.apache.kafka.streams.LagInfo;\n-import org.apache.kafka.streams.StreamsBuilder;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n-import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n-import org.apache.kafka.streams.kstream.KTable;\n-import org.apache.kafka.streams.kstream.Materialized;\n-import org.apache.kafka.streams.processor.StateRestoreListener;\n-import org.apache.kafka.streams.processor.internals.StreamThread;\n-import org.apache.kafka.test.IntegrationTest;\n-import org.apache.kafka.test.TestUtils;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.ClassRule;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.experimental.categories.Category;\n-import org.junit.rules.TestName;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-@Category({IntegrationTest.class})\n-public class LagFetchIntegrationTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNTIxNQ=="}, "originalCommit": null, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDU3Njc4OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/HighAvailabilityStreamsPartitionAssignorTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoxMDowNFrOGLBGsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QyMjowMTo0MFrOGM5myw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNTYxOQ==", "bodyText": "I moved this and other tests from StreamsPartitionAssignorTest that had been guarded to only actually run when parameterized with \"high availability\"", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414205619", "createdAt": "2020-04-24T00:10:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/HighAvailabilityStreamsPartitionAssignorTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.ListOffsetsResult;\n+import org.apache.kafka.clients.admin.ListOffsetsResult.ListOffsetsResultInfo;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.Assignment;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.GroupSubscription;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.Subscription;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.internals.KafkaFutureImpl;\n+import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.StreamsConfig.InternalConfig;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignmentInfo;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorError;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo;\n+import org.apache.kafka.test.MockClientSupplier;\n+import org.apache.kafka.test.MockInternalTopicManager;\n+import org.apache.kafka.test.MockKeyValueStoreBuilder;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.easymock.EasyMock;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptyMap;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singletonList;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.EMPTY_CHANGELOG_END_OFFSETS;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.EMPTY_TASKS;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n+import static org.easymock.EasyMock.anyObject;\n+import static org.easymock.EasyMock.expect;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertTrue;\n+\n+public class HighAvailabilityStreamsPartitionAssignorTest {\n+\n+    private final List<PartitionInfo> infos = asList(\n+        new PartitionInfo(\"topic1\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic1\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic1\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 3, Node.noNode(), new Node[0], new Node[0])\n+    );\n+\n+    private final Cluster metadata = new Cluster(\n+        \"cluster\",\n+        singletonList(Node.noNode()),\n+        infos,\n+        emptySet(),\n+        emptySet());\n+\n+    private final StreamsPartitionAssignor partitionAssignor = new StreamsPartitionAssignor();\n+    private final MockClientSupplier mockClientSupplier = new MockClientSupplier();\n+    private static final String USER_END_POINT = \"localhost:8080\";\n+    private static final String APPLICATION_ID = \"stream-partition-assignor-test\";\n+\n+    private TaskManager taskManager;\n+    private Admin adminClient;\n+    private StreamsConfig streamsConfig = new StreamsConfig(configProps());\n+    private final InternalTopologyBuilder builder = new InternalTopologyBuilder();\n+    private final StreamsMetadataState streamsMetadataState = EasyMock.createNiceMock(StreamsMetadataState.class);\n+    private final Map<String, Subscription> subscriptions = new HashMap<>();\n+\n+    private final AtomicInteger assignmentError = new AtomicInteger();\n+    private final AtomicLong nextProbingRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n+    private final MockTime time = new MockTime();\n+\n+    private Map<String, Object> configProps() {\n+        final Map<String, Object> configurationMap = new HashMap<>();\n+        configurationMap.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID);\n+        configurationMap.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, USER_END_POINT);\n+        configurationMap.put(InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);\n+        configurationMap.put(InternalConfig.STREAMS_METADATA_STATE_FOR_PARTITION_ASSIGNOR, streamsMetadataState);\n+        configurationMap.put(InternalConfig.STREAMS_ADMIN_CLIENT, adminClient);\n+        configurationMap.put(InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentError);\n+        configurationMap.put(InternalConfig.NEXT_PROBING_REBALANCE_MS, nextProbingRebalanceMs);\n+        configurationMap.put(InternalConfig.TIME, time);\n+        configurationMap.put(AssignorConfiguration.INTERNAL_TASK_ASSIGNOR_CLASS, HighAvailabilityTaskAssignor.class.getName());\n+        return configurationMap;\n+    }\n+\n+    // Make sure to complete setting up any mocks (such as TaskManager or AdminClient) before configuring the assignor\n+    private void configureDefaultPartitionAssignor() {\n+        configurePartitionAssignorWith(emptyMap());\n+    }\n+\n+    // Make sure to complete setting up any mocks (such as TaskManager or AdminClient) before configuring the assignor\n+    private void configurePartitionAssignorWith(final Map<String, Object> props) {\n+        final Map<String, Object> configMap = configProps();\n+        configMap.putAll(props);\n+\n+        streamsConfig = new StreamsConfig(configMap);\n+        partitionAssignor.configure(configMap);\n+        EasyMock.replay(taskManager, adminClient);\n+\n+        overwriteInternalTopicManagerWithMock();\n+    }\n+\n+    // Useful for tests that don't care about the task offset sums\n+    private void createMockTaskManager(final Set<TaskId> activeTasks) {\n+        createMockTaskManager(getTaskOffsetSums(activeTasks));\n+    }\n+\n+    private void createMockTaskManager(final Map<TaskId, Long> taskOffsetSums) {\n+        taskManager = EasyMock.createNiceMock(TaskManager.class);\n+        expect(taskManager.builder()).andReturn(builder).anyTimes();\n+        expect(taskManager.getTaskOffsetSums()).andReturn(taskOffsetSums).anyTimes();\n+        expect(taskManager.processId()).andReturn(UUID_1).anyTimes();\n+        builder.setApplicationId(APPLICATION_ID);\n+        builder.buildTopology();\n+    }\n+\n+    // If you don't care about setting the end offsets for each specific topic partition, the helper method\n+    // getTopicPartitionOffsetMap is useful for building this input map for all partitions\n+    private void createMockAdminClient(final Map<TopicPartition, Long> changelogEndOffsets) {\n+        adminClient = EasyMock.createMock(AdminClient.class);\n+\n+        final ListOffsetsResult result = EasyMock.createNiceMock(ListOffsetsResult.class);\n+        final KafkaFutureImpl<Map<TopicPartition, ListOffsetsResultInfo>> allFuture = new KafkaFutureImpl<>();\n+        allFuture.complete(changelogEndOffsets.entrySet().stream().collect(Collectors.toMap(\n+            Entry::getKey,\n+            t -> {\n+                final ListOffsetsResultInfo info = EasyMock.createNiceMock(ListOffsetsResultInfo.class);\n+                expect(info.offset()).andStubReturn(t.getValue());\n+                EasyMock.replay(info);\n+                return info;\n+            }))\n+        );\n+\n+        expect(adminClient.listOffsets(anyObject())).andStubReturn(result);\n+        expect(result.all()).andReturn(allFuture);\n+\n+        EasyMock.replay(result);\n+    }\n+\n+    private void overwriteInternalTopicManagerWithMock() {\n+        final MockInternalTopicManager mockInternalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);\n+        partitionAssignor.setInternalTopicManager(mockInternalTopicManager);\n+    }\n+\n+    @Before\n+    public void setUp() {\n+        createMockAdminClient(EMPTY_CHANGELOG_END_OFFSETS);\n+    }\n+\n+\n+    @Test\n+    public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceAndTriggerRebalanceIfEndOffsetFetchFailsAndHighAvailabilityEnabled() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjE3OTkxNQ==", "bodyText": "Can we remove the AndHighAvailabiltiyEnabled suffix from the test name? And/or just generally shorten it if you have a better idea", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416179915", "createdAt": "2020-04-27T22:01:40Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/HighAvailabilityStreamsPartitionAssignorTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.ListOffsetsResult;\n+import org.apache.kafka.clients.admin.ListOffsetsResult.ListOffsetsResultInfo;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.Assignment;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.GroupSubscription;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.Subscription;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.internals.KafkaFutureImpl;\n+import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.StreamsConfig.InternalConfig;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignmentInfo;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorError;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo;\n+import org.apache.kafka.test.MockClientSupplier;\n+import org.apache.kafka.test.MockInternalTopicManager;\n+import org.apache.kafka.test.MockKeyValueStoreBuilder;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.easymock.EasyMock;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptyMap;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singletonList;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.EMPTY_CHANGELOG_END_OFFSETS;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.EMPTY_TASKS;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n+import static org.easymock.EasyMock.anyObject;\n+import static org.easymock.EasyMock.expect;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertTrue;\n+\n+public class HighAvailabilityStreamsPartitionAssignorTest {\n+\n+    private final List<PartitionInfo> infos = asList(\n+        new PartitionInfo(\"topic1\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic1\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic1\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 3, Node.noNode(), new Node[0], new Node[0])\n+    );\n+\n+    private final Cluster metadata = new Cluster(\n+        \"cluster\",\n+        singletonList(Node.noNode()),\n+        infos,\n+        emptySet(),\n+        emptySet());\n+\n+    private final StreamsPartitionAssignor partitionAssignor = new StreamsPartitionAssignor();\n+    private final MockClientSupplier mockClientSupplier = new MockClientSupplier();\n+    private static final String USER_END_POINT = \"localhost:8080\";\n+    private static final String APPLICATION_ID = \"stream-partition-assignor-test\";\n+\n+    private TaskManager taskManager;\n+    private Admin adminClient;\n+    private StreamsConfig streamsConfig = new StreamsConfig(configProps());\n+    private final InternalTopologyBuilder builder = new InternalTopologyBuilder();\n+    private final StreamsMetadataState streamsMetadataState = EasyMock.createNiceMock(StreamsMetadataState.class);\n+    private final Map<String, Subscription> subscriptions = new HashMap<>();\n+\n+    private final AtomicInteger assignmentError = new AtomicInteger();\n+    private final AtomicLong nextProbingRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n+    private final MockTime time = new MockTime();\n+\n+    private Map<String, Object> configProps() {\n+        final Map<String, Object> configurationMap = new HashMap<>();\n+        configurationMap.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID);\n+        configurationMap.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, USER_END_POINT);\n+        configurationMap.put(InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);\n+        configurationMap.put(InternalConfig.STREAMS_METADATA_STATE_FOR_PARTITION_ASSIGNOR, streamsMetadataState);\n+        configurationMap.put(InternalConfig.STREAMS_ADMIN_CLIENT, adminClient);\n+        configurationMap.put(InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentError);\n+        configurationMap.put(InternalConfig.NEXT_PROBING_REBALANCE_MS, nextProbingRebalanceMs);\n+        configurationMap.put(InternalConfig.TIME, time);\n+        configurationMap.put(AssignorConfiguration.INTERNAL_TASK_ASSIGNOR_CLASS, HighAvailabilityTaskAssignor.class.getName());\n+        return configurationMap;\n+    }\n+\n+    // Make sure to complete setting up any mocks (such as TaskManager or AdminClient) before configuring the assignor\n+    private void configureDefaultPartitionAssignor() {\n+        configurePartitionAssignorWith(emptyMap());\n+    }\n+\n+    // Make sure to complete setting up any mocks (such as TaskManager or AdminClient) before configuring the assignor\n+    private void configurePartitionAssignorWith(final Map<String, Object> props) {\n+        final Map<String, Object> configMap = configProps();\n+        configMap.putAll(props);\n+\n+        streamsConfig = new StreamsConfig(configMap);\n+        partitionAssignor.configure(configMap);\n+        EasyMock.replay(taskManager, adminClient);\n+\n+        overwriteInternalTopicManagerWithMock();\n+    }\n+\n+    // Useful for tests that don't care about the task offset sums\n+    private void createMockTaskManager(final Set<TaskId> activeTasks) {\n+        createMockTaskManager(getTaskOffsetSums(activeTasks));\n+    }\n+\n+    private void createMockTaskManager(final Map<TaskId, Long> taskOffsetSums) {\n+        taskManager = EasyMock.createNiceMock(TaskManager.class);\n+        expect(taskManager.builder()).andReturn(builder).anyTimes();\n+        expect(taskManager.getTaskOffsetSums()).andReturn(taskOffsetSums).anyTimes();\n+        expect(taskManager.processId()).andReturn(UUID_1).anyTimes();\n+        builder.setApplicationId(APPLICATION_ID);\n+        builder.buildTopology();\n+    }\n+\n+    // If you don't care about setting the end offsets for each specific topic partition, the helper method\n+    // getTopicPartitionOffsetMap is useful for building this input map for all partitions\n+    private void createMockAdminClient(final Map<TopicPartition, Long> changelogEndOffsets) {\n+        adminClient = EasyMock.createMock(AdminClient.class);\n+\n+        final ListOffsetsResult result = EasyMock.createNiceMock(ListOffsetsResult.class);\n+        final KafkaFutureImpl<Map<TopicPartition, ListOffsetsResultInfo>> allFuture = new KafkaFutureImpl<>();\n+        allFuture.complete(changelogEndOffsets.entrySet().stream().collect(Collectors.toMap(\n+            Entry::getKey,\n+            t -> {\n+                final ListOffsetsResultInfo info = EasyMock.createNiceMock(ListOffsetsResultInfo.class);\n+                expect(info.offset()).andStubReturn(t.getValue());\n+                EasyMock.replay(info);\n+                return info;\n+            }))\n+        );\n+\n+        expect(adminClient.listOffsets(anyObject())).andStubReturn(result);\n+        expect(result.all()).andReturn(allFuture);\n+\n+        EasyMock.replay(result);\n+    }\n+\n+    private void overwriteInternalTopicManagerWithMock() {\n+        final MockInternalTopicManager mockInternalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);\n+        partitionAssignor.setInternalTopicManager(mockInternalTopicManager);\n+    }\n+\n+    @Before\n+    public void setUp() {\n+        createMockAdminClient(EMPTY_CHANGELOG_END_OFFSETS);\n+    }\n+\n+\n+    @Test\n+    public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceAndTriggerRebalanceIfEndOffsetFetchFailsAndHighAvailabilityEnabled() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNTYxOQ=="}, "originalCommit": null, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDU4MDEyOnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoxMToxM1rOGLBIZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoxMToxM1rOGLBIZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNjA1Mg==", "bodyText": "This is why I moved these methods to their own test, so that this class can focus on verifying behavior that is invariant with respect to the parameterized assignor.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414206052", "createdAt": "2020-04-24T00:11:13Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -1832,102 +1834,6 @@ public void shouldThrowIllegalStateExceptionIfAnyTopicsMissingFromChangelogEndOf\n         assertThrows(IllegalStateException.class, () -> partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)));\n     }\n \n-    @Test\n-    public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceAndTriggerRebalanceIfEndOffsetFetchFailsAndHighAvailabilityEnabled() {\n-        if (highAvailabilityEnabled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDU5OTgxOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoxODo0NFrOGLBSoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoxODo0NFrOGLBSoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwODY3Mw==", "bodyText": "I made a bunch of changes to this test, because it was pretty brittle with respect to changes in the HighAvailabilityTaskAssignor. For context, this is the second time I've touched the assignment code since we introduced the HATA, and it's the second time I've had to deal with irrelevant test failures in this class.\nFirst, I replaced the ClientState mocks with \"real\" ClientStates, constructed to represent the desired scenario for each test. Mocks are really more appropriate for isolating a component from external components (like mocking a remote service). Mocking data types leads to verifying that a specific set of queries happens against the data type, which is likely to break any time the logic under test changes in any way. Another problem with data-type mocks is that they can violate the invariants of the data type itself. For example, you can mock a list that both isEmpty and contains items. In our case, we threw NPEs in the assignor that could never happen in production when the mocked assigned/standby tasks didn't agree with the assigned tasks or the stateful assigned tasks weren't mocked to agree with the lags. Now, we just construct a ClientState for each client, representing the desired scenario and make assertions on the resulting assignment.\nSecond, the tests as written rely heavily on shared mutable fields inserted into shared mutable collections to build the assignor. This can be a good way to minimize the text inside the test method, which lets readers focus on the proper logic of the test itself. However, it makes it harder to understand the full context of a test, and it also raises the possibility of tests polluting each others' environments. Since in this particular case, localizing all the setup code is about as compact as factoring it out, I went ahead and minimized the shared fields, and eliminated the mutability, the tests are self-contained.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414208673", "createdAt": "2020-04-24T00:18:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDYxNDc2OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoyNDowNFrOGLBaXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoyNDowNFrOGLBaXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIxMDY1Mw==", "bodyText": "These first tests are really the only ones to change. They're still asserting the same basic fact, but previousAssignmentIsValid is now an internal method, so we instead make assertions about the black-box semantics of the assignor, instead of a specific \"visible for testing\" method.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414210653", "createdAt": "2020-04-24T00:24:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {\n-    private long acceptableRecoveryLag = 100L;\n-    private int balanceFactor = 1;\n-    private int maxWarmupReplicas = 2;\n-    private int numStandbyReplicas = 0;\n-    private long probingRebalanceInterval = 60 * 1000L;\n-\n-    private Map<UUID, ClientState> clientStates = new HashMap<>();\n-    private Set<TaskId> allTasks = new HashSet<>();\n-    private Set<TaskId> statefulTasks = new HashSet<>();\n-\n-    private ClientState client1;\n-    private ClientState client2;\n-    private ClientState client3;\n-    \n-    private HighAvailabilityTaskAssignor taskAssignor;\n-\n-    private void createTaskAssignor() {\n-        final AssignmentConfigs configs = new AssignmentConfigs(\n-            acceptableRecoveryLag,\n-            balanceFactor,\n-            maxWarmupReplicas,\n-            numStandbyReplicas,\n-            probingRebalanceInterval\n-        );\n-        taskAssignor = new HighAvailabilityTaskAssignor(\n-            clientStates,\n-            allTasks,\n-            statefulTasks,\n-            configs);\n-    }\n+    private final AssignmentConfigs configWithoutStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 0,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n+\n+    private final AssignmentConfigs configWithStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 1,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n \n-    @Test\n-    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n-        client1 = EasyMock.createNiceMock(ClientState.class);\n-        expect(client1.prevActiveTasks()).andReturn(singleton(TASK_0_0));\n-        expect(client1.prevStandbyTasks()).andStubReturn(EMPTY_TASKS);\n-        replay(client1);\n-        allTasks =  mkSet(TASK_0_0, TASK_0_1);\n-        clientStates = singletonMap(UUID_1, client1);\n-        createTaskAssignor();\n \n-        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    @Test\n+    public void shouldComputeNewAssignmentIfThereAreUnassignedActiveTasks() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDYxODA0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoyNTozMVrOGLBcJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoyNTozMVrOGLBcJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIxMTExMA==", "bodyText": "Since I added the new assignor, I added a regression test to verify its most important special function. Most of its validity is verified by the parameterized StreamsPartitionAssignorTest.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414211110", "createdAt": "2020-04-24T00:25:31Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.UUID;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+\n+public class PriorTaskAssignorTest {\n+\n+    private final Map<UUID, ClientState> clients = new TreeMap<>();\n+\n+    @Test\n+    public void shouldViolateBalanceToPreserveActiveTaskStickiness() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NDc1MzQxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMTozMToyNVrOGLCnVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNToyNDo0OVrOGLc95A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMDM1OQ==", "bodyText": "Should we put this with the other Streams internal configs? And/or follow the pattern of prefix+suffixing with __ ?", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414230359", "createdAt": "2020-04-24T01:31:25Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -41,8 +42,8 @@\n import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n \n public final class AssignorConfiguration {\n-    public static final String HIGH_AVAILABILITY_ENABLED_CONFIG = \"internal.high.availability.enabled\";\n-    private final boolean highAvailabilityEnabled;\n+    public static final String INTERNAL_TASK_ASSIGNOR_CLASS = \"internal.task.assignor.class\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY1NTgzNw==", "bodyText": "Oh, yeah, good idea.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414655837", "createdAt": "2020-04-24T15:16:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -41,8 +42,8 @@\n import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n \n public final class AssignorConfiguration {\n-    public static final String HIGH_AVAILABILITY_ENABLED_CONFIG = \"internal.high.availability.enabled\";\n-    private final boolean highAvailabilityEnabled;\n+    public static final String INTERNAL_TASK_ASSIGNOR_CLASS = \"internal.task.assignor.class\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMDM1OQ=="}, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2MjExNg==", "bodyText": "Ok, I moved it to org.apache.kafka.streams.StreamsConfig.InternalConfig#INTERNAL_TASK_ASSIGNOR_CLASS. I made an ad-hoc decision not to add the underscores, though, because this config is different than the other internal configs. I added comments to InternalConfig to explain the difference.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414662116", "createdAt": "2020-04-24T15:24:49Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -41,8 +42,8 @@\n import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n \n public final class AssignorConfiguration {\n-    public static final String HIGH_AVAILABILITY_ENABLED_CONFIG = \"internal.high.availability.enabled\";\n-    private final boolean highAvailabilityEnabled;\n+    public static final String INTERNAL_TASK_ASSIGNOR_CLASS = \"internal.task.assignor.class\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMDM1OQ=="}, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3OTI5NDQ5OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozMjoxNFrOGLrcAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoxMDo1OFrOGM85wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg5OTIwMw==", "bodyText": "This is not a TODO. I'm planning to leave the test like this. (Just opening the floor for objections)", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414899203", "createdAt": "2020-04-24T22:32:14Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -147,6 +149,9 @@ private void shouldFetchLagsDuringRebalancing(final String optimization) throws\n         // create stream threads\n         for (int i = 0; i < 2; i++) {\n             final Properties props = (Properties) streamsConfiguration.clone();\n+            // this test relies on the second instance getting the standby, so we specify\n+            // an assignor with this contract.\n+            props.put(StreamsConfig.InternalConfig.INTERNAL_TASK_ASSIGNOR_CLASS, PriorTaskAssignor.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjE3ODg2NQ==", "bodyText": "It would be nice to take advantage of the now-pluggable assignor and write a test utility assignor that allows you to specify the assignment you want and the validate the inputs that you get. Obviously beyond the scope of the current PR, just a thought I had. Using the PriorTaskAssignor seems like the next best thing so \ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416178865", "createdAt": "2020-04-27T21:59:45Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -147,6 +149,9 @@ private void shouldFetchLagsDuringRebalancing(final String optimization) throws\n         // create stream threads\n         for (int i = 0; i < 2; i++) {\n             final Properties props = (Properties) streamsConfiguration.clone();\n+            // this test relies on the second instance getting the standby, so we specify\n+            // an assignor with this contract.\n+            props.put(StreamsConfig.InternalConfig.INTERNAL_TASK_ASSIGNOR_CLASS, PriorTaskAssignor.class.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg5OTIwMw=="}, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIxNzM3MA==", "bodyText": "Yep, I had a similar thought, just ran out of motivation after debugging the integration tests.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416217370", "createdAt": "2020-04-27T23:27:03Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -147,6 +149,9 @@ private void shouldFetchLagsDuringRebalancing(final String optimization) throws\n         // create stream threads\n         for (int i = 0; i < 2; i++) {\n             final Properties props = (Properties) streamsConfiguration.clone();\n+            // this test relies on the second instance getting the standby, so we specify\n+            // an assignor with this contract.\n+            props.put(StreamsConfig.InternalConfig.INTERNAL_TASK_ASSIGNOR_CLASS, PriorTaskAssignor.class.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg5OTIwMw=="}, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIzMzkyMg==", "bodyText": "Very understandable", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416233922", "createdAt": "2020-04-28T00:10:58Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -147,6 +149,9 @@ private void shouldFetchLagsDuringRebalancing(final String optimization) throws\n         // create stream threads\n         for (int i = 0; i < 2; i++) {\n             final Properties props = (Properties) streamsConfiguration.clone();\n+            // this test relies on the second instance getting the standby, so we specify\n+            // an assignor with this contract.\n+            props.put(StreamsConfig.InternalConfig.INTERNAL_TASK_ASSIGNOR_CLASS, PriorTaskAssignor.class.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg5OTIwMw=="}, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3OTMwMTIwOnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/services/streams.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNTowOVrOGLrfvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNTowOVrOGLrfvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMDE1OQ==", "bodyText": "I've added this as a general mechanism in a couple of places to pass specific configs into Streams, so we don't have to make new constructors for every different parameterization.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414900159", "createdAt": "2020-04-24T22:35:09Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -477,6 +477,10 @@ def __init__(self, test_context, kafka):\n                                                                  \"\")\n         self.UPGRADE_FROM = None\n         self.UPGRADE_TO = None\n+        self.extra_properties = {}\n+\n+    def set_config(self, key, value):\n+        self.extra_properties[key] = value", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3OTMwMzE0OnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/services/streams.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNjowMVrOGLrg3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNjowMVrOGLrg3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMDQ0Nw==", "bodyText": "These will become follow-on tasks to fix each test. Thankfully, there aren't many.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414900447", "createdAt": "2020-04-24T22:36:01Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -562,6 +568,8 @@ def prop_file(self):\n                       consumer_property.SESSION_TIMEOUT_MS: 60000}\n \n         properties['input.topic'] = self.INPUT_TOPIC\n+        # TODO KIP-441: consider rewriting the test for HighAvailabilityTaskAssignor\n+        properties['internal.task.assignor.class'] = \"org.apache.kafka.streams.processor.internals.assignment.StickyTaskAssignor\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3OTMwNjIwOnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/tests/streams/streams_broker_bounce_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNzowNlrOGLrifQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNzowNlrOGLrifQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMDg2MQ==", "bodyText": "This accounted for most of the test failures, and it's already fixed on trunk.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414900861", "createdAt": "2020-04-24T22:37:06Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/tests/streams/streams_broker_bounce_test.py", "diffHunk": "@@ -164,7 +164,7 @@ def setup_system(self, start_processor=True, num_threads=3):\n \n         # Start test harness\n         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)\n-        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka, num_threads)\n+        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka, \"at_least_once\", num_threads)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3OTMwNzUzOnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNzo0NFrOGLrjOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNzo0NFrOGLrjOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMTA1MA==", "bodyText": "This one already had a different mechanism to add more configs, so I just left it alone.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414901050", "createdAt": "2020-04-24T22:37:44Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py", "diffHunk": "@@ -144,7 +144,11 @@ def test_streams_runs_with_broker_down_initially(self):\n     def test_streams_should_scale_in_while_brokers_down(self):\n         self.kafka.start()\n \n-        configs = self.get_configs(extra_configs=\",application.id=shutdown_with_broker_down\")\n+        # TODO KIP-441: consider rewriting the test for HighAvailabilityTaskAssignor\n+        configs = self.get_configs(\n+            extra_configs=\",application.id=shutdown_with_broker_down\" +\n+                          \",internal.task.assignor.class=org.apache.kafka.streams.processor.internals.assignment.StickyTaskAssignor\"\n+        )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NjU1NTUxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzowODozMlrOGMiPTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QyMTo0MToxNlrOGM478A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc5NzA3MQ==", "bodyText": "prop:\nCould we package this logic into a factory method to make the code more readable?\nfinal TaskAssignor taskAssignor = createTaskAssignor(boolean lagComputationSuccessful);", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415797071", "createdAt": "2020-04-27T13:08:32Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {\n+            log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n+                         + \"trigger another rebalance to retry.\");\n+            setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n+            taskAssignor = new PriorTaskAssignor();\n         } else {\n-            taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, false);\n+            taskAssignor = this.taskAssignor.get();\n         }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjE2ODk0NA==", "bodyText": "sure!", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416168944", "createdAt": "2020-04-27T21:41:16Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {\n+            log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n+                         + \"trigger another rebalance to retry.\");\n+            setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n+            taskAssignor = new PriorTaskAssignor();\n         } else {\n-            taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, false);\n+            taskAssignor = this.taskAssignor.get();\n         }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc5NzA3MQ=="}, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NjY5MjIyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzozNTowNFrOGMjehA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzozNTowNFrOGMjehA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxNzM0OA==", "bodyText": "req: Please add a unit test.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415817348", "createdAt": "2020-04-27T13:35:04Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -86,6 +90,22 @@ private ClientState(final Set<TaskId> activeTasks,\n         this.capacity = capacity;\n     }\n \n+    public ClientState(final Set<TaskId> previousActiveTasks,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NjcwMzEyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzozNzoyMlrOGMjk4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QyMToyNzozNFrOGM4dQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxODk3OQ==", "bodyText": "You are an exemplary boy scout!", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415818979", "createdAt": "2020-04-27T13:37:22Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -16,49 +16,50 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n-import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.UUID;\n import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n-import java.util.Map;\n-import java.util.Set;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n+import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjE2MTA5MQ==", "bodyText": "haha, how'd you know? ;)", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416161091", "createdAt": "2020-04-27T21:27:34Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -16,49 +16,50 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n-import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.UUID;\n import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n-import java.util.Map;\n-import java.util.Set;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n+import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxODk3OQ=="}, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NzA0NDg1OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDo0MDowMVrOGMmuHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDo0MDowMVrOGMmuHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg3MDQ5NA==", "bodyText": "prop:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        rebalancePending = new HighAvailabilityTaskAssignor().assign(harness.clientStates,\n          \n          \n            \n                                                                                     allTasks,\n          \n          \n            \n                                                                                     harness.statefulTaskEndOffsetSums.keySet(),\n          \n          \n            \n                                                                                     configs);\n          \n          \n            \n                        rebalancePending = new HighAvailabilityTaskAssignor().assign(\n          \n          \n            \n                            harness.clientStates,\n          \n          \n            \n                            allTasks,\n          \n          \n            \n                            harness.statefulTaskEndOffsetSums.keySet(),\n          \n          \n            \n                            configs\n          \n          \n            \n                        );", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415870494", "createdAt": "2020-04-27T14:40:01Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -416,11 +416,10 @@ private static void testForConvergence(final Harness harness,\n             iteration++;\n             harness.prepareForNextRebalance();\n             harness.recordBefore(iteration);\n-            rebalancePending = new HighAvailabilityTaskAssignor(\n-                harness.clientStates, allTasks,\n-                harness.statefulTaskEndOffsetSums.keySet(),\n-                configs\n-            ).assign();\n+            rebalancePending = new HighAvailabilityTaskAssignor().assign(harness.clientStates,\n+                                                                         allTasks,\n+                                                                         harness.statefulTaskEndOffsetSums.keySet(),\n+                                                                         configs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NzA0OTY0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDo0MDo1NFrOGMmxFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDo0MDo1NFrOGMmxFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg3MTI1Mw==", "bodyText": "prop:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final boolean followupRebalanceNeeded = assign(TASK_0_0,\n          \n          \n            \n                                                                   TASK_0_1,\n          \n          \n            \n                                                                   TASK_0_2,\n          \n          \n            \n                                                                   new TaskId(1, 0),\n          \n          \n            \n                                                                   new TaskId(1, 1),\n          \n          \n            \n                                                                   new TaskId(1, 2),\n          \n          \n            \n                                                                   new TaskId(2, 0),\n          \n          \n            \n                                                                   new TaskId(2, 1),\n          \n          \n            \n                                                                   new TaskId(2, 2),\n          \n          \n            \n                                                                   new TaskId(3, 0),\n          \n          \n            \n                                                                   new TaskId(3, 1),\n          \n          \n            \n                                                                   new TaskId(3, 2));\n          \n          \n            \n                    final boolean followupRebalanceNeeded = assign(\n          \n          \n            \n                        TASK_0_0,\n          \n          \n            \n                        TASK_0_1,\n          \n          \n            \n                        TASK_0_2,\n          \n          \n            \n                        new TaskId(1, 0),\n          \n          \n            \n                        new TaskId(1, 1),\n          \n          \n            \n                        new TaskId(1, 2),\n          \n          \n            \n                        new TaskId(2, 0),\n          \n          \n            \n                        new TaskId(2, 1),\n          \n          \n            \n                        new TaskId(2, 2),\n          \n          \n            \n                        new TaskId(3, 0),\n          \n          \n            \n                        new TaskId(3, 1),\n          \n          \n            \n                        new TaskId(3, 2)\n          \n          \n            \n                    );", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415871253", "createdAt": "2020-04-27T14:40:54Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java", "diffHunk": "@@ -350,20 +350,20 @@ public void shouldAssignMoreTasksToClientWithMoreCapacity() {\n         createClient(UUID_2, 2);\n         createClient(UUID_1, 1);\n \n-        final StickyTaskAssignor taskAssignor = createTaskAssignor(TASK_0_0,\n-                                                                            TASK_0_1,\n-                                                                            TASK_0_2,\n-                                                                            new TaskId(1, 0),\n-                                                                            new TaskId(1, 1),\n-                                                                            new TaskId(1, 2),\n-                                                                            new TaskId(2, 0),\n-                                                                            new TaskId(2, 1),\n-                                                                            new TaskId(2, 2),\n-                                                                            new TaskId(3, 0),\n-                                                                            new TaskId(3, 1),\n-                                                                            new TaskId(3, 2));\n-\n-        taskAssignor.assign();\n+        final boolean followupRebalanceNeeded = assign(TASK_0_0,\n+                                                       TASK_0_1,\n+                                                       TASK_0_2,\n+                                                       new TaskId(1, 0),\n+                                                       new TaskId(1, 1),\n+                                                       new TaskId(1, 2),\n+                                                       new TaskId(2, 0),\n+                                                       new TaskId(2, 1),\n+                                                       new TaskId(2, 2),\n+                                                       new TaskId(3, 0),\n+                                                       new TaskId(3, 1),\n+                                                       new TaskId(3, 2));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 371}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4OTUwMTc1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoxMjowNlrOGM87uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoxMjowNlrOGM87uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIzNDQyNQ==", "bodyText": "I renamed the PriorTaskAssignor and added a Javadoc to make its role clear.\nNote that \"PriorTaskAssignor\" would be an appropriate behavioral name, except that it also always returns \"true\", and that it must ignore the lags, which is what makes it a \"fallback\" assignor here.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416234425", "createdAt": "2020-04-28T00:12:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+/**\n+ * A special task assignor implementation to be used as a fallback in case the\n+ * configured assignor couldn't be invoked.\n+ *\n+ * Specifically, this assignor must:\n+ * 1. ignore the task lags in the ClientState map\n+ * 2. always return true, indicating that a follow-up rebalance is needed\n+ */\n+public class FallbackPriorTaskAssignor implements TaskAssignor {\n+    private final StickyTaskAssignor delegate;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4OTUyNTkyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoyMTozMlrOGM9JBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMzoyMTowM1rOGNAxIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIzNzgyOQ==", "bodyText": "We should probably rename this to probingRebalanceRequired or so on, see comment on FallbackPriorTaskAssignor", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416237829", "createdAt": "2020-04-28T00:21:32Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -712,31 +712,32 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n-        } else {\n-            taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, false);\n-        }\n-        final boolean followupRebalanceNeeded = taskAssignor.assign();\n+        final TaskAssignor taskAssignor = createTaskAssignor(lagComputationSuccessful);\n+\n+        final boolean followupRebalanceNeeded = taskAssignor.assign(clientStates,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI5NzI0OA==", "bodyText": "Yeah, seems legit.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416297248", "createdAt": "2020-04-28T03:21:03Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -712,31 +712,32 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n-        } else {\n-            taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, false);\n-        }\n-        final boolean followupRebalanceNeeded = taskAssignor.assign();\n+        final TaskAssignor taskAssignor = createTaskAssignor(lagComputationSuccessful);\n+\n+        final boolean followupRebalanceNeeded = taskAssignor.assign(clientStates,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIzNzgyOQ=="}, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4OTU4MjUzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDo0MToyNVrOGM9mfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMjoyNjo1OFrOGM_uaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI0NTM3NA==", "bodyText": "Returning true here will schedule a followup rebalance at the probing interval, but we also schedule a followup rebalance immediately before instantiating this assignor (line 735). Is this intentional? IIUC your proposal was to trigger a followup rebalance right away, which we do by means of the assignment error code.\nOf course, this is in memory so if the instance crashes and restarts we lose this information. I think we should actually avoid using the REBALANCE_NEEDED error code inside the assign method, and only allow. it during onAssignment. If we know that a followup rebalance is needed during assign we should just encode the nextScheduledRebalance with the current time", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416245374", "createdAt": "2020-04-28T00:41:25Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+/**\n+ * A special task assignor implementation to be used as a fallback in case the\n+ * configured assignor couldn't be invoked.\n+ *\n+ * Specifically, this assignor must:\n+ * 1. ignore the task lags in the ClientState map\n+ * 2. always return true, indicating that a follow-up rebalance is needed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI4MDE3MQ==", "bodyText": "Ah, this is a good point. Actually, I overlooked line 735. I'll remove that one.\nMy proposal actually was to just wait for the probing rebalance interval in case the lag computation failed. It seems like this should be ok, since Streams will still make progress in the mean time, and it avoids the pathological case where we could just constantly rebalance if the end-offsets API is down for some reason.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416280171", "createdAt": "2020-04-28T02:26:58Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+/**\n+ * A special task assignor implementation to be used as a fallback in case the\n+ * configured assignor couldn't be invoked.\n+ *\n+ * Specifically, this assignor must:\n+ * 1. ignore the task lags in the ClientState map\n+ * 2. always return true, indicating that a follow-up rebalance is needed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI0NTM3NA=="}, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5Mjk2MTAzOnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxNjozOTo1OFrOGNdHdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMDo1NTozM1rOGNmjRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjc2MTcxOQ==", "bodyText": "req: I think, you can now restrict access to previousAssignmentIsValid() to private.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416761719", "createdAt": "2020-04-28T16:39:58Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {\n-    private long acceptableRecoveryLag = 100L;\n-    private int balanceFactor = 1;\n-    private int maxWarmupReplicas = 2;\n-    private int numStandbyReplicas = 0;\n-    private long probingRebalanceInterval = 60 * 1000L;\n-\n-    private Map<UUID, ClientState> clientStates = new HashMap<>();\n-    private Set<TaskId> allTasks = new HashSet<>();\n-    private Set<TaskId> statefulTasks = new HashSet<>();\n-\n-    private ClientState client1;\n-    private ClientState client2;\n-    private ClientState client3;\n-    \n-    private HighAvailabilityTaskAssignor taskAssignor;\n-\n-    private void createTaskAssignor() {\n-        final AssignmentConfigs configs = new AssignmentConfigs(\n-            acceptableRecoveryLag,\n-            balanceFactor,\n-            maxWarmupReplicas,\n-            numStandbyReplicas,\n-            probingRebalanceInterval\n-        );\n-        taskAssignor = new HighAvailabilityTaskAssignor(\n-            clientStates,\n-            allTasks,\n-            statefulTasks,\n-            configs);\n-    }\n+    private final AssignmentConfigs configWithoutStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 0,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n+\n+    private final AssignmentConfigs configWithStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 1,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n \n-    @Test\n-    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n-        client1 = EasyMock.createNiceMock(ClientState.class);\n-        expect(client1.prevActiveTasks()).andReturn(singleton(TASK_0_0));\n-        expect(client1.prevStandbyTasks()).andStubReturn(EMPTY_TASKS);\n-        replay(client1);\n-        allTasks =  mkSet(TASK_0_0, TASK_0_1);\n-        clientStates = singletonMap(UUID_1, client1);\n-        createTaskAssignor();\n \n-        assertFalse(taskAssignor.previousAssignmentIsValid());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjgxMDcxMw==", "bodyText": "Or just remove it completely \ud83d\ude09", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416810713", "createdAt": "2020-04-28T17:54:19Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {\n-    private long acceptableRecoveryLag = 100L;\n-    private int balanceFactor = 1;\n-    private int maxWarmupReplicas = 2;\n-    private int numStandbyReplicas = 0;\n-    private long probingRebalanceInterval = 60 * 1000L;\n-\n-    private Map<UUID, ClientState> clientStates = new HashMap<>();\n-    private Set<TaskId> allTasks = new HashSet<>();\n-    private Set<TaskId> statefulTasks = new HashSet<>();\n-\n-    private ClientState client1;\n-    private ClientState client2;\n-    private ClientState client3;\n-    \n-    private HighAvailabilityTaskAssignor taskAssignor;\n-\n-    private void createTaskAssignor() {\n-        final AssignmentConfigs configs = new AssignmentConfigs(\n-            acceptableRecoveryLag,\n-            balanceFactor,\n-            maxWarmupReplicas,\n-            numStandbyReplicas,\n-            probingRebalanceInterval\n-        );\n-        taskAssignor = new HighAvailabilityTaskAssignor(\n-            clientStates,\n-            allTasks,\n-            statefulTasks,\n-            configs);\n-    }\n+    private final AssignmentConfigs configWithoutStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 0,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n+\n+    private final AssignmentConfigs configWithStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 1,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n \n-    @Test\n-    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n-        client1 = EasyMock.createNiceMock(ClientState.class);\n-        expect(client1.prevActiveTasks()).andReturn(singleton(TASK_0_0));\n-        expect(client1.prevStandbyTasks()).andStubReturn(EMPTY_TASKS);\n-        replay(client1);\n-        allTasks =  mkSet(TASK_0_0, TASK_0_1);\n-        clientStates = singletonMap(UUID_1, client1);\n-        createTaskAssignor();\n \n-        assertFalse(taskAssignor.previousAssignmentIsValid());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjc2MTcxOQ=="}, "originalCommit": {"oid": "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjkxNjI5NQ==", "bodyText": "Since you have a follow-on PR that touches this method, I'll leave it alone and just proceed to merge. We should consider both of these options in the follow-on.\nThanks!", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416916295", "createdAt": "2020-04-28T20:55:33Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {\n-    private long acceptableRecoveryLag = 100L;\n-    private int balanceFactor = 1;\n-    private int maxWarmupReplicas = 2;\n-    private int numStandbyReplicas = 0;\n-    private long probingRebalanceInterval = 60 * 1000L;\n-\n-    private Map<UUID, ClientState> clientStates = new HashMap<>();\n-    private Set<TaskId> allTasks = new HashSet<>();\n-    private Set<TaskId> statefulTasks = new HashSet<>();\n-\n-    private ClientState client1;\n-    private ClientState client2;\n-    private ClientState client3;\n-    \n-    private HighAvailabilityTaskAssignor taskAssignor;\n-\n-    private void createTaskAssignor() {\n-        final AssignmentConfigs configs = new AssignmentConfigs(\n-            acceptableRecoveryLag,\n-            balanceFactor,\n-            maxWarmupReplicas,\n-            numStandbyReplicas,\n-            probingRebalanceInterval\n-        );\n-        taskAssignor = new HighAvailabilityTaskAssignor(\n-            clientStates,\n-            allTasks,\n-            statefulTasks,\n-            configs);\n-    }\n+    private final AssignmentConfigs configWithoutStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 0,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n+\n+    private final AssignmentConfigs configWithStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 1,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n \n-    @Test\n-    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n-        client1 = EasyMock.createNiceMock(ClientState.class);\n-        expect(client1.prevActiveTasks()).andReturn(singleton(TASK_0_0));\n-        expect(client1.prevStandbyTasks()).andStubReturn(EMPTY_TASKS);\n-        replay(client1);\n-        allTasks =  mkSet(TASK_0_0, TASK_0_1);\n-        clientStates = singletonMap(UUID_1, client1);\n-        createTaskAssignor();\n \n-        assertFalse(taskAssignor.previousAssignmentIsValid());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjc2MTcxOQ=="}, "originalCommit": {"oid": "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60"}, "originalPosition": 97}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2776, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}