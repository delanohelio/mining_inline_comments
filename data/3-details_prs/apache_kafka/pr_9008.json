{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3NTY2MTY0", "number": 9008, "title": "KAFKA-9629 Use generated protocol for Fetch API", "bodyText": "This change makes use of the generated protocols for FetchRequest and FetchResponse. The main challenge here was how to allow the transferrable bytes of the record set to be directly sent to the outgoing response without copying into a buffer.\nThe proposed solution is similar to the existing multi-send object used in FetchResponse. However, a new writer class RecordsWriter was introduced to allow interleaving of ByteBufferSend (for headers and other non-record fields) along with RecordsSend-s which implement the efficient byte transfer.\nAnother change introduced here is that FetchRequest and FetchResponse do not maintain their own copies of the fields from the message. Instead, they hold a reference to the generated message class (FetchRequestData and FetchResponseData). Read-only copies of different forms of the message data are created once open construction to allow for efficient access using the existing class methods.\nFor example, in FetchRequest we hold the FetchRequestData, but also compute and hold:\n    private final FetchRequestData fetchRequestData;\n\n    // These are immutable read-only structures derived from FetchRequestData\n    private final Map<TopicPartition, PartitionData> fetchData;\n    private final List<TopicPartition> toForget;\n    private final FetchMetadata metadata;\nAnd in FetchResponse, we similarly hold:\n    private final FetchResponseData fetchResponseData;\n    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\nIf we want, we could deprecate all the accessors on FetchRequest/FetchResponse and force callers to use the #data() method. This would eliminate the need for these additional data structures.\nFinally, most of the other changes are fixing up tests that were actually using invalid default values for protocol messages (which are now enforced, thanks to the generated classes) as well as rectifying the JSON schema to match what the actual defined Schemas were (e.g., FETCH_RESPONSE_V11)", "createdAt": "2020-07-10T17:50:50Z", "url": "https://github.com/apache/kafka/pull/9008", "merged": true, "mergeCommit": {"oid": "4cd2396db31418c90005c998d9107ad40df055b2"}, "closed": true, "closedAt": "2020-07-30T17:29:40Z", "author": {"login": "mumrah"}, "timelineItems": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABczlst4gH2gAyNDQ3NTY2MTY0OjU2ZTQxNTY5MzBiYjhhNDQ0NWNkMjAyM2UxN2ZhODRkODYyYmI5MTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc5wRBWAFqTQ1NzgyNzE5MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "56e4156930bb8a4445cd2023e17fa84d862bb919", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/56e4156930bb8a4445cd2023e17fa84d862bb919", "committedDate": "2020-07-10T15:51:33Z", "message": "KAFKA-10265 Use the generated messages for FetchRequest and FetchResponse"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/04538af26ab9f036d547f4b6f34bdffb720e8007", "committedDate": "2020-07-10T17:38:07Z", "message": "Fix compile errors and checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzExMDc0", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-446711074", "createdAt": "2020-07-10T21:29:23Z", "commit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMToyOToyNFrOGwGJaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxODowMzozNlrOGwzwAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4NTU0NQ==", "bodyText": "Can we revert some of these renamings? We intentionally changed them in #8802.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453085545", "createdAt": "2020-07-10T21:29:24Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/FetchRequest.json", "diffHunk": "@@ -49,41 +49,41 @@\n   \"fields\": [\n     { \"name\": \"ReplicaId\", \"type\": \"int32\", \"versions\": \"0+\",\n       \"about\": \"The broker ID of the follower, of -1 if this request is from a consumer.\" },\n-    { \"name\": \"MaxWaitMs\", \"type\": \"int32\", \"versions\": \"0+\",\n+    { \"name\": \"MaxWaitTime\", \"type\": \"int32\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4NjcyOA==", "bodyText": "nit: I guess we didn't need this?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453086728", "createdAt": "2020-07-10T21:32:45Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -366,225 +368,164 @@ public FetchResponse(Errors error,\n                          LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n                          int throttleTimeMs,\n                          int sessionId) {\n-        this.error = error;\n-        this.responseData = responseData;\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.sessionId = sessionId;\n+        this.fetchResponseData = toMessage(throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        this.responseDataMap = responseData;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(Struct struct) {\n-        LinkedHashMap<TopicPartition, PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n-        for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                Struct partitionResponseHeader = partitionResponse.getStruct(PARTITION_HEADER_KEY_NAME);\n-                int partition = partitionResponseHeader.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionResponseHeader.get(ERROR_CODE));\n-                long highWatermark = partitionResponseHeader.get(HIGH_WATERMARK);\n-                long lastStableOffset = partitionResponseHeader.getOrElse(LAST_STABLE_OFFSET, INVALID_LAST_STABLE_OFFSET);\n-                long logStartOffset = partitionResponseHeader.getOrElse(LOG_START_OFFSET, INVALID_LOG_START_OFFSET);\n-                Optional<Integer> preferredReadReplica = Optional.of(\n-                    partitionResponseHeader.getOrElse(PREFERRED_READ_REPLICA, INVALID_PREFERRED_REPLICA_ID)\n-                ).filter(Predicate.isEqual(INVALID_PREFERRED_REPLICA_ID).negate());\n-\n-                BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);\n-                if (!(baseRecords instanceof MemoryRecords))\n-                    throw new IllegalStateException(\"Unknown records type found: \" + baseRecords.getClass());\n-                MemoryRecords records = (MemoryRecords) baseRecords;\n-\n-                List<AbortedTransaction> abortedTransactions = null;\n-                if (partitionResponseHeader.hasField(ABORTED_TRANSACTIONS_KEY_NAME)) {\n-                    Object[] abortedTransactionsArray = partitionResponseHeader.getArray(ABORTED_TRANSACTIONS_KEY_NAME);\n-                    if (abortedTransactionsArray != null) {\n-                        abortedTransactions = new ArrayList<>(abortedTransactionsArray.length);\n-                        for (Object abortedTransactionObj : abortedTransactionsArray) {\n-                            Struct abortedTransactionStruct = (Struct) abortedTransactionObj;\n-                            long producerId = abortedTransactionStruct.get(PRODUCER_ID);\n-                            long firstOffset = abortedTransactionStruct.get(FIRST_OFFSET);\n-                            abortedTransactions.add(new AbortedTransaction(producerId, firstOffset));\n-                        }\n-                    }\n-                }\n-\n-                PartitionData<MemoryRecords> partitionData = new PartitionData<>(error, highWatermark, lastStableOffset,\n-                        logStartOffset, preferredReadReplica, abortedTransactions, records);\n-                responseData.put(new TopicPartition(topic, partition), partitionData);\n-            }\n-        }\n-        return new FetchResponse<>(Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0)), responseData,\n-                struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME), struct.getOrElse(SESSION_ID, INVALID_SESSION_ID));\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        this.fetchResponseData = fetchResponseData;\n+        this.responseDataMap = toResponseDataMap(fetchResponseData);\n     }\n \n     @Override\n     public Struct toStruct(short version) {\n-        return toStruct(version, throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        return fetchResponseData.toStruct(version);\n     }\n \n     @Override\n     protected Send toSend(String dest, ResponseHeader responseHeader, short apiVersion) {\n-        Struct responseHeaderStruct = responseHeader.toStruct();\n-        Struct responseBodyStruct = toStruct(apiVersion);\n+        // Generate the Sends for the response fields and records\n+        ArrayDeque<Send> sends = new ArrayDeque<>();\n+        RecordsWriter writer = new RecordsWriter(dest, sends::add);\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        fetchResponseData.size(cache, apiVersion);\n+        fetchResponseData.write(writer, cache, apiVersion);\n+        writer.flush();\n+\n+        // Compute the total size of all the Sends and write it out along with the header in the first Send\n+        ResponseHeaderData responseHeaderData = responseHeader.data();\n+\n+        //Struct responseHeaderStruct = responseHeader.toStruct();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4ODA2Nw==", "bodyText": "nit: in all of the other classes, we just use the name data. Can we do the same here?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453088067", "createdAt": "2020-07-10T21:36:38Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -218,23 +220,19 @@\n             SESSION_ID,\n             new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V6)));\n \n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[] {FETCH_RESPONSE_V0, FETCH_RESPONSE_V1, FETCH_RESPONSE_V2,\n-            FETCH_RESPONSE_V3, FETCH_RESPONSE_V4, FETCH_RESPONSE_V5, FETCH_RESPONSE_V6,\n-            FETCH_RESPONSE_V7, FETCH_RESPONSE_V8, FETCH_RESPONSE_V9, FETCH_RESPONSE_V10,\n-            FETCH_RESPONSE_V11};\n-    }\n-\n     public static final long INVALID_HIGHWATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n-    private final int throttleTimeMs;\n-    private final Errors error;\n-    private final int sessionId;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseData;\n+    private final FetchResponseData fetchResponseData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA5NDMyMw==", "bodyText": "We can get rid of all the stuff above too, right?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453094323", "createdAt": "2020-07-10T21:55:21Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -218,23 +220,19 @@\n             SESSION_ID,\n             new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V6)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA5NTczMQ==", "bodyText": "Probably better to save for a follow-up, but potentially we can get rid of this conversion by using FetchablePartitionResponse directly in the broker.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453095731", "createdAt": "2020-07-10T21:59:45Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -366,225 +368,164 @@ public FetchResponse(Errors error,\n                          LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n                          int throttleTimeMs,\n                          int sessionId) {\n-        this.error = error;\n-        this.responseData = responseData;\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.sessionId = sessionId;\n+        this.fetchResponseData = toMessage(throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA5NzUzMw==", "bodyText": "Similarly, we can get rid of all this.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453097533", "createdAt": "2020-07-10T22:03:02Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java", "diffHunk": "@@ -209,30 +210,21 @@\n             FORGOTTEN_TOPIC_DATA_V7,\n             RACK_ID);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyMjc3Ng==", "bodyText": "I don't think FileRecords and MemoryRecords instances can be compared directly, if that's what the question is about.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453822776", "createdAt": "2020-07-13T17:46:49Z", "author": {"login": "hachikuji"}, "path": "generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java", "diffHunk": "@@ -2078,6 +2103,11 @@ private void generateFieldEquals(FieldSpec field) {\n                 buffer.printf(\"if (!Arrays.equals(this.%s, other.%s)) return false;%n\",\n                     field.camelCaseName(), field.camelCaseName());\n             }\n+        } else if (field.type().isRecords()) {\n+            // TODO is this valid for record instances?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgzMDMxNg==", "bodyText": "Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push toSend into the generated class as well. That will be necessary if we ever want to get of the current AbstractRequest and AbstractResponse types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453830316", "createdAt": "2020-07-13T17:59:30Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -366,225 +368,164 @@ public FetchResponse(Errors error,\n                          LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n                          int throttleTimeMs,\n                          int sessionId) {\n-        this.error = error;\n-        this.responseData = responseData;\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.sessionId = sessionId;\n+        this.fetchResponseData = toMessage(throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        this.responseDataMap = responseData;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(Struct struct) {\n-        LinkedHashMap<TopicPartition, PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n-        for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                Struct partitionResponseHeader = partitionResponse.getStruct(PARTITION_HEADER_KEY_NAME);\n-                int partition = partitionResponseHeader.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionResponseHeader.get(ERROR_CODE));\n-                long highWatermark = partitionResponseHeader.get(HIGH_WATERMARK);\n-                long lastStableOffset = partitionResponseHeader.getOrElse(LAST_STABLE_OFFSET, INVALID_LAST_STABLE_OFFSET);\n-                long logStartOffset = partitionResponseHeader.getOrElse(LOG_START_OFFSET, INVALID_LOG_START_OFFSET);\n-                Optional<Integer> preferredReadReplica = Optional.of(\n-                    partitionResponseHeader.getOrElse(PREFERRED_READ_REPLICA, INVALID_PREFERRED_REPLICA_ID)\n-                ).filter(Predicate.isEqual(INVALID_PREFERRED_REPLICA_ID).negate());\n-\n-                BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);\n-                if (!(baseRecords instanceof MemoryRecords))\n-                    throw new IllegalStateException(\"Unknown records type found: \" + baseRecords.getClass());\n-                MemoryRecords records = (MemoryRecords) baseRecords;\n-\n-                List<AbortedTransaction> abortedTransactions = null;\n-                if (partitionResponseHeader.hasField(ABORTED_TRANSACTIONS_KEY_NAME)) {\n-                    Object[] abortedTransactionsArray = partitionResponseHeader.getArray(ABORTED_TRANSACTIONS_KEY_NAME);\n-                    if (abortedTransactionsArray != null) {\n-                        abortedTransactions = new ArrayList<>(abortedTransactionsArray.length);\n-                        for (Object abortedTransactionObj : abortedTransactionsArray) {\n-                            Struct abortedTransactionStruct = (Struct) abortedTransactionObj;\n-                            long producerId = abortedTransactionStruct.get(PRODUCER_ID);\n-                            long firstOffset = abortedTransactionStruct.get(FIRST_OFFSET);\n-                            abortedTransactions.add(new AbortedTransaction(producerId, firstOffset));\n-                        }\n-                    }\n-                }\n-\n-                PartitionData<MemoryRecords> partitionData = new PartitionData<>(error, highWatermark, lastStableOffset,\n-                        logStartOffset, preferredReadReplica, abortedTransactions, records);\n-                responseData.put(new TopicPartition(topic, partition), partitionData);\n-            }\n-        }\n-        return new FetchResponse<>(Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0)), responseData,\n-                struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME), struct.getOrElse(SESSION_ID, INVALID_SESSION_ID));\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        this.fetchResponseData = fetchResponseData;\n+        this.responseDataMap = toResponseDataMap(fetchResponseData);\n     }\n \n     @Override\n     public Struct toStruct(short version) {\n-        return toStruct(version, throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        return fetchResponseData.toStruct(version);\n     }\n \n     @Override\n     protected Send toSend(String dest, ResponseHeader responseHeader, short apiVersion) {\n-        Struct responseHeaderStruct = responseHeader.toStruct();\n-        Struct responseBodyStruct = toStruct(apiVersion);\n+        // Generate the Sends for the response fields and records\n+        ArrayDeque<Send> sends = new ArrayDeque<>();\n+        RecordsWriter writer = new RecordsWriter(dest, sends::add);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgzMjcwNw==", "bodyText": "Is it worth extending ByteBufferAccessor or not?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453832707", "createdAt": "2020-07-13T18:03:36Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/RecordsReader.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.record.BaseRecords;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Implementation of Readable which reads from a byte buffer and can read records as {@link MemoryRecords}\n+ *\n+ * @see org.apache.kafka.common.requests.FetchResponse\n+ */\n+public class RecordsReader implements Readable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 31}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6094e4e201d4b15db31605143841df3545414071", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/6094e4e201d4b15db31605143841df3545414071", "committedDate": "2020-07-13T18:36:39Z", "message": "Feedback from PR"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NTA4NDM2", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-447508436", "createdAt": "2020-07-13T18:44:32Z", "commit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxODo0NDozMlrOGw1Mog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxODo0NDozMlrOGw1Mog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg1NjQxOA==", "bodyText": "s/Responses/TopicResponses?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r453856418", "createdAt": "2020-07-13T18:44:32Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/resources/common/message/FetchResponse.json", "diffHunk": "@@ -47,33 +47,35 @@\n       \"about\": \"The top level response error code.\" },\n     { \"name\": \"SessionId\", \"type\": \"int32\", \"versions\": \"7+\", \"default\": \"0\", \"ignorable\": false,\n       \"about\": \"The fetch session ID, or 0 if this is not part of a fetch session.\" },\n-    { \"name\": \"Topics\", \"type\": \"[]FetchableTopicResponse\", \"versions\": \"0+\",\n+    { \"name\": \"Responses\", \"type\": \"[]FetchableTopicResponse\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04538af26ab9f036d547f4b6f34bdffb720e8007"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41d03d1e52da1906b8479cbd369eb95954c09ff8", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/41d03d1e52da1906b8479cbd369eb95954c09ff8", "committedDate": "2020-07-13T19:46:36Z", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-9629-fetch-api-generated-protocol"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c98083b2d673b9acde687627fd8e3da94588195", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/5c98083b2d673b9acde687627fd8e3da94588195", "committedDate": "2020-07-14T15:00:02Z", "message": "Fix re-ordering of topic partitions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ca460c2ac2a0b71c5565c927ba21bce8b7749f1", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/8ca460c2ac2a0b71c5565c927ba21bce8b7749f1", "committedDate": "2020-07-14T15:00:22Z", "message": "Use generated message class for serialization in FetchRequest also"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3808a96aa8ef182c5cabebfba6debe5a303c2b5e", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/3808a96aa8ef182c5cabebfba6debe5a303c2b5e", "committedDate": "2020-07-14T15:01:00Z", "message": "Feedback from PR"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4NzE5Mjgw", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-448719280", "createdAt": "2020-07-15T08:02:58Z", "commit": {"oid": "3808a96aa8ef182c5cabebfba6debe5a303c2b5e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwODowMjo1OFrOGxy3FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwODowMjo1OFrOGxy3FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg2NjcwOA==", "bodyText": "@mumrah Have we considered dropping the PartitionData class entirely in favour of using FetchRequestData .FetchPartition directly in the broker? The main difference is that FetchPartition does not have an Optional for the leader epoch but returns the default value (-1) instead.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r454866708", "createdAt": "2020-07-15T08:02:58Z", "author": {"login": "dajac"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java", "diffHunk": "@@ -273,6 +99,28 @@ public boolean equals(Object o) {\n         }\n     }\n \n+    private Map<TopicPartition, PartitionData> toPartitionDataMap(List<FetchRequestData.FetchTopic> fetchableTopics) {\n+       Map<TopicPartition, PartitionData> result = new LinkedHashMap<>();\n+        fetchableTopics.forEach(fetchTopic -> fetchTopic.partitions().forEach(fetchPartition -> {\n+            Optional<Integer> leaderEpoch = Optional.of(fetchPartition.currentLeaderEpoch())\n+                .filter(epoch -> epoch != RecordBatch.NO_PARTITION_LEADER_EPOCH);\n+            result.put(new TopicPartition(fetchTopic.topic(), fetchPartition.partition()),\n+                new PartitionData(fetchPartition.fetchOffset(), fetchPartition.logStartOffset(),\n+                    fetchPartition.partitionMaxBytes(), leaderEpoch));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3808a96aa8ef182c5cabebfba6debe5a303c2b5e"}, "originalPosition": 247}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "efdadc5ab419806b6165cc2ce90a23e36117d007", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/efdadc5ab419806b6165cc2ce90a23e36117d007", "committedDate": "2020-07-15T18:01:17Z", "message": "Add jmh benchmarks and fix checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5MzAwMzIx", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-449300321", "createdAt": "2020-07-15T20:20:35Z", "commit": {"oid": "efdadc5ab419806b6165cc2ce90a23e36117d007"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMDoyMDozNlrOGyOacQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMDoyMDozNlrOGyOacQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxODEyOQ==", "bodyText": "Can we please have benchmarks for both forConsumer and forReplica fetch requests?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r455318129", "createdAt": "2020-07-15T20:20:36Z", "author": {"login": "lbradstreet"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchRequestBenchmark.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.jmh.common;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.network.Send;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.requests.ByteBufferChannel;\n+import org.apache.kafka.common.requests.FetchRequest;\n+import org.apache.kafka.common.requests.RequestHeader;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Level;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 5)\n+@Measurement(iterations = 15)\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+public class FetchRequestBenchmark {\n+    @Param({\"1000\"})\n+    private int topicCount;\n+\n+    @Param({\"20\"})\n+    private int partitionCount;\n+\n+    Map<TopicPartition, FetchRequest.PartitionData> fetchData;\n+\n+    RequestHeader header;\n+\n+    FetchRequest request;\n+\n+\n+    @Setup(Level.Trial)\n+    public void setup() {\n+        this.fetchData = new HashMap<>();\n+        for (int topicIdx = 0; topicIdx < topicCount; topicIdx++) {\n+            for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n+                FetchRequest.PartitionData partitionData = new FetchRequest.PartitionData(\n+                    0, 0, 4096, Optional.empty());\n+                fetchData.put(new TopicPartition(String.format(\"topic-%04d\", topicIdx), partitionId), partitionData);\n+            }\n+        }\n+\n+        this.header = new RequestHeader(ApiKeys.FETCH, ApiKeys.FETCH.latestVersion(), \"jmh-benchmark\", 100);\n+        this.request = FetchRequest.Builder.forConsumer(0, 0, fetchData).build(ApiKeys.FETCH.latestVersion());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efdadc5ab419806b6165cc2ce90a23e36117d007"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5MzU0MDc0", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-449354074", "createdAt": "2020-07-15T21:26:40Z", "commit": {"oid": "efdadc5ab419806b6165cc2ce90a23e36117d007"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMToyNjo0MFrOGyRFPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMToyNjo0MFrOGyRFPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM2MTg1NQ==", "bodyText": "Let's add a smaller topic or partition count param benchmark. 20,000 partitions in a fetch request is larger than we would normally see :)", "url": "https://github.com/apache/kafka/pull/9008#discussion_r455361855", "createdAt": "2020-07-15T21:26:40Z", "author": {"login": "lbradstreet"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchRequestBenchmark.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.jmh.common;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.network.Send;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.requests.ByteBufferChannel;\n+import org.apache.kafka.common.requests.FetchRequest;\n+import org.apache.kafka.common.requests.RequestHeader;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Level;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 5)\n+@Measurement(iterations = 15)\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+public class FetchRequestBenchmark {\n+    @Param({\"1000\"})", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efdadc5ab419806b6165cc2ce90a23e36117d007"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5OTAzNjY0", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-449903664", "createdAt": "2020-07-16T14:27:51Z", "commit": {"oid": "efdadc5ab419806b6165cc2ce90a23e36117d007"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNDoyNzo1MVrOGytlmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNDoyNzo1MVrOGytlmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTgyODg5MA==", "bodyText": "Typo \"Forgotton\"", "url": "https://github.com/apache/kafka/pull/9008#discussion_r455828890", "createdAt": "2020-07-16T14:27:51Z", "author": {"login": "lbradstreet"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java", "diffHunk": "@@ -273,6 +99,28 @@ public boolean equals(Object o) {\n         }\n     }\n \n+    private Map<TopicPartition, PartitionData> toPartitionDataMap(List<FetchRequestData.FetchTopic> fetchableTopics) {\n+        Map<TopicPartition, PartitionData> result = new LinkedHashMap<>();\n+        fetchableTopics.forEach(fetchTopic -> fetchTopic.partitions().forEach(fetchPartition -> {\n+            Optional<Integer> leaderEpoch = Optional.of(fetchPartition.currentLeaderEpoch())\n+                .filter(epoch -> epoch != RecordBatch.NO_PARTITION_LEADER_EPOCH);\n+            result.put(new TopicPartition(fetchTopic.topic(), fetchPartition.partition()),\n+                new PartitionData(fetchPartition.fetchOffset(), fetchPartition.logStartOffset(),\n+                    fetchPartition.partitionMaxBytes(), leaderEpoch));\n+        }));\n+        return Collections.unmodifiableMap(result);\n+    }\n+\n+    private List<TopicPartition> toForgottonTopicList(List<FetchRequestData.ForgottenTopic> forgottenTopics) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efdadc5ab419806b6165cc2ce90a23e36117d007"}, "originalPosition": 252}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "538ed0010f048d48517057aa932c5f377b6c00d7", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/538ed0010f048d48517057aa932c5f377b6c00d7", "committedDate": "2020-07-16T15:53:49Z", "message": "Fix sizeOf for records in message class generator"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "155621b97fc1c1c32ec4998165fcaf1858bd1cd4", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/155621b97fc1c1c32ec4998165fcaf1858bd1cd4", "committedDate": "2020-07-16T16:17:11Z", "message": "Don't re-create the whole message on static size method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/89de5083ecbee44f730bf1f15b422c01c2114976", "committedDate": "2020-07-16T20:58:06Z", "message": "Update benchmarks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMTkxNDg4", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-450191488", "createdAt": "2020-07-16T20:27:57Z", "commit": {"oid": "155621b97fc1c1c32ec4998165fcaf1858bd1cd4"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMDoyNzo1OFrOGy7j-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMToxOTo0MFrOGy9K-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA1Nzg0OQ==", "bodyText": "nit: could probably change this to use ifPresent", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456057849", "createdAt": "2020-07-16T20:27:58Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1249,26 +1249,26 @@ private CompletedFetch initializeCompletedFetch(CompletedFetch nextCompletedFetc\n                     }\n                 }\n \n-                if (partition.highWatermark >= 0) {\n-                    log.trace(\"Updating high watermark for partition {} to {}\", tp, partition.highWatermark);\n-                    subscriptions.updateHighWatermark(tp, partition.highWatermark);\n+                if (partition.highWatermark() >= 0) {\n+                    log.trace(\"Updating high watermark for partition {} to {}\", tp, partition.highWatermark());\n+                    subscriptions.updateHighWatermark(tp, partition.highWatermark());\n                 }\n \n-                if (partition.logStartOffset >= 0) {\n-                    log.trace(\"Updating log start offset for partition {} to {}\", tp, partition.logStartOffset);\n-                    subscriptions.updateLogStartOffset(tp, partition.logStartOffset);\n+                if (partition.logStartOffset() >= 0) {\n+                    log.trace(\"Updating log start offset for partition {} to {}\", tp, partition.logStartOffset());\n+                    subscriptions.updateLogStartOffset(tp, partition.logStartOffset());\n                 }\n \n-                if (partition.lastStableOffset >= 0) {\n-                    log.trace(\"Updating last stable offset for partition {} to {}\", tp, partition.lastStableOffset);\n-                    subscriptions.updateLastStableOffset(tp, partition.lastStableOffset);\n+                if (partition.lastStableOffset() >= 0) {\n+                    log.trace(\"Updating last stable offset for partition {} to {}\", tp, partition.lastStableOffset());\n+                    subscriptions.updateLastStableOffset(tp, partition.lastStableOffset());\n                 }\n \n-                if (partition.preferredReadReplica.isPresent()) {\n-                    subscriptions.updatePreferredReadReplica(completedFetch.partition, partition.preferredReadReplica.get(), () -> {\n+                if (partition.preferredReadReplica().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "155621b97fc1c1c32ec4998165fcaf1858bd1cd4"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3NjkyNw==", "bodyText": "Is there an advantage to pulling this up? Seems like we still need to update a bunch more classes. Until we have all the protocols converted, it might be safer to find another approach.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456076927", "createdAt": "2020-07-16T21:04:15Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/AbstractRequestResponse.java", "diffHunk": "@@ -16,5 +16,16 @@\n  */\n package org.apache.kafka.common.requests;\n \n+import org.apache.kafka.common.protocol.ApiMessage;\n+\n public interface AbstractRequestResponse {\n+    /**\n+     * Return the auto-generated `Message` instance if this request/response relies on one for\n+     * serialization/deserialization. If this class has not yet been updated to rely on the auto-generated protocol\n+     * classes, return `null`.\n+     * @return\n+     */\n+    default ApiMessage data() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MTM3NA==", "bodyText": "Are we overriding this so that we save the conversion to Struct? As far as I can tell, there's nothing specific to FetchRequest below. I wonder if we can move this implementation to AbstractRequest.serialize so that we save the conversion to Struct for all APIs that have been converted?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456081374", "createdAt": "2020-07-16T21:13:38Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java", "diffHunk": "@@ -492,74 +327,51 @@ public int maxBytes() {\n     }\n \n     public boolean isFromFollower() {\n-        return replicaId >= 0;\n+        return replicaId() >= 0;\n     }\n \n     public IsolationLevel isolationLevel() {\n-        return isolationLevel;\n+        return IsolationLevel.forId(data.isolationLevel());\n     }\n \n     public FetchMetadata metadata() {\n         return metadata;\n     }\n \n     public String rackId() {\n-        return rackId;\n+        return data.rackId();\n     }\n \n     public static FetchRequest parse(ByteBuffer buffer, short version) {\n-        return new FetchRequest(ApiKeys.FETCH.parseRequest(version, buffer), version);\n+        ByteBufferAccessor accessor = new ByteBufferAccessor(buffer);\n+        FetchRequestData message = new FetchRequestData();\n+        message.read(accessor, version);\n+        return new FetchRequest(message, version);\n+    }\n+\n+    @Override\n+    public ByteBuffer serialize(RequestHeader header) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "originalPosition": 445}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4NDIxNg==", "bodyText": "In the parsing logic, we still convert to struct first before calling AbstractRequest.parseRequest. I think we could bypass the Struct conversion by changing AbstractRequest.parseRequest to take the ByteBuffer instead of the Struct.\n    public static AbstractRequest parseRequest(ApiKeys apiKey, short apiVersion, ByteBuffer buffer) {\nThen in the fetch case, we could just call this method.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456084216", "createdAt": "2020-07-16T21:19:40Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java", "diffHunk": "@@ -492,74 +327,51 @@ public int maxBytes() {\n     }\n \n     public boolean isFromFollower() {\n-        return replicaId >= 0;\n+        return replicaId() >= 0;\n     }\n \n     public IsolationLevel isolationLevel() {\n-        return isolationLevel;\n+        return IsolationLevel.forId(data.isolationLevel());\n     }\n \n     public FetchMetadata metadata() {\n         return metadata;\n     }\n \n     public String rackId() {\n-        return rackId;\n+        return data.rackId();\n     }\n \n     public static FetchRequest parse(ByteBuffer buffer, short version) {\n-        return new FetchRequest(ApiKeys.FETCH.parseRequest(version, buffer), version);\n+        ByteBufferAccessor accessor = new ByteBufferAccessor(buffer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "originalPosition": 438}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMzQ3NzUz", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-450347753", "createdAt": "2020-07-17T03:19:46Z", "commit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMzoxOTo0NlrOGzD_Xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMzoxOTo0NlrOGzD_Xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE5NTkzNA==", "bodyText": "This seems pretty inefficient (creating a lambda for each byte we write). Are we sure about it?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456195934", "createdAt": "2020-07-17T03:19:46Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/RecordsWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.network.ByteBufferSend;\n+import org.apache.kafka.common.network.Send;\n+import org.apache.kafka.common.record.BaseRecords;\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.function.Consumer;\n+\n+/**\n+ * Implementation of Writable which produces a sequence of {@link Send} objects. This allows for deferring the transfer\n+ * of data from a record-set's file channel to the eventual socket channel.\n+ *\n+ * Excepting {@link #writeRecords(BaseRecords)}, calls to the write methods on this class will append to a byte array\n+ * according to the format specified in {@link DataOutput}. When a call is made to writeRecords, any previously written\n+ * bytes will be flushed as a new {@link ByteBufferSend} to the given Send consumer. After flushing the pending bytes,\n+ * another Send is passed to the consumer which wraps the underlying record-set's transfer logic.\n+ *\n+ * For example,\n+ *\n+ * <pre>\n+ *     recordsWritable.writeInt(10);\n+ *     recordsWritable.writeRecords(records1);\n+ *     recordsWritable.writeInt(20);\n+ *     recordsWritable.writeRecords(records2);\n+ *     recordsWritable.writeInt(30);\n+ *     recordsWritable.flush();\n+ * </pre>\n+ *\n+ * Will pass 5 Send objects to the consumer given in the constructor. Care must be taken by callers to flush any\n+ * pending bytes at the end of the writing sequence to ensure everything is flushed to the consumer. This class is\n+ * intended to be used with {@link org.apache.kafka.common.record.MultiRecordsSend}.\n+ *\n+ * @see org.apache.kafka.common.requests.FetchResponse\n+ */\n+public class RecordsWriter implements Writable {\n+    private final String dest;\n+    private final Consumer<Send> sendConsumer;\n+    private final ByteArrayOutputStream byteArrayOutputStream;\n+    private final DataOutput output;\n+\n+    public RecordsWriter(String dest, Consumer<Send> sendConsumer) {\n+        this.dest = dest;\n+        this.sendConsumer = sendConsumer;\n+        this.byteArrayOutputStream = new ByteArrayOutputStream();\n+        this.output = new DataOutputStream(this.byteArrayOutputStream);\n+    }\n+\n+    @Override\n+    public void writeByte(byte val) {\n+        writeQuietly(() -> output.writeByte(val));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMzQ4MTg0", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-450348184", "createdAt": "2020-07-17T03:21:27Z", "commit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMzoyMToyOFrOGzEA6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMzoyMToyOFrOGzEA6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE5NjMzMQ==", "bodyText": "Is this and the respective writeRecords in the base interface needed? It seems like they're only implemented in two specific cases. Could we not downcast for those cases?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456196331", "createdAt": "2020-07-17T03:21:28Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/Readable.java", "diffHunk": "@@ -35,6 +36,10 @@\n     int readUnsignedVarint();\n     ByteBuffer readByteBuffer(int length);\n \n+    default BaseRecords readRecords(int length) {\n+        throw new UnsupportedOperationException(\"Not implemented\");\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMzQ5NTcw", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-450349570", "createdAt": "2020-07-17T03:26:55Z", "commit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMzoyNjo1NVrOGzEF4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMzoyNjo1NVrOGzEF4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE5NzYwMQ==", "bodyText": "This creates a copy of the underlying bytes, can we avoid it?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456197601", "createdAt": "2020-07-17T03:26:55Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/RecordsWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.network.ByteBufferSend;\n+import org.apache.kafka.common.network.Send;\n+import org.apache.kafka.common.record.BaseRecords;\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.function.Consumer;\n+\n+/**\n+ * Implementation of Writable which produces a sequence of {@link Send} objects. This allows for deferring the transfer\n+ * of data from a record-set's file channel to the eventual socket channel.\n+ *\n+ * Excepting {@link #writeRecords(BaseRecords)}, calls to the write methods on this class will append to a byte array\n+ * according to the format specified in {@link DataOutput}. When a call is made to writeRecords, any previously written\n+ * bytes will be flushed as a new {@link ByteBufferSend} to the given Send consumer. After flushing the pending bytes,\n+ * another Send is passed to the consumer which wraps the underlying record-set's transfer logic.\n+ *\n+ * For example,\n+ *\n+ * <pre>\n+ *     recordsWritable.writeInt(10);\n+ *     recordsWritable.writeRecords(records1);\n+ *     recordsWritable.writeInt(20);\n+ *     recordsWritable.writeRecords(records2);\n+ *     recordsWritable.writeInt(30);\n+ *     recordsWritable.flush();\n+ * </pre>\n+ *\n+ * Will pass 5 Send objects to the consumer given in the constructor. Care must be taken by callers to flush any\n+ * pending bytes at the end of the writing sequence to ensure everything is flushed to the consumer. This class is\n+ * intended to be used with {@link org.apache.kafka.common.record.MultiRecordsSend}.\n+ *\n+ * @see org.apache.kafka.common.requests.FetchResponse\n+ */\n+public class RecordsWriter implements Writable {\n+    private final String dest;\n+    private final Consumer<Send> sendConsumer;\n+    private final ByteArrayOutputStream byteArrayOutputStream;\n+    private final DataOutput output;\n+\n+    public RecordsWriter(String dest, Consumer<Send> sendConsumer) {\n+        this.dest = dest;\n+        this.sendConsumer = sendConsumer;\n+        this.byteArrayOutputStream = new ByteArrayOutputStream();\n+        this.output = new DataOutputStream(this.byteArrayOutputStream);\n+    }\n+\n+    @Override\n+    public void writeByte(byte val) {\n+        writeQuietly(() -> output.writeByte(val));\n+    }\n+\n+    @Override\n+    public void writeShort(short val) {\n+        writeQuietly(() -> output.writeShort(val));\n+    }\n+\n+    @Override\n+    public void writeInt(int val) {\n+        writeQuietly(() -> output.writeInt(val));\n+    }\n+\n+    @Override\n+    public void writeLong(long val) {\n+        writeQuietly(() -> output.writeLong(val));\n+\n+    }\n+\n+    @Override\n+    public void writeDouble(double val) {\n+        writeQuietly(() -> ByteUtils.writeDouble(val, output));\n+\n+    }\n+\n+    @Override\n+    public void writeByteArray(byte[] arr) {\n+        writeQuietly(() -> output.write(arr));\n+    }\n+\n+    @Override\n+    public void writeUnsignedVarint(int i) {\n+        writeQuietly(() -> ByteUtils.writeUnsignedVarint(i, output));\n+    }\n+\n+    @Override\n+    public void writeByteBuffer(ByteBuffer src) {\n+        writeQuietly(() -> output.write(src.array(), src.position(), src.remaining()));\n+    }\n+\n+    @FunctionalInterface\n+    private interface IOExceptionThrowingRunnable {\n+        void run() throws IOException;\n+    }\n+\n+    private void writeQuietly(IOExceptionThrowingRunnable runnable) {\n+        try {\n+            runnable.run();\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Writable encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeRecords(BaseRecords records) {\n+        flush();\n+        sendConsumer.accept(records.toSend(dest));\n+    }\n+\n+    /**\n+     * Flush any pending bytes as a ByteBufferSend and reset the buffer\n+     */\n+    public void flush() {\n+        ByteBufferSend send = new ByteBufferSend(dest,\n+                ByteBuffer.wrap(byteArrayOutputStream.toByteArray()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "originalPosition": 137}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7878289a11d1efb500440c3218b4cac6670791b1", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/7878289a11d1efb500440c3218b4cac6670791b1", "committedDate": "2020-07-17T16:06:51Z", "message": "Pull up readRecords and writeRecords out of the base interfaces"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "701619dbfd23f1b41c3376f33c9615107dc1e8f3", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/701619dbfd23f1b41c3376f33c9615107dc1e8f3", "committedDate": "2020-07-17T16:16:31Z", "message": "Add a struct -> message benchmark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82a0d46ecea2d18a59de64fbb18c8a40f1c05e3e", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/82a0d46ecea2d18a59de64fbb18c8a40f1c05e3e", "committedDate": "2020-07-17T19:59:55Z", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-9629-fetch-api-generated-protocol"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38b2ebf3c3343d7d6a5c354096fc24c6bbf73535", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/38b2ebf3c3343d7d6a5c354096fc24c6bbf73535", "committedDate": "2020-07-17T20:18:27Z", "message": "Re-add storage error conversion logic"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwOTQyOTk0", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-450942994", "createdAt": "2020-07-17T20:37:05Z", "commit": {"oid": "38b2ebf3c3343d7d6a5c354096fc24c6bbf73535"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMDozNzowNVrOGzgfQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMDozNzowNVrOGzgfQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY2Mjg1MA==", "bodyText": "Using null as the default seems reasonable to me. We also have MemoryRecords.EMPTY which we could use.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r456662850", "createdAt": "2020-07-17T20:37:05Z", "author": {"login": "hachikuji"}, "path": "generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java", "diffHunk": "@@ -2397,6 +2460,9 @@ private String fieldDefault(FieldSpec field) {\n                 headerGenerator.addImport(MessageGenerator.BYTES_CLASS);\n                 return \"Bytes.EMPTY\";\n             }\n+        } else if (field.type().isRecords()) {\n+            // TODO should we use some special EmptyRecords class instead?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38b2ebf3c3343d7d6a5c354096fc24c6bbf73535"}, "originalPosition": 175}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "efa54501144764fb7159f1e9211fecd88dd34401", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/efa54501144764fb7159f1e9211fecd88dd34401", "committedDate": "2020-07-18T00:55:03Z", "message": "Use ByteBufferOutputStream for auto-resizing buffer in RecordsWriter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2514f5ae54b4f51dad11b6ff23ecafc28362f7bd", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/2514f5ae54b4f51dad11b6ff23ecafc28362f7bd", "committedDate": "2020-07-18T01:11:57Z", "message": "Remove some TODOs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxNjM3NzY4", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-451637768", "createdAt": "2020-07-20T14:05:08Z", "commit": {"oid": "2514f5ae54b4f51dad11b6ff23ecafc28362f7bd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNDowNTowOFrOG0OeTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNDowNTowOFrOG0OeTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzQxNjI2OQ==", "bodyText": "I ended up having to keep a separate mark here since ByteBufferOutputStream doesn't keep the mark when it replaces the underlying buffer. I also didn't want to mess with that class in this PR since it had quite a lot of usages. We could look into fixing that as a follow-on", "url": "https://github.com/apache/kafka/pull/9008#discussion_r457416269", "createdAt": "2020-07-20T14:05:08Z", "author": {"login": "mumrah"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/RecordsWriter.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.network.ByteBufferSend;\n+import org.apache.kafka.common.network.Send;\n+import org.apache.kafka.common.record.BaseRecords;\n+import org.apache.kafka.common.utils.ByteBufferOutputStream;\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.function.Consumer;\n+\n+/**\n+ * Implementation of Writable which produces a sequence of {@link Send} objects. This allows for deferring the transfer\n+ * of data from a record-set's file channel to the eventual socket channel.\n+ *\n+ * Excepting {@link #writeRecords(BaseRecords)}, calls to the write methods on this class will append to a byte array\n+ * according to the format specified in {@link DataOutput}. When a call is made to writeRecords, any previously written\n+ * bytes will be flushed as a new {@link ByteBufferSend} to the given Send consumer. After flushing the pending bytes,\n+ * another Send is passed to the consumer which wraps the underlying record-set's transfer logic.\n+ *\n+ * For example,\n+ *\n+ * <pre>\n+ *     recordsWritable.writeInt(10);\n+ *     recordsWritable.writeRecords(records1);\n+ *     recordsWritable.writeInt(20);\n+ *     recordsWritable.writeRecords(records2);\n+ *     recordsWritable.writeInt(30);\n+ *     recordsWritable.flush();\n+ * </pre>\n+ *\n+ * Will pass 5 Send objects to the consumer given in the constructor. Care must be taken by callers to flush any\n+ * pending bytes at the end of the writing sequence to ensure everything is flushed to the consumer. This class is\n+ * intended to be used with {@link org.apache.kafka.common.record.MultiRecordsSend}.\n+ *\n+ * @see org.apache.kafka.common.requests.FetchResponse\n+ */\n+public class RecordsWriter implements Writable {\n+    private final String dest;\n+    private final Consumer<Send> sendConsumer;\n+    private final ByteBufferOutputStream byteArrayOutputStream;\n+    private final DataOutput output;\n+    private int mark;\n+\n+    public RecordsWriter(String dest, Consumer<Send> sendConsumer) {\n+        this.dest = dest;\n+        this.sendConsumer = sendConsumer;\n+        this.byteArrayOutputStream = new ByteBufferOutputStream(32);\n+        this.output = new DataOutputStream(this.byteArrayOutputStream);\n+        this.mark = 0;\n+    }\n+\n+    @Override\n+    public void writeByte(byte val) {\n+        try {\n+            output.writeByte(val);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeShort(short val) {\n+        try {\n+            output.writeShort(val);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeInt(int val) {\n+        try {\n+            output.writeInt(val);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeLong(long val) {\n+        try {\n+            output.writeLong(val);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeDouble(double val) {\n+        try {\n+            ByteUtils.writeDouble(val, output);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeByteArray(byte[] arr) {\n+        try {\n+            output.write(arr);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeUnsignedVarint(int i) {\n+        try {\n+            ByteUtils.writeUnsignedVarint(i, output);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void writeByteBuffer(ByteBuffer src) {\n+        try {\n+            output.write(src.array(), src.position(), src.remaining());\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"RecordsWriter encountered an IO error\", e);\n+        }\n+    }\n+\n+    public void writeRecords(BaseRecords records) {\n+        flush();\n+        sendConsumer.accept(records.toSend(dest));\n+    }\n+\n+    /**\n+     * Flush any pending bytes as a ByteBufferSend and reset the buffer\n+     */\n+    public void flush() {\n+        ByteBuffer buf = byteArrayOutputStream.buffer();\n+        int end = buf.position();\n+        int len = end - mark;\n+\n+        if (len > 0) {\n+            buf.position(mark);\n+            ByteBuffer slice = buf.slice();\n+            slice.limit(len);\n+            ByteBufferSend send = new ByteBufferSend(dest, slice);\n+            sendConsumer.accept(send);\n+        }\n+\n+        buf.position(end);\n+        mark = end;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2514f5ae54b4f51dad11b6ff23ecafc28362f7bd"}, "originalPosition": 167}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e198797840bfb4679ba81f6da42aa267f6156409", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/e198797840bfb4679ba81f6da42aa267f6156409", "committedDate": "2020-07-20T14:39:49Z", "message": "Clean up and comment ByteBuffer usage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf3bf3360c0dab8402e6c5adbbb2ce9bc0ac8374", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/cf3bf3360c0dab8402e6c5adbbb2ce9bc0ac8374", "committedDate": "2020-07-21T20:44:19Z", "message": "Allocated a larger initial buffer for FetchResponse and grow it at 2x"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78cd0124dc0573819e169c432b45929efd933665", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/78cd0124dc0573819e169c432b45929efd933665", "committedDate": "2020-07-23T02:02:45Z", "message": "Use ByteBuffer with a single allocation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU2MTM5MTM0", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-456139134", "createdAt": "2020-07-27T21:12:47Z", "commit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMToxMjo0N1rOG3zxqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0MjoxNVrOG32ICQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE3MzE2Mg==", "bodyText": "I guess the implicit expectation is that if the protocol does not support the read_committed isolation level, then it wouldn't have transactional data anyway, so reverting to read_uncommitted is safe. Can't find a fault with that.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461173162", "createdAt": "2020-07-27T21:12:47Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/FetchRequest.json", "diffHunk": "@@ -55,35 +55,35 @@\n       \"about\": \"The minimum bytes to accumulate in the response.\" },\n     { \"name\": \"MaxBytes\", \"type\": \"int32\", \"versions\": \"3+\", \"default\": \"0x7fffffff\", \"ignorable\": true,\n       \"about\": \"The maximum bytes to fetch.  See KIP-74 for cases where this limit may not be honored.\" },\n-    { \"name\": \"IsolationLevel\", \"type\": \"int8\", \"versions\": \"4+\", \"default\": \"0\", \"ignorable\": false,\n+    { \"name\": \"IsolationLevel\", \"type\": \"int8\", \"versions\": \"4+\", \"default\": \"0\", \"ignorable\": true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIwMTM2Mg==", "bodyText": "More of a side question, but is this length guaranteed to be less than the buffer size? Wondering if it is worth adding range checking.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461201362", "createdAt": "2020-07-27T22:15:08Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/RecordsReader.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.record.BaseRecords;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Implementation of Readable which reads from a byte buffer and can read records as {@link MemoryRecords}\n+ *\n+ * @see org.apache.kafka.common.requests.FetchResponse\n+ */\n+public class RecordsReader implements Readable {\n+    private final ByteBuffer buf;\n+\n+    public RecordsReader(ByteBuffer buf) {\n+        this.buf = buf;\n+    }\n+\n+    @Override\n+    public byte readByte() {\n+        return buf.get();\n+    }\n+\n+    @Override\n+    public short readShort() {\n+        return buf.getShort();\n+    }\n+\n+    @Override\n+    public int readInt() {\n+        return buf.getInt();\n+    }\n+\n+    @Override\n+    public long readLong() {\n+        return buf.getLong();\n+    }\n+\n+    @Override\n+    public double readDouble() {\n+        return ByteUtils.readDouble(buf);\n+    }\n+\n+    @Override\n+    public void readArray(byte[] arr) {\n+        buf.get(arr);\n+    }\n+\n+    @Override\n+    public int readUnsignedVarint() {\n+        return ByteUtils.readUnsignedVarint(buf);\n+    }\n+\n+    @Override\n+    public ByteBuffer readByteBuffer(int length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIwMjg3MQ==", "bodyText": "Could we rename totalSize so that it is clear that it does not cover the record sizes. Maybe totalOverheadSize or totalNonRecordSize or something like that.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461202871", "createdAt": "2020-07-27T22:19:10Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/RecordsWriter.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.network.ByteBufferSend;\n+import org.apache.kafka.common.network.Send;\n+import org.apache.kafka.common.record.BaseRecords;\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.DataOutput;\n+import java.nio.ByteBuffer;\n+import java.util.function.Consumer;\n+\n+/**\n+ * Implementation of Writable which produces a sequence of {@link Send} objects. This allows for deferring the transfer\n+ * of data from a record-set's file channel to the eventual socket channel.\n+ *\n+ * Excepting {@link #writeRecords(BaseRecords)}, calls to the write methods on this class will append to a byte array\n+ * according to the format specified in {@link DataOutput}. When a call is made to writeRecords, any previously written\n+ * bytes will be flushed as a new {@link ByteBufferSend} to the given Send consumer. After flushing the pending bytes,\n+ * another Send is passed to the consumer which wraps the underlying record-set's transfer logic.\n+ *\n+ * For example,\n+ *\n+ * <pre>\n+ *     recordsWritable.writeInt(10);\n+ *     recordsWritable.writeRecords(records1);\n+ *     recordsWritable.writeInt(20);\n+ *     recordsWritable.writeRecords(records2);\n+ *     recordsWritable.writeInt(30);\n+ *     recordsWritable.flush();\n+ * </pre>\n+ *\n+ * Will pass 5 Send objects to the consumer given in the constructor. Care must be taken by callers to flush any\n+ * pending bytes at the end of the writing sequence to ensure everything is flushed to the consumer. This class is\n+ * intended to be used with {@link org.apache.kafka.common.record.MultiRecordsSend}.\n+ *\n+ * @see org.apache.kafka.common.requests.FetchResponse\n+ */\n+public class RecordsWriter implements Writable {\n+    private final String dest;\n+    private final Consumer<Send> sendConsumer;\n+    private final ByteBuffer buffer;\n+    private int mark;\n+\n+    public RecordsWriter(String dest, int totalSize, Consumer<Send> sendConsumer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIwMzI4Nw==", "bodyText": "nit: any reason not to stick with the same constructor convention as the other requests?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461203287", "createdAt": "2020-07-27T22:20:10Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java", "diffHunk": "@@ -146,7 +147,7 @@ public static AbstractRequest parseRequest(ApiKeys apiKey, short apiVersion, Str\n             case PRODUCE:\n                 return new ProduceRequest(struct, apiVersion);\n             case FETCH:\n-                return new FetchRequest(struct, apiVersion);\n+                return new FetchRequest(new FetchRequestData(struct, apiVersion), apiVersion);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIwMzY2MQ==", "bodyText": "@mumrah Do we need this for this PR or can we leave this for #7409?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461203661", "createdAt": "2020-07-27T22:21:09Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/AbstractRequestResponse.java", "diffHunk": "@@ -16,5 +16,16 @@\n  */\n package org.apache.kafka.common.requests;\n \n+import org.apache.kafka.common.protocol.ApiMessage;\n+\n public interface AbstractRequestResponse {\n+    /**\n+     * Return the auto-generated `Message` instance if this request/response relies on one for\n+     * serialization/deserialization. If this class has not yet been updated to rely on the auto-generated protocol\n+     * classes, return `null`.\n+     * @return\n+     */\n+    default ApiMessage data() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3NjkyNw=="}, "originalCommit": {"oid": "89de5083ecbee44f730bf1f15b422c01c2114976"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIwNTQyNw==", "bodyText": "nit: wonder if this should be RecordsWritable for consistency with Writable.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461205427", "createdAt": "2020-07-27T22:25:41Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/RecordsWriter.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.network.ByteBufferSend;\n+import org.apache.kafka.common.network.Send;\n+import org.apache.kafka.common.record.BaseRecords;\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.DataOutput;\n+import java.nio.ByteBuffer;\n+import java.util.function.Consumer;\n+\n+/**\n+ * Implementation of Writable which produces a sequence of {@link Send} objects. This allows for deferring the transfer\n+ * of data from a record-set's file channel to the eventual socket channel.\n+ *\n+ * Excepting {@link #writeRecords(BaseRecords)}, calls to the write methods on this class will append to a byte array\n+ * according to the format specified in {@link DataOutput}. When a call is made to writeRecords, any previously written\n+ * bytes will be flushed as a new {@link ByteBufferSend} to the given Send consumer. After flushing the pending bytes,\n+ * another Send is passed to the consumer which wraps the underlying record-set's transfer logic.\n+ *\n+ * For example,\n+ *\n+ * <pre>\n+ *     recordsWritable.writeInt(10);\n+ *     recordsWritable.writeRecords(records1);\n+ *     recordsWritable.writeInt(20);\n+ *     recordsWritable.writeRecords(records2);\n+ *     recordsWritable.writeInt(30);\n+ *     recordsWritable.flush();\n+ * </pre>\n+ *\n+ * Will pass 5 Send objects to the consumer given in the constructor. Care must be taken by callers to flush any\n+ * pending bytes at the end of the writing sequence to ensure everything is flushed to the consumer. This class is\n+ * intended to be used with {@link org.apache.kafka.common.record.MultiRecordsSend}.\n+ *\n+ * @see org.apache.kafka.common.requests.FetchResponse\n+ */\n+public class RecordsWriter implements Writable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIwOTU2Ng==", "bodyText": "nit: not a big deal, but I feel like calling flush should really be the responsibility of write.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461209566", "createdAt": "2020-07-27T22:36:36Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -366,225 +255,128 @@ public FetchResponse(Errors error,\n                          LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n                          int throttleTimeMs,\n                          int sessionId) {\n-        this.error = error;\n-        this.responseData = responseData;\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.sessionId = sessionId;\n+        this.data = toMessage(throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        this.responseDataMap = responseData;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(Struct struct) {\n-        LinkedHashMap<TopicPartition, PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n-        for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                Struct partitionResponseHeader = partitionResponse.getStruct(PARTITION_HEADER_KEY_NAME);\n-                int partition = partitionResponseHeader.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionResponseHeader.get(ERROR_CODE));\n-                long highWatermark = partitionResponseHeader.get(HIGH_WATERMARK);\n-                long lastStableOffset = partitionResponseHeader.getOrElse(LAST_STABLE_OFFSET, INVALID_LAST_STABLE_OFFSET);\n-                long logStartOffset = partitionResponseHeader.getOrElse(LOG_START_OFFSET, INVALID_LOG_START_OFFSET);\n-                Optional<Integer> preferredReadReplica = Optional.of(\n-                    partitionResponseHeader.getOrElse(PREFERRED_READ_REPLICA, INVALID_PREFERRED_REPLICA_ID)\n-                ).filter(Predicate.isEqual(INVALID_PREFERRED_REPLICA_ID).negate());\n-\n-                BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);\n-                if (!(baseRecords instanceof MemoryRecords))\n-                    throw new IllegalStateException(\"Unknown records type found: \" + baseRecords.getClass());\n-                MemoryRecords records = (MemoryRecords) baseRecords;\n-\n-                List<AbortedTransaction> abortedTransactions = null;\n-                if (partitionResponseHeader.hasField(ABORTED_TRANSACTIONS_KEY_NAME)) {\n-                    Object[] abortedTransactionsArray = partitionResponseHeader.getArray(ABORTED_TRANSACTIONS_KEY_NAME);\n-                    if (abortedTransactionsArray != null) {\n-                        abortedTransactions = new ArrayList<>(abortedTransactionsArray.length);\n-                        for (Object abortedTransactionObj : abortedTransactionsArray) {\n-                            Struct abortedTransactionStruct = (Struct) abortedTransactionObj;\n-                            long producerId = abortedTransactionStruct.get(PRODUCER_ID);\n-                            long firstOffset = abortedTransactionStruct.get(FIRST_OFFSET);\n-                            abortedTransactions.add(new AbortedTransaction(producerId, firstOffset));\n-                        }\n-                    }\n-                }\n-\n-                PartitionData<MemoryRecords> partitionData = new PartitionData<>(error, highWatermark, lastStableOffset,\n-                        logStartOffset, preferredReadReplica, abortedTransactions, records);\n-                responseData.put(new TopicPartition(topic, partition), partitionData);\n-            }\n-        }\n-        return new FetchResponse<>(Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0)), responseData,\n-                struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME), struct.getOrElse(SESSION_ID, INVALID_SESSION_ID));\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        this.data = fetchResponseData;\n+        this.responseDataMap = toResponseDataMap(fetchResponseData);\n     }\n \n     @Override\n     public Struct toStruct(short version) {\n-        return toStruct(version, throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        return data.toStruct(version);\n     }\n \n     @Override\n-    protected Send toSend(String dest, ResponseHeader responseHeader, short apiVersion) {\n-        Struct responseHeaderStruct = responseHeader.toStruct();\n-        Struct responseBodyStruct = toStruct(apiVersion);\n-\n-        // write the total size and the response header\n-        ByteBuffer buffer = ByteBuffer.allocate(responseHeaderStruct.sizeOf() + 4);\n-        buffer.putInt(responseHeaderStruct.sizeOf() + responseBodyStruct.sizeOf());\n-        responseHeaderStruct.writeTo(buffer);\n+    public Send toSend(String dest, ResponseHeader responseHeader, short apiVersion) {\n+        // Generate the Sends for the response fields and records\n+        ArrayDeque<Send> sends = new ArrayDeque<>();\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        int totalRecordSize = data.responses().stream()\n+                .flatMap(fetchableTopicResponse -> fetchableTopicResponse.partitionResponses().stream())\n+                .mapToInt(fetchablePartitionResponse -> fetchablePartitionResponse.recordSet().sizeInBytes())\n+                .sum();\n+        int totalMessageSize = data.size(cache, apiVersion);\n+\n+        RecordsWriter writer = new RecordsWriter(dest, totalMessageSize - totalRecordSize, sends::add);\n+        data.write(writer, cache, apiVersion);\n+        writer.flush();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 480}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMDA0NA==", "bodyText": "Instead of the cast, could we add a validation check?", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461210044", "createdAt": "2020-07-27T22:37:45Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -366,225 +255,128 @@ public FetchResponse(Errors error,\n                          LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n                          int throttleTimeMs,\n                          int sessionId) {\n-        this.error = error;\n-        this.responseData = responseData;\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.sessionId = sessionId;\n+        this.data = toMessage(throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        this.responseDataMap = responseData;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(Struct struct) {\n-        LinkedHashMap<TopicPartition, PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n-        for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                Struct partitionResponseHeader = partitionResponse.getStruct(PARTITION_HEADER_KEY_NAME);\n-                int partition = partitionResponseHeader.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionResponseHeader.get(ERROR_CODE));\n-                long highWatermark = partitionResponseHeader.get(HIGH_WATERMARK);\n-                long lastStableOffset = partitionResponseHeader.getOrElse(LAST_STABLE_OFFSET, INVALID_LAST_STABLE_OFFSET);\n-                long logStartOffset = partitionResponseHeader.getOrElse(LOG_START_OFFSET, INVALID_LOG_START_OFFSET);\n-                Optional<Integer> preferredReadReplica = Optional.of(\n-                    partitionResponseHeader.getOrElse(PREFERRED_READ_REPLICA, INVALID_PREFERRED_REPLICA_ID)\n-                ).filter(Predicate.isEqual(INVALID_PREFERRED_REPLICA_ID).negate());\n-\n-                BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);\n-                if (!(baseRecords instanceof MemoryRecords))\n-                    throw new IllegalStateException(\"Unknown records type found: \" + baseRecords.getClass());\n-                MemoryRecords records = (MemoryRecords) baseRecords;\n-\n-                List<AbortedTransaction> abortedTransactions = null;\n-                if (partitionResponseHeader.hasField(ABORTED_TRANSACTIONS_KEY_NAME)) {\n-                    Object[] abortedTransactionsArray = partitionResponseHeader.getArray(ABORTED_TRANSACTIONS_KEY_NAME);\n-                    if (abortedTransactionsArray != null) {\n-                        abortedTransactions = new ArrayList<>(abortedTransactionsArray.length);\n-                        for (Object abortedTransactionObj : abortedTransactionsArray) {\n-                            Struct abortedTransactionStruct = (Struct) abortedTransactionObj;\n-                            long producerId = abortedTransactionStruct.get(PRODUCER_ID);\n-                            long firstOffset = abortedTransactionStruct.get(FIRST_OFFSET);\n-                            abortedTransactions.add(new AbortedTransaction(producerId, firstOffset));\n-                        }\n-                    }\n-                }\n-\n-                PartitionData<MemoryRecords> partitionData = new PartitionData<>(error, highWatermark, lastStableOffset,\n-                        logStartOffset, preferredReadReplica, abortedTransactions, records);\n-                responseData.put(new TopicPartition(topic, partition), partitionData);\n-            }\n-        }\n-        return new FetchResponse<>(Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0)), responseData,\n-                struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME), struct.getOrElse(SESSION_ID, INVALID_SESSION_ID));\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        this.data = fetchResponseData;\n+        this.responseDataMap = toResponseDataMap(fetchResponseData);\n     }\n \n     @Override\n     public Struct toStruct(short version) {\n-        return toStruct(version, throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n+        return data.toStruct(version);\n     }\n \n     @Override\n-    protected Send toSend(String dest, ResponseHeader responseHeader, short apiVersion) {\n-        Struct responseHeaderStruct = responseHeader.toStruct();\n-        Struct responseBodyStruct = toStruct(apiVersion);\n-\n-        // write the total size and the response header\n-        ByteBuffer buffer = ByteBuffer.allocate(responseHeaderStruct.sizeOf() + 4);\n-        buffer.putInt(responseHeaderStruct.sizeOf() + responseBodyStruct.sizeOf());\n-        responseHeaderStruct.writeTo(buffer);\n+    public Send toSend(String dest, ResponseHeader responseHeader, short apiVersion) {\n+        // Generate the Sends for the response fields and records\n+        ArrayDeque<Send> sends = new ArrayDeque<>();\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        int totalRecordSize = data.responses().stream()\n+                .flatMap(fetchableTopicResponse -> fetchableTopicResponse.partitionResponses().stream())\n+                .mapToInt(fetchablePartitionResponse -> fetchablePartitionResponse.recordSet().sizeInBytes())\n+                .sum();\n+        int totalMessageSize = data.size(cache, apiVersion);\n+\n+        RecordsWriter writer = new RecordsWriter(dest, totalMessageSize - totalRecordSize, sends::add);\n+        data.write(writer, cache, apiVersion);\n+        writer.flush();\n+\n+        // Compute the total size of all the Sends and write it out along with the header in the first Send\n+        ResponseHeaderData responseHeaderData = responseHeader.data();\n+\n+        int headerSize = responseHeaderData.size(cache, responseHeader.headerVersion());\n+        int bodySize = (int) sends.stream().mapToLong(Send::size).sum();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 486}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMTY1Nw==", "bodyText": "I'm wondering if this should be ignorable. When this is set, the leader returns no data, so it relies crucially on the follower redirecting.", "url": "https://github.com/apache/kafka/pull/9008#discussion_r461211657", "createdAt": "2020-07-27T22:42:15Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/FetchResponse.json", "diffHunk": "@@ -43,37 +43,39 @@\n   \"fields\": [\n     { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"1+\", \"ignorable\": true,\n       \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n-    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"7+\", \"ignorable\": false,\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"7+\", \"ignorable\": true,\n       \"about\": \"The top level response error code.\" },\n     { \"name\": \"SessionId\", \"type\": \"int32\", \"versions\": \"7+\", \"default\": \"0\", \"ignorable\": false,\n       \"about\": \"The fetch session ID, or 0 if this is not part of a fetch session.\" },\n-    { \"name\": \"Topics\", \"type\": \"[]FetchableTopicResponse\", \"versions\": \"0+\",\n+    { \"name\": \"Responses\", \"type\": \"[]FetchableTopicResponse\", \"versions\": \"0+\",\n       \"about\": \"The response topics.\", \"fields\": [\n-      { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+      { \"name\": \"Topic\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n         \"about\": \"The topic name.\" },\n-      { \"name\": \"Partitions\", \"type\": \"[]FetchablePartitionResponse\", \"versions\": \"0+\",\n+      { \"name\": \"PartitionResponses\", \"type\": \"[]FetchablePartitionResponse\", \"versions\": \"0+\",\n         \"about\": \"The topic partitions.\", \"fields\": [\n-        { \"name\": \"PartitionIndex\", \"type\": \"int32\", \"versions\": \"0+\",\n-          \"about\": \"The partiiton index.\" },\n-        { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n-          \"about\": \"The error code, or 0 if there was no fetch error.\" },\n-        { \"name\": \"HighWatermark\", \"type\": \"int64\", \"versions\": \"0+\",\n-          \"about\": \"The current high water mark.\" },\n-        { \"name\": \"LastStableOffset\", \"type\": \"int64\", \"versions\": \"4+\", \"default\": \"-1\", \"ignorable\": true,\n-          \"about\": \"The last stable offset (or LSO) of the partition. This is the last offset such that the state of all transactional records prior to this offset have been decided (ABORTED or COMMITTED)\" },\n-        { \"name\": \"LogStartOffset\", \"type\": \"int64\", \"versions\": \"5+\", \"default\": \"-1\", \"ignorable\": true,\n-          \"about\": \"The current log start offset.\" },\n-        { \"name\": \"Aborted\", \"type\": \"[]AbortedTransaction\", \"versions\": \"4+\", \"nullableVersions\": \"4+\", \"ignorable\": false,\n-          \"about\": \"The aborted transactions.\",  \"fields\": [\n-          { \"name\": \"ProducerId\", \"type\": \"int64\", \"versions\": \"4+\", \"entityType\": \"producerId\",\n-            \"about\": \"The producer id associated with the aborted transaction.\" },\n-          { \"name\": \"FirstOffset\", \"type\": \"int64\", \"versions\": \"4+\",\n-            \"about\": \"The first offset in the aborted transaction.\" }\n+        { \"name\":  \"PartitionHeader\", \"type\": \"PartitionHeader\", \"versions\": \"0+\",\n+          \"fields\":  [\n+          { \"name\": \"Partition\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The partition index.\" },\n+          { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+            \"about\": \"The error code, or 0 if there was no fetch error.\" },\n+          { \"name\": \"HighWatermark\", \"type\": \"int64\", \"versions\": \"0+\",\n+            \"about\": \"The current high water mark.\" },\n+          { \"name\": \"LastStableOffset\", \"type\": \"int64\", \"versions\": \"4+\", \"default\": \"-1\", \"ignorable\": true,\n+            \"about\": \"The last stable offset (or LSO) of the partition. This is the last offset such that the state of all transactional records prior to this offset have been decided (ABORTED or COMMITTED)\" },\n+          { \"name\": \"LogStartOffset\", \"type\": \"int64\", \"versions\": \"5+\", \"default\": \"-1\", \"ignorable\": true,\n+            \"about\": \"The current log start offset.\" },\n+          { \"name\": \"AbortedTransactions\", \"type\": \"[]AbortedTransaction\", \"versions\": \"4+\", \"nullableVersions\": \"4+\", \"ignorable\": true,\n+            \"about\": \"The aborted transactions.\",  \"fields\": [\n+            { \"name\": \"ProducerId\", \"type\": \"int64\", \"versions\": \"4+\", \"entityType\": \"producerId\",\n+              \"about\": \"The producer id associated with the aborted transaction.\" },\n+            { \"name\": \"FirstOffset\", \"type\": \"int64\", \"versions\": \"4+\",\n+              \"about\": \"The first offset in the aborted transaction.\" }\n+          ]},\n+          { \"name\": \"PreferredReadReplica\", \"type\": \"int32\", \"versions\": \"11+\", \"default\": \"-1\", \"ignorable\": true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78cd0124dc0573819e169c432b45929efd933665"}, "originalPosition": 53}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "507eb047ba0f652f7781d80e35c8d8c262ccb9f5", "author": {"user": {"login": "mumrah", "name": "David Arthur"}}, "url": "https://github.com/apache/kafka/commit/507eb047ba0f652f7781d80e35c8d8c262ccb9f5", "committedDate": "2020-07-29T16:31:05Z", "message": "PR feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3ODI3MTkx", "url": "https://github.com/apache/kafka/pull/9008#pullrequestreview-457827191", "createdAt": "2020-07-29T19:33:48Z", "commit": {"oid": "507eb047ba0f652f7781d80e35c8d8c262ccb9f5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1262, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}