{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc2MjMwMTU0", "number": 9232, "title": "KAFKA-9924: Add remaining property-based RocksDB metrics as described in KIP-607", "bodyText": "This commit adds the remaining property-based RocksDB metrics as described in KIP-607, except for num-entries-active-mem-table, which was added in PR #9177.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-08-31T11:14:47Z", "url": "https://github.com/apache/kafka/pull/9232", "merged": true, "mergeCommit": {"oid": "c04000cab1e98c206c5410ef68d00df1d9129182"}, "closed": true, "closedAt": "2020-09-02T22:32:18Z", "author": {"login": "cadonna"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5paXIAH2gAyNDc2MjMwMTU0OmRkNjQ3NTM1MzM5YTlkMGNjNjAwM2NhYTVmMjk3MTk1ZmQ2NGQ2MTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdEicEnAH2gAyNDc2MjMwMTU0OmZkMThhNDNiNzFkMzAwZjc2NGNlMmFlYmEzZDc1ODYyOTkwZWNjOTM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "dd647535339a9d0cc6003caa5f297195fd64d615", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/dd647535339a9d0cc6003caa5f297195fd64d615", "committedDate": "2020-07-29T11:34:40Z", "message": "Add wrapper around BlockBasedTableConfig to make cache accessible"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "634d18b15ff400dced3b1af6b43c98630e115d8d", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/634d18b15ff400dced3b1af6b43c98630e115d8d", "committedDate": "2020-07-29T11:34:40Z", "message": "Refactor RocksDBMetricsRecorder and instantiation of RocksDBMetricsRecordingTrigger"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "committedDate": "2020-07-30T08:51:12Z", "message": "Add unit test when user specifies new table format config"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0afb35d8bba0d76adf9002bd9db14025cd0ca340", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/0afb35d8bba0d76adf9002bd9db14025cd0ca340", "committedDate": "2020-07-30T10:25:54Z", "message": "Make RocksDB recording trigger member variable final"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4331821d84ecc8873f814cd691e9bb13b7762fe2", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/4331821d84ecc8873f814cd691e9bb13b7762fe2", "committedDate": "2020-07-31T13:46:51Z", "message": "Make warning regarding RocksDB's table configuration clearer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34dac91f6249d809df5b366eaf82596dde3d5b3b", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/34dac91f6249d809df5b366eaf82596dde3d5b3b", "committedDate": "2020-07-31T13:50:02Z", "message": "Remove unused parameter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91b3430b9db78b0cff834d6197f509f65a639dcd", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/91b3430b9db78b0cff834d6197f509f65a639dcd", "committedDate": "2020-08-10T20:15:57Z", "message": "Throw exception instead of log a warning"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebab7af0bb7ea98af111aa626874f678d9fe3c56", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/ebab7af0bb7ea98af111aa626874f678d9fe3c56", "committedDate": "2020-08-11T19:41:48Z", "message": "Allow other table formats than block-based tables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c967a48033bcf92438e8fc419a6a8b4835ba665", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/5c967a48033bcf92438e8fc419a6a8b4835ba665", "committedDate": "2020-08-12T08:36:07Z", "message": "Improve statistics handling in metrics recorder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb15c3a5bf91f9a46d6914739b617ca2291f52da", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/fb15c3a5bf91f9a46d6914739b617ca2291f52da", "committedDate": "2020-08-12T15:27:07Z", "message": "Change guard to avoid recording when statistics are null"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "committedDate": "2020-08-12T15:31:38Z", "message": "Add methods to add gauge metrics on state store level"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86bcf00fc34a840160029178ca3b0ca77d4f9443", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/86bcf00fc34a840160029178ca3b0ca77d4f9443", "committedDate": "2020-08-12T15:31:38Z", "message": "Add metric num-entries-active-mem-table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d6759aa75bf1ecf75de5114fb6ac8be50259535", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/8d6759aa75bf1ecf75de5114fb6ac8be50259535", "committedDate": "2020-08-13T17:22:29Z", "message": "Add methods to expose property-based RocksDB metrics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa486862db47294a18ba3c41b0e5bf172fca2cb1", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/fa486862db47294a18ba3c41b0e5bf172fca2cb1", "committedDate": "2020-08-19T08:44:06Z", "message": "Add property-based metrics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eeec8879b2ef9b21a65cfec45c7dbc9397bc27bb", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/eeec8879b2ef9b21a65cfec45c7dbc9397bc27bb", "committedDate": "2020-08-20T16:49:12Z", "message": "Add more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bbd7548f9f2a15e5d3ecec8d89e5b4ea5d515bbf", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/bbd7548f9f2a15e5d3ecec8d89e5b4ea5d515bbf", "committedDate": "2020-08-31T10:46:48Z", "message": "Merge remote-tracking branch 'upstream/trunk' into AK9924-add-metrics2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f6802d79b2f9b372060ed32dfd091615443ea9a1", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/f6802d79b2f9b372060ed32dfd091615443ea9a1", "committedDate": "2020-08-31T11:10:00Z", "message": "Remove unused parameter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9ca85b919216839d9e8d5bdb81003031a2e9462", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/f9ca85b919216839d9e8d5bdb81003031a2e9462", "committedDate": "2020-08-31T11:40:07Z", "message": "Improve code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/91d16685e1460ec903956b40f5eedf1fb21e16b9", "committedDate": "2020-08-31T11:45:26Z", "message": "Clean up code"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MDc5OTE1", "url": "https://github.com/apache/kafka/pull/9232#pullrequestreview-479079915", "createdAt": "2020-08-31T23:49:12Z", "commit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo0OToxMlrOHKOBxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo1MjozMlrOHKOFnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3NzYzOQ==", "bodyText": "Hmm, why we need the second condition to determine singleCache = false here?", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480477639", "createdAt": "2020-08-31T23:49:12Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODAyMw==", "bodyText": "nit: verifyConsistentSegmentValueProviders?", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480478023", "createdAt": "2020-08-31T23:50:26Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {\n+                singleCache = false;\n+            } else if (singleCache && cache != valueProviders.cache || !singleCache && cache == valueProviders.cache) {\n+                throw new IllegalStateException(\"Caches for store \" + storeName + \" of task \" + taskId +\n+                    \" are either not all distinct or do not all refer to the same cache. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+        }\n+    }\n+\n+    private void verifyIfSomeAreNull(final String segmentName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODYyMQ==", "bodyText": "Is this a piggy-backed fix to wrap RocksDBException here?", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480478621", "createdAt": "2020-08-31T23:52:32Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -174,22 +241,163 @@ private void initSensors(final StreamsMetricsImpl streamsMetrics, final RocksDBM\n         numberOfFileErrorsSensor = RocksDBMetrics.numberOfFileErrorsSensor(streamsMetrics, metricContext);\n     }\n \n-    private void initGauges(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext) {\n-        RocksDBMetrics.addNumEntriesActiveMemTableMetric(streamsMetrics, metricContext, (metricsConfig, now) -> {\n+    private void initGauges(final StreamsMetricsImpl streamsMetrics,\n+                            final RocksDBMetricContext metricContext) {\n+        RocksDBMetrics.addNumImmutableMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addCurSizeActiveMemTable(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addCurSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumEntriesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumDeletesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumEntriesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumDeletesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addMemTableFlushPending(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(MEMTABLE_FLUSH_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningFlushesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_FLUSHES)\n+        );\n+        RocksDBMetrics.addCompactionPendingMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(COMPACTION_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningCompactionsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_COMPACTIONS)\n+        );\n+        RocksDBMetrics.addEstimatePendingCompactionBytesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_BYTES_OF_PENDING_COMPACTION)\n+        );\n+        RocksDBMetrics.addTotalSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(TOTAL_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addLiveSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(LIVE_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addNumLiveVersionMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_LIVE_VERSIONS)\n+        );\n+        RocksDBMetrics.addEstimateNumKeysMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_NUMBER_OF_KEYS)\n+        );\n+        RocksDBMetrics.addEstimateTableReadersMemMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_MEMORY_OF_TABLE_READERS)\n+        );\n+        RocksDBMetrics.addBackgroundErrorsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_BACKGROUND_ERRORS)\n+        );\n+        RocksDBMetrics.addBlockCacheCapacityMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(CAPACITY_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCacheUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(USAGE_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCachePinnedUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(PINNED_USAGE_OF_BLOCK_CACHE)\n+        );\n+    }\n+\n+    private Gauge<BigInteger> gaugeToComputeSumOfProperties(final String propertyName) {\n+        return (metricsConfig, now) -> {\n             BigInteger result = BigInteger.valueOf(0);\n             for (final DbAndCacheAndStatistics valueProvider : storeToValueProviders.values()) {\n                 try {\n                     // values of RocksDB properties are of type unsigned long in C++, i.e., in Java we need to use\n                     // BigInteger and construct the object from the byte representation of the value\n                     result = result.add(new BigInteger(1, longToBytes(\n-                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE))));\n+                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + propertyName)\n+                    )));\n+                } catch (final RocksDBException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 261}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd18a43b71d300f764ce2aeba3d75862990ecc93", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/fd18a43b71d300f764ce2aeba3d75862990ecc93", "committedDate": "2020-09-01T07:40:22Z", "message": "Include feedback"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 848, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}