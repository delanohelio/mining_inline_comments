{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzMzE0NDg2", "number": 8218, "title": "KAFKA-9441: Unify committing within TaskManager", "bodyText": "With KIP-447 we need to commit all tasks at once using eos-beta as all tasks share the same producer. Right now, each task is committed individually (by making multiple calls to consumer.commitSync or individual calls to producer.commitTransaction for the corresponding task producer).\nTo allow for a unified commit logic, we move the commit logic into TaskManager. For non-eos, we collect all offset to be committed and do a single call to consumer.commitSync -- for eos-alpha we still commit each transaction individually (but now triggered by the TaskManager to have all code in one place).\nTo allow for a unified commit logic, we need to split existing method on Task interface in pre/post parts to allow committing in between, in particular:\n\ncommit() -> prepareCommit() and postCommit()\nsuspend() -> prepareSuspend() and suspend() (we keep the name as it still does suspend the task)\ncloseClean() -> prepareCloseClean() and closeClean(Map<TopicPartitions, Long> checkpoint) (we keep the name as it still does close the task)\ncloseDirty() -> prepareCloseDirty() and closeDirty() (we keep the name as it still does close the task)\n\nprepareCloseClean() returns checkpoint information to allow checkpointing after the TaskManager did the commit within closeClean().\nIn a follow up PR, we will introduce eso-beta and commit a single transaction over all task using the shared producer.\nCall for review @guozhangwang @abbccdda", "createdAt": "2020-03-04T03:10:26Z", "url": "https://github.com/apache/kafka/pull/8218", "merged": true, "mergeCommit": {"oid": "89cd2f2a0b21368297323437fd15ba6341e4707b"}, "closed": true, "closedAt": "2020-03-19T18:31:52Z", "author": {"login": "mjsax"}, "timelineItems": {"totalCount": 40, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcKOWCaAFqTM2ODQ3OTEzMA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcPH7AeAH2gAyMzgzMzE0NDg2OjRiMDNhNDg3YTUxODc1NmY3YzlhY2QxZjMxNzdiMTJiNDA3ODY3ZDU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4NDc5MTMw", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-368479130", "createdAt": "2020-03-04T03:11:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMzoxMToyN1rOFxesrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMzoyNjowMlrOFxe46Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyNzUwMA==", "bodyText": "This is not really relevant for this PR, but we need to add it for KIP-447 eventually, thus I just include it in this PR.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387427500", "createdAt": "2020-03-04T03:11:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -275,6 +277,18 @@\n     @SuppressWarnings(\"WeakerAccess\")\n     public static final String UPGRADE_FROM_23 = \"2.3\";\n \n+    /**\n+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.4.x}.\n+     */\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String UPGRADE_FROM_24 = \"2.4\";\n+\n+    /**\n+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.5.x}.\n+     */\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String UPGRADE_FROM_25 = \"2.5\";\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyNzYzOA==", "bodyText": "We moved this to TaskManager", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387427638", "createdAt": "2020-03-04T03:12:04Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java", "diffHunk": "@@ -45,8 +44,6 @@\n                      final Serializer<V> valueSerializer,\n                      final StreamPartitioner<? super K, ? super V> partitioner);\n \n-    void commit(final Map<TopicPartition, OffsetAndMetadata> offsets);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyNzgyMw==", "bodyText": "On suspend() and prepareCommit() we don't commit yet, but return the offsets that need to be committed", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387427823", "createdAt": "2020-03-04T03:12:59Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -102,8 +103,9 @@ public void completeRestoration() {\n     }\n \n     @Override\n-    public void suspend() {\n+    public Map<TopicPartition, OffsetAndMetadata> suspend() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyNzkxMQ==", "bodyText": "We don't commit and thus don't throw any longer", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387427911", "createdAt": "2020-03-04T03:13:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -294,29 +296,24 @@ public void resume() {\n         }\n     }\n \n-    /**\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyODQxNg==", "bodyText": "Frankly, not sure if this is correct any longer. What do we want to record with this sensor exactly? Flushing can be expensive and we might want to record it as part of committing -- but I am not sure.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387428416", "createdAt": "2020-03-04T03:15:41Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -338,12 +335,14 @@ private void commitState() {\n         final long startNs = time.nanoseconds();\n \n         stateMgr.flush();\n-\n         recordCollector.flush();\n \n-        // we need to preserve the original partitions times before calling commit\n-        // because all partition times are reset to -1 during close\n+        commitSensor.record(time.nanoseconds() - startNs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyODc4Ng==", "bodyText": "I am not happy with this rewrite (but as I know that John did some changes in this class in another PR, I just did this hack her for now -- needs some cleanup after a rebase)", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387428786", "createdAt": "2020-03-04T03:17:11Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -59,9 +61,8 @@\n     private boolean transactionInFlight = false;\n     private boolean transactionInitialized = false;\n \n-    public StreamsProducer(final LogContext logContext,\n-                           final Producer<byte[], byte[]> producer) {\n-        this(logContext, producer, null, null);\n+    public StreamsProducer(final Producer<byte[], byte[]> producer) {\n+        this(null, producer, null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyOTA2Nw==", "bodyText": "We could also do a second loop over all tasks, after calling commit(..) below -- not sure if this is ok as-is?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387429067", "createdAt": "2020-03-04T03:18:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -481,14 +486,16 @@ int commitAll() {\n         if (rebalanceInProgress) {\n             return -1;\n         } else {\n-            int commits = 0;\n+            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n             for (final Task task : tasks.values()) {\n                 if (task.commitNeeded()) {\n-                    task.commit();\n-                    commits++;\n+                    consumedOffsetsAndMetadataPerTask.put(task.id(), task.prepareCommit());\n+                    task.markCommitted();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQzMDU1Ng==", "bodyText": "Moved both tests to TaskManagerTest", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387430556", "createdAt": "2020-03-04T03:25:43Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -239,49 +231,6 @@ public void shouldPassThroughRecordHeaderToSerializer() {\n         }\n     }\n \n-    @Test\n-    public void shouldCommitViaConsumerIfEosDisabled() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQzMDYzMw==", "bodyText": "move all 4 tests to TaskManagerTest", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387430633", "createdAt": "2020-03-04T03:26:02Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -506,103 +445,12 @@ public void shouldThrowStreamsExceptionOnSubsequentCallIfFatalEvenWithContinueEx\n         assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n     }\n \n-    @Test\n-    public void shouldThrowTaskMigratedExceptionOnCommitFailed() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTk3Mzk1", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369197395", "createdAt": "2020-03-04T23:23:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyMzoxM1rOFyBRkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyMzoxM1rOFyBRkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NDAwMQ==", "bodyText": "We need to allow committing in SUSPENDED state now as we first suspend all tasks and than commit. Cf. TaskManager#handleRevocation()", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387994001", "createdAt": "2020-03-04T23:23:13Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -294,29 +295,19 @@ public void resume() {\n         }\n     }\n \n-    /**\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n     @Override\n-    public void commit() {\n+    public Map<TopicPartition, OffsetAndMetadata> prepareCommit() {\n         switch (state()) {\n             case RUNNING:\n             case RESTORING:\n-                commitState();\n-\n-                // this is an optimization for non-EOS only\n-                if (eosDisabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n-                }\n-\n-                log.info(\"Committed\");\n+            case SUSPENDED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTk3Njkz", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369197693", "createdAt": "2020-03-04T23:23:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyMzo1OVrOFyBSnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyMzo1OVrOFyBSnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NDI2OQ==", "bodyText": "MInor improvement: we include writing the checkpoint and the caller can indicate if it should be written or not.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387994269", "createdAt": "2020-03-04T23:23:59Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -334,16 +325,22 @@ public void commit() {\n      * @throws TaskMigratedException if committing offsets failed (non-EOS)\n      *                               or if the task producer got fenced (EOS)\n      */\n-    private void commitState() {\n+    private void commitState(final boolean writeCheckpoint) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTk4MjQ4", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369198248", "createdAt": "2020-03-04T23:25:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyNToxNVrOFyBUSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyNToxNVrOFyBUSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NDY5Nw==", "bodyText": "This issues was introduced in the PR that introduced StreamsProducer -- we forgot to close them. Fixing this on the side.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387994697", "createdAt": "2020-03-04T23:25:15Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -409,6 +411,10 @@ public void close() {\n                 } catch (final Throwable e) {\n                     log.error(\"Failed to close producer due to the following error:\", e);\n                 }\n+            } else {\n+                for (final StreamsProducer streamsProducer : taskProducers.values()) {\n+                    streamsProducer.close();\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTk4NjQ1", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369198645", "createdAt": "2020-03-04T23:26:10Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyNjoxMFrOFyBVpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyNjoxMFrOFyBVpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NTA0NA==", "bodyText": "We call closeClean below -- just fixing the comment here for now (\\cc @guozhangwang)\nNote that we don't commit offsets for this case any longer -- previously, committing offsets \"might\" have been done with closeClean() (even if I believe that the task would be marked as \"commitNeeded == false\"). We don't let the TaskManager commit offsets here, as it should not be required.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387995044", "createdAt": "2020-03-04T23:26:10Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -138,7 +142,7 @@ void handleCorruption(final Map<TaskId, Set<TopicPartition>> taskWithChangelogs)\n             // this call is idempotent so even if the task is only CREATED we can still call it\n             changelogReader.remove(task.changelogPartitions());\n \n-            // mark corrupted partitions to not be checkpointed, and then close the task as dirty\n+            // mark corrupted partitions to not be checkpointed, and then close the task and revive the task", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTk5NjM1", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369199635", "createdAt": "2020-03-04T23:28:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyODoyNlrOFyBY4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyODoyNlrOFyBY4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NTg3NQ==", "bodyText": "Similar to above: this issue was introduced in the StreamsProducer PR. We nee to close the producer when we remove it.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387995875", "createdAt": "2020-03-04T23:28:26Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -194,7 +198,9 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                     // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n                     task.closeDirty();\n                 } finally {\n-                    taskProducers.remove(task.id());\n+                    if (taskProducers.containsKey(task.id())) {\n+                        taskProducers.remove(task.id()).close();\n+                    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTk5NzQ5", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369199749", "createdAt": "2020-03-04T23:28:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyODo0NFrOFyBZOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyODo0NFrOFyBZOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NTk2MQ==", "bodyText": "as above", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387995961", "createdAt": "2020-03-04T23:28:44Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -345,7 +352,9 @@ void handleLostAll() {\n                 cleanupTask(task);\n                 task.closeDirty();\n                 iterator.remove();\n-                taskProducers.remove(task.id());\n+                if (taskProducers.containsKey(task.id())) {\n+                    taskProducers.remove(task.id()).close();\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTk5OTY5", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369199969", "createdAt": "2020-03-04T23:29:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyOToxNlrOFyBZ5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzoyOToxNlrOFyBZ5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NjEzMg==", "bodyText": "Not sure why we use an iterator here. Simplifying the code with a for-loop", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387996132", "createdAt": "2020-03-04T23:29:16Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -401,9 +410,12 @@ private void cleanupTask(final Task task) {\n \n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n-        final Iterator<Task> iterator = tasks.values().iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MjAwNzUy", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369200752", "createdAt": "2020-03-04T23:31:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzozMToxNlrOFyBcfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzozMToxNlrOFyBcfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5Njc5Nw==", "bodyText": "We need to commit explicitly in TTD now to mimic the TaskManger. Hence, we need access to the consumer and streamsProducer", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387996797", "createdAt": "2020-03-04T23:31:16Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -213,7 +214,9 @@\n     ProcessorTopology processorTopology;\n     ProcessorTopology globalTopology;\n \n+    private final MockConsumer<byte[], byte[]> consumer;\n     private final MockProducer<byte[], byte[]> producer;\n+    private final StreamsProducer streamsProducer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MjIyMDU3", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369222057", "createdAt": "2020-03-05T00:30:01Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTg0NjUz", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-369184653", "createdAt": "2020-03-04T22:54:42Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMjo1NDo0MlrOFyAo5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwMToyNTo1OFrOFyDhdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk4MzU5MA==", "bodyText": "Did you already update the KIP for the new config?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387983590", "createdAt": "2020-03-04T22:54:42Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -283,10 +297,26 @@\n \n     /**\n      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG \"processing.guarantee\"} for exactly-once processing guarantees.\n+     * <p>\n+     * Enabling exactly-once processing semantics requires broker version 0.11.0 or higher.\n+     * If you enable this feature, Kafka Streams will use a producer per task\n+     * (instead a producer per thread as for the {@link #AT_LEAST_ONCE} case).\n+     *\n+     * @see #EXACTLY_ONCE_BETA\n      */\n     @SuppressWarnings(\"WeakerAccess\")\n     public static final String EXACTLY_ONCE = \"exactly_once\";\n \n+    /**\n+     * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG \"processing.guarantee\"} for exactly-once processing guarantees.\n+     * <p>\n+     * Enabling exactly-once (beta) requires broker version 2.5 or higher.\n+     * In contrast to {@link #EXACTLY_ONCE} Kafka Streams uses a producer per thread model,\n+     * similar to the {@link #AT_LEAST_ONCE} case.\n+     */\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String EXACTLY_ONCE_BETA = \"exactly_once_beta\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk4NDI3MA==", "bodyText": "What's the benefit of building this as a static helper?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r387984270", "createdAt": "2020-03-04T22:56:19Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -707,6 +710,35 @@ public static String getSharedAdminClientId(final String clientId) {\n         return clientId + \"-admin\";\n     }\n \n+    static boolean eosAlphaEnabled(final StreamsConfig config) {\n+        return EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+    }\n+\n+    public static boolean eosBetaEnabled(final StreamsConfig config) {\n+        return EXACTLY_ONCE_BETA.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+    }\n+\n+    public static boolean eosEnabled(final StreamsConfig config) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxNTg1Nw==", "bodyText": "Why do we start to suppress warnings?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388015857", "createdAt": "2020-03-05T00:33:39Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -504,8 +497,7 @@ public boolean isProcessable(final long wallClockTime) {\n      * @return true if this method processes a record, false if it does not process a record.\n      * @throws TaskMigratedException if the task producer got fenced (EOS only)\n      */\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxNjUyMw==", "bodyText": "Sounds good, just mark that depending on John's fix, we probably don't need to handle this.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388016523", "createdAt": "2020-03-05T00:35:52Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -409,6 +411,10 @@ public void close() {\n                 } catch (final Throwable e) {\n                     log.error(\"Failed to close producer due to the following error:\", e);\n                 }\n+            } else {\n+                for (final StreamsProducer streamsProducer : taskProducers.values()) {\n+                    streamsProducer.close();\n+                }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NDY5Nw=="}, "originalCommit": null, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxODAxNQ==", "bodyText": "Add a comment describing the new return statement.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388018015", "createdAt": "2020-03-05T00:41:10Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -232,17 +232,16 @@ public void completeRestoration() {\n      *                               or if the task producer got fenced (EOS)\n      */\n     @Override\n-    public void suspend() {\n+    public Map<TopicPartition, OffsetAndMetadata> suspend() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxOTQyMw==", "bodyText": "I would prefer a second loop to guarantee a consistent reflection on the task committed state.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388019423", "createdAt": "2020-03-05T00:45:58Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -481,14 +486,16 @@ int commitAll() {\n         if (rebalanceInProgress) {\n             return -1;\n         } else {\n-            int commits = 0;\n+            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n             for (final Task task : tasks.values()) {\n                 if (task.commitNeeded()) {\n-                    task.commit();\n-                    commits++;\n+                    consumedOffsetsAndMetadataPerTask.put(task.id(), task.prepareCommit());\n+                    task.markCommitted();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyOTA2Nw=="}, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxOTgyMA==", "bodyText": "In EOS beta, we should be able to send out a batch commit instead of individual ones?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388019820", "createdAt": "2020-03-05T00:47:32Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -500,17 +514,43 @@ int maybeCommitActiveTasksPerUserRequested() {\n         if (rebalanceInProgress) {\n             return -1;\n         } else {\n-            int commits = 0;\n+            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n             for (final Task task : activeTaskIterable()) {\n                 if (task.commitRequested() && task.commitNeeded()) {\n-                    task.commit();\n-                    commits++;\n+                    consumedOffsetsAndMetadataPerTask.put(task.id(), task.prepareCommit());\n+                    task.markCommitted();\n                 }\n             }\n-            return commits;\n+            commit(consumedOffsetsAndMetadataPerTask);\n+\n+            return consumedOffsetsAndMetadataPerTask.size();\n         }\n     }\n \n+    private void commit(final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> offsetsPerTask) {\n+        if (eosEnabled) {\n+            for (final Map.Entry<TaskId, Map<TopicPartition, OffsetAndMetadata>> taskToCommit : offsetsPerTask.entrySet()) {\n+                taskProducers.get(taskToCommit.getKey()).commitTransaction(taskToCommit.getValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyMjEzMg==", "bodyText": "I don't think we need to test assertFalse(task.commitNeeded() as its outcome is controlled by task.markCommitted. So we only need to do it once.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388022132", "createdAt": "2020-03-05T00:55:25Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -684,44 +686,47 @@ public void shouldRespectCommitNeeded() {\n         assertTrue(task.process(0L));\n         assertTrue(task.commitNeeded());\n \n-        task.commit();\n+        task.prepareCommit();\n+        assertTrue(task.commitNeeded());\n+\n+        task.markCommitted();\n         assertFalse(task.commitNeeded());\n \n         assertTrue(task.maybePunctuateStreamTime());\n         assertTrue(task.commitNeeded());\n \n-        task.commit();\n+        task.prepareCommit();\n+        assertTrue(task.commitNeeded());\n+\n+        task.markCommitted();\n         assertFalse(task.commitNeeded());\n \n         time.sleep(10);\n         assertTrue(task.maybePunctuateSystemTime());\n         assertTrue(task.commitNeeded());\n \n-        task.commit();\n+        task.prepareCommit();\n+        assertTrue(task.commitNeeded());\n+\n+        task.markCommitted();\n         assertFalse(task.commitNeeded());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyMjQxOQ==", "bodyText": "Why do we no longer have the mock verification?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388022419", "createdAt": "2020-03-05T00:56:24Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -733,9 +738,9 @@ public void shouldCommitConsumerPositionIfRecordQueueIsEmpty() {\n \n         task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0L)));\n         task.process(0L);\n-        task.commit();\n+        final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = task.prepareCommit();\n \n-        verify(recordCollector);\n+        assertThat(offsetsAndMetadata, equalTo(mkMap(mkEntry(partition1, new OffsetAndMetadata(3L, encodeTimestamp(0L))))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyNjY5MA==", "bodyText": "Should we do expectLastCall here?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388026690", "createdAt": "2020-03-05T01:11:22Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -441,10 +458,54 @@ public void shouldCommitActiveAndStandbyTasks() {\n     }\n \n     @Test\n+    public void shouldCommitViaConsumerIfEosDisabled() {\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setCommitNeeded();\n+        taskManager.tasks().put(taskId01, task01);\n+\n+        consumer.commitSync(new HashMap<>());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyODY2NQ==", "bodyText": "We should also verify the thrown cause", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388028665", "createdAt": "2020-03-05T01:18:20Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -812,7 +877,79 @@ public void closeClean() {\n         assertThat(thrown.getCause().getMessage(), equalTo(null));\n     }\n \n+    @Test\n+    public void shouldThrowTaskMigratedExceptionOnCommitFailed() {\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setCommitNeeded();\n+        taskManager.tasks().put(taskId01, task01);\n+\n+        consumer.commitSync(new HashMap<>());\n+        expectLastCall().andThrow(new CommitFailedException());\n+        replay(consumer);\n+\n+        final TaskMigratedException thrown = assertThrows(\n+            TaskMigratedException.class,\n+            () -> taskManager.commitAll()\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"Consumer committing offsets failed, indicating the corresponding thread is no longer part of the group; it means all tasks belonging to this thread should be migrated.\"));\n+    }\n+\n+    @Test\n+    public void shouldThrowStreamsExceptionOnCommitTimeout() {\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setCommitNeeded();\n+        taskManager.tasks().put(taskId01, task01);\n+\n+        consumer.commitSync(new HashMap<>());\n+        expectLastCall().andThrow(new TimeoutException());\n+        replay(consumer);\n+\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> taskManager.commitAll()\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"Timed out while committing offsets via consumer\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyODgyMA==", "bodyText": "Same here, for verifying the thrown cause", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388028820", "createdAt": "2020-03-05T01:18:46Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -812,7 +877,79 @@ public void closeClean() {\n         assertThat(thrown.getCause().getMessage(), equalTo(null));\n     }\n \n+    @Test\n+    public void shouldThrowTaskMigratedExceptionOnCommitFailed() {\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setCommitNeeded();\n+        taskManager.tasks().put(taskId01, task01);\n+\n+        consumer.commitSync(new HashMap<>());\n+        expectLastCall().andThrow(new CommitFailedException());\n+        replay(consumer);\n+\n+        final TaskMigratedException thrown = assertThrows(\n+            TaskMigratedException.class,\n+            () -> taskManager.commitAll()\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"Consumer committing offsets failed, indicating the corresponding thread is no longer part of the group; it means all tasks belonging to this thread should be migrated.\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyOTE2NA==", "bodyText": "What's the reasoning her for only wrapping the consumer offset commit case here, not for EOS case?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388029164", "createdAt": "2020-03-05T01:20:06Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -500,17 +514,43 @@ int maybeCommitActiveTasksPerUserRequested() {\n         if (rebalanceInProgress) {\n             return -1;\n         } else {\n-            int commits = 0;\n+            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n             for (final Task task : activeTaskIterable()) {\n                 if (task.commitRequested() && task.commitNeeded()) {\n-                    task.commit();\n-                    commits++;\n+                    consumedOffsetsAndMetadataPerTask.put(task.id(), task.prepareCommit());\n+                    task.markCommitted();\n                 }\n             }\n-            return commits;\n+            commit(consumedOffsetsAndMetadataPerTask);\n+\n+            return consumedOffsetsAndMetadataPerTask.size();\n         }\n     }\n \n+    private void commit(final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> offsetsPerTask) {\n+        if (eosEnabled) {\n+            for (final Map.Entry<TaskId, Map<TopicPartition, OffsetAndMetadata>> taskToCommit : offsetsPerTask.entrySet()) {\n+                taskProducers.get(taskToCommit.getKey()).commitTransaction(taskToCommit.getValue());\n+            }\n+        } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyOTQ3MQ==", "bodyText": "Always feels better for one less parameter :)", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388029471", "createdAt": "2020-03-05T01:21:10Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java", "diffHunk": "@@ -200,8 +200,7 @@ private KeyValueStoreTestDriver(final StateSerdes<K, V> serdes) {\n         final RecordCollector recordCollector = new RecordCollectorImpl(\n             logContext,\n             new TaskId(0, 0),\n-            consumer,\n-            new StreamsProducer(logContext, producer),\n+            new StreamsProducer(producer),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyOTk1OA==", "bodyText": "Makes sense to me.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388029958", "createdAt": "2020-03-05T01:22:58Z", "author": {"login": "abbccdda"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -213,7 +214,9 @@\n     ProcessorTopology processorTopology;\n     ProcessorTopology globalTopology;\n \n+    private final MockConsumer<byte[], byte[]> consumer;\n     private final MockProducer<byte[], byte[]> producer;\n+    private final StreamsProducer streamsProducer;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5Njc5Nw=="}, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAzMDI1MA==", "bodyText": "Checkmark for proving the 6 tests are all migrated.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388030250", "createdAt": "2020-03-05T01:23:57Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -506,103 +445,12 @@ public void shouldThrowStreamsExceptionOnSubsequentCallIfFatalEvenWithContinueEx\n         assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n     }\n \n-    @Test\n-    public void shouldThrowTaskMigratedExceptionOnCommitFailed() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQzMDYzMw=="}, "originalCommit": null, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAzMDgzNw==", "bodyText": "Probably need to change after rebase", "url": "https://github.com/apache/kafka/pull/8218#discussion_r388030837", "createdAt": "2020-03-05T01:25:58Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -194,7 +198,9 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                     // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n                     task.closeDirty();\n                 } finally {\n-                    taskProducers.remove(task.id());\n+                    if (taskProducers.containsKey(task.id())) {\n+                        taskProducers.remove(task.id()).close();\n+                    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzk5NTg3NQ=="}, "originalCommit": null, "originalPosition": 60}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDM3NTEw", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372437510", "createdAt": "2020-03-11T02:20:42Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMDo0MlrOF0nLkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMDo0MlrOF0nLkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjIxMQ==", "bodyText": "This is an open question: we don't want to remove this sensor however it was unclear to me how to handle this metric after we split \"task committing\" into three steps (prepareCommit; taskManager#commit; postCommit).", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390712211", "createdAt": "2020-03-11T02:20:42Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -92,7 +92,6 @@\n     private final Sensor closeTaskSensor;\n     private final Sensor processLatencySensor;\n     private final Sensor punctuateLatencySensor;\n-    private final Sensor commitSensor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDM3OTQ2", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372437946", "createdAt": "2020-03-11T02:22:12Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMjoxM1rOF0nM-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMjoxM1rOF0nM-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjU3MQ==", "bodyText": "Simplification to avoid passing in eosEnabled and reducing constructor parameter list -- we just piggy back on the application.id that shall be null for non-eos.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390712571", "createdAt": "2020-03-11T02:22:13Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -58,25 +58,24 @@\n     private boolean transactionInitialized = false;\n \n     public StreamsProducer(final Producer<byte[], byte[]> producer,\n-                           final boolean eosEnabled,\n-                           final LogContext logContext,\n-                           final String applicationId) {\n-        log = logContext.logger(getClass());\n+                           final String applicationId,\n+                           final LogContext logContext) {\n+        log = Objects.requireNonNull(logContext, \"logContext cannot be null\").logger(getClass());\n         logPrefix = logContext.logPrefix().trim();\n \n         this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n         this.applicationId = applicationId;\n-        this.eosEnabled = eosEnabled;\n+        this.eosEnabled = applicationId != null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDM4MDYy", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372438062", "createdAt": "2020-03-11T02:22:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMjozNVrOF0nNWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMjozNVrOF0nNWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjY2NQ==", "bodyText": "Avoid redundant logging.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390712665", "createdAt": "2020-03-11T02:22:35Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -58,25 +58,24 @@\n     private boolean transactionInitialized = false;\n \n     public StreamsProducer(final Producer<byte[], byte[]> producer,\n-                           final boolean eosEnabled,\n-                           final LogContext logContext,\n-                           final String applicationId) {\n-        log = logContext.logger(getClass());\n+                           final String applicationId,\n+                           final LogContext logContext) {\n+        log = Objects.requireNonNull(logContext, \"logContext cannot be null\").logger(getClass());\n         logPrefix = logContext.logPrefix().trim();\n \n         this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n         this.applicationId = applicationId;\n-        this.eosEnabled = eosEnabled;\n+        this.eosEnabled = applicationId != null;\n     }\n \n     private String formatException(final String message) {\n-        return message + \" [\" + logPrefix + \", \" + (eosEnabled ? \"eos\" : \"alo\") + \"]\";\n+        return message + \" [\" + logPrefix + \"]\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDM4MjAy", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372438202", "createdAt": "2020-03-11T02:23:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMzowNlrOF0nNwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMzowNlrOF0nNwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjc3MA==", "bodyText": "Side cleanup: All those method can actually be package private.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390712770", "createdAt": "2020-03-11T02:23:06Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -58,25 +58,24 @@\n     private boolean transactionInitialized = false;\n \n     public StreamsProducer(final Producer<byte[], byte[]> producer,\n-                           final boolean eosEnabled,\n-                           final LogContext logContext,\n-                           final String applicationId) {\n-        log = logContext.logger(getClass());\n+                           final String applicationId,\n+                           final LogContext logContext) {\n+        log = Objects.requireNonNull(logContext, \"logContext cannot be null\").logger(getClass());\n         logPrefix = logContext.logPrefix().trim();\n \n         this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n         this.applicationId = applicationId;\n-        this.eosEnabled = eosEnabled;\n+        this.eosEnabled = applicationId != null;\n     }\n \n     private String formatException(final String message) {\n-        return message + \" [\" + logPrefix + \", \" + (eosEnabled ? \"eos\" : \"alo\") + \"]\";\n+        return message + \" [\" + logPrefix + \"]\";\n     }\n \n     /**\n      * @throws IllegalStateException if EOS is disabled\n      */\n-    public void initTransaction() {\n+    void initTransaction() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDM4NDM2", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372438436", "createdAt": "2020-03-11T02:23:55Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMzo1NVrOF0nOiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMzo1NVrOF0nOiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjk3MQ==", "bodyText": "Removing this state -- this is an open question if I did this correctly. \\cc @vvcephei", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390712971", "createdAt": "2020-03-11T02:23:55Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -71,7 +72,6 @@\n         RESTORING(2, 3, 4),    // 1\n         RUNNING(3, 4),         // 2\n         SUSPENDED(1, 4),       // 3\n-        CLOSING(4, 5),         // 4, we allow CLOSING to transit to itself to make close idempotent", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDM5Njgz", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372439683", "createdAt": "2020-03-11T02:27:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyNzo1OVrOF0nSqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyNzo1OVrOF0nSqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDAyNw==", "bodyText": "After we addressed the question how we want to do metrics, we can update this tests", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390714027", "createdAt": "2020-03-11T02:27:59Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -385,30 +387,30 @@ public void shouldConstructMetricsWithBuiltInMetricsVersionLatest() {\n     private void testMetrics(final String builtInMetricsVersion) {\n         task = createStatelessTask(createConfig(false, \"100\"), builtInMetricsVersion);\n \n-        assertNotNull(getMetric(\n-            \"commit\",\n-            \"%s-latency-avg\",\n-            task.id().toString(),\n-            builtInMetricsVersion\n-        ));\n-        assertNotNull(getMetric(\n-            \"commit\",\n-            \"%s-latency-max\",\n-            task.id().toString(),\n-            builtInMetricsVersion\n-        ));\n-        assertNotNull(getMetric(\n-            \"commit\",\n-            \"%s-rate\",\n-            task.id().toString(),\n-            builtInMetricsVersion\n-        ));\n-        assertNotNull(getMetric(\n-            \"commit\",\n-            \"%s-total\",\n-            task.id().toString(),\n-            builtInMetricsVersion\n-        ));\n+//        assertNotNull(getMetric(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDQwMjI5", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372440229", "createdAt": "2020-03-11T02:29:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyOTo1OVrOF0nUeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyOTo1OVrOF0nUeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDQ4OA==", "bodyText": "Because we make app method in StreamsProducer package private but need access to commit() we add TestDriverProducer to get access.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390714488", "createdAt": "2020-03-11T02:29:59Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -213,7 +214,9 @@\n     ProcessorTopology processorTopology;\n     ProcessorTopology globalTopology;\n \n+    private final MockConsumer<byte[], byte[]> consumer;\n     private final MockProducer<byte[], byte[]> producer;\n+    private final TestDriverProducer testDriverProducer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDQwNTM3", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372440537", "createdAt": "2020-03-11T02:31:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozMTowOVrOF0nVdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozMTowOVrOF0nVdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDc0MQ==", "bodyText": "Just added to give public access to commitTransaction() to TTD (it's more elegant than making StreamsProducer#commitTransaction public IMHO)", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390714741", "createdAt": "2020-03-11T02:31:09Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/processor/internals/TestDriverProducer.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.utils.LogContext;\n+\n+import java.util.Map;\n+\n+public class TestDriverProducer extends StreamsProducer {\n+\n+    public TestDriverProducer(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final LogContext logContext) {\n+        super(producer, applicationId, logContext);\n+    }\n+\n+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNDU0OTUz", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-372454953", "createdAt": "2020-03-11T03:27:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMzoyNzowN1rOF0oG8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQyMjoyMDo0MFrOF1LSIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyNzQwOQ==", "bodyText": "nit: we could log thread-id here for easier log search.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r390727409", "createdAt": "2020-03-11T03:27:07Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -80,32 +80,42 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                       final KafkaClientSupplier clientSupplier,\n                       final String threadId,\n                       final Logger log) {\n-        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n         this.builder = builder;\n         this.config = config;\n         this.streamsMetrics = streamsMetrics;\n         this.stateDirectory = stateDirectory;\n         this.storeChangelogReader = storeChangelogReader;\n+        this.cache = cache;\n         this.time = time;\n+        this.clientSupplier = clientSupplier;\n+        this.threadId = threadId;\n         this.log = log;\n \n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+\n         if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n             threadProducer = null;\n             taskProducers = new HashMap<>();\n         } else {\n+            log.info(\"Creating thread producer client\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIzMzc3Mw==", "bodyText": "Could we internalize this state check inside the task to simplify the logic here?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391233773", "createdAt": "2020-03-11T20:05:06Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -182,30 +193,49 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 cleanupTask(task);\n \n                 try {\n-                    task.closeClean();\n+                    checkpointPerTask.put(task, task.prepareCloseClean());\n+                    if (task.state() != CREATED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIzNDI0OA==", "bodyText": "Similarly here, this state check could be internalized.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391234248", "createdAt": "2020-03-11T20:05:36Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -320,13 +363,27 @@ boolean tryToCompleteRestoration() {\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n         final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n \n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n         for (final Task task : tasks.values()) {\n             if (remainingPartitions.containsAll(task.inputPartitions())) {\n-                task.suspend();\n+                task.prepareSuspend();\n+                if (task.state() != CREATED) {\n+                    consumedOffsetsAndMetadataPerTask.put(task.id(), task.committableOffsetsAndMetadata());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIzNjUwOQ==", "bodyText": "nit: let's order the functions as\nprepareCloseClean\ncloseClean\nprepareCloseDirty\ncloseDirty", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391236509", "createdAt": "2020-03-11T20:07:57Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -123,35 +123,41 @@ public boolean isValidTransition(final State newState) {\n     boolean commitNeeded();\n \n     /**\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void commit();\n+    void prepareCommit();\n+\n+    void postCommit();\n \n     /**\n      * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void suspend();\n+    void prepareSuspend();\n \n+    void suspend();\n     /**\n+     *\n      * @throws StreamsException fatal error, should close the thread\n      */\n     void resume();\n \n     /**\n-     * Close a task that we still own. Commit all progress and close the task gracefully.\n+     * Close a task that we still own and prepare it for committing\n      * Throws an exception if this couldn't be done.\n      *\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void closeClean();\n+    Map<TopicPartition, Long> prepareCloseClean();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIzNzA3OA==", "bodyText": "Prepare to uncleanly close a task that we may not own.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391237078", "createdAt": "2020-03-11T20:08:36Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -123,35 +123,41 @@ public boolean isValidTransition(final State newState) {\n     boolean commitNeeded();\n \n     /**\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void commit();\n+    void prepareCommit();\n+\n+    void postCommit();\n \n     /**\n      * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void suspend();\n+    void prepareSuspend();\n \n+    void suspend();\n     /**\n+     *\n      * @throws StreamsException fatal error, should close the thread\n      */\n     void resume();\n \n     /**\n-     * Close a task that we still own. Commit all progress and close the task gracefully.\n+     * Close a task that we still own and prepare it for committing\n      * Throws an exception if this couldn't be done.\n      *\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void closeClean();\n+    Map<TopicPartition, Long> prepareCloseClean();\n \n     /**\n      * Close a task that we may not own. Discard any uncommitted progress and close the task.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0MTUyOA==", "bodyText": "Having a prepareCloseDirty makes the calling of closeDirty a bit cumbersome as we always need to call prepareCloseDirty first. To simplify or just do a reminder, I have two suggestions:\n\nInternally create a task state called PREPARE_CLOSE or just a boolean like closeDirtyPrepared as the state check, so that closeDirty will throw illegal state if the flag is false\nFollowing #1, instead of throw, if we don't see the prepareClose is being called, the closeDirty will invoke prepareCloseDirty first internally.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391241528", "createdAt": "2020-03-11T20:13:52Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -144,6 +149,7 @@ void handleCorruption(final Map<TaskId, Collection<TopicPartition>> taskWithChan\n             final Collection<TopicPartition> corruptedPartitions = entry.getValue();\n             task.markChangelogAsCorrupted(corruptedPartitions);\n \n+            task.prepareCloseDirty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0MzkxMA==", "bodyText": "@guozhangwang For my own education, why we do and here instead of just checking commitRequested?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391243910", "createdAt": "2020-03-11T20:17:03Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -555,14 +653,46 @@ int maybeCommitActiveTasksPerUserRequested() {\n         if (rebalanceInProgress) {\n             return -1;\n         } else {\n-            int commits = 0;\n+            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n             for (final Task task : activeTaskIterable()) {\n                 if (task.commitRequested() && task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0NDcyOQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391244729", "createdAt": "2020-03-11T20:18:43Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -500,17 +514,43 @@ int maybeCommitActiveTasksPerUserRequested() {\n         if (rebalanceInProgress) {\n             return -1;\n         } else {\n-            int commits = 0;\n+            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n             for (final Task task : activeTaskIterable()) {\n                 if (task.commitRequested() && task.commitNeeded()) {\n-                    task.commit();\n-                    commits++;\n+                    consumedOffsetsAndMetadataPerTask.put(task.id(), task.prepareCommit());\n+                    task.markCommitted();\n                 }\n             }\n-            return commits;\n+            commit(consumedOffsetsAndMetadataPerTask);\n+\n+            return consumedOffsetsAndMetadataPerTask.size();\n         }\n     }\n \n+    private void commit(final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> offsetsPerTask) {\n+        if (eosEnabled) {\n+            for (final Map.Entry<TaskId, Map<TopicPartition, OffsetAndMetadata>> taskToCommit : offsetsPerTask.entrySet()) {\n+                taskProducers.get(taskToCommit.getKey()).commitTransaction(taskToCommit.getValue());\n+            }\n+        } else {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyOTE2NA=="}, "originalCommit": null, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI2MDI0Mw==", "bodyText": "Comment here for no better place: Standby task always returns an empty committableOffsetsAndMetadata, then why do we still need to check commitNeeded for it? Shouldn't it always set to false?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391260243", "createdAt": "2020-03-11T20:50:44Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -115,48 +121,45 @@ public void resume() {\n      * 1. flush store\n      * 2. write checkpoint file\n      *\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n     @Override\n-    public void commit() {\n-        switch (state()) {\n-            case RUNNING:\n-                stateMgr.flush();\n-\n-                // since there's no written offsets we can checkpoint with empty map,\n-                // and the state current offset would be used to checkpoint\n-                stateMgr.checkpoint(Collections.emptyMap());\n-\n-                offsetSnapshotSinceLastCommit = new HashMap<>(stateMgr.changelogOffsets());\n-\n-                log.info(\"Committed\");\n-                break;\n-\n-            case CLOSING:\n-                // do nothing and also not throw\n-                log.trace(\"Skip committing since task is closing\");\n-\n-                break;\n-\n-            default:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while committing standby task \" + id);\n+    public void prepareCommit() {\n+        if (state() == State.RUNNING) {\n+            stateMgr.flush();\n+            log.info(\"Task ready for committing\");\n+        } else {\n+            throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing standby task \" + id + \" for committing \");\n+        }\n+    }\n \n+    @Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI2MTE1OA==", "bodyText": "logContext  is not used.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391261158", "createdAt": "2020-03-11T20:52:34Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -80,32 +80,42 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                       final KafkaClientSupplier clientSupplier,\n                       final String threadId,\n                       final Logger log) {\n-        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n         this.builder = builder;\n         this.config = config;\n         this.streamsMetrics = streamsMetrics;\n         this.stateDirectory = stateDirectory;\n         this.storeChangelogReader = storeChangelogReader;\n+        this.cache = cache;\n         this.time = time;\n+        this.clientSupplier = clientSupplier;\n+        this.threadId = threadId;\n         this.log = log;\n \n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+\n         if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n             threadProducer = null;\n             taskProducers = new HashMap<>();\n         } else {\n+            log.info(\"Creating thread producer client\");\n+\n             final String threadProducerClientId = getThreadProducerClientId(threadId);\n             final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n-            log.info(\"Creating thread producer client\");\n+\n+            final String logPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final LogContext logContext = new LogContext(logPrefix);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI2MTkyOQ==", "bodyText": "Should we attempt to add more fine-grained metrics for 3 stages then?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391261929", "createdAt": "2020-03-11T20:54:06Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -92,7 +92,6 @@\n     private final Sensor closeTaskSensor;\n     private final Sensor processLatencySensor;\n     private final Sensor punctuateLatencySensor;\n-    private final Sensor commitSensor;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjIxMQ=="}, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI2NTcxMQ==", "bodyText": "Could we add a @return for this method? Also we should comment about the different indications when we return an empty map vs null.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391265711", "createdAt": "2020-03-11T21:01:37Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -410,54 +410,88 @@ public void closeDirty() {\n      * @throws TaskMigratedException if committing offsets failed (non-EOS)\n      *                               or if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI2NzEwOQ==", "bodyText": "Remove if", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391267109", "createdAt": "2020-03-11T21:04:39Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -410,54 +410,88 @@ public void closeDirty() {\n      * @throws TaskMigratedException if committing offsets failed (non-EOS)\n      *                               or if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {\n+        final Map<TopicPartition, Long> checkpoint;\n+\n         if (state() == State.CREATED) {\n             // the task is created and not initialized, just re-write the checkpoint file\n-            executeAndMaybeSwallow(clean, () -> {\n-                stateMgr.checkpoint(Collections.emptyMap());\n-            }, \"state manager checkpoint\", log);\n-\n-            transitionTo(State.CLOSING);\n+            checkpoint = Collections.emptyMap();\n         } else if (state() == State.RUNNING) {\n             closeTopology(clean);\n \n             if (clean) {\n-                commitState();\n-                // whenever we have successfully committed state, it is safe to checkpoint\n-                // the state as well no matter if EOS is enabled or not\n-                stateMgr.checkpoint(checkpointableOffsets());\n+                stateMgr.flush();\n+                recordCollector.flush();\n+                checkpoint = checkpointableOffsets();\n             } else {\n+                checkpoint = null; // `null` indicates to not write a checkpoint\n                 executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\", log);\n             }\n-\n-            transitionTo(State.CLOSING);\n         } else if (state() == State.RESTORING) {\n-            executeAndMaybeSwallow(clean, () -> {\n-                stateMgr.flush();\n-                stateMgr.checkpoint(Collections.emptyMap());\n-            }, \"state manager flush and checkpoint\", log);\n-\n-            transitionTo(State.CLOSING);\n+            executeAndMaybeSwallow(clean, stateMgr::flush, \"state manager flush\", log);\n+            checkpoint = Collections.emptyMap();\n         } else if (state() == State.SUSPENDED) {\n-            // do not need to commit / checkpoint, since when suspending we've already committed the state\n-            transitionTo(State.CLOSING);\n+            // if `SUSPENDED` do not need to checkpoint, since when suspending we've already committed the state", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI4Njk4OA==", "bodyText": "I feel a bit weird here as we don't need prepareCloseClean anymore. This API usage is a little complicated upon when we should do it and we don't.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391286988", "createdAt": "2020-03-11T21:47:16Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -182,30 +193,49 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 cleanupTask(task);\n \n                 try {\n-                    task.closeClean();\n+                    checkpointPerTask.put(task, task.prepareCloseClean());\n+                    if (task.state() != CREATED) {\n+                        consumedOffsetsAndMetadataPerTask.put(task.id(), task.committableOffsetsAndMetadata());\n+                    }\n                 } catch (final RuntimeException e) {\n                     final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n                     log.error(uncleanMessage, e);\n                     taskCloseExceptions.put(task.id(), e);\n                     // We've already recorded the exception (which is the point of clean).\n                     // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    task.closeDirty();\n-                } finally {\n-                    if (task.isActive()) {\n-                        try {\n-                            activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n-                        } catch (final RuntimeException e) {\n-                            final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n-                            log.error(uncleanMessage, e);\n-                            taskCloseExceptions.putIfAbsent(task.id(), e);\n-                        }\n-                    }\n+                    task.prepareCloseDirty();\n+                    dirtyTasks.add(task);\n                 }\n \n                 iterator.remove();\n             }\n         }\n \n+        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        }\n+\n+        for (final Map.Entry<Task, Map<TopicPartition, Long>> taskAndCheckpoint : checkpointPerTask.entrySet()) {\n+            final Task task = taskAndCheckpoint.getKey();\n+            try {\n+                task.closeClean(checkpointPerTask.get(task));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI4NzEzMg==", "bodyText": "Similarly for closeDirty and prepareCloseDirty", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391287132", "createdAt": "2020-03-11T21:47:37Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -182,30 +193,49 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 cleanupTask(task);\n \n                 try {\n-                    task.closeClean();\n+                    checkpointPerTask.put(task, task.prepareCloseClean());\n+                    if (task.state() != CREATED) {\n+                        consumedOffsetsAndMetadataPerTask.put(task.id(), task.committableOffsetsAndMetadata());\n+                    }\n                 } catch (final RuntimeException e) {\n                     final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n                     log.error(uncleanMessage, e);\n                     taskCloseExceptions.put(task.id(), e);\n                     // We've already recorded the exception (which is the point of clean).\n                     // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    task.closeDirty();\n-                } finally {\n-                    if (task.isActive()) {\n-                        try {\n-                            activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n-                        } catch (final RuntimeException e) {\n-                            final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n-                            log.error(uncleanMessage, e);\n-                            taskCloseExceptions.putIfAbsent(task.id(), e);\n-                        }\n-                    }\n+                    task.prepareCloseDirty();\n+                    dirtyTasks.add(task);\n                 }\n \n                 iterator.remove();\n             }\n         }\n \n+        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        }\n+\n+        for (final Map.Entry<Task, Map<TopicPartition, Long>> taskAndCheckpoint : checkpointPerTask.entrySet()) {\n+            final Task task = taskAndCheckpoint.getKey();\n+            try {\n+                task.closeClean(checkpointPerTask.get(task));\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                // We've already recorded the exception (which is the point of clean).\n+                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n+                task.closeDirty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI4OTc4Ng==", "bodyText": "nit; 228 - 229 could be merged.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391289786", "createdAt": "2020-03-11T21:54:04Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "diffHunk": "@@ -222,7 +225,8 @@ public void shouldDoNothingWithCreatedStateOnClose() {\n         final MetricName metricName = setupCloseTaskMetric();\n \n         task = createStandbyTask();\n-        task.closeClean();\n+        final Map<TopicPartition, Long> checkpoint = task.prepareCloseClean();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5MTEwMg==", "bodyText": "Also the above step #4 is no longer correct, the commit is done on TaskManager now.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391291102", "createdAt": "2020-03-11T21:57:16Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -410,54 +410,88 @@ public void closeDirty() {\n      * @throws TaskMigratedException if committing offsets failed (non-EOS)\n      *                               or if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5NTIyMQ==", "bodyText": "Do we need to keep a task once it is failed to clean close? Why couldn't we just close it dirty immediately after we see the exception?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391295221", "createdAt": "2020-03-11T22:08:20Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -432,24 +490,54 @@ private void cleanupTask(final Task task) {\n \n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n-        final Iterator<Task> iterator = tasks.values().iterator();\n-        while (iterator.hasNext()) {\n-            final Task task = iterator.next();\n+\n+        final Map<Task, Map<TopicPartition, Long>> checkpointPerTask = new HashMap<>();\n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+        final Set<Task> dirtyTasks = new HashSet<>();\n+\n+        for (final Task task : tasks.values()) {\n             cleanupTask(task);\n \n             if (clean) {\n                 try {\n-                    task.closeClean();\n+                    checkpointPerTask.put(task, task.prepareCloseClean());\n+                    if (task.state() != CREATED) {\n+                        consumedOffsetsAndMetadataPerTask.put(task.id(), task.committableOffsetsAndMetadata());\n+                    }\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    task.closeDirty();\n+                    task.prepareCloseDirty();\n+                    dirtyTasks.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    task.closeDirty();\n+                    task.prepareCloseDirty();\n+                    dirtyTasks.add(task);\n                 }\n             } else {\n+                task.prepareCloseDirty();\n+                dirtyTasks.add(task);\n+            }\n+        }\n+\n+        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        }\n+\n+        for (final Map.Entry<Task, Map<TopicPartition, Long>> taskAndCheckpoint : checkpointPerTask.entrySet()) {\n+            final Task task = taskAndCheckpoint.getKey();\n+            try {\n+                task.closeClean(checkpointPerTask.get(task));\n+            } catch (final RuntimeException e) {\n+                firstException.compareAndSet(null, e);\n                 task.closeDirty();\n             }\n+        }\n+\n+        for (final Task task : dirtyTasks) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5Njg5NQ==", "bodyText": "Why do we need to move these tests?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391296895", "createdAt": "2020-03-11T22:11:16Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "diffHunk": "@@ -88,44 +88,11 @@ public void before() {\n         eosStreamsProducer.initTransaction();\n     }\n \n-    @Test", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5ODc4NQ==", "bodyText": "Looks like we lack test coverage for TimeoutException and KafkaException cases", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391298785", "createdAt": "2020-03-11T22:13:39Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -364,11 +365,8 @@ private void commitState() {\n             final long partitionTime = partitionTimes.get(partition);\n             consumedOffsetsAndMetadata.put(partition, new OffsetAndMetadata(offset, encodeTimestamp(partitionTime)));\n         }\n-        recordCollector.commit(consumedOffsetsAndMetadata);\n \n-        commitNeeded = false;\n-        commitRequested = false;\n-        commitSensor.record(time.nanoseconds() - startNs);\n+        return consumedOffsetsAndMetadata;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMwMTY3OA==", "bodyText": "We don't have unit test coverage for this exception case", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391301678", "createdAt": "2020-03-11T22:17:34Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -247,6 +277,19 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n         changelogReader.transitToRestoreActive();\n     }\n \n+    private void cleanUpTaskProducer(final Task task,\n+                                     final Map<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (task.isActive()) {\n+            try {\n+                activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n+            } catch (final RuntimeException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMwMjIyNw==", "bodyText": "We lack unit test coverage for this case", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391302227", "createdAt": "2020-03-11T22:18:14Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -182,30 +193,49 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 cleanupTask(task);\n \n                 try {\n-                    task.closeClean();\n+                    checkpointPerTask.put(task, task.prepareCloseClean());\n+                    if (task.state() != CREATED) {\n+                        consumedOffsetsAndMetadataPerTask.put(task.id(), task.committableOffsetsAndMetadata());\n+                    }\n                 } catch (final RuntimeException e) {\n                     final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n                     log.error(uncleanMessage, e);\n                     taskCloseExceptions.put(task.id(), e);\n                     // We've already recorded the exception (which is the point of clean).\n                     // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    task.closeDirty();\n-                } finally {\n-                    if (task.isActive()) {\n-                        try {\n-                            activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n-                        } catch (final RuntimeException e) {\n-                            final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n-                            log.error(uncleanMessage, e);\n-                            taskCloseExceptions.putIfAbsent(task.id(), e);\n-                        }\n-                    }\n+                    task.prepareCloseDirty();\n+                    dirtyTasks.add(task);\n                 }\n \n                 iterator.remove();\n             }\n         }\n \n+        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        }\n+\n+        for (final Map.Entry<Task, Map<TopicPartition, Long>> taskAndCheckpoint : checkpointPerTask.entrySet()) {\n+            final Task task = taskAndCheckpoint.getKey();\n+            try {\n+                task.closeClean(checkpointPerTask.get(task));\n+            } catch (final RuntimeException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMwMzcxMg==", "bodyText": "Could we verify the assignment stack and lost stack separately, by doing handleAssignment verify first before calling handleLost", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391303712", "createdAt": "2020-03-11T22:20:40Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -212,23 +227,24 @@ public void shouldCloseActiveTasksWhenHandlingLostTasks() {\n         final Task task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n         final Task task01 = new StateMachineTask(taskId01, taskId01Partitions, false);\n \n+        // `handleAssignment`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNzk4NDA1", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-373798405", "createdAt": "2020-03-12T18:22:32Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxODoyMjozMlrOF1qK_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxOToxODowNlrOF1r78w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgwOTc5MA==", "bodyText": "nit: add a check that taskId exists in taskProducers to make sure we do not return null.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391809790", "createdAt": "2020-03-12T18:22:32Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -80,32 +80,42 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                       final KafkaClientSupplier clientSupplier,\n                       final String threadId,\n                       final Logger log) {\n-        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n         this.builder = builder;\n         this.config = config;\n         this.streamsMetrics = streamsMetrics;\n         this.stateDirectory = stateDirectory;\n         this.storeChangelogReader = storeChangelogReader;\n+        this.cache = cache;\n         this.time = time;\n+        this.clientSupplier = clientSupplier;\n+        this.threadId = threadId;\n         this.log = log;\n \n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+\n         if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n             threadProducer = null;\n             taskProducers = new HashMap<>();\n         } else {\n+            log.info(\"Creating thread producer client\");\n+\n             final String threadProducerClientId = getThreadProducerClientId(threadId);\n             final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n-            log.info(\"Creating thread producer client\");\n+\n+            final String logPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n             threadProducer = clientSupplier.getProducer(producerConfigs);\n             taskProducers = Collections.emptyMap();\n         }\n+    }\n \n-\n-        this.cache = cache;\n-        this.threadId = threadId;\n-        this.clientSupplier = clientSupplier;\n-\n-        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    StreamsProducer streamsProducerForTask(final TaskId taskId) {\n+        if (threadProducer != null) {\n+            throw new IllegalStateException(\"Producer per thread is used\");\n+        }\n+        return taskProducers.get(taskId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgxMjI0Mg==", "bodyText": "Subjectively I'd +1 that adding one more parameter to avoid piggy-backing on the applicationId is better.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391812242", "createdAt": "2020-03-12T18:27:10Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -58,25 +58,24 @@\n     private boolean transactionInitialized = false;\n \n     public StreamsProducer(final Producer<byte[], byte[]> producer,\n-                           final boolean eosEnabled,\n-                           final LogContext logContext,\n-                           final String applicationId) {\n-        log = logContext.logger(getClass());\n+                           final String applicationId,\n+                           final LogContext logContext) {\n+        log = Objects.requireNonNull(logContext, \"logContext cannot be null\").logger(getClass());\n         logPrefix = logContext.logPrefix().trim();\n \n         this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n         this.applicationId = applicationId;\n-        this.eosEnabled = eosEnabled;\n+        this.eosEnabled = applicationId != null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjU3MQ=="}, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTQ3Mw==", "bodyText": "Actually on a second thought, I'm wondering if the following inside TaskManager is cleaner:\nfor (task <- taskManager.activeTasks)\n    task.recordCollector().commit(taskToCommit.getTask(task.id);\n\nInstead of:\nactiveTaskCreator.streamsProducerForTask(taskToCommit.getKey()).commitTransaction(taskToCommit.getValue());\n\nMy gut feeling is that it is cleaner to not access the task creator for its created stream-producers (and hence here we need to change the task-producer map to streamsProducers), but just access each task's record collector and call its commit --- today we already have a StreamTask#recordCollector method.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391829473", "createdAt": "2020-03-12T18:59:20Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -80,32 +80,42 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                       final KafkaClientSupplier clientSupplier,\n                       final String threadId,\n                       final Logger log) {\n-        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n         this.builder = builder;\n         this.config = config;\n         this.streamsMetrics = streamsMetrics;\n         this.stateDirectory = stateDirectory;\n         this.storeChangelogReader = storeChangelogReader;\n+        this.cache = cache;\n         this.time = time;\n+        this.clientSupplier = clientSupplier;\n+        this.threadId = threadId;\n         this.log = log;\n \n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+\n         if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n             threadProducer = null;\n             taskProducers = new HashMap<>();\n         } else {\n+            log.info(\"Creating thread producer client\");\n+\n             final String threadProducerClientId = getThreadProducerClientId(threadId);\n             final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n-            log.info(\"Creating thread producer client\");\n+\n+            final String logPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n             threadProducer = clientSupplier.getProducer(producerConfigs);\n             taskProducers = Collections.emptyMap();\n         }\n+    }\n \n-\n-        this.cache = cache;\n-        this.threadId = threadId;\n-        this.clientSupplier = clientSupplier;\n-\n-        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    StreamsProducer streamsProducerForTask(final TaskId taskId) {\n+        if (threadProducer != null) {\n+            throw new IllegalStateException(\"Producer per thread is used\");\n+        }\n+        return taskProducers.get(taskId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgwOTc5MA=="}, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTg1NA==", "bodyText": "Please see my other comment above --- I think it is cleaner to just call foreach(active-task) task.recordCollector.commit inside the task-manager; and inside RecordCollectorImpl we check that eosEnabled is always true, otherwise illegal-state thrown.\nIn the next PR where we have the thread-producer, we could then only create a single recordCollector object that is shared among all active tasks and wraps the thread-producer, and then the caller taskManager code then can just get one active task and call its record-collector's commit function knowing that is sufficient to actually commit for all tasks since everyone is using the same record-collector.\nWDYT?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391829854", "createdAt": "2020-03-12T19:00:10Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java", "diffHunk": "@@ -45,8 +44,6 @@\n                      final Serializer<V> valueSerializer,\n                      final StreamPartitioner<? super K, ? super V> partitioner);\n \n-    void commit(final Map<TopicPartition, OffsetAndMetadata> offsets);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyNzYzOA=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzODA3OA==", "bodyText": "This is a meta comment: I think we can consolidate prepareCommit and prepareClose and prepareSuspend here by introducing the clean parameters to the function, since their logic are very similar (for the part that they diffs a bit, it can be pushed to post logic), and on task-manager during commit:\n\nfor each task -> task.prepareCommit(true)\ncommit\nfor each task -> task.postCommit(true)\n\nDuring close:\nif (clean)\n1) for each task -> task.prepareCommit(true)\n2) commit()\n3) for each task -> task.postCommit(true)\nelse\n1) for each task -> task.prepareCommit(false)\n// do not commit\n3) for each task -> task.postCommit(false)\n4) tasks.close(flag)\nAnd the same for suspension.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391838078", "createdAt": "2020-03-12T19:16:48Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -115,48 +121,45 @@ public void resume() {\n      * 1. flush store\n      * 2. write checkpoint file\n      *\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n     @Override\n-    public void commit() {\n-        switch (state()) {\n-            case RUNNING:\n-                stateMgr.flush();\n-\n-                // since there's no written offsets we can checkpoint with empty map,\n-                // and the state current offset would be used to checkpoint\n-                stateMgr.checkpoint(Collections.emptyMap());\n-\n-                offsetSnapshotSinceLastCommit = new HashMap<>(stateMgr.changelogOffsets());\n-\n-                log.info(\"Committed\");\n-                break;\n-\n-            case CLOSING:\n-                // do nothing and also not throw\n-                log.trace(\"Skip committing since task is closing\");\n-\n-                break;\n-\n-            default:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while committing standby task \" + id);\n+    public void prepareCommit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzODcwNw==", "bodyText": "I actually think that we can remove this DEBUG-level per-task commit metrics, since we already have the INFO-level per-thread commit metric and this one does not provide much more additional information?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r391838707", "createdAt": "2020-03-12T19:18:06Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -92,7 +92,6 @@\n     private final Sensor closeTaskSensor;\n     private final Sensor processLatencySensor;\n     private final Sensor punctuateLatencySensor;\n-    private final Sensor commitSensor;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjIxMQ=="}, "originalCommit": null, "originalPosition": 20}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0MDM1NjY2", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-374035666", "createdAt": "2020-03-13T03:50:05Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMzo1MDowNVrOF124Iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMzo1MDowNVrOF124Iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjAxNzk1NQ==", "bodyText": "This is not introduced in this PR, but: while thinking about it, I realized for RESTORING state we do not need to rely on eosDisabled to checkpoint, in fact we can always checkpoint during RESTORING here.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r392017955", "createdAt": "2020-03-13T03:50:05Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -293,18 +302,31 @@ public void resume() {\n         }\n     }\n \n-    /**\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n     @Override\n-    public void commit() {\n+    public void prepareCommit() {\n+        switch (state()) {\n+            case RUNNING:\n+            case RESTORING:\n+                stateMgr.flush();\n+                recordCollector.flush();\n+\n+                log.info(\"Prepared task for committing\");\n+\n+                break;\n+\n+            default:\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n+        }\n+    }\n+\n+    @Override\n+    public void postCommit() {\n         switch (state()) {\n             case RUNNING:\n             case RESTORING:\n-                commitState();\n+                commitNeeded = false;\n+                commitRequested = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MzU2MTEw", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-375356110", "createdAt": "2020-03-16T16:00:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxNjowMDoyMFrOF266Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxNzozMTo1MVrOF2-vvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzMjU1OA==", "bodyText": "We need a unit test for this function.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393132558", "createdAt": "2020-03-16T16:00:20Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -80,32 +80,44 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                       final KafkaClientSupplier clientSupplier,\n                       final String threadId,\n                       final Logger log) {\n-        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n         this.builder = builder;\n         this.config = config;\n         this.streamsMetrics = streamsMetrics;\n         this.stateDirectory = stateDirectory;\n         this.storeChangelogReader = storeChangelogReader;\n+        this.cache = cache;\n         this.time = time;\n+        this.clientSupplier = clientSupplier;\n+        this.threadId = threadId;\n         this.log = log;\n \n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+\n         if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n             threadProducer = null;\n             taskProducers = new HashMap<>();\n         } else {\n+            log.info(\"Creating thread producer client\");\n+\n             final String threadProducerClientId = getThreadProducerClientId(threadId);\n             final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n-            log.info(\"Creating thread producer client\");\n+\n             threadProducer = clientSupplier.getProducer(producerConfigs);\n             taskProducers = Collections.emptyMap();\n         }\n+    }\n \n+    StreamsProducer streamsProducerForTask(final TaskId taskId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzNjU5NQ==", "bodyText": "we could just do one log in front: log.info(\"Prepare suspending {}\", state());", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393136595", "createdAt": "2020-03-16T16:06:09Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -231,27 +228,40 @@ public void completeRestoration() {\n      *                               or if the task producer got fenced (EOS)\n      */\n     @Override\n-    public void suspend() {\n-        if (state() == State.CREATED || state() == State.CLOSING || state() == State.SUSPENDED) {\n+    public void prepareSuspend() {\n+        if (state() == State.CREATED || state() == State.SUSPENDED) {\n             // do nothing\n-            log.trace(\"Skip suspending since state is {}\", state());\n+            log.trace(\"Skip prepare suspending since state is {}\", state());\n         } else if (state() == State.RUNNING) {\n             closeTopology(true);\n \n-            commitState();\n-            // whenever we have successfully committed state during suspension, it is safe to checkpoint\n-            // the state as well no matter if EOS is enabled or not\n-            stateMgr.checkpoint(checkpointableOffsets());\n+            stateMgr.flush();\n+            recordCollector.flush();\n \n-            // we should also clear any buffered records of a task when suspending it\n+            log.info(\"Prepare suspending running\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzOTU3NQ==", "bodyText": "Do we have unit test to check the checkpoint status after postCommit()?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393139575", "createdAt": "2020-03-16T16:10:32Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -293,18 +302,31 @@ public void resume() {\n         }\n     }\n \n-    /**\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n     @Override\n-    public void commit() {\n+    public void prepareCommit() {\n+        switch (state()) {\n+            case RUNNING:\n+            case RESTORING:\n+                stateMgr.flush();\n+                recordCollector.flush();\n+\n+                log.info(\"Prepared task for committing\");\n+\n+                break;\n+\n+            default:\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n+        }\n+    }\n+\n+    @Override\n+    public void postCommit() {\n         switch (state()) {\n             case RUNNING:\n             case RESTORING:\n-                commitState();\n+                commitNeeded = false;\n+                commitRequested = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjAxNzk1NQ=="}, "originalCommit": null, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE4MDM4MA==", "bodyText": "I couldn't fully follow this idea, just playing devil advocates here, if we think meta code comments actually hinder the readability of internal class, why not just remove all the internal function meta comments, as they would get outdated anyway? For me the return type comment is still valuable for understandability. If the comment gets outdated, we should just update it. cc @guozhangwang if the idea here makes sense, or we could get a consensus on what needs to be done in internal class comments, and what's not.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393180380", "createdAt": "2020-03-16T17:06:41Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -410,54 +410,88 @@ public void closeDirty() {\n      * @throws TaskMigratedException if committing offsets failed (non-EOS)\n      *                               or if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5MTEwMg=="}, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE4MzMxMA==", "bodyText": "nit: { could be reduced.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393183310", "createdAt": "2020-03-16T17:11:13Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -400,65 +422,88 @@ public void closeDirty() {\n      *  1. first close topology to make sure all cached records in the topology are processed\n      *  2. then flush the state, send any left changelog records\n      *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     *  6. then if we are closing on EOS and dirty, wipe out the state store directory\n-     *  7. finally release the state manager lock\n      * </pre>\n      *\n-     * @param clean    shut down cleanly (ie, incl. flush and commit) if {@code true} --\n+     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n      *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n+     * @throws TaskMigratedException if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {\n+        final Map<TopicPartition, Long> checkpoint;\n+\n         if (state() == State.CREATED) {\n             // the task is created and not initialized, just re-write the checkpoint file\n-            executeAndMaybeSwallow(clean, () -> {\n-                stateMgr.checkpoint(Collections.emptyMap());\n-            }, \"state manager checkpoint\", log);\n-\n-            transitionTo(State.CLOSING);\n+            checkpoint = Collections.emptyMap();\n         } else if (state() == State.RUNNING) {\n             closeTopology(clean);\n \n             if (clean) {\n-                commitState();\n-                // whenever we have successfully committed state, it is safe to checkpoint\n-                // the state as well no matter if EOS is enabled or not\n-                stateMgr.checkpoint(checkpointableOffsets());\n+                stateMgr.flush();\n+                recordCollector.flush();\n+                checkpoint = checkpointableOffsets();\n             } else {\n+                checkpoint = null; // `null` indicates to not write a checkpoint\n                 executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\", log);\n             }\n-\n-            transitionTo(State.CLOSING);\n         } else if (state() == State.RESTORING) {\n-            executeAndMaybeSwallow(clean, () -> {\n-                stateMgr.flush();\n-                stateMgr.checkpoint(Collections.emptyMap());\n-            }, \"state manager flush and checkpoint\", log);\n-\n-            transitionTo(State.CLOSING);\n+            executeAndMaybeSwallow(clean, stateMgr::flush, \"state manager flush\", log);\n+            checkpoint = Collections.emptyMap();\n         } else if (state() == State.SUSPENDED) {\n-            // do not need to commit / checkpoint, since when suspending we've already committed the state\n-            transitionTo(State.CLOSING);\n+            // if `SUSPENDED` do not need to checkpoint, since when suspending we've already committed the state\n+            checkpoint = null; // `null` indicates to not write a checkpoint\n+        } else {\n+            throw new IllegalStateException(\"Illegal state \" + state() + \" while prepare closing active task \" + id);\n         }\n \n-        if (state() == State.CLOSING) {\n-            // if EOS is enabled, we wipe out the whole state store for unclean close\n-            // since they are invalid to use anymore\n-            final boolean wipeStateStore = !clean && !eosDisabled;\n+        return checkpoint;\n+    }\n \n-            // first close state manager (which is idempotent) then close the record collector (which could throw),\n-            // if the latter throws and we re-close dirty which would close the state manager again.\n-            executeAndMaybeSwallow(clean, () -> {\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean,\n-                        wipeStateStore, stateMgr, stateDirectory, TaskType.ACTIVE);\n-            }, \"state manager close\", log);\n+    /**\n+     * <pre>\n+     * the following order must be followed:\n+     *  1. checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n+     *  2. then if we are closing on EOS and dirty, wipe out the state store directory\n+     *  3. finally release the state manager lock\n+     * </pre>\n+     */\n+    private void close(final boolean clean,\n+                       final Map<TopicPartition, Long> checkpoint) {\n+        if (clean && checkpoint != null) {\n+            executeAndMaybeSwallow(clean, () -> stateMgr.checkpoint(checkpoint), \"state manager checkpoint\", log);\n+        }\n \n-            executeAndMaybeSwallow(clean, recordCollector::close, \"record collector close\", log);\n-        } else {\n-            throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);\n+        switch (state()) {\n+            case CREATED:\n+            case RUNNING:\n+            case RESTORING:\n+            case SUSPENDED:\n+                // if EOS is enabled, we wipe out the whole state store for unclean close\n+                // since they are invalid to use anymore\n+                final boolean wipeStateStore = !clean && !eosDisabled;\n+\n+                // first close state manager (which is idempotent) then close the record collector (which could throw),\n+                // if the latter throws and we re-close dirty which would close the state manager again.\n+                executeAndMaybeSwallow(\n+                    clean,\n+                    () -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE5NTQ1NQ==", "bodyText": "Yea, a TODO is also ok.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393195455", "createdAt": "2020-03-16T17:31:51Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -144,6 +149,7 @@ void handleCorruption(final Map<TaskId, Collection<TopicPartition>> taskWithChan\n             final Collection<TopicPartition> corruptedPartitions = entry.getValue();\n             task.markChangelogAsCorrupted(corruptedPartitions);\n \n+            task.prepareCloseDirty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0MTUyOA=="}, "originalCommit": null, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NjEwNzgw", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-375610780", "createdAt": "2020-03-16T21:59:23Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NjE0MzA0", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-375614304", "createdAt": "2020-03-16T22:07:16Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQyMjowNzoxNlrOF3HVYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwMDo1MTozMVrOF3KfQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMzNjE2MA==", "bodyText": "For the next PR (all other comments with this tag means no changes required for this PR): my understanding is that we would make the thread-producer also a StreamsProducer instead of a KafkaProducer which would be used to commitTransaction under eosBeta, is that right?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393336160", "createdAt": "2020-03-16T22:07:16Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -80,32 +80,44 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                       final KafkaClientSupplier clientSupplier,\n                       final String threadId,\n                       final Logger log) {\n-        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n         this.builder = builder;\n         this.config = config;\n         this.streamsMetrics = streamsMetrics;\n         this.stateDirectory = stateDirectory;\n         this.storeChangelogReader = storeChangelogReader;\n+        this.cache = cache;\n         this.time = time;\n+        this.clientSupplier = clientSupplier;\n+        this.threadId = threadId;\n         this.log = log;\n \n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+\n         if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n             threadProducer = null;\n             taskProducers = new HashMap<>();\n         } else {\n+            log.info(\"Creating thread producer client\");\n+\n             final String threadProducerClientId = getThreadProducerClientId(threadId);\n             final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n-            log.info(\"Creating thread producer client\");\n+\n             threadProducer = clientSupplier.getProducer(producerConfigs);\n             taskProducers = Collections.emptyMap();\n         }\n+    }\n \n+    StreamsProducer streamsProducerForTask(final TaskId taskId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzMjU1OA=="}, "originalCommit": null, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMzNjYxOA==", "bodyText": "nit: we can have a wrapped StreamsProducer#close / metrics, and then #kafkaProducer would be for testing-only.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393336618", "createdAt": "2020-03-16T22:08:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -178,18 +194,18 @@ void closeThreadProducerIfNeeded() {\n             try {\n                 threadProducer.close();\n             } catch (final RuntimeException e) {\n-                throw new StreamsException(\"Thread Producer encounter unexpected error trying to close\", e);\n+                throw new StreamsException(\"Thread Producer encounter error trying to close\", e);\n             }\n         }\n     }\n \n     void closeAndRemoveTaskProducerIfNeeded(final TaskId id) {\n-        final Producer<byte[], byte[]> producer = taskProducers.remove(id);\n-        if (producer != null) {\n+        final StreamsProducer taskProducer = taskProducers.remove(id);\n+        if (taskProducer != null) {\n             try {\n-                producer.close();\n+                taskProducer.kafkaProducer().close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMzNjkzNA==", "bodyText": "After syncing offline about this, I think I'm convinced now that moving this logic into TaskManager is better.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393336934", "createdAt": "2020-03-16T22:09:16Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java", "diffHunk": "@@ -45,8 +44,6 @@\n                      final Serializer<V> valueSerializer,\n                      final StreamPartitioner<? super K, ? super V> partitioner);\n \n-    void commit(final Map<TopicPartition, OffsetAndMetadata> offsets);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQyNzYzOA=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMzOTA2Nw==", "bodyText": "I think we should just let the prepareXX function to return the map of partitions -> partition-timestamp to indicate if it should be included in the map of committing offsets, so that we do not need to leak the state into task-manager here. Also we only need to call mainConsumer.position once for all tasks -- please see my other comment above.\nAlso: we should not try to commit state if we are in RESTORING but only flushing store and writing checkpoints (I think this is already the behavior in trunk), since the partitions are paused from the main-consumer before restoration is done --- maybe it is partly causing some unit test flakiness.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393339067", "createdAt": "2020-03-16T22:14:56Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -320,13 +363,27 @@ boolean tryToCompleteRestoration() {\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n         final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n \n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n         for (final Task task : tasks.values()) {\n             if (remainingPartitions.containsAll(task.inputPartitions())) {\n-                task.suspend();\n+                task.prepareSuspend();\n+                if (task.state() != CREATED) {\n+                    consumedOffsetsAndMetadataPerTask.put(task.id(), task.committableOffsetsAndMetadata());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIzNDI0OA=="}, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMzOTIxMw==", "bodyText": "SG.\nI think in this PR we still can do the change to let prepareXX to return the map of partitions -> partition-timestamp to indicate whether this task should be included in committing.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393339213", "createdAt": "2020-03-16T22:15:23Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -115,48 +121,45 @@ public void resume() {\n      * 1. flush store\n      * 2. write checkpoint file\n      *\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n     @Override\n-    public void commit() {\n-        switch (state()) {\n-            case RUNNING:\n-                stateMgr.flush();\n-\n-                // since there's no written offsets we can checkpoint with empty map,\n-                // and the state current offset would be used to checkpoint\n-                stateMgr.checkpoint(Collections.emptyMap());\n-\n-                offsetSnapshotSinceLastCommit = new HashMap<>(stateMgr.changelogOffsets());\n-\n-                log.info(\"Committed\");\n-                break;\n-\n-            case CLOSING:\n-                // do nothing and also not throw\n-                log.trace(\"Skip committing since task is closing\");\n-\n-                break;\n-\n-            default:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while committing standby task \" + id);\n+    public void prepareCommit() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzODA3OA=="}, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM0MTk3Mg==", "bodyText": "In either eos-alpha or eos-beta or non-eos, we can just loop over all the \"committable partitions\" and call mainConsumer#position once, so this function can be extracted out of the task as a per-task call.\nMore specifically, in the prepareXX calls, we know based on the state of the task and clean flag whether or not we should commit the source topic offsets for this task, so we can let the prepareXX function to return Map<TopicPartition, Long> partitionTimes encoding the extracted timestamps for each partition instead of void --- when we decided not to commit we return an empty map. And then inside TaskManager we just use the mainConsumer to call position once and then pass that to the commitOffsetsOrTransaction call.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393341972", "createdAt": "2020-03-16T22:23:14Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -313,35 +334,23 @@ public void commit() {\n \n                 break;\n \n-            case CLOSING:\n-                // do nothing\n+            case RESTORING:\n+                commitNeeded = false;\n+                commitRequested = false;\n+\n+                stateMgr.checkpoint(checkpointableOffsets());\n+\n+                log.info(\"Committed\");\n+\n                 break;\n \n             default:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while committing standby task \" + id);\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while post committing active task \" + id);\n         }\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. flush the state, send any left changelog records\n-     *  2. then flush the record collector\n-     *  3. then commit the record collector -- for EOS this is the synchronization barrier\n-     * </pre>\n-     *\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n-    private void commitState() {\n-        final long startNs = time.nanoseconds();\n-\n-        stateMgr.flush();\n-\n-        recordCollector.flush();\n-\n-        // we need to preserve the original partitions times before calling commit\n-        // because all partition times are reset to -1 during close\n+    @Override\n+    public Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM0NDYxMQ==", "bodyText": "nit: we can do if / else if / else here still and move the closeTaskSensor.record(); / transitionTo(State.CLOSED); to avoid duplication.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393344611", "createdAt": "2020-03-16T22:28:40Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -166,29 +169,70 @@ public void closeDirty() {\n      * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    private void close(final boolean clean) {\n+    private void prepareClose(final boolean clean) {\n         if (state() == State.CREATED) {\n             // the task is created and not initialized, do nothing\n-            transitionTo(State.CLOSING);\n-        } else {\n-            if (state() == State.RUNNING) {\n-                if (clean) {\n-                    commit();\n-                }\n+            return;\n+        }\n \n-                transitionTo(State.CLOSING);\n+        if (state() == State.RUNNING) {\n+            if (clean) {\n+                stateMgr.flush();\n             }\n+        } else {\n+            throw new IllegalStateException(\"Illegal state \" + state() + \" while closing standby task \" + id);\n+        }\n+    }\n+\n+    @Override\n+    public void closeClean(final Map<TopicPartition, Long> checkpoint) {\n+        Objects.requireNonNull(checkpoint);\n+        close(true, checkpoint);\n+\n+        log.info(\"Closed clean\");\n+    }\n+\n+    @Override\n+    public void closeDirty() {\n+        close(false, null);\n+\n+        log.info(\"Closed dirty\");\n+    }\n \n-            if (state() == State.CLOSING) {\n-                executeAndMaybeSwallow(clean, () -> {\n-                    StateManagerUtil.closeStateManager(log, logPrefix, clean,\n-                        false, stateMgr, stateDirectory, TaskType.STANDBY);\n-                }, \"state manager close\", log);\n+    private void close(final boolean clean,\n+                       final Map<TopicPartition, Long> checkpoint) {\n+        if (state() == State.CREATED) {\n+            // the task is created and not initialized, do nothing\n+            closeTaskSensor.record();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM0NTAzNw==", "bodyText": "Ditto here, I think if / else if / else is more readable.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393345037", "createdAt": "2020-03-16T22:29:26Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -166,29 +169,70 @@ public void closeDirty() {\n      * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    private void close(final boolean clean) {\n+    private void prepareClose(final boolean clean) {\n         if (state() == State.CREATED) {\n             // the task is created and not initialized, do nothing\n-            transitionTo(State.CLOSING);\n-        } else {\n-            if (state() == State.RUNNING) {\n-                if (clean) {\n-                    commit();\n-                }\n+            return;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3MTc4MQ==", "bodyText": "For the next PR: as I mentioned in the last commit I feel prepareSuspend and prepareClose can be consolidated with prepareCommit but in the next PR these logic would be changed again for eos-beta so maybe we cannot do that any more, so I'm fine with keeping as-is and we can revisit to see if we can really do this refactoring or not in the next PR when we did the eos-beta.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393371781", "createdAt": "2020-03-16T23:47:56Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -231,27 +228,40 @@ public void completeRestoration() {\n      *                               or if the task producer got fenced (EOS)\n      */\n     @Override\n-    public void suspend() {\n-        if (state() == State.CREATED || state() == State.CLOSING || state() == State.SUSPENDED) {\n+    public void prepareSuspend() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3MzAyMw==", "bodyText": "Yes we are unnecessarily checkpointing here --- the reason is that EOS flag was original striped out of task and only processor-state-manager knows about it; now since we get this EOS flag back to task (sigh.. :) we can add this additional check.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393373023", "createdAt": "2020-03-16T23:52:01Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -410,54 +410,88 @@ public void closeDirty() {\n      * @throws TaskMigratedException if committing offsets failed (non-EOS)\n      *                               or if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI2NTcxMQ=="}, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3NDQ2Ng==", "bodyText": "I would suggest not restricting ourselves to some specific rules about comments :) Personally I tried to avoid the one line comment explaining one line code type of comments inside a function since it should be obvious, rather I'd add some comments for a block or several blocks if I fear it maybe hard to read by itself. I think you guys should just make your best judgement here.\nAnd for internal functions, I agree that we do not necessarily need to write java-docs, and this one, for example, I wrote the java-doc as part of the tech debt cleanup just to remind what operations MUST be considered here inside closing / suspending etc so that later on when we change the function itself by other contributors, they would use it as a reference to check if they mistakenly missed some steps or re-ordered some steps. However if we are going to split this function into multiple, instead of just re-structuring the function as a whole, then although I have my preference I'd leave to you guys if you want to add the javadoc for both pre/post of you feel now it is too obvious to bother :)", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393374466", "createdAt": "2020-03-16T23:57:18Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -410,54 +410,88 @@ public void closeDirty() {\n      * @throws TaskMigratedException if committing offsets failed (non-EOS)\n      *                               or if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5MTEwMg=="}, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3NzI1Ng==", "bodyText": "For the next PR: I see the reason I return the checkpoint is that we are now extracting the committing out of the task and I need to remember if we need to checkpoint and if yes which offsets after we've flushed and before we checkpoint, but since the state of the task would not change before / after the commit during close.\nMore specifically we only have three cases: 1) to not write checkpoint, 2) write checkpoints for written offsets (changelogs) only, 3) write checkpoint for written and consumed offsets. And no matter which case it is during the preClose, it would always be the same in the post, so why do we need to return it to task-manager, book-keep there, and then after commit to pass it back to tasks?", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393377256", "createdAt": "2020-03-17T00:07:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -400,65 +422,88 @@ public void closeDirty() {\n      *  1. first close topology to make sure all cached records in the topology are processed\n      *  2. then flush the state, send any left changelog records\n      *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     *  6. then if we are closing on EOS and dirty, wipe out the state store directory\n-     *  7. finally release the state manager lock\n      * </pre>\n      *\n-     * @param clean    shut down cleanly (ie, incl. flush and commit) if {@code true} --\n+     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n      *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n+     * @throws TaskMigratedException if the task producer got fenced (EOS)\n      */\n-    private void close(final boolean clean) {\n+    private Map<TopicPartition, Long> prepareClose(final boolean clean) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3ODc3OQ==", "bodyText": "nit: we should emphasize that PrepareClose and close calls should be implemented idempotent since we may call it multiple times if a task close clean first and then fail and then close dirty.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393378779", "createdAt": "2020-03-17T00:14:27Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -125,35 +125,41 @@ public boolean isValidTransition(final State newState) {\n     boolean commitNeeded();\n \n     /**\n-     * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void commit();\n+    void prepareCommit();\n+\n+    void postCommit();\n \n     /**\n      * @throws TaskMigratedException all the task has been migrated\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void suspend();\n+    void prepareSuspend();\n \n+    void suspend();\n     /**\n+     *\n      * @throws StreamsException fatal error, should close the thread\n      */\n     void resume();\n \n     /**\n-     * Close a task that we still own. Commit all progress and close the task gracefully.\n+     * Prepare to close a task that we still own and prepare it for committing", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NDI2Ng==", "bodyText": "For the next PR: I think we can save prepareClose (or more accurately, merge prepareClose and close together again) if we make a state diagram change that only suspended state can transit to closed state, i.e. at task-manager level whenever we want to close a task we call its suspend function first, which would, depending on its state, be a no-op, or flushing, or closing topology etc, and then after that the task is always in SUSPENDED state, and then we call \"commit\" if necessary, and then we call close (a minor thing is that today when the state is in SUSPENDED we would omit committing inside task, and we need to lift this restriction; and also the transition actions to transit to SUSPENDED need to rely on the clean flag, hence we need suspend(clean-flag)).\nAND we can further merge prepareSuspend and suspend as well by just making the checkpointing logic as part of post-commit instead of post-suspend, since as I mentioned above you only have three cases:\n\ndo not need to checkpoint: if you are in CREATED.\ncheckpoint written and consumed offsets: if you are in RUNNING, in which you need to commit offsets as well.\ncheckpoint only store offsets: if you are in RESTORING, and in which case you do not need to commit offsets.\n\nIn fact, if we are not in the RUNNING state yet, the consumedOffsets as well as recordCollector#offsets() are always going to be empty, so it is always safe to call stateMgr.checkpoint(checkpointableOffsets()) and not condition on the state and call stateMgr.checkpoint(emptySet()).\nAnd if we now allow committing in SUSPENDED state as part of closing (i.e. suspend -> commit -> close), similar rules apply: if we are suspending from a RESTORING state, then in postCommit while we ``stateMgr.checkpoint(checkpointableOffsets())thecheckpointableOffsets` would always be empty; if we are suspending from a RUNNING state it would contain some offsets.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393384266", "createdAt": "2020-03-17T00:37:48Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -144,6 +149,7 @@ void handleCorruption(final Map<TaskId, Collection<TopicPartition>> taskWithChan\n             final Collection<TopicPartition> corruptedPartitions = entry.getValue();\n             task.markChangelogAsCorrupted(corruptedPartitions);\n \n+            task.prepareCloseDirty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0MTUyOA=="}, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NTQzOA==", "bodyText": "See my other comments: we should not commit in CREATED, RESTORING and SUSPENDED state, and it's better just to let the prepareXX function to indicate if there's anything to commit based on its state internally than letting task-manager to branch on the task state -- more specifically, here the prepareClose call should not return the map of checkpoints but the map of partition -> partition-timestamps (if empty it means nothing to commit), since the checkpoint map are not needed at task-manager at all and post commit, if the offsets should be empty it would still be empty.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393385438", "createdAt": "2020-03-17T00:41:49Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -182,30 +193,49 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 cleanupTask(task);\n \n                 try {\n-                    task.closeClean();\n+                    checkpointPerTask.put(task, task.prepareCloseClean());\n+                    if (task.state() != CREATED) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIzMzc3Mw=="}, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NzQ5MQ==", "bodyText": "Same here: not only CREATED, but also RESTORING and SUSPENDED tasks should not be included in consumedOffsetsAndMetadataPerTask and we should not let the task-manager to peek its state.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393387491", "createdAt": "2020-03-17T00:50:23Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -497,24 +555,49 @@ private void cleanupTask(final Task task) {\n \n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n-        final Iterator<Task> iterator = tasks.values().iterator();\n-        while (iterator.hasNext()) {\n-            final Task task = iterator.next();\n+\n+        final Map<Task, Map<TopicPartition, Long>> checkpointPerTask = new HashMap<>();\n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+\n+        for (final Task task : tasks.values()) {\n             cleanupTask(task);\n \n             if (clean) {\n                 try {\n-                    task.closeClean();\n+                    checkpointPerTask.put(task, task.prepareCloseClean());\n+                    if (task.state() != CREATED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4Nzg0MA==", "bodyText": "\"as above\" :)", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393387840", "createdAt": "2020-03-17T00:51:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -555,14 +653,46 @@ int maybeCommitActiveTasksPerUserRequested() {\n         if (rebalanceInProgress) {\n             return -1;\n         } else {\n-            int commits = 0;\n+            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n             for (final Task task : activeTaskIterable()) {\n                 if (task.commitRequested() && task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0MzkxMA=="}, "originalCommit": null, "originalPosition": 283}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2MzIzOTg1", "url": "https://github.com/apache/kafka/pull/8218#pullrequestreview-376323985", "createdAt": "2020-03-17T19:03:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxOTowMzoxNlrOF3qGRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxOToyNTozOFrOF3qy5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNTczMg==", "bodyText": "Thanks for the cleanup!", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393905732", "createdAt": "2020-03-17T19:03:16Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -61,7 +61,6 @@\n import static org.hamcrest.CoreMatchers.is;\n import static org.hamcrest.MatcherAssert.assertThat;\n \n-@SuppressWarnings(\"unchecked\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNjk3MQ==", "bodyText": "Why we have to transit to SUSPENDED before prepare-closing? Originally we want to check that CREATED state can still trigger close.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393906971", "createdAt": "2020-03-17T19:05:38Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -264,7 +265,10 @@ public void shouldAttemptToDeleteStateDirectoryWhenCloseDirtyAndEosEnabled() thr\n         ctrl.replay();\n \n         task = createStatefulTask(createConfig(true, \"100\"), true, stateManager);\n-        task.transitionTo(Task.State.CLOSING);\n+        task.transitionTo(Task.State.RESTORING);\n+        task.transitionTo(Task.State.RUNNING);\n+        task.transitionTo(Task.State.SUSPENDED);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNzc1NA==", "bodyText": "Not introduced in this PR: could we add test checking CLOSED state should not commit as well?\nAlso checking SUSPENDED state close-call is no-op.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393907754", "createdAt": "2020-03-17T19:07:06Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -1292,7 +1275,14 @@ public void shouldThrowStreamsExceptionWhenFetchCommittedFailed() {\n     public void shouldThrowIfCommittingOnIllegalState() {\n         task = createStatelessTask(createConfig(false, \"100\"), StreamsConfig.METRICS_LATEST);\n \n-        assertThrows(IllegalStateException.class, task::commit);\n+        assertThrows(IllegalStateException.class, task::prepareCommit);\n+    }\n+\n+    @Test\n+    public void shouldThrowIfPostCommittingOnIllegalState() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxMTUzMg==", "bodyText": "Why making commitTransaction is less elegant? I thought that was fine since StreamsProducer is inside the internals package anyways? In fact, in TTD we have access to InternalTopologyBuilder accessing it functions (we used to also have a wrapper of InternalTopologyBuilder which we removed later) so I thought that was the agreed pattern.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393911532", "createdAt": "2020-03-17T19:14:37Z", "author": {"login": "guozhangwang"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/processor/internals/TestDriverProducer.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.utils.LogContext;\n+\n+import java.util.Map;\n+\n+public class TestDriverProducer extends StreamsProducer {\n+\n+    public TestDriverProducer(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final LogContext logContext) {\n+        super(producer, applicationId, logContext);\n+    }\n+\n+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDc0MQ=="}, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxNzE1Ng==", "bodyText": "This is a meta comment: since we moved the commit logic out of the tasks into task-manager already, we should add the check that:\n\ninside the task manager, if the commit failed with fatal errors, the corresponding follow-up steps (postCommit, suspend, closeClean) should be skipped, and the exception is thrown out of the task-manager to thread\nif commit failed with fenced errors, follow-up steps are also skipped (tasks state should be un-changed) and the task-migration exception is thrown out of the task-manager.", "url": "https://github.com/apache/kafka/pull/8218#discussion_r393917156", "createdAt": "2020-03-17T19:25:38Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -812,7 +877,79 @@ public void closeClean() {\n         assertThat(thrown.getCause().getMessage(), equalTo(null));\n     }\n \n+    @Test\n+    public void shouldThrowTaskMigratedExceptionOnCommitFailed() {\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setCommitNeeded();\n+        taskManager.tasks().put(taskId01, task01);\n+\n+        consumer.commitSync(new HashMap<>());\n+        expectLastCall().andThrow(new CommitFailedException());\n+        replay(consumer);\n+\n+        final TaskMigratedException thrown = assertThrows(\n+            TaskMigratedException.class,\n+            () -> taskManager.commitAll()\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"Consumer committing offsets failed, indicating the corresponding thread is no longer part of the group; it means all tasks belonging to this thread should be migrated.\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyODgyMA=="}, "originalCommit": null, "originalPosition": 216}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5de5b1250020bac04d4763cfe54d5ad78e2168e", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/b5de5b1250020bac04d4763cfe54d5ad78e2168e", "committedDate": "2020-03-18T17:04:03Z", "message": "KAFKA-9441: Unify committing within TaskManager"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20021441a3b69b3bc26b6608457fcb1fec779087", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/20021441a3b69b3bc26b6608457fcb1fec779087", "committedDate": "2020-03-18T17:04:03Z", "message": "Github comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2f2c04daa41d692095cb20063b45a31850cc1cb", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/a2f2c04daa41d692095cb20063b45a31850cc1cb", "committedDate": "2020-03-18T17:04:03Z", "message": "Github comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "45e5633f16c6305e5d645e36da3fa9917b53c8df", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/45e5633f16c6305e5d645e36da3fa9917b53c8df", "committedDate": "2020-03-18T17:04:03Z", "message": "Fix bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "693b33d6f5365579c85f42d7a9663a2db491b711", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/693b33d6f5365579c85f42d7a9663a2db491b711", "committedDate": "2020-03-18T17:04:03Z", "message": "Github comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "693b33d6f5365579c85f42d7a9663a2db491b711", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/693b33d6f5365579c85f42d7a9663a2db491b711", "committedDate": "2020-03-18T17:04:03Z", "message": "Github comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b03a487a518756f7c9acd1f3177b12b407867d5", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/4b03a487a518756f7c9acd1f3177b12b407867d5", "committedDate": "2020-03-19T08:48:44Z", "message": "Bug fix"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 372, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}