{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM3MzMzMjAx", "number": 9736, "title": "Add configurable workloads and E2E latency tracking to Trogdor.", "bodyText": "This allows us to run highly granular and configurable workloads to directly simulate customer scenarios and measure the end to end latency all within Trogdor.\nThis creates a new ConfigurableProducer workload that can be used to tune many parts of the workload better than the ProduceBench workload.\nThis also creates all the helper classes for the ConfigurableProducer workload.\nThis adds a new parameter to the ConsumeBench workload to allow for processing of records after polling them.\nThis also adds a new E2E latency test utilizing the new record processor within the ConsumeBench workload.\n\nConfigurableProducer workload\nThe ConfigurableProducer workload allows for customized and even variable configurations in terms of messages per second, message size, batch size, key size, and even the ability to target a specific partition out of a topic.\nThe parameters that differ from ProduceBenchSpec:\n\nflushGenerator - Used to instruct the KafkaProducer when to issue flushes.  This allows us to simulate variable batching since batch flushing is not currently exposed within the KafkaProducer class.\nthroughputGenerator - Used to throttle the ConfigurableProducerWorker based on a calculated number of messages within a window.\nactiveTopic - This class only supports execution against a single topic at a time.  If more than one topic is specified, the ConfigurableProducerWorker will throw an error.\nactivePartition - Specify a specific partition number within the activeTopic to run load against, or specify -1 to allow use of all partitions.\n\nHere is an example spec:\n{\n    \"startMs\": 1606949497662,\n    \"durationMs\": 3600000,\n    \"producerNode\": \"trogdor-agent-0\",\n    \"bootstrapServers\": \"some.example.kafka.server:9091\",\n    \"flushGenerator\": {\n        \"type\": \"gaussian\",\n        \"messagesPerFlushAverage\": 16,\n        \"messagesPerFlushDeviation\": 4\n    },\n    \"throughputGenerator\": {\n        \"type\": \"gaussian\",\n        \"messagesPerSecondAverage\": 500,\n        \"messagesPerSecondDeviation\": 50,\n        \"windowsUntilRateChange\": 100,\n        \"windowSizeMs\": 100\n    },\n    \"keyGenerator\": {\n        \"type\": \"constant\",\n        \"size\": 8\n    },\n    \"valueGenerator\": {\n        \"type\": \"gaussianTimestampRandom\",\n        \"messageSizeAverage\": 512,\n        \"messageSizeDeviation\": 100,\n        \"timestampBytes\": 8,\n        \"messagesUntilSizeChange\": 100\n    },\n    \"producerConf\": {\n        \"acks\": \"all\"\n    },\n    \"commonClientConf\": {},\n    \"adminClientConf\": {},\n    \"activeTopic\": {\n        \"topic0\": {\n            \"numPartitions\": 100,\n            \"replicationFactor\": 3,\n            \"configs\": {\n                \"retention.ms\": \"1800000\"\n            }\n        }\n    },\n    \"activePartition\": 5\n}\n\nThis example spec performed the following:\n\nRan on trogdor-agent-0 for 1 hour starting at 2020-12-02 22:51:37.662 GMT\nProduced with acks=all to Partition 5 of topic0 on kafka server some.example.kafka.server:9091.\nThe average batch had 16 messages, with a standard deviation of 4 messages.\nThe message had a 8-bit constant key, with an average value of 512 bytes and a standard deviation of 100 bytes.\nThe messages had millisecond timestamps embedded in the first 8-bytes of the value.\nThe average throughput was 500 messages/second, with a window of 100ms and a deviation of 50 messages/second.\n\n\nConsumeBench workload changes\nThis commit adds the ability for the ConsumeBench workloads to optionally process the records returned through the consumer poll call.  This is done by specifying a new recordProcessor parameter.  The record processor's status is then included in the workload's status.\nRecordProcessor\nThis interface provides the ability to optionally process records after the ConsumeBench workload polls them.  The interface provides for the ability to include additional data in the status output.\nCurrently there are 2 processing methods:\n\nDisabled, by not specifying this parameter.\ntimestamp will use TimestampRecordProcessor to process records containing a timestamp in the first several bytes of the message.\n\nTimestampRecordProcessor\nThis includes a TimestampRecordProcessor class to process records containing a timestamp in the first several bytes of the message.   This class will process records containing timestamps and generate a histogram based on the data.  It will then be present in the status from the ConsumeBenchWorker class.  This must be used with a timestamped PayloadGenerator implementation.\nHere's an example spec:\n{\n   \"type\": \"timestampRandom\",\n   \"timestampBytes\": 8,\n   \"histogramMaxMs\": 10000,\n   \"histogramMinMs\": 0,\n   \"histogramStepMs\": 1\n}\n\nThis will track total E2E latency up to 10 seconds, using 1ms resolution and a timestamp size of 8 bytes.\n\nFlushGenerator\nA FlushGenerator is used to facilitate flushing the KafkaProducers on a cadence specified by the user.  This is useful to simulate a specific number of messages in a batch regardless of the message size, since batch flushing is not exposed in the KafkaProducer.\nCurrently there are 3 flushing methods:\n\nDisabled, by not specifying this parameter.\nconstant will use ConstantFlushGenerator to keep the number of messages per batch constant.\ngaussian will use GaussianFlushGenerator to vary the number of messages per batch on a normal distribution.\n\nConstantFlushGenerator\nThis generator will flush the producer after a specific number of messages.  This does not directly control when KafkaProducer will batch, this only makes best effort.  This also cannot tell when a KafkaProducer batch is closed.  If the KafkaProducer sends a batch before this executes, this will continue to execute on its own cadence.\nHere is an example spec:\n{\n   \"type\": \"constant\",\n   \"messagesPerFlush\": 16\n}\n\nThis example will flush the producer every 16 messages.\nGaussianFlushGenerator\nThis generator will flush the producer after a specific number of messages, determined by a gaussian distribution.  This does not directly control when KafkaProducer will batch, this only makes best effort.  This also cannot tell when a KafkaProducer batch is closed.  If the KafkaProducer sends a batch before this executes, this will continue to execute on its own cadence.\nHere is an example spec:\n{\n   \"type\": \"gaussian\",\n   \"messagesPerFlushAverage\": 16,\n   \"messagesPerFlushDeviation\": 4\n}\n\nThis example will flush the producer on average every 16 messages, assuming linger.ms and batch.size allow for\nit.  That average changes based on a normal distribution after each flush:\n\nAn average of the flushes will be at 16 messages.\n~68% of the flushes are at between 12 and 20 messages.\n~95% of the flushes are at between 8 and 24 messages.\n~99% of the flushes are at between 4 and 28 messages.\n\n\nThroughputGenerator\nSimilar to the throttle class, except a simpler design.  This interface is used to facilitate running a configurable number of messages per second by throttling if the throughput goes above a certain amount.\nCurrently there are 2 throughput methods:\n\nconstant will use ConstantThroughputGenerator to keep the number of messages per second constant.\ngaussian will use GaussianThroughputGenerator to vary the number of messages per second on a normal distribution.\n\nConstantThroughputGenerator\nThis throughput generator configures constant throughput.  The lower the window size, the smoother the traffic will be. Using a 100ms window offers no noticeable spikes in traffic while still being long enough to avoid too much overhead.\nDue to binary nature of throughput in terms of messages sent in a window, each window will send at least 1 message, and each window sends the same number of messages, rounded down. For example, 99 messages per second with a 100ms window will only send 90 messages per second, or 9 messages per window. Another example, in order to send only 5 messages per second, a window size of 200ms is required. In cases like these, both the messagesPerSecond and windowSizeMs parameters should be adjusted together to achieve more accurate throughput.\nHere is an example spec:\n{\n   \"type\": \"constant\",\n   \"messagesPerSecond\": 500,\n   \"windowSizeMs\": 100\n}\n\nThis will produce a workload that runs 500 messages per second, with a maximum resolution of 50 messages per 100 millisecond.\nGaussianThroughputGenerator\nThis throughput generator configures throughput with a gaussian normal distribution on a per-window basis. You can specify how many windows to keep the throughput at the rate before changing. All traffic will follow a gaussian distribution centered around messagesPerSecondAverage with a deviation of messagesPerSecondDeviation.  The lower the window size, the smoother the traffic will be. Using a 100ms window offers no noticeable spikes in traffic while still being long enough to avoid too much overhead.\nDue to binary nature of throughput in terms of messages sent in a window, this does not work well for an average throughput of less than 5 messages per window.  In cases where you want lower throughput, the windowSizeMs must be adjusted accordingly.\nHere is an example spec:\n{\n   \"type\": \"gaussian\",\n   \"messagesPerSecondAverage\": 500,\n   \"messagesPerSecondDeviation\": 50,\n   \"windowsUntilRateChange\": 100,\n   \"windowSizeMs\": 100\n}\n\nThis will produce a workload that runs on average 500 messages per second, however that speed will change every 10 seconds due to the windowSizeMs * windowsUntilRateChange parameters. The throughput will have the following normal distribution:\n\nAn average of the throughput windows of 500 messages per second.\n~68% of the throughput windows are between 450 and 550 messages per second.\n~95% of the throughput windows are between 400 and 600 messages per second.\n~99% of the throughput windows are between 350 and 650 messages per second.\n\n\nAdditional Payload Generators\nThis implementation also offers additional payload generators to facilitate the tests these workloads are designed to run.  These are also compatible with the existing ProduceBench workloads.\nTimestampRandomPayloadGenerator\nThis generator generates timestamped pseudo-random payloads that can be reproduced from run to run.  The guarantees are the same as those of java.util.Random.  The timestamp used for this class is in milliseconds since epoch, encoded directly to the first several bytes of the payload. This should be used in conjunction with TimestampRecordProcessor in the Consumer to measure true end-to-end latency of a system.\n\nsize - The size in bytes of each message.\ntimestampBytes - The amount of bytes to use for the timestamp.  Usually 8.\nseed - Used to initialize Random() to remove some non-determinism.\n\nHere is an example spec:\n{\n   \"type\": \"timestampRandom\",\n   \"size\": 512,\n   \"timestampBytes\": 8\n}\n\nThis will generate a 512-byte random message with the first 8 bytes encoded with the timestamp.\nGaussianTimestampRandomPayloadGenerator\nThis class behaves identically to TimestampRandomPayloadGenerator, except the message size follows a gaussian distribution.  This should be used in conjunction with TimestampRecordProcessor in the Consumer to measure true end-to-end latency of a system.\n\nmessageSizeAverage - The average size in bytes of each message.\nmessageSizeDeviation - The standard deviation to use when calculating message size.\ntimestampBytes - The amount of bytes to use for the timestamp.  Usually 8.\nmessagesUntilSizeChange - The number of messages to keep at the same size.\nseed - Used to initialize Random() to remove some non-determinism.\n\nHere is an example spec:\n{\n   \"type\": \"gaussianTimestampRandom\",\n   \"messageSizeAverage\": 512,\n   \"messageSizeDeviation\": 100,\n   \"timestampBytes\": 8,\n   \"messagesUntilSizeChange\": 100\n}\n\nThis will generate messages on a gaussian distribution with an average size each 512-bytes and the first 8 bytes encoded with the timestamp.  The message sizes will have a standard deviation of 100 bytes, and the size will only change every 100 messages.  The distribution of messages will be as follows:\n\nThe average size of the messages are 512 bytes.\n~68% of the messages are between 412 and 612 bytes\n~95% of the messages are between 312 and 712 bytes\n~99% of the messages are between 212 and 812 bytes\n\n\nTesting\nNew Functionality\nThe ConfigurableProducer workload was tested by running various scenarios and verifying the metrics within the Kafka cluster matched the scenario as defined.\nExisting Functionality\nThe ConsumeBench workload was tested without specifying the recordProcessor parameter to verify it still behaves as it did prior to this patch set.  All other code paths are in a new workload.", "createdAt": "2020-12-11T19:53:16Z", "url": "https://github.com/apache/kafka/pull/9736", "merged": true, "mergeCommit": {"oid": "baef516789f0d80ce1faba5d257a905ecb27e6fe"}, "closed": true, "closedAt": "2020-12-18T21:03:59Z", "author": {"login": "scott-hendricks"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdlNcfTgH2gAyNTM3MzMzMjAxOjhlYWQyY2VhYjcyMThkZDQ3ZGQ0NDc0OThhZDM2MDEyNjRiNGM1Mzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdmK7B2gH2gAyNTM3MzMzMjAxOjY1OTFjMjI4NGFjZDMxZmRlNzNlNzc0YzQyNjlmMjc4YzdkNWQwY2M=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8ead2ceab7218dd47dd447498ad3601264b4c537", "author": {"user": {"login": "scott-hendricks", "name": "Scott Hendricks"}}, "url": "https://github.com/apache/kafka/commit/8ead2ceab7218dd47dd447498ad3601264b4c537", "committedDate": "2020-12-11T19:52:19Z", "message": "Add a Timestamp Payload Generator and Record Processor."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bca4ee1dabce1b90760e6a3ca1204960623d1b4b", "author": {"user": {"login": "scott-hendricks", "name": "Scott Hendricks"}}, "url": "https://github.com/apache/kafka/commit/bca4ee1dabce1b90760e6a3ca1204960623d1b4b", "committedDate": "2020-12-11T20:39:17Z", "message": "Fix spotbugsMain output."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNjI2MDM0", "url": "https://github.com/apache/kafka/pull/9736#pullrequestreview-550626034", "createdAt": "2020-12-11T22:45:05Z", "commit": {"oid": "bca4ee1dabce1b90760e6a3ca1204960623d1b4b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMjo0NTowNlrOIETKGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMjo0NTowNlrOIETKGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTM3OTA5OA==", "bodyText": "it would be good to have a default for timestampBytes, I think.  Basically if this gets set to 0, set it to 8 instead?\nAlso, when would we want to change the number of timestamp bytes?", "url": "https://github.com/apache/kafka/pull/9736#discussion_r541379098", "createdAt": "2020-12-11T22:45:06Z", "author": {"login": "cmccabe"}, "path": "tools/src/main/java/org/apache/kafka/trogdor/workload/GaussianTimestampRandomPayloadGenerator.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.trogdor.workload;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.kafka.common.utils.Time;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.Random;\n+\n+/**\n+ * This class behaves identically to TimestampRandomPayloadGenerator, except the message size follows a gaussian\n+ * distribution.\n+ *\n+ * This should be used in conjunction with TimestampRecordProcessor in the Consumer to measure true end-to-end latency\n+ * of a system.\n+ *\n+ * `messageSizeAverage` - The average size in bytes of each message.\n+ * `messageSizeDeviation` - The standard deviation to use when calculating message size.\n+ * `timestampBytes` - The amount of bytes to use for the timestamp.  Usually 8.\n+ * `messagesUntilSizeChange` - The number of messages to keep at the same size.\n+ * `seed` - Used to initialize Random() to remove some non-determinism.\n+ *\n+ * Here is an example spec:\n+ *\n+ * {\n+ *    \"type\": \"gaussianTimestampRandom\",\n+ *    \"messageSizeAverage\": 512,\n+ *    \"messageSizeDeviation\": 100,\n+ *    \"timestampBytes\": 8,\n+ *    \"messagesUntilSizeChange\": 100\n+ * }\n+ *\n+ * This will generate messages on a gaussian distribution with an average size each 512-bytes and the first 8 bytes\n+ * encoded with the timestamp.  The message sizes will have a standard deviation of 100 bytes, and the size will only\n+ * change every 100 messages.  The distribution of messages will be as follows:\n+ *\n+ *    The average size of the messages are 512 bytes.\n+ *    ~68% of the messages are between 412 and 612 bytes\n+ *    ~95% of the messages are between 312 and 712 bytes\n+ *    ~99% of the messages are between 212 and 812 bytes\n+ */\n+\n+public class GaussianTimestampRandomPayloadGenerator implements PayloadGenerator {\n+    private final int messageSizeAverage;\n+    private final int messageSizeDeviation;\n+    private final int timestampBytes;\n+    private final int messagesUntilSizeChange;\n+    private final long seed;\n+\n+    private final Random random = new Random();\n+    private final ByteBuffer buffer;\n+\n+    private int messageTracker = 0;\n+    private int messageSize = 0;\n+\n+    @JsonCreator\n+    public GaussianTimestampRandomPayloadGenerator(@JsonProperty(\"messageSizeAverage\") int messageSizeAverage,\n+                                                   @JsonProperty(\"messageSizeDeviation\") int messageSizeDeviation,\n+                                                   @JsonProperty(\"timestampBytes\") int timestampBytes,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bca4ee1dabce1b90760e6a3ca1204960623d1b4b"}, "originalPosition": 77}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6591c2284acd31fde73e774c4269f278c7d5d0cc", "author": {"user": {"login": "scott-hendricks", "name": "Scott Hendricks"}}, "url": "https://github.com/apache/kafka/commit/6591c2284acd31fde73e774c4269f278c7d5d0cc", "committedDate": "2020-12-14T19:29:53Z", "message": "Remove timestampBytes from E2E Latency Tests."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2262, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}