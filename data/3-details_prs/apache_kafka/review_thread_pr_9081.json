{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU2NzY4ODc0", "number": 9081, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMTo1Mjo1M1rOESNdaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwNDo0MDo0OVrOESphAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NTMwMzQ1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMTo1Mjo1M1rOG3RS5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMTozNDoyOVrOG35Y5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwODIyOA==", "bodyText": "javadoc for this method should be updated as well.", "url": "https://github.com/apache/kafka/pull/9081#discussion_r460608228", "createdAt": "2020-07-27T01:52:53Z", "author": {"login": "huxihx"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "diffHunk": "@@ -687,7 +687,7 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs\n         throwIfProducerClosed();\n         TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, groupMetadata);\n         sender.wakeup();\n-        result.await();\n+        result.await(maxBlockTimeMs, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25a7cebe917889cb645ff87c6effc869cfc51a95"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2NTEyNQ==", "bodyText": "I wrote some description related to TimeoutException and InterruptedException", "url": "https://github.com/apache/kafka/pull/9081#discussion_r461265125", "createdAt": "2020-07-28T01:34:29Z", "author": {"login": "sasakitoa"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "diffHunk": "@@ -687,7 +687,7 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs\n         throwIfProducerClosed();\n         TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, groupMetadata);\n         sender.wakeup();\n-        result.await();\n+        result.await(maxBlockTimeMs, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwODIyOA=="}, "originalCommit": {"oid": "25a7cebe917889cb645ff87c6effc869cfc51a95"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NTMxNTEyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMjowMDoyN1rOG3RZVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMTozMzoyM1rOG35XwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwOTg3Ng==", "bodyText": "Using Map(new TopicPartition(topic1, 0) -> new OffsetAndMetadata(0)).asJava is better. No need to import scala.collection.mutable package.", "url": "https://github.com/apache/kafka/pull/9081#discussion_r460609876", "createdAt": "2020-07-27T02:00:27Z", "author": {"login": "huxihx"}, "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "diffHunk": "@@ -406,6 +406,26 @@ class TransactionsTest extends KafkaServerTestHarness {\n     TestUtils.waitUntilTrue(() => offsetAndMetadata.equals(consumer.committed(Set(tp).asJava).get(tp)), \"cannot read committed offset\")\n   }\n \n+  @Test(expected = classOf[TimeoutException])\n+  def testSendOffsetsToTransactionTimeout(): Unit = {\n+    val producer = createTransactionalProducer(\"transactionProducer\", maxBlockMs = 1000)\n+    producer.initTransactions()\n+    producer.beginTransaction()\n+    producer.send(new ProducerRecord[Array[Byte], Array[Byte]](topic1, \"foo\".getBytes, \"bar\".getBytes))\n+\n+    for (i <- 0 until servers.size)\n+      killBroker(i)\n+\n+    val offsets = new mutable.HashMap[TopicPartition, OffsetAndMetadata]().asJava\n+    offsets.put(new TopicPartition(topic1, 0), new OffsetAndMetadata(0))\n+    try {\n+      producer.sendOffsetsToTransaction(offsets, \"test-group\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25a7cebe917889cb645ff87c6effc869cfc51a95"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2NDgzMw==", "bodyText": "Replaced from mutable.HashMap to Map", "url": "https://github.com/apache/kafka/pull/9081#discussion_r461264833", "createdAt": "2020-07-28T01:33:23Z", "author": {"login": "sasakitoa"}, "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "diffHunk": "@@ -406,6 +406,26 @@ class TransactionsTest extends KafkaServerTestHarness {\n     TestUtils.waitUntilTrue(() => offsetAndMetadata.equals(consumer.committed(Set(tp).asJava).get(tp)), \"cannot read committed offset\")\n   }\n \n+  @Test(expected = classOf[TimeoutException])\n+  def testSendOffsetsToTransactionTimeout(): Unit = {\n+    val producer = createTransactionalProducer(\"transactionProducer\", maxBlockMs = 1000)\n+    producer.initTransactions()\n+    producer.beginTransaction()\n+    producer.send(new ProducerRecord[Array[Byte], Array[Byte]](topic1, \"foo\".getBytes, \"bar\".getBytes))\n+\n+    for (i <- 0 until servers.size)\n+      killBroker(i)\n+\n+    val offsets = new mutable.HashMap[TopicPartition, OffsetAndMetadata]().asJava\n+    offsets.put(new TopicPartition(topic1, 0), new OffsetAndMetadata(0))\n+    try {\n+      producer.sendOffsetsToTransaction(offsets, \"test-group\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwOTg3Ng=="}, "originalCommit": {"oid": "25a7cebe917889cb645ff87c6effc869cfc51a95"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTg5ODcyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwNDozOTo1M1rOG38VYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNjowNDoyNlrOG4Ty0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMxMzM3Nw==", "bodyText": "nit: could use servers.indices", "url": "https://github.com/apache/kafka/pull/9081#discussion_r461313377", "createdAt": "2020-07-28T04:39:53Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "diffHunk": "@@ -406,6 +406,26 @@ class TransactionsTest extends KafkaServerTestHarness {\n     TestUtils.waitUntilTrue(() => offsetAndMetadata.equals(consumer.committed(Set(tp).asJava).get(tp)), \"cannot read committed offset\")\n   }\n \n+  @Test(expected = classOf[TimeoutException])\n+  def testSendOffsetsToTransactionTimeout(): Unit = {\n+    val producer = createTransactionalProducer(\"transactionProducer\", maxBlockMs = 1000)\n+    producer.initTransactions()\n+    producer.beginTransaction()\n+    producer.send(new ProducerRecord[Array[Byte], Array[Byte]](topic1, \"foo\".getBytes, \"bar\".getBytes))\n+\n+    for (i <- 0 until servers.size)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d5e4984f4c5287513b47ddecf6ccb7de9e90268"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTY5Nzc0Nw==", "bodyText": "Modified from size to indices, thanks.", "url": "https://github.com/apache/kafka/pull/9081#discussion_r461697747", "createdAt": "2020-07-28T16:04:26Z", "author": {"login": "sasakitoa"}, "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "diffHunk": "@@ -406,6 +406,26 @@ class TransactionsTest extends KafkaServerTestHarness {\n     TestUtils.waitUntilTrue(() => offsetAndMetadata.equals(consumer.committed(Set(tp).asJava).get(tp)), \"cannot read committed offset\")\n   }\n \n+  @Test(expected = classOf[TimeoutException])\n+  def testSendOffsetsToTransactionTimeout(): Unit = {\n+    val producer = createTransactionalProducer(\"transactionProducer\", maxBlockMs = 1000)\n+    producer.initTransactions()\n+    producer.beginTransaction()\n+    producer.send(new ProducerRecord[Array[Byte], Array[Byte]](topic1, \"foo\".getBytes, \"bar\".getBytes))\n+\n+    for (i <- 0 until servers.size)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMxMzM3Nw=="}, "originalCommit": {"oid": "3d5e4984f4c5287513b47ddecf6ccb7de9e90268"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTkwMDE3OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwNDo0MDo0OVrOG38WPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNjoxMToxMVrOG4UFSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMxMzU5Nw==", "bodyText": "Do we have unit test coverage for other transaction API max blocking as well? Do you mind adding them as separate tests and share the same module?", "url": "https://github.com/apache/kafka/pull/9081#discussion_r461313597", "createdAt": "2020-07-28T04:40:49Z", "author": {"login": "abbccdda"}, "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "diffHunk": "@@ -406,6 +406,26 @@ class TransactionsTest extends KafkaServerTestHarness {\n     TestUtils.waitUntilTrue(() => offsetAndMetadata.equals(consumer.committed(Set(tp).asJava).get(tp)), \"cannot read committed offset\")\n   }\n \n+  @Test(expected = classOf[TimeoutException])\n+  def testSendOffsetsToTransactionTimeout(): Unit = {\n+    val producer = createTransactionalProducer(\"transactionProducer\", maxBlockMs = 1000)\n+    producer.initTransactions()\n+    producer.beginTransaction()\n+    producer.send(new ProducerRecord[Array[Byte], Array[Byte]](topic1, \"foo\".getBytes, \"bar\".getBytes))\n+\n+    for (i <- 0 until servers.size)\n+      killBroker(i)\n+\n+    try {\n+      producer.sendOffsetsToTransaction(Map(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d5e4984f4c5287513b47ddecf6ccb7de9e90268"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTcwMjQ3Mg==", "bodyText": "I added some timeout tests for initTransaction, commitTransction, abortTransaction using same base method.\nIs this implementation correct what you intended?", "url": "https://github.com/apache/kafka/pull/9081#discussion_r461702472", "createdAt": "2020-07-28T16:11:11Z", "author": {"login": "sasakitoa"}, "path": "core/src/test/scala/integration/kafka/api/TransactionsTest.scala", "diffHunk": "@@ -406,6 +406,26 @@ class TransactionsTest extends KafkaServerTestHarness {\n     TestUtils.waitUntilTrue(() => offsetAndMetadata.equals(consumer.committed(Set(tp).asJava).get(tp)), \"cannot read committed offset\")\n   }\n \n+  @Test(expected = classOf[TimeoutException])\n+  def testSendOffsetsToTransactionTimeout(): Unit = {\n+    val producer = createTransactionalProducer(\"transactionProducer\", maxBlockMs = 1000)\n+    producer.initTransactions()\n+    producer.beginTransaction()\n+    producer.send(new ProducerRecord[Array[Byte], Array[Byte]](topic1, \"foo\".getBytes, \"bar\".getBytes))\n+\n+    for (i <- 0 until servers.size)\n+      killBroker(i)\n+\n+    try {\n+      producer.sendOffsetsToTransaction(Map(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMxMzU5Nw=="}, "originalCommit": {"oid": "3d5e4984f4c5287513b47ddecf6ccb7de9e90268"}, "originalPosition": 15}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2022, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}