{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE4MjAwNjAy", "number": 9582, "title": "KAFKA-6687: rewrite topology to allow reading the same topic multiple times in the DSL", "bodyText": "Needed to fix this on the side in order to more easily set up some experiments, so here's the PR.\nAllows a user to create multiple KStreams and/or KTables from the same topic, collection of topics, or pattern. At the moment this isn't possible since we can only consume from a topic once, and each source topic maps to a single source node in the topology. The \"fix\" is just to rewrite the logical plan and merge any duplicate source nodes into a single node before it gets compiled into the physical topology.\nThe one exception is when the stream/table are subscribed to an overlapping-but-unequal collection of topics, which I left as future work (with a TODO in the comments describing a possible solution). If the offset reset policy doesn't match we just throw a TopologyException.\nedit: tables are much more complicated so I opted to restrict things to just multiple KStreams for now on and consider allowing multiple KTables (or KStream+KTable) as followup work", "createdAt": "2020-11-10T04:29:23Z", "url": "https://github.com/apache/kafka/pull/9582", "merged": true, "mergeCommit": {"oid": "cb3dc6785bc8263d8e1fc695e499e1eeb9df52a4"}, "closed": true, "closedAt": "2020-11-13T22:35:27Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdbCgPcgBqjM5NzcwNDgxMDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdcMg3AAFqTUzMDM1NDk4MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzIwMzQ0", "url": "https://github.com/apache/kafka/pull/9582#pullrequestreview-527720344", "createdAt": "2020-11-11T00:25:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDoyNTowNlrOHw01OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDoyNTowNlrOHw01OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1OTI4OA==", "bodyText": "Saw this and at first I thought it was broken because it only considers pattern-subscribed topics that happened to explicitly configure an offset reset policy. Unless I'm missing something here, that makes no sense and we should consider all  source patterns and whether they overlap.\nBut then I started thinking, why does it matter if they overlap? Just because one pattern is a substring of another does not mean that they'll match the same topics. So I think that we should actually just remove this restriction altogether. Am I missing anything here?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r520959288", "createdAt": "2020-11-11T00:25:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -410,18 +410,6 @@ public final void addSource(final Topology.AutoOffsetReset offsetReset,\n             }\n         }\n \n-        for (final Pattern otherPattern : earliestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-\n-        for (final Pattern otherPattern : latestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4OTI3Njky", "url": "https://github.com/apache/kafka/pull/9582#pullrequestreview-528927692", "createdAt": "2020-11-12T10:02:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMDowMjo0OVrOHxzUPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzo0MDoxMFrOHx7L8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTk4MzAzOQ==", "bodyText": "Could you please add a try-catch clause to better document the test?\nFor example:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void shouldAllowReadingFromSameTopic() {\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    builder.build();\n          \n          \n            \n                }\n          \n          \n            \n                public void shouldAllowReadingFromSameTopic() {\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    \n          \n          \n            \n                    try {\n          \n          \n            \n                        builder.build();\n          \n          \n            \n                    } catch (final TopologyException topologyException) {\n          \n          \n            \n                        fail(\"TopologyException not expected\");\n          \n          \n            \n                    }\n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nThis applies also to the other tests.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r521983039", "createdAt": "2020-11-12T10:02:49Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAwMTIwOQ==", "bodyText": "We could avoid the instanceof and the casting if we introduce a RootGraphNode with a method sourceNodes(). Since a root can only have source nodes and state stores as children, we could make the topology code in general a bit more type safe. As far as I can see that would need some additional changes outside the scope of this PR. So, feel free to not consider this comment for this PR and we can do another PR for that.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522001209", "createdAt": "2020-11-12T10:30:57Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));\n+\n+        for (final StreamsGraphNode graphNode : root.children()) {\n+            if (graphNode instanceof StreamSourceNode) {\n+                final StreamSourceNode<?, ?> currentSourceNode = (StreamSourceNode<?, ?>) graphNode;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAyNzk0Mg==", "bodyText": "Just to be clear. This improves the situation but it is not a complete solution, right? Assume we have a topic topicA. Patterns topic* and topi* both  match topicA but they are different when compared with this comparator. In that case a TopologyException would be thrown in the InternalTopologyBuilder, right?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522027942", "createdAt": "2020-11-12T11:14:37Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA5NTEzOA==", "bodyText": "Do we really need this comment and the comment on line 81. We get the same information when we navigate to the call and to the implementation of the methods with the difference that comments can start to be outdated without us noticing it.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522095138", "createdAt": "2020-11-12T13:12:56Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,21 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times\n+    public void merge(final StreamSourceNode<?, ?> other) {\n+        final AutoOffsetReset resetPolicy = consumedInternal.offsetResetPolicy();\n+        if (resetPolicy != null && !resetPolicy.equals(other.consumedInternal().offsetResetPolicy())) {\n+            log.error(\"Tried to merge source nodes {} and {} which are subscribed to the same topic/pattern, but \"\n+                          + \"the offset reset policies do not match\", this, other);\n+            throw new TopologyException(\"Can't configure different offset reset policies on the same input topic(s)\");\n+        }\n+        for (final StreamsGraphNode otherChild : other.children()) {\n+            // Move children from other to this, these calls take care of resetting the child's parents to this", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwNDkyMA==", "bodyText": "I agree on the first part.\nRegarding the second part, I had similiar thoughts when I wrote my comment in mergeDuplicateSourceNodes().\nBut I might also be missing something here.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522104920", "createdAt": "2020-11-12T13:28:56Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -410,18 +410,6 @@ public final void addSource(final Topology.AutoOffsetReset offsetReset,\n             }\n         }\n \n-        for (final Pattern otherPattern : earliestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-\n-        for (final Pattern otherPattern : latestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1OTI4OA=="}, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwNjA0Mg==", "bodyText": "Please use a collection with at least two topics to test the loop over the collections.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522106042", "createdAt": "2020-11-12T13:30:37Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowSubscribingToSamePattern() {\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowReadingFromSameCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(Collections.singletonList(\"topic\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwODA0Nw==", "bodyText": "What should happen if this reset policy is null and the other is not null? I guess we should also throw in that case, don't we?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522108047", "createdAt": "2020-11-12T13:33:55Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,21 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times\n+    public void merge(final StreamSourceNode<?, ?> other) {\n+        final AutoOffsetReset resetPolicy = consumedInternal.offsetResetPolicy();\n+        if (resetPolicy != null && !resetPolicy.equals(other.consumedInternal().offsetResetPolicy())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwOTkxNg==", "bodyText": "What should happen in this case? See also my comment in merge().\n    @Test\n    public void shouldThrowWhenSubscribedToATopicWithSetAndUnsetResetPolicies() {\n        builder.stream(\"topic\");\n        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n        assertThrows(TopologyException.class, builder::build);\n    }", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522109916", "createdAt": "2020-11-12T13:36:53Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowSubscribingToSamePattern() {\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowReadingFromSameCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldNotAllowReadingFromOverlappingAndUnequalCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(asList(\"topic\", \"anotherTopic\"));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithDifferentResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.LATEST));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithSetAndUnsetResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\");\n+        assertThrows(TopologyException.class, builder::build);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjExMTk4NA==", "bodyText": "Could you also add a test with two patterns with the same string but one with a set reset policy and one with unset reset policy like you did for the non-pattern case. Just to make it clear it should also throw in that case.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522111984", "createdAt": "2020-11-12T13:40:10Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowSubscribingToSamePattern() {\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowReadingFromSameCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldNotAllowReadingFromOverlappingAndUnequalCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(asList(\"topic\", \"anotherTopic\"));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithDifferentResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.LATEST));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithSetAndUnsetResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\");\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToAPatternWithDifferentResetPolicies() {\n+        builder.stream(Pattern.compile(\"some-regex\"), Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(Pattern.compile(\"some-regex\"), Consumed.with(AutoOffsetReset.LATEST));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5ODU1NDM5", "url": "https://github.com/apache/kafka/pull/9582#pullrequestreview-529855439", "createdAt": "2020-11-13T08:16:33Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoxNjozM1rOHyj1mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoxNjozM1rOHyj1mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc3ODAxMA==", "bodyText": "I would also remove this comment.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522778010", "createdAt": "2020-11-13T08:16:33Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,22 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5ODU3ODEw", "url": "https://github.com/apache/kafka/pull/9582#pullrequestreview-529857810", "createdAt": "2020-11-13T08:20:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoyMDo0MVrOHykGWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoyMDo0MVrOHykGWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc4MjI5OQ==", "bodyText": "Could you please extract this part in its own method since we use it also in a couple of other tests?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522782299", "createdAt": "2020-11-13T08:20:41Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +899,103 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowStreamsFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        try {\n+            builder.build();\n+        } catch (final TopologyException topologyException) {\n+            fail(\"TopologyException not expected\");\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5ODU5MDc0", "url": "https://github.com/apache/kafka/pull/9582#pullrequestreview-529859074", "createdAt": "2020-11-13T08:22:44Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMjAwOTI0", "url": "https://github.com/apache/kafka/pull/9582#pullrequestreview-530200924", "createdAt": "2020-11-13T16:19:45Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cade4f52f7ac487ac7e84b3e4babdfed4deac91a", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cade4f52f7ac487ac7e84b3e4babdfed4deac91a", "committedDate": "2020-11-13T19:12:50Z", "message": "merge source nodes and remove duplicates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf3d911f4eb17511a5594fccfea2f4ab94ed051f", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cf3d911f4eb17511a5594fccfea2f4ab94ed051f", "committedDate": "2020-11-13T19:12:50Z", "message": "improve error message"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "603eb0527579d7c08e2f57a9cd27a6cab4ddaf1d", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/603eb0527579d7c08e2f57a9cd27a6cab4ddaf1d", "committedDate": "2020-11-13T19:12:50Z", "message": "tests and minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac03bddcf15468f5f7026e1dec6fae9b796eada1", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/ac03bddcf15468f5f7026e1dec6fae9b796eada1", "committedDate": "2020-11-13T19:12:50Z", "message": "fix Pattern subscription"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9483ae9aef4bb2b0627d14003a9f4145617b8008", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/9483ae9aef4bb2b0627d14003a9f4145617b8008", "committedDate": "2020-11-13T19:12:50Z", "message": "simplify"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7eaa50be9d8a08405308207cab1390a909cbc4fa", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/7eaa50be9d8a08405308207cab1390a909cbc4fa", "committedDate": "2020-11-13T19:12:50Z", "message": "more unit test coverage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ad0fb057089039874b72aa62d772bebf49e24a6", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/3ad0fb057089039874b72aa62d772bebf49e24a6", "committedDate": "2020-11-13T19:12:50Z", "message": "unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "70a9cb690011eba3112866d0a6cc86b366ec0ef5", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/70a9cb690011eba3112866d0a6cc86b366ec0ef5", "committedDate": "2020-11-13T19:12:50Z", "message": "remove weird regex check and fix letter in test exception"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf32d6925b2ad8506179c2694595067cc26e1b43", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cf32d6925b2ad8506179c2694595067cc26e1b43", "committedDate": "2020-11-13T19:28:33Z", "message": "review feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "cf32d6925b2ad8506179c2694595067cc26e1b43", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cf32d6925b2ad8506179c2694595067cc26e1b43", "committedDate": "2020-11-13T19:28:33Z", "message": "review feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMzU0OTgx", "url": "https://github.com/apache/kafka/pull/9582#pullrequestreview-530354981", "createdAt": "2020-11-13T19:41:51Z", "commit": {"oid": "cf32d6925b2ad8506179c2694595067cc26e1b43"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxOTo0MTo1MlrOHy8mzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxOTo0MTo1MlrOHy8mzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzgyMQ==", "bodyText": "Filed https://issues.apache.org/jira/browse/KAFKA-10721", "url": "https://github.com/apache/kafka/pull/9582#discussion_r523183821", "createdAt": "2020-11-13T19:41:52Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));\n+\n+        for (final StreamsGraphNode graphNode : root.children()) {\n+            if (graphNode instanceof StreamSourceNode) {\n+                final StreamSourceNode<?, ?> currentSourceNode = (StreamSourceNode<?, ?>) graphNode;\n+\n+                if (currentSourceNode.topicPattern() != null) {\n+                    if (!patternsToSourceNodes.containsKey(currentSourceNode.topicPattern())) {\n+                        patternsToSourceNodes.put(currentSourceNode.topicPattern(), currentSourceNode);\n+                    } else {\n+                        final StreamSourceNode<?, ?> mainSourceNode = patternsToSourceNodes.get(currentSourceNode.topicPattern());\n+                        mainSourceNode.merge(currentSourceNode);\n+                        root.removeChild(graphNode);\n+                    }\n+                } else {\n+                    for (final String topic : currentSourceNode.topicNames()) {\n+                        if (!topicsToSourceNodes.containsKey(topic)) {\n+                            topicsToSourceNodes.put(topic, currentSourceNode);\n+                        } else {\n+                            final StreamSourceNode<?, ?> mainSourceNode = topicsToSourceNodes.get(topic);\n+                            // TODO we only merge source nodes if the subscribed topic(s) are an exact match, so it's still not\n+                            // possible to subscribe to topicA in one KStream and topicA + topicB in another. We could achieve\n+                            // this by splitting these source nodes into one topic per node and routing to the subscribed children", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf32d6925b2ad8506179c2694595067cc26e1b43"}, "originalPosition": 54}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2752, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}