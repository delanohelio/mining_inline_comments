{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0OTMzNTg5", "number": 9553, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMjo1MTowOVrOFCGCmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDo0MDo1MVrOFIWF4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NzQwNDQyOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMjo1MTowOVrOIBEfxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMjo1MTowOVrOIBEfxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk5MzE1Ng==", "bodyText": "Fix this logic to be more conservative. It should be a fatal error if one is negative and the other is positive.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r537993156", "createdAt": "2020-12-08T02:51:09Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1037,6 +1047,14 @@ private boolean handleFetchResponse(\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n                 });\n+            } else if (partitionResponse.snapshotId().epoch() >= 0 && partitionResponse.snapshotId().endOffset() >= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "460992277664c781c08627514d8391bbad8cf5af"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3Nzg5ODI5OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwNjowODo1MVrOIBImbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwNjowODo1MVrOIBImbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODA2MDM5Nw==", "bodyText": "The follower needs to update the log start offset and log end offset after it has successfully fetched a snapshot. I want to implement this part in this JIRA: https://issues.apache.org/jira/browse/KAFKA-10820", "url": "https://github.com/apache/kafka/pull/9553#discussion_r538060397", "createdAt": "2020-12-08T06:08:51Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n+            snapshot.read(buffer, partitionSnapshot.position());\n+            buffer.flip();\n+\n+            long snapshotSize = snapshot.sizeInBytes();\n+\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    addQuorumLeader(responsePartitionSnapshot)\n+                        .snapshotId()\n+                        .setEndOffset(snapshotId.offset)\n+                        .setEpoch(snapshotId.epoch);\n+\n+                    return responsePartitionSnapshot\n+                        .setSize(snapshotSize)\n+                        .setPosition(partitionSnapshot.position())\n+                        .setBytes(buffer);\n+                }\n+            );\n+        }\n+    }\n+\n+    private boolean handleFetchSnapshotResponse(\n+        RaftResponse.Inbound responseMetadata,\n+        long currentTimeMs\n+    ) throws IOException {\n+        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n+        Errors topLevelError = Errors.forCode(data.errorCode());\n+        if (topLevelError != Errors.NONE) {\n+            // TODO: check what values this expression returns\n+            return handleTopLevelError(topLevelError, responseMetadata);\n+        }\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return false;\n+        }\n+\n+        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            return false;\n+        }\n+\n+        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+\n+        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n+        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n+        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n+        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n+\n+        Optional<Boolean> handled = maybeHandleCommonResponse(\n+            error, responseLeaderId, responseEpoch, currentTimeMs);\n+        if (handled.isPresent()) {\n+            // TODO: check what values this expression returns\n+            return handled.get();\n+        }\n+\n+        FollowerState state = quorum.followerStateOrThrow();\n+\n+        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n+            partitionSnapshot.snapshotId().endOffset() < 0 ||\n+            partitionSnapshot.snapshotId().epoch() < 0) {\n+\n+            /* The leader deleted the snapshot before the follower could download it. Start over by\n+             * reseting the fetching snapshot state and sending another fetch request.\n+             */\n+            state.setFetchingSnapshot(Optional.empty());\n+            state.resetFetchTimeout(currentTimeMs);\n+            return true;\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+\n+        RawSnapshotWriter snapshot;\n+        if (state.fetchingSnapshot().isPresent()) {\n+            snapshot = state.fetchingSnapshot().get();\n+        } else {\n+            throw new IllegalStateException(\"Received unexpected fetch snapshot response: \" + partitionSnapshot);\n+        }\n+\n+        if (!snapshot.snapshotId().equals(snapshotId)) {\n+            throw new IllegalStateException(String.format(\"Received fetch snapshot response with an invalid id. Expected %s; Received %s\", snapshot.snapshotId(), snapshotId));\n+        }\n+        if (snapshot.sizeInBytes() != partitionSnapshot.position()) {\n+            throw new IllegalStateException(String.format(\"Received fetch snapshot response with an invalid position. Expected %s; Received %s\", snapshot.sizeInBytes(), partitionSnapshot.position()));\n+        }\n+\n+        snapshot.append(partitionSnapshot.bytes());\n+\n+        if (snapshot.sizeInBytes() == partitionSnapshot.size()) {\n+            // Finished fetching the snapshot.\n+            snapshot.freeze();\n+            state.setFetchingSnapshot(Optional.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 260}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTcyMzk0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchSnapshotResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNjo1Nzo0MFrOIKCBZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNjo1Nzo0MFrOIKCBZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4OTc5Ng==", "bodyText": "nit: maybe withTopLevelError?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547389796", "createdAt": "2020-12-22T16:57:40Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchSnapshotResponse.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.UnaryOperator;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.FetchSnapshotResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.Message;\n+\n+final public class FetchSnapshotResponse extends AbstractResponse {\n+    public final FetchSnapshotResponseData data;\n+\n+    public FetchSnapshotResponse(FetchSnapshotResponseData data) {\n+        super(ApiKeys.FETCH_SNAPSHOT);\n+\n+        this.data = data;\n+    }\n+\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        Map<Errors, Integer> errors = new HashMap<>();\n+\n+        Errors topLevelError = Errors.forCode(data.errorCode());\n+        if (topLevelError != Errors.NONE) {\n+            errors.put(topLevelError, 1);\n+        }\n+\n+        for (FetchSnapshotResponseData.TopicSnapshot topicResponse : data.topics()) {\n+            for (FetchSnapshotResponseData.PartitionSnapshot partitionResponse : topicResponse.partitions()) {\n+                errors.compute(Errors.forCode(partitionResponse.errorCode()),\n+                    (error, count) -> count == null ? 1 : count + 1);\n+            }\n+        }\n+\n+        return errors;\n+    }\n+\n+    @Override\n+    public int throttleTimeMs() {\n+        return data.throttleTimeMs();\n+    }\n+\n+    @Override\n+    protected Message data() {\n+        return data;\n+    }\n+\n+    /**\n+     * Creates a FetchSnapshotResponseData with a top level error.\n+     *\n+     * @param error the top level error\n+     * @return the created fetch snapshot response data\n+     */\n+    public static FetchSnapshotResponseData withTopError(Errors error) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTczNjI1OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/LeaderState.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzowMToyOVrOIKCIxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMDowNToyMFrOIKHvPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5MTY4Ng==", "bodyText": "Maybe we could add a default no-op implementation to EpochState?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547391686", "createdAt": "2020-12-22T17:01:29Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/LeaderState.java", "diffHunk": "@@ -287,4 +287,7 @@ public String name() {\n         return \"Leader\";\n     }\n \n+    @Override\n+    public void close() {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MzQ1Mg==", "bodyText": "I prefer to have each of the epoch states explicitly opt out of this close method. This makes it clear that this state doesn't have any resource that it wishes to clean/close during a transition. Instead of future code changes forgetting to override this method. What do you think?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547483452", "createdAt": "2020-12-22T20:05:20Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/LeaderState.java", "diffHunk": "@@ -287,4 +287,7 @@ public String name() {\n         return \"Leader\";\n     }\n \n+    @Override\n+    public void close() {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5MTY4Ng=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc0NDYyOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzowNDoxNlrOIKCN4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxOTozOToxNlrOIKHDig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Mjk5NA==", "bodyText": "Isn't this already done by buildFetchResponse?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547392994", "createdAt": "2020-12-22T17:04:16Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -941,6 +949,8 @@ private FetchResponseData tryCompleteFetchRequest(\n     ) {\n         Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n         if (errorOpt.isPresent()) {\n+            // TODO: The replica should return what information it knows about the current epoch and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ3MjI2Ng==", "bodyText": "Yes. This was a note to me to confirm this as it wasn't clear to me when I first read the code.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547472266", "createdAt": "2020-12-22T19:39:16Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -941,6 +949,8 @@ private FetchResponseData tryCompleteFetchRequest(\n     ) {\n         Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n         if (errorOpt.isPresent()) {\n+            // TODO: The replica should return what information it knows about the current epoch and", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Mjk5NA=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc1NTYzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzowNzo1NFrOIKCUww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMzozMTo1NlrOIKMNgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NDc1NQ==", "bodyText": "Hmm.. The leader has sent a bad response. I think logging an error and retrying might be better than crashing the followers.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547394755", "createdAt": "2020-12-22T17:07:54Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1037,6 +1047,35 @@ private boolean handleFetchResponse(\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n                 });\n+            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n+                       partitionResponse.snapshotId().endOffset() >= 0) {\n+                // The leader is asking us to fetch a snapshot\n+\n+                if (partitionResponse.snapshotId().epoch() < 0) {\n+                    throw new KafkaException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ3NTk3OA==", "bodyText": "Yeah. I am not sure. If these exception are thrown then it means that the epoch is \"valid\" but the end offset is not \"valid\" or vice versa. I think this could happen because of either a buggy remote replica or corrupted data in the remote replica.\nI guess you can argue that it is safe to keep retrying.\nIn the case that the Fetch never succeed do we want to transition to the candidate the state? If so, let me add a test that confirms that.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547475978", "createdAt": "2020-12-22T19:48:24Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1037,6 +1047,35 @@ private boolean handleFetchResponse(\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n                 });\n+            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n+                       partitionResponse.snapshotId().endOffset() >= 0) {\n+                // The leader is asking us to fetch a snapshot\n+\n+                if (partitionResponse.snapshotId().epoch() < 0) {\n+                    throw new KafkaException(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NDc1NQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0ODgzMQ==", "bodyText": "Hmm.. I would rather not handle this case specially by transitioning to a candidate. It's not a crazy idea, but I think we should attempt that in a separate PR and consider errors more holistically.\nI would suggest that we log an error saying that the remote replica seemed to return an invalid response and just keep fetching. Then a user can see the log message and restart the remote replica.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547548831", "createdAt": "2020-12-22T23:04:49Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1037,6 +1047,35 @@ private boolean handleFetchResponse(\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n                 });\n+            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n+                       partitionResponse.snapshotId().endOffset() >= 0) {\n+                // The leader is asking us to fetch a snapshot\n+\n+                if (partitionResponse.snapshotId().epoch() < 0) {\n+                    throw new KafkaException(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NDc1NQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NjczOA==", "bodyText": "I would suggest that we log an error saying that the remote replica seemed to return an invalid response and just keep fetching. Then a user can see the log message and restart the remote replica.\n\nYeah. This is what I implemented and added a test for it. In other words.\n\nLog an error message\nTell the raft client that the response was handle successfully but the fetch timer was not reset\n\nIn practice this results in the follower continuing to send Fetch requests. After fetchTimeoutMs the follower will transition to candidate as the existing client code does. See https://github.com/apache/kafka/pull/9553/files#diff-86474ad1438150630c21b29a3da2f6dd79d1357e33ac034f00e5fcef0f2e889cR350\nLet me know if this is what you were thinking.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547556738", "createdAt": "2020-12-22T23:31:56Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1037,6 +1047,35 @@ private boolean handleFetchResponse(\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n                 });\n+            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n+                       partitionResponse.snapshotId().endOffset() >= 0) {\n+                // The leader is asking us to fetch a snapshot\n+\n+                if (partitionResponse.snapshotId().epoch() < 0) {\n+                    throw new KafkaException(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NDc1NQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc2NDIwOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoxMDoyNVrOIKCZww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODowOTozNVrOIKEIdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NjAzNQ==", "bodyText": "nit: I guess you could use leaderValidation.ifPresent", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547396035", "createdAt": "2020-12-22T17:10:25Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyNDM3NA==", "bodyText": "Hmm. I don't think so. ifPresent returns void and this part of the code wants to return a FetchSnapshotResponseData if there was a validation error. We can do a map followed by a isPresent and get but I think this is more consistent with the rest of the code.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547424374", "createdAt": "2020-12-22T18:09:35Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NjAzNQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc2OTcyOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoxMjoxMVrOIKCdBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMzoyMzo0OVrOIKMEcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Njg2OQ==", "bodyText": "Is it worth validating that partitionSnapshot.position() is non-negative?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547396869", "createdAt": "2020-12-22T17:12:11Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n+            snapshot.read(buffer, partitionSnapshot.position());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzNzgxNg==", "bodyText": "Good catch. In the KIP we added POSITION_OUT_OF_RANGE which I am not using any where in this PR.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547437816", "createdAt": "2020-12-22T18:30:45Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n+            snapshot.read(buffer, partitionSnapshot.position());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Njg2OQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NDQxNg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547554416", "createdAt": "2020-12-22T23:23:49Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n+            snapshot.read(buffer, partitionSnapshot.position());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Njg2OQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc3Njg3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/resources/common/message/FetchSnapshotResponse.json", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoxNDoxNVrOIKChUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QwMjoxMTozNlrOILnmHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Nzk3MA==", "bodyText": "Can you remind me if we are planning to change the type to \"records\"? I don't think we will get the benefit of sendfile unless we do so.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547397970", "createdAt": "2020-12-22T17:14:15Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/resources/common/message/FetchSnapshotResponse.json", "diffHunk": "@@ -0,0 +1,59 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 59,\n+  \"type\": \"response\",\n+  \"name\": \"FetchSnapshotResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\", \"ignorable\": true,\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\", \"ignorable\": false,\n+      \"about\": \"The top level response error code.\" },\n+    { \"name\": \"Topics\", \"type\": \"[]TopicSnapshot\", \"versions\": \"0+\",\n+      \"about\": \"The topics to fetch.\", \"fields\": [\n+      { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+        \"about\": \"The name of the topic to fetch.\" },\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionSnapshot\", \"versions\": \"0+\",\n+        \"about\": \"The partitions to fetch.\", \"fields\": [\n+        { \"name\": \"Index\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The partition index.\" },\n+        { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+          \"about\": \"The error code, or 0 if there was no fetch error.\" },\n+        { \"name\": \"SnapshotId\", \"type\": \"SnapshotId\", \"versions\": \"0+\",\n+          \"about\": \"The snapshot endOffset and epoch fetched\",\n+          \"fields\": [\n+          { \"name\": \"EndOffset\", \"type\": \"int64\", \"versions\": \"0+\" },\n+          { \"name\": \"Epoch\", \"type\": \"int32\", \"versions\": \"0+\" }\n+        ]},\n+        { \"name\": \"CurrentLeader\", \"type\": \"LeaderIdAndEpoch\",\n+          \"versions\": \"0+\", \"taggedVersions\": \"0+\", \"tag\": 0, \"fields\": [\n+          { \"name\": \"LeaderId\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The ID of the current leader or -1 if the leader is unknown.\"},\n+          { \"name\": \"LeaderEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The latest known leader epoch\"}\n+        ]},\n+        { \"name\": \"Size\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The total size of the snapshot.\" },\n+        { \"name\": \"Position\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The starting byte position within the snapshot included in the Bytes field.\" },\n+        { \"name\": \"Bytes\", \"type\": \"bytes\", \"versions\": \"0+\", \"zeroCopy\": true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ3MDc3OQ==", "bodyText": "Yeah. I created this issue: https://issues.apache.org/jira/browse/KAFKA-10694\nI think we have two options:\n\nUse \"records\" as you have suggested.\nSimilar to \"records\" introduce a new type called for example Memory that has two implementations one that encapsulates ByteBuffer and another that encapsulates a file and a start position. This is similar to \"records\", MemoryRecords and FileRecords without the requirement that the start needs to be a valid RecordBatch.\n\nI want to play around with this soon and restart the conversation on that issue and PR.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547470779", "createdAt": "2020-12-22T19:36:38Z", "author": {"login": "jsancio"}, "path": "clients/src/main/resources/common/message/FetchSnapshotResponse.json", "diffHunk": "@@ -0,0 +1,59 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 59,\n+  \"type\": \"response\",\n+  \"name\": \"FetchSnapshotResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\", \"ignorable\": true,\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\", \"ignorable\": false,\n+      \"about\": \"The top level response error code.\" },\n+    { \"name\": \"Topics\", \"type\": \"[]TopicSnapshot\", \"versions\": \"0+\",\n+      \"about\": \"The topics to fetch.\", \"fields\": [\n+      { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+        \"about\": \"The name of the topic to fetch.\" },\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionSnapshot\", \"versions\": \"0+\",\n+        \"about\": \"The partitions to fetch.\", \"fields\": [\n+        { \"name\": \"Index\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The partition index.\" },\n+        { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+          \"about\": \"The error code, or 0 if there was no fetch error.\" },\n+        { \"name\": \"SnapshotId\", \"type\": \"SnapshotId\", \"versions\": \"0+\",\n+          \"about\": \"The snapshot endOffset and epoch fetched\",\n+          \"fields\": [\n+          { \"name\": \"EndOffset\", \"type\": \"int64\", \"versions\": \"0+\" },\n+          { \"name\": \"Epoch\", \"type\": \"int32\", \"versions\": \"0+\" }\n+        ]},\n+        { \"name\": \"CurrentLeader\", \"type\": \"LeaderIdAndEpoch\",\n+          \"versions\": \"0+\", \"taggedVersions\": \"0+\", \"tag\": 0, \"fields\": [\n+          { \"name\": \"LeaderId\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The ID of the current leader or -1 if the leader is unknown.\"},\n+          { \"name\": \"LeaderEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The latest known leader epoch\"}\n+        ]},\n+        { \"name\": \"Size\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The total size of the snapshot.\" },\n+        { \"name\": \"Position\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The starting byte position within the snapshot included in the Bytes field.\" },\n+        { \"name\": \"Bytes\", \"type\": \"bytes\", \"versions\": \"0+\", \"zeroCopy\": true,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Nzk3MA=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk2MDA2Mw==", "bodyText": "@jsancio Can we add an index file to the snapshot file so that the FetchSnaphotResponseData can also satisfy the requirement that the start is a valid RecordBatch? I can help with some work of this issue: https://issues.apache.org/jira/browse/KAFKA-10694.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r548960063", "createdAt": "2020-12-26T08:40:57Z", "author": {"login": "dengziming"}, "path": "clients/src/main/resources/common/message/FetchSnapshotResponse.json", "diffHunk": "@@ -0,0 +1,59 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 59,\n+  \"type\": \"response\",\n+  \"name\": \"FetchSnapshotResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\", \"ignorable\": true,\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\", \"ignorable\": false,\n+      \"about\": \"The top level response error code.\" },\n+    { \"name\": \"Topics\", \"type\": \"[]TopicSnapshot\", \"versions\": \"0+\",\n+      \"about\": \"The topics to fetch.\", \"fields\": [\n+      { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+        \"about\": \"The name of the topic to fetch.\" },\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionSnapshot\", \"versions\": \"0+\",\n+        \"about\": \"The partitions to fetch.\", \"fields\": [\n+        { \"name\": \"Index\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The partition index.\" },\n+        { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+          \"about\": \"The error code, or 0 if there was no fetch error.\" },\n+        { \"name\": \"SnapshotId\", \"type\": \"SnapshotId\", \"versions\": \"0+\",\n+          \"about\": \"The snapshot endOffset and epoch fetched\",\n+          \"fields\": [\n+          { \"name\": \"EndOffset\", \"type\": \"int64\", \"versions\": \"0+\" },\n+          { \"name\": \"Epoch\", \"type\": \"int32\", \"versions\": \"0+\" }\n+        ]},\n+        { \"name\": \"CurrentLeader\", \"type\": \"LeaderIdAndEpoch\",\n+          \"versions\": \"0+\", \"taggedVersions\": \"0+\", \"tag\": 0, \"fields\": [\n+          { \"name\": \"LeaderId\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The ID of the current leader or -1 if the leader is unknown.\"},\n+          { \"name\": \"LeaderEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The latest known leader epoch\"}\n+        ]},\n+        { \"name\": \"Size\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The total size of the snapshot.\" },\n+        { \"name\": \"Position\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The starting byte position within the snapshot included in the Bytes field.\" },\n+        { \"name\": \"Bytes\", \"type\": \"bytes\", \"versions\": \"0+\", \"zeroCopy\": true,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Nzk3MA=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1Mzk4MQ==", "bodyText": "@dengziming, Thanks for the comment. The FetchSnapshotRequest differs from FetchRequest in that Position is a byte offset. Because of this we don't need an Offset to Byte index.\nIf we implement option 1 in my comment above, we can still use Records by making sure that:\n\nThe leaders send at least one entire RecordBatch in the FetchSnapshotResponse.\nThe followers only write complete RecordBatches by not writing bytes sent by the leader that are not complete RecordBatch.\n\nThis guarantees that the Position sent in FetchSnapshotRequest is RecordBatch aligned and it always makes progress because the leader sends at least one complete RecordBatch.\nHaving said that, I think we should try and implement option 2. Unfortunately, the Jira https://issues.apache.org/jira/browse/KAFKA-10694 doesn't document option 2.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r549053981", "createdAt": "2020-12-27T02:11:36Z", "author": {"login": "jsancio"}, "path": "clients/src/main/resources/common/message/FetchSnapshotResponse.json", "diffHunk": "@@ -0,0 +1,59 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 59,\n+  \"type\": \"response\",\n+  \"name\": \"FetchSnapshotResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\", \"ignorable\": true,\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\", \"ignorable\": false,\n+      \"about\": \"The top level response error code.\" },\n+    { \"name\": \"Topics\", \"type\": \"[]TopicSnapshot\", \"versions\": \"0+\",\n+      \"about\": \"The topics to fetch.\", \"fields\": [\n+      { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+        \"about\": \"The name of the topic to fetch.\" },\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionSnapshot\", \"versions\": \"0+\",\n+        \"about\": \"The partitions to fetch.\", \"fields\": [\n+        { \"name\": \"Index\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The partition index.\" },\n+        { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+          \"about\": \"The error code, or 0 if there was no fetch error.\" },\n+        { \"name\": \"SnapshotId\", \"type\": \"SnapshotId\", \"versions\": \"0+\",\n+          \"about\": \"The snapshot endOffset and epoch fetched\",\n+          \"fields\": [\n+          { \"name\": \"EndOffset\", \"type\": \"int64\", \"versions\": \"0+\" },\n+          { \"name\": \"Epoch\", \"type\": \"int32\", \"versions\": \"0+\" }\n+        ]},\n+        { \"name\": \"CurrentLeader\", \"type\": \"LeaderIdAndEpoch\",\n+          \"versions\": \"0+\", \"taggedVersions\": \"0+\", \"tag\": 0, \"fields\": [\n+          { \"name\": \"LeaderId\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The ID of the current leader or -1 if the leader is unknown.\"},\n+          { \"name\": \"LeaderEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n+            \"about\": \"The latest known leader epoch\"}\n+        ]},\n+        { \"name\": \"Size\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The total size of the snapshot.\" },\n+        { \"name\": \"Position\", \"type\": \"int64\", \"versions\": \"0+\",\n+          \"about\": \"The starting byte position within the snapshot included in the Bytes field.\" },\n+        { \"name\": \"Bytes\", \"type\": \"bytes\", \"versions\": \"0+\", \"zeroCopy\": true,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5Nzk3MA=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc4MzA3OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoxNTo0N1rOIKCkyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMDowMDowMFrOIKHmew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5ODg1Nw==", "bodyText": "Address TODO?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547398857", "createdAt": "2020-12-22T17:15:47Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n+            snapshot.read(buffer, partitionSnapshot.position());\n+            buffer.flip();\n+\n+            long snapshotSize = snapshot.sizeInBytes();\n+\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    addQuorumLeader(responsePartitionSnapshot)\n+                        .snapshotId()\n+                        .setEndOffset(snapshotId.offset)\n+                        .setEpoch(snapshotId.epoch);\n+\n+                    return responsePartitionSnapshot\n+                        .setSize(snapshotSize)\n+                        .setPosition(partitionSnapshot.position())\n+                        .setBytes(buffer);\n+                }\n+            );\n+        }\n+    }\n+\n+    private boolean handleFetchSnapshotResponse(\n+        RaftResponse.Inbound responseMetadata,\n+        long currentTimeMs\n+    ) throws IOException {\n+        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n+        Errors topLevelError = Errors.forCode(data.errorCode());\n+        if (topLevelError != Errors.NONE) {\n+            // TODO: check what values this expression returns", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MTIxMQ==", "bodyText": "Done. Excuse the noise.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547481211", "createdAt": "2020-12-22T20:00:00Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n+            snapshot.read(buffer, partitionSnapshot.position());\n+            buffer.flip();\n+\n+            long snapshotSize = snapshot.sizeInBytes();\n+\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    addQuorumLeader(responsePartitionSnapshot)\n+                        .snapshotId()\n+                        .setEndOffset(snapshotId.offset)\n+                        .setEpoch(snapshotId.epoch);\n+\n+                    return responsePartitionSnapshot\n+                        .setSize(snapshotSize)\n+                        .setPosition(partitionSnapshot.position())\n+                        .setBytes(buffer);\n+                }\n+            );\n+        }\n+    }\n+\n+    private boolean handleFetchSnapshotResponse(\n+        RaftResponse.Inbound responseMetadata,\n+        long currentTimeMs\n+    ) throws IOException {\n+        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n+        Errors topLevelError = Errors.forCode(data.errorCode());\n+        if (topLevelError != Errors.NONE) {\n+            // TODO: check what values this expression returns", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5ODg1Nw=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 194}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc4OTIzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoxNzozNlrOIKCoZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoxNzozNlrOIKCoZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5OTc4MQ==", "bodyText": "A log message would probably be helpful. It's probably worth doing one full pass over the logic here to see where we could add extra logging.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547399781", "createdAt": "2020-12-22T17:17:36Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.min(data.maxBytes(), maxSnapshotSize));\n+            snapshot.read(buffer, partitionSnapshot.position());\n+            buffer.flip();\n+\n+            long snapshotSize = snapshot.sizeInBytes();\n+\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    addQuorumLeader(responsePartitionSnapshot)\n+                        .snapshotId()\n+                        .setEndOffset(snapshotId.offset)\n+                        .setEpoch(snapshotId.epoch);\n+\n+                    return responsePartitionSnapshot\n+                        .setSize(snapshotSize)\n+                        .setPosition(partitionSnapshot.position())\n+                        .setBytes(buffer);\n+                }\n+            );\n+        }\n+    }\n+\n+    private boolean handleFetchSnapshotResponse(\n+        RaftResponse.Inbound responseMetadata,\n+        long currentTimeMs\n+    ) throws IOException {\n+        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n+        Errors topLevelError = Errors.forCode(data.errorCode());\n+        if (topLevelError != Errors.NONE) {\n+            // TODO: check what values this expression returns\n+            return handleTopLevelError(topLevelError, responseMetadata);\n+        }\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return false;\n+        }\n+\n+        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            return false;\n+        }\n+\n+        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+\n+        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n+        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n+        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n+        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n+\n+        Optional<Boolean> handled = maybeHandleCommonResponse(\n+            error, responseLeaderId, responseEpoch, currentTimeMs);\n+        if (handled.isPresent()) {\n+            // TODO: check what values this expression returns\n+            return handled.get();\n+        }\n+\n+        FollowerState state = quorum.followerStateOrThrow();\n+\n+        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n+            partitionSnapshot.snapshotId().endOffset() < 0 ||\n+            partitionSnapshot.snapshotId().epoch() < 0) {\n+\n+            /* The leader deleted the snapshot before the follower could download it. Start over by", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 228}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc5ODk5OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoyMDo1M1rOIKCucQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMDowMToyNFrOIKHovA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwMTMyOQ==", "bodyText": "Can we move this logic to pollFollower somehow?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547401329", "createdAt": "2020-12-22T17:20:53Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1629,16 +1872,29 @@ private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) throws\n             transitionToCandidate(currentTimeMs);\n             return 0L;\n         } else {\n-            long backoffMs = maybeSendRequest(\n-                currentTimeMs,\n-                state.leaderId(),\n-                this::buildFetchRequest\n-            );\n+            long backoffMs;\n+            if (state.fetchingSnapshot().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 337}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MTc4OA==", "bodyText": "I cleaned this up by moving it to a method we can reuse. Let me know if that addresses your concern.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547481788", "createdAt": "2020-12-22T20:01:24Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1629,16 +1872,29 @@ private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) throws\n             transitionToCandidate(currentTimeMs);\n             return 0L;\n         } else {\n-            long backoffMs = maybeSendRequest(\n-                currentTimeMs,\n-                state.leaderId(),\n-                this::buildFetchRequest\n-            );\n+            long backoffMs;\n+            if (state.fetchingSnapshot().isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwMTMyOQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 337}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTgwMjQ3OnYy", "diffSide": "RIGHT", "path": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoyMjowOVrOIKCwqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMDowODo1NlrOIKH1Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwMTg5OQ==", "bodyText": "I guess you're still planning to implement these?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547401899", "createdAt": "2020-12-22T17:22:09Z", "author": {"login": "hachikuji"}, "path": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java", "diffHunk": "@@ -0,0 +1,765 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.message.FetchResponseData;\n+import org.apache.kafka.common.message.FetchSnapshotRequestData;\n+import org.apache.kafka.common.message.FetchSnapshotResponseData;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.requests.FetchSnapshotRequest;\n+import org.apache.kafka.common.requests.FetchSnapshotResponse;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.raft.internals.StringSerde;\n+import org.apache.kafka.snapshot.RawSnapshotReader;\n+import org.apache.kafka.snapshot.RawSnapshotWriter;\n+import org.apache.kafka.snapshot.SnapshotWriter;\n+import org.apache.kafka.snapshot.SnapshotWriterTest;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.Disabled;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+final public class KafkaRaftClientSnapshotTest {\n+    @Test\n+    public void testMissingFetchSnapshotRequest() throws Exception {\n+        int localId = 0;\n+        int epoch = 2;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        context.deliverRequest(\n+            fetchSnapshotRequest(\n+                context.metadataPartition,\n+                epoch,\n+                new OffsetAndEpoch(0, 0),\n+                Integer.MAX_VALUE,\n+                0\n+            )\n+        );\n+\n+        context.client.poll();\n+\n+        FetchSnapshotResponseData.PartitionSnapshot response =  context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n+        assertEquals(Errors.SNAPSHOT_NOT_FOUND, Errors.forCode(response.errorCode()));\n+    }\n+\n+    @Test\n+    public void testUnknownFetchSnapshotRequest() throws Exception {\n+        int localId = 0;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+        int epoch = 2;\n+        TopicPartition topicPartition = new TopicPartition(\"unknown\", 0);\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        context.deliverRequest(\n+            fetchSnapshotRequest(\n+                topicPartition,\n+                epoch,\n+                new OffsetAndEpoch(0, 0),\n+                Integer.MAX_VALUE,\n+                0\n+            )\n+        );\n+\n+        context.client.poll();\n+\n+        FetchSnapshotResponseData.PartitionSnapshot response =  context.assertSentFetchSnapshotResponse(topicPartition).get();\n+        assertEquals(Errors.UNKNOWN_TOPIC_OR_PARTITION, Errors.forCode(response.errorCode()));\n+    }\n+\n+    @Test\n+    public void testFetchSnapshotRequestAsLeader() throws Exception {\n+        int localId = 0;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+        int epoch = 2;\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n+        List<String> records = Arrays.asList(\"foo\", \"bar\");\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId)) {\n+            snapshot.append(records);\n+            snapshot.freeze();\n+        }\n+\n+        try (RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get()) {\n+            context.deliverRequest(\n+                fetchSnapshotRequest(\n+                    context.metadataPartition,\n+                    epoch,\n+                    snapshotId,\n+                    Integer.MAX_VALUE,\n+                    0\n+                )\n+            );\n+\n+            context.client.poll();\n+\n+            FetchSnapshotResponseData.PartitionSnapshot response =  context\n+                .assertSentFetchSnapshotResponse(context.metadataPartition)\n+                .get();\n+\n+            assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n+            assertEquals(snapshot.sizeInBytes(), response.size());\n+            assertEquals(0, response.position());\n+            assertEquals(snapshot.sizeInBytes(), response.bytes().remaining());\n+\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n+            snapshot.read(buffer, 0);\n+            buffer.flip();\n+\n+            assertEquals(buffer.slice(), response.bytes());\n+        }\n+    }\n+\n+    @Test\n+    public void testPartialFetchSnapshotRequestAsLeader() throws Exception {\n+        int localId = 0;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+        int epoch = 2;\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n+        List<String> records = Arrays.asList(\"foo\", \"bar\");\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId)) {\n+            snapshot.append(records);\n+            snapshot.freeze();\n+        }\n+\n+        try (RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get()) {\n+            // Fetch half of the snapshot\n+            context.deliverRequest(\n+                fetchSnapshotRequest(\n+                    context.metadataPartition,\n+                    epoch,\n+                    snapshotId,\n+                    Math.toIntExact(snapshot.sizeInBytes() / 2),\n+                    0\n+                )\n+            );\n+\n+            context.client.poll();\n+\n+            FetchSnapshotResponseData.PartitionSnapshot response = context\n+                .assertSentFetchSnapshotResponse(context.metadataPartition)\n+                .get();\n+\n+            assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n+            assertEquals(snapshot.sizeInBytes(), response.size());\n+            assertEquals(0, response.position());\n+            assertEquals(snapshot.sizeInBytes() / 2, response.bytes().remaining());\n+\n+            ByteBuffer snapshotBuffer = ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n+            snapshot.read(snapshotBuffer, 0);\n+            snapshotBuffer.flip();\n+\n+            ByteBuffer responseBuffer = ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n+            responseBuffer.put(response.bytes());\n+\n+            ByteBuffer expectedBytes = snapshotBuffer.duplicate();\n+            expectedBytes.limit(Math.toIntExact(snapshot.sizeInBytes() / 2));\n+\n+            assertEquals(expectedBytes, responseBuffer.duplicate().flip());\n+\n+            // Fetch the remainder of the snapshot\n+            context.deliverRequest(\n+                fetchSnapshotRequest(\n+                    context.metadataPartition,\n+                    epoch,\n+                    snapshotId,\n+                    Integer.MAX_VALUE,\n+                    responseBuffer.position()\n+                )\n+            );\n+\n+            context.client.poll();\n+\n+            response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n+            assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n+            assertEquals(snapshot.sizeInBytes(), response.size());\n+            assertEquals(responseBuffer.position(), response.position());\n+            assertEquals(snapshot.sizeInBytes() - (snapshot.sizeInBytes() / 2), response.bytes().remaining());\n+\n+            responseBuffer.put(response.bytes());\n+            assertEquals(snapshotBuffer, responseBuffer.flip());\n+        }\n+    }\n+\n+    @Test\n+    public void testFetchSnapshotRequestAsFollower() throws IOException {\n+        int localId = 0;\n+        int leaderId = localId + 1;\n+        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n+        int epoch = 2;\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n+\n+        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n+            .withElectedLeader(epoch, leaderId)\n+            .build();\n+\n+        context.deliverRequest(\n+            fetchSnapshotRequest(\n+                context.metadataPartition,\n+                epoch,\n+                snapshotId,\n+                Integer.MAX_VALUE,\n+                0\n+            )\n+        );\n+\n+        context.client.poll();\n+\n+        FetchSnapshotResponseData.PartitionSnapshot response =  context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n+        assertEquals(Errors.NOT_LEADER_OR_FOLLOWER, Errors.forCode(response.errorCode()));\n+        assertEquals(epoch, response.currentLeader().leaderEpoch());\n+        assertEquals(leaderId, response.currentLeader().leaderId());\n+    }\n+\n+    @Disabled\n+    @Test\n+    public void testFetchSnapshotRequestWithOlderEpoch() throws IOException {\n+        assertTrue(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NTAyMw==", "bodyText": "Yes. Doing that now...", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547485023", "createdAt": "2020-12-22T20:08:56Z", "author": {"login": "jsancio"}, "path": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java", "diffHunk": "@@ -0,0 +1,765 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.message.FetchResponseData;\n+import org.apache.kafka.common.message.FetchSnapshotRequestData;\n+import org.apache.kafka.common.message.FetchSnapshotResponseData;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.requests.FetchSnapshotRequest;\n+import org.apache.kafka.common.requests.FetchSnapshotResponse;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.raft.internals.StringSerde;\n+import org.apache.kafka.snapshot.RawSnapshotReader;\n+import org.apache.kafka.snapshot.RawSnapshotWriter;\n+import org.apache.kafka.snapshot.SnapshotWriter;\n+import org.apache.kafka.snapshot.SnapshotWriterTest;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.Disabled;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+final public class KafkaRaftClientSnapshotTest {\n+    @Test\n+    public void testMissingFetchSnapshotRequest() throws Exception {\n+        int localId = 0;\n+        int epoch = 2;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        context.deliverRequest(\n+            fetchSnapshotRequest(\n+                context.metadataPartition,\n+                epoch,\n+                new OffsetAndEpoch(0, 0),\n+                Integer.MAX_VALUE,\n+                0\n+            )\n+        );\n+\n+        context.client.poll();\n+\n+        FetchSnapshotResponseData.PartitionSnapshot response =  context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n+        assertEquals(Errors.SNAPSHOT_NOT_FOUND, Errors.forCode(response.errorCode()));\n+    }\n+\n+    @Test\n+    public void testUnknownFetchSnapshotRequest() throws Exception {\n+        int localId = 0;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+        int epoch = 2;\n+        TopicPartition topicPartition = new TopicPartition(\"unknown\", 0);\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        context.deliverRequest(\n+            fetchSnapshotRequest(\n+                topicPartition,\n+                epoch,\n+                new OffsetAndEpoch(0, 0),\n+                Integer.MAX_VALUE,\n+                0\n+            )\n+        );\n+\n+        context.client.poll();\n+\n+        FetchSnapshotResponseData.PartitionSnapshot response =  context.assertSentFetchSnapshotResponse(topicPartition).get();\n+        assertEquals(Errors.UNKNOWN_TOPIC_OR_PARTITION, Errors.forCode(response.errorCode()));\n+    }\n+\n+    @Test\n+    public void testFetchSnapshotRequestAsLeader() throws Exception {\n+        int localId = 0;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+        int epoch = 2;\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n+        List<String> records = Arrays.asList(\"foo\", \"bar\");\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId)) {\n+            snapshot.append(records);\n+            snapshot.freeze();\n+        }\n+\n+        try (RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get()) {\n+            context.deliverRequest(\n+                fetchSnapshotRequest(\n+                    context.metadataPartition,\n+                    epoch,\n+                    snapshotId,\n+                    Integer.MAX_VALUE,\n+                    0\n+                )\n+            );\n+\n+            context.client.poll();\n+\n+            FetchSnapshotResponseData.PartitionSnapshot response =  context\n+                .assertSentFetchSnapshotResponse(context.metadataPartition)\n+                .get();\n+\n+            assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n+            assertEquals(snapshot.sizeInBytes(), response.size());\n+            assertEquals(0, response.position());\n+            assertEquals(snapshot.sizeInBytes(), response.bytes().remaining());\n+\n+            ByteBuffer buffer = ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n+            snapshot.read(buffer, 0);\n+            buffer.flip();\n+\n+            assertEquals(buffer.slice(), response.bytes());\n+        }\n+    }\n+\n+    @Test\n+    public void testPartialFetchSnapshotRequestAsLeader() throws Exception {\n+        int localId = 0;\n+        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n+        int epoch = 2;\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n+        List<String> records = Arrays.asList(\"foo\", \"bar\");\n+\n+        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n+\n+        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId)) {\n+            snapshot.append(records);\n+            snapshot.freeze();\n+        }\n+\n+        try (RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get()) {\n+            // Fetch half of the snapshot\n+            context.deliverRequest(\n+                fetchSnapshotRequest(\n+                    context.metadataPartition,\n+                    epoch,\n+                    snapshotId,\n+                    Math.toIntExact(snapshot.sizeInBytes() / 2),\n+                    0\n+                )\n+            );\n+\n+            context.client.poll();\n+\n+            FetchSnapshotResponseData.PartitionSnapshot response = context\n+                .assertSentFetchSnapshotResponse(context.metadataPartition)\n+                .get();\n+\n+            assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n+            assertEquals(snapshot.sizeInBytes(), response.size());\n+            assertEquals(0, response.position());\n+            assertEquals(snapshot.sizeInBytes() / 2, response.bytes().remaining());\n+\n+            ByteBuffer snapshotBuffer = ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n+            snapshot.read(snapshotBuffer, 0);\n+            snapshotBuffer.flip();\n+\n+            ByteBuffer responseBuffer = ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n+            responseBuffer.put(response.bytes());\n+\n+            ByteBuffer expectedBytes = snapshotBuffer.duplicate();\n+            expectedBytes.limit(Math.toIntExact(snapshot.sizeInBytes() / 2));\n+\n+            assertEquals(expectedBytes, responseBuffer.duplicate().flip());\n+\n+            // Fetch the remainder of the snapshot\n+            context.deliverRequest(\n+                fetchSnapshotRequest(\n+                    context.metadataPartition,\n+                    epoch,\n+                    snapshotId,\n+                    Integer.MAX_VALUE,\n+                    responseBuffer.position()\n+                )\n+            );\n+\n+            context.client.poll();\n+\n+            response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n+            assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n+            assertEquals(snapshot.sizeInBytes(), response.size());\n+            assertEquals(responseBuffer.position(), response.position());\n+            assertEquals(snapshot.sizeInBytes() - (snapshot.sizeInBytes() / 2), response.bytes().remaining());\n+\n+            responseBuffer.put(response.bytes());\n+            assertEquals(snapshotBuffer, responseBuffer.flip());\n+        }\n+    }\n+\n+    @Test\n+    public void testFetchSnapshotRequestAsFollower() throws IOException {\n+        int localId = 0;\n+        int leaderId = localId + 1;\n+        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n+        int epoch = 2;\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n+\n+        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n+            .withElectedLeader(epoch, leaderId)\n+            .build();\n+\n+        context.deliverRequest(\n+            fetchSnapshotRequest(\n+                context.metadataPartition,\n+                epoch,\n+                snapshotId,\n+                Integer.MAX_VALUE,\n+                0\n+            )\n+        );\n+\n+        context.client.poll();\n+\n+        FetchSnapshotResponseData.PartitionSnapshot response =  context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n+        assertEquals(Errors.NOT_LEADER_OR_FOLLOWER, Errors.forCode(response.errorCode()));\n+        assertEquals(epoch, response.currentLeader().leaderEpoch());\n+        assertEquals(leaderId, response.currentLeader().leaderId());\n+    }\n+\n+    @Disabled\n+    @Test\n+    public void testFetchSnapshotRequestWithOlderEpoch() throws IOException {\n+        assertTrue(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwMTg5OQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 247}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTgyMjcwOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoyODoxNVrOIKC8fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0NDozNFrOIKDbCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwNDkyNQ==", "bodyText": "Just checking my understanding. This patch adds the logic to respond to the snapshot id from a fetch response and to handle send/handle snapshots when needed. However, since it does not contain the logic to set the snapshot id in the fetch request handler, none of this logic will get exercised by the simulation test. Is that right?", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547404925", "createdAt": "2020-12-22T17:28:15Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMjc0Ng==", "bodyText": "Correct. I am handling this case in https://issues.apache.org/jira/browse/KAFKA-10761. I think that after implementing that issue we should have an end-to-end working Raft Client with snapshot support that we test in the simulations.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547412746", "createdAt": "2020-12-22T17:44:34Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwNDkyNQ=="}, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjA5MDU3OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1MDo0MVrOIKFfoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1MDo0MVrOIKFfoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NjY4OA==", "bodyText": "I created this issue: https://issues.apache.org/jira/browse/KAFKA-10884\nWe can fix this for Raft Client's implementation of both Fetch and FetchSnapshot at the same time.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547446688", "createdAt": "2020-12-22T18:50:41Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1101,6 +1140,174 @@ private DescribeQuorumResponseData handleDescribeQuorumRequest(\n         );\n     }\n \n+    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n+        RaftRequest.Inbound requestMetadata\n+    ) throws IOException {\n+        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n+\n+        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n+            return FetchSnapshotResponse.withTopError(Errors.INVALID_REQUEST);\n+        }\n+\n+        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n+            .forTopicPartition(data, log.topicPartition());\n+        if (!partitionSnapshotOpt.isPresent()) {\n+            // The Raft client assumes that there is only one topic partition.\n+            TopicPartition unknownTopicPartition = new TopicPartition(\n+                data.topics().get(0).name(),\n+                data.topics().get(0).partitions().get(0).partition()\n+            );\n+\n+            return FetchSnapshotResponse.singleton(\n+                unknownTopicPartition,\n+                responsePartitionSnapshot -> {\n+                    return responsePartitionSnapshot\n+                        .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());\n+                }\n+            );\n+        }\n+\n+        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n+        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n+                partitionSnapshot.currentLeaderEpoch()\n+        );\n+        if (leaderValidation.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(leaderValidation.get().code());\n+                }\n+            );\n+        }\n+\n+        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n+            partitionSnapshot.snapshotId().endOffset(),\n+            partitionSnapshot.snapshotId().epoch()\n+        );\n+        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n+        if (!snapshotOpt.isPresent()) {\n+            return FetchSnapshotResponse.singleton(\n+                log.topicPartition(),\n+                responsePartitionSnapshot -> {\n+                    return addQuorumLeader(responsePartitionSnapshot)\n+                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n+                }\n+            );\n+        }\n+\n+        try (RawSnapshotReader snapshot = snapshotOpt.get()) {\n+            int maxSnapshotSize;\n+            try {\n+                maxSnapshotSize = Math.toIntExact(snapshot.sizeInBytes());\n+            } catch (ArithmeticException e) {\n+                maxSnapshotSize = Integer.MAX_VALUE;\n+            }\n+\n+            // TODO: Make sure that we also limit based on the fetch max bytes configuration", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79fed6115ce20aedee3cea0cc9a4a8c5ba1bcca2"}, "originalPosition": 163}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0Mjk0ODgyOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDo0MDo1MVrOIKNTWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMTozMjowOFrOIKOU1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3NDYxOQ==", "bodyText": "Returning false seemed more appropriate. That ensures we will get a backoff before the next retry.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547574619", "createdAt": "2020-12-23T00:40:51Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1055,14 +1055,14 @@ private boolean handleFetchResponse(\n                         partitionResponse.snapshotId().endOffset(),\n                         partitionResponse.snapshotId().epoch()\n                     );\n-                    return false;\n+                    return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2393cf75bb0f3854ae10d629c09207a5c0675640"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU5MTM4Mw==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9553#discussion_r547591383", "createdAt": "2020-12-23T01:32:08Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1055,14 +1055,14 @@ private boolean handleFetchResponse(\n                         partitionResponse.snapshotId().endOffset(),\n                         partitionResponse.snapshotId().epoch()\n                     );\n-                    return false;\n+                    return true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3NDYxOQ=="}, "originalCommit": {"oid": "2393cf75bb0f3854ae10d629c09207a5c0675640"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3792, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}