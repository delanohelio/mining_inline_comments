{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYxNTgxOTkz", "number": 7929, "title": "KAFKA-9393: DeleteRecords may cause extreme lock contention for large partition directories", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-9393\nThis PR avoids a performance issue with DeleteRecords when a partition directory contains high numbers of files. Previously, DeleteRecords would iterate the partition directory searching for producer state snapshot files. With this change, the iteration is removed in favor of keeping a 1:1 mapping between producer state snapshot file and segment file. A segment files corresponding producer state snapshot file is now deleted when the segment file is deleted.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-01-10T19:03:54Z", "url": "https://github.com/apache/kafka/pull/7929", "merged": true, "mergeCommit": {"oid": "24290de82821d16f7d163d086f5cfa88cec2b976"}, "closed": true, "closedAt": "2020-10-09T21:25:02Z", "author": {"login": "gardnervickers"}, "timelineItems": {"totalCount": 33, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb7Ik_HAFqTM0NDM4MDY1Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQ2GF0gH2gAyMzYxNTgxOTkzOmFlOTliNGViZjIwODc1YmY2OWNlNGQ5OTI1ODZjYzU3MDdjNmEwZWU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzgwNjU3", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-344380657", "createdAt": "2020-01-17T06:16:05Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNjoxNjowNVrOFevtqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNjoxNjowNVrOFevtqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzc4MzMzNg==", "bodyText": "This should be moved to loadLog() since it only needs to run at startup. Also, the set of offsets to delete should exclude the log end offset, since that is snapshotted out during shutdown.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r367783336", "createdAt": "2020-01-17T06:16:05Z", "author": {"login": "gardnervickers"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -870,6 +870,10 @@ class Log(@volatile var dir: File,\n   }\n \n   private def loadProducerState(lastOffset: Long, reloadFromCleanShutdown: Boolean): Unit = lock synchronized {\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ4OTI0NjU3", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-348924657", "createdAt": "2020-01-27T19:40:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxOTo0MDozNlrOFiPEUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QyMjozNjowOFrOFiT4tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTQ0Mjc2OQ==", "bodyText": "nit: I'd suggest a more descriptive name like segmentBaseOffsets", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371442769", "createdAt": "2020-01-27T19:40:36Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTQ0MzU2Nw==", "bodyText": "nit: braces are probably not needed", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371443567", "createdAt": "2020-01-27T19:42:05Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -622,7 +623,8 @@ class LogSegment private[log] (val log: FileRecords,\n       () => delete(log.deleteIfExists _, \"log\", log.file, logIfMissing = true),\n       () => delete(offsetIndex.deleteIfExists _, \"offset index\", lazyOffsetIndex.file, logIfMissing = true),\n       () => delete(timeIndex.deleteIfExists _, \"time index\", lazyTimeIndex.file, logIfMissing = true),\n-      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false)\n+      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false),\n+      () => delete(() => {Files.deleteIfExists(producerStateSnapshot.toPath)}, \"producer state snapshot\", producerStateSnapshot, logIfMissing = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUxMTgyOA==", "bodyText": "Just mentioning this as an observation. Unlike the index files, which have a lifecycle clearly tied to the segment, producer snapshots are a bit of a mix. They are created in Log when a segment is rolled, but deletion is the job of the segment. I'm partially wondering if it would be clearer to handle both operations in the same context.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371511828", "createdAt": "2020-01-27T22:12:11Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUyMDE0MA==", "bodyText": "Can we add a log message when we delete snapshot files?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371520140", "createdAt": "2020-01-27T22:32:18Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -759,6 +759,15 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(keepOffsets: Iterable[Long]): Unit = {\n+    val expected = keepOffsets.map(Log.producerSnapshotFile(logDir, _)).toSet ++ latestSnapshotFile\n+    val actual = listSnapshotFiles.toSet\n+    actual.diff(expected).foreach(file => Files.deleteIfExists(file.toPath))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTUyMTcxOA==", "bodyText": "Was there a reason you chose to cleanup the orphaned files before loading producer state? I'm not sure it matters too much, but it does mean that we won't remove the snapshot from the log end offset until the following restart.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r371521718", "createdAt": "2020-01-27T22:36:08Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -310,6 +310,10 @@ class Log(@volatile var dir: File,\n     // from scratch.\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val keepOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(keepOffsets)\n     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzMzY2NjY0", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-353366664", "createdAt": "2020-02-04T23:06:17Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzNDEzNjc4", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-353413678", "createdAt": "2020-02-05T01:20:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwMToyMDowMFrOFlpBzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwMToyOToxOFrOFlpLPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzgzOQ==", "bodyText": "So, this means that we will be removing the producer snapshot corresponding to log end offset. One of the potential impact is that if the broker has an immediate hard failure after this point, the recovery will have to rebuild the producer snapshot for the active segment, which can be expensive. We probably want to think if there is any other impact.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375013839", "createdAt": "2020-02-05T01:20:00Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -311,6 +311,10 @@ class Log(@volatile var dir: File,\n     if (!producerStateManager.isEmpty)\n       throw new IllegalStateException(\"Producer state must be empty during log initialization\")\n     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)\n+    // Pass the set of loaded segment base offsets to diff against the set of producer state snapshot\n+    // files. Any orphaned files should be cleaned up.\n+    val segmentBaseOffsets = logSegments.map(_.baseOffset)\n+    producerStateManager.cleanupOrphanSnapshotFiles(segmentBaseOffsets)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTY2Mg==", "bodyText": "Hmm, should we include fileSuffix in producerStateSnapshot too?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375015662", "createdAt": "2020-02-05T01:26:49Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -653,11 +655,13 @@ object LogSegment {\n   def open(dir: File, baseOffset: Long, config: LogConfig, time: Time, fileAlreadyExists: Boolean = false,\n            initFileSize: Int = 0, preallocate: Boolean = false, fileSuffix: String = \"\"): LogSegment = {\n     val maxIndexSize = config.maxIndexSize\n+    val producerStateSnapshot = Log.producerSnapshotFile(dir, baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNjI1NA==", "bodyText": "Hmm, it seems that we are missing the handling of producerStateSnapshot in a few methods like changeFileSuffixes(), updateDir(), and potentially flush().", "url": "https://github.com/apache/kafka/pull/7929#discussion_r375016254", "createdAt": "2020-02-05T01:29:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -57,6 +57,7 @@ class LogSegment private[log] (val log: FileRecords,\n                                val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                                val lazyTimeIndex: LazyIndex[TimeIndex],\n                                val txnIndex: TransactionIndex,\n+                               val producerStateSnapshot: File,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/be858cd5a3f9be029ec2a7368024028403c976c5", "committedDate": "2020-03-10T05:36:33Z", "message": "KAFKA-9393: Address PR feedback."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczOTQ3MjA2", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-373947206", "createdAt": "2020-03-12T22:41:17Z", "commit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjo0MToxOFrOF1yVPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMzoyODowMVrOF1zKfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0MzQ4NA==", "bodyText": "unused import CoreUtils", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391943484", "createdAt": "2020-03-12T22:41:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -23,7 +23,7 @@ import java.nio.file.{Files, StandardOpenOption}\n \n import kafka.log.Log.offsetFromFile\n import kafka.server.LogOffsetMetadata\n-import kafka.utils.{Logging, nonthreadsafe, threadsafe}\n+import kafka.utils.{CoreUtils, Logging, nonthreadsafe, threadsafe}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NzU3Mg==", "bodyText": "Since this is called on broker restart, it seems that we no longer need to call deleteSnapshotFiles() in truncateAndReload(), truncateHead() and truncate() ?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391947572", "createdAt": "2020-03-12T22:54:28Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -753,6 +753,18 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(segmentBaseOffsets: Iterable[Long]): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0ODQ0Nw==", "bodyText": "We don't need to clear the producer snapshot here. However, it seems that we still need to do other things in truncateHead() like removeUnreplicatedTransactions()?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391948447", "createdAt": "2020-03-12T22:57:26Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1260,7 +1265,6 @@ class Log(@volatile var dir: File,\n           info(s\"Incrementing log start offset to $newLogStartOffset\")\n           updateLogStartOffset(newLogStartOffset)\n           leaderEpochCache.foreach(_.truncateFromStart(logStartOffset))\n-          producerStateManager.truncateHead(newLogStartOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0OTA4MQ==", "bodyText": "segments base offset => segment's base offset", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391949081", "createdAt": "2020-03-12T22:59:31Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -47,6 +47,7 @@ import scala.math._\n  * @param lazyOffsetIndex The offset index\n  * @param lazyTimeIndex The timestamp index\n  * @param txnIndex The transaction index\n+ * @param producerStateSnapshot The producer state snapshot file matching this segments base offset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk1NDM0NQ==", "bodyText": "For consistency, should we cover producerStateSnapshot in lastModified_= ?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391954345", "createdAt": "2020-03-12T23:17:28Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -622,7 +631,8 @@ class LogSegment private[log] (val log: FileRecords,\n       () => delete(log.deleteIfExists _, \"log\", log.file, logIfMissing = true),\n       () => delete(offsetIndex.deleteIfExists _, \"offset index\", lazyOffsetIndex.file, logIfMissing = true),\n       () => delete(timeIndex.deleteIfExists _, \"time index\", lazyTimeIndex.file, logIfMissing = true),\n-      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false)\n+      () => delete(txnIndex.deleteIfExists _, \"transaction index\", txnIndex.file, logIfMissing = false),\n+      () => delete(() =>  Files.deleteIfExists(producerStateSnapshot.toPath), \"producer state snapshot\", producerStateSnapshot, logIfMissing = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk1NzExOA==", "bodyText": "Just to be clear. You mentioned\n\"I choose to implement #2 as it was simpler, and instead of just deleting the highest producer state snapshot file without a corresponding segment file, I delete all producer state snapshot files without a corresponding segment file.\"\nBut, we are still keeping the snapshot file with the largest offset?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r391957118", "createdAt": "2020-03-12T23:28:01Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -753,6 +753,18 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.\n+   */\n+  def cleanupOrphanSnapshotFiles(segmentBaseOffsets: Iterable[Long]): Unit = {\n+    val expected = segmentBaseOffsets.map(Log.producerSnapshotFile(logDir, _)).toSet ++ latestSnapshotFile", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be858cd5a3f9be029ec2a7368024028403c976c5"}, "originalPosition": 30}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2MzUzMTY1", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-426353165", "createdAt": "2020-06-08T15:23:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNToyMzo0NFrOGgjsnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNToyMzo0NFrOGgjsnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5MjQ3Nw==", "bodyText": "Nit: case and newline are unnecessary here.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r436792477", "createdAt": "2020-06-08T15:23:44Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2237,7 +2209,11 @@ class Log(@volatile private var _dir: File,\n     def deleteSegments(): Unit = {\n       info(s\"Deleting segments ${segments.mkString(\",\")}\")\n       maybeHandleIOException(s\"Error while deleting segments for $topicPartition in dir ${dir.getParent}\") {\n-        segments.foreach(_.deleteIfExists())\n+        segments.foreach {\n+          case segment =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMTAxMDEw", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-431101010", "createdAt": "2020-06-16T01:38:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTozODowOFrOGkIQYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMTo1MjoxN1rOGkIfBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUzNzE4Nw==", "bodyText": "What's keepOffsets?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440537187", "createdAt": "2020-06-16T01:38:08Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -751,6 +751,25 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n       None\n   }\n \n-  private def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n+  private[log] def listSnapshotFiles: Seq[File] = ProducerStateManager.listSnapshotFiles(logDir)\n \n+  /**\n+   * Remove any producer state snapshot files which do not have a corresponding offset provided\n+   * in keepOffsets. The latest snapshot file will always be kept.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUzODc3MA==", "bodyText": "It doesn't seem that we generate producer snapshot files for those new segments?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440538770", "createdAt": "2020-06-16T01:44:01Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2421,6 +2396,7 @@ class Log(@volatile private var _dir: File,\n         newSegments.foreach { splitSegment =>\n           splitSegment.close()\n           splitSegment.deleteIfExists()\n+          producerStateManager.deleteIfExists(splitSegment.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUzOTI5MQ==", "bodyText": "Hmm, the cleaned segment has the same base offset as the first segment. So, we don't want to delete that snapshot file.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440539291", "createdAt": "2020-06-16T01:45:58Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -595,8 +595,10 @@ private[log] class Cleaner(val id: Int,\n       log.replaceSegments(List(cleaned), segments)\n     } catch {\n       case e: LogCleaningAbortedException =>\n-        try cleaned.deleteIfExists()\n-        catch {\n+        try {\n+          cleaned.deleteIfExists()\n+          log.producerStateManager.deleteIfExists(cleaned.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU0MDkzMw==", "bodyText": "Hmm, this can be a bit tricky. When we replace old segments with a new segment in LogCleaner, each of the old segment will be deleted. However, the first old segment has the same offset as the new segment. So, we don't want to just delete the producer snapshot corresponding to the first old segment.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r440540933", "createdAt": "2020-06-16T01:52:17Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2237,7 +2209,10 @@ class Log(@volatile private var _dir: File,\n     def deleteSegments(): Unit = {\n       info(s\"Deleting segments ${segments.mkString(\",\")}\")\n       maybeHandleIOException(s\"Error while deleting segments for $topicPartition in dir ${dir.getParent}\") {\n-        segments.foreach(_.deleteIfExists())\n+        segments.foreach { segment =>\n+          segment.deleteIfExists()\n+          producerStateManager.deleteIfExists(segment.baseOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzMzY2ODM1", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-443366835", "createdAt": "2020-07-06T19:56:21Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff9dc5bfc8233874c222413afb780b179e0c4639", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/ff9dc5bfc8233874c222413afb780b179e0c4639", "committedDate": "2020-10-01T21:37:52Z", "message": "Rework how producer state snapshot files are maintained:\n\n- On broker startup, only the most recent producer state snapshot and producer state snapshots associated with a segment file are retained.\n\n- Producer state snapshot files are managed via an in-memory index inside of the ProducerStateManager to reduce lookup costs.\n\n- Producer state snapshots are kept around for the lifetime of their associated segment to prevent large truncations from deleting all producer state snapshot files.\n\n- Producer state snapshots are now deleted asynchronously as part of the thread which\n  deletes segment and index files. Snapshot file deletion occurs when segment deletion occurs."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "ff9dc5bfc8233874c222413afb780b179e0c4639", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/ff9dc5bfc8233874c222413afb780b179e0c4639", "committedDate": "2020-10-01T21:37:52Z", "message": "Rework how producer state snapshot files are maintained:\n\n- On broker startup, only the most recent producer state snapshot and producer state snapshots associated with a segment file are retained.\n\n- Producer state snapshot files are managed via an in-memory index inside of the ProducerStateManager to reduce lookup costs.\n\n- Producer state snapshots are kept around for the lifetime of their associated segment to prevent large truncations from deleting all producer state snapshot files.\n\n- Producer state snapshots are now deleted asynchronously as part of the thread which\n  deletes segment and index files. Snapshot file deletion occurs when segment deletion occurs."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0c0b9caf3ae300d2ff65abcb8f187783e21d7af", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/b0c0b9caf3ae300d2ff65abcb8f187783e21d7af", "committedDate": "2020-10-02T19:03:23Z", "message": "Add tests verifying that deleting stray snapshots does not remove the\n\nlargest stray snapshot."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0c6475e19028c43ba52b12513ae691fa0b678fd", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/c0c6475e19028c43ba52b12513ae691fa0b678fd", "committedDate": "2020-10-05T14:44:57Z", "message": "The snapshots var maintained by the PoducerStateManager does not need to be volatile as it is only a set once during log initialization."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28ec7f1ebb79be0eef4f6242c3cef2789a471913", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/28ec7f1ebb79be0eef4f6242c3cef2789a471913", "committedDate": "2020-10-05T16:17:45Z", "message": "When deleting cleaned segment files, do not delete the\ncorresponding producer state snapshot file if the deleted snapshot\nfile is being replaced with a clean version."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab7cc894ad1f7bfdb4c2ca3dd84a0e2827f4cf0e", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/ab7cc894ad1f7bfdb4c2ca3dd84a0e2827f4cf0e", "committedDate": "2020-10-05T17:03:34Z", "message": "When advancing the log start offset, ensure that the lastMapOffset is advanced in the ProducerStateManager."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a233f30a53b1bd5ef94ae1e79c5ce79566f9b74", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/1a233f30a53b1bd5ef94ae1e79c5ce79566f9b74", "committedDate": "2020-10-05T20:52:36Z", "message": "Merge branch 'trunk' of github.com:apache/kafka into KAFKA-9393-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d22dfddcaaa5c5f304e8dfbc40a5286be2cc6823", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/d22dfddcaaa5c5f304e8dfbc40a5286be2cc6823", "committedDate": "2020-10-06T12:26:54Z", "message": "Cleanup ProducerStateManager\n\n- Use `snapshot` instead of `file` for snapshot name when loading producer state.\n- Use `removeAndDeleteSnapshot` where applicable instead of doing it manuallly."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/1fd21b9e9bfaf5668f6cae3d5819d11c191bf359", "committedDate": "2020-10-06T12:29:33Z", "message": "Clarify comment for LogManagerTest."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0Mjg3MjU3", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-504287257", "createdAt": "2020-10-07T21:39:53Z", "commit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozOTo1M1rOHeGfiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNjo0ODoxM1rOHendzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNTcwNQ==", "bodyText": "ProducerStateManager.listSnapshotFiles() could just be listSnapshotFiles() ?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501325705", "createdAt": "2020-10-07T21:39:53Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -496,6 +491,53 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   // completed transactions whose markers are at offsets above the high watermark\n   private val unreplicatedTxns = new util.TreeMap[Long, TxnMetadata]\n \n+  /**\n+   * Load producer state snapshots by scanning the _logDir.\n+   */\n+  private def loadSnapshots(): ConcurrentSkipListMap[java.lang.Long, SnapshotFile] = {\n+    val tm = new ConcurrentSkipListMap[java.lang.Long, SnapshotFile]()\n+    for (f <- ProducerStateManager.listSnapshotFiles(_logDir)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNjEyMA==", "bodyText": "Incomplete sentence after \"but not to remove\".", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501326120", "createdAt": "2020-10-07T21:40:55Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -496,6 +491,53 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   // completed transactions whose markers are at offsets above the high watermark\n   private val unreplicatedTxns = new util.TreeMap[Long, TxnMetadata]\n \n+  /**\n+   * Load producer state snapshots by scanning the _logDir.\n+   */\n+  private def loadSnapshots(): ConcurrentSkipListMap[java.lang.Long, SnapshotFile] = {\n+    val tm = new ConcurrentSkipListMap[java.lang.Long, SnapshotFile]()\n+    for (f <- ProducerStateManager.listSnapshotFiles(_logDir)) {\n+      tm.put(f.offset, f)\n+    }\n+    tm\n+  }\n+\n+  /**\n+   * Scans the log directory, gathering all producer state snapshot files. Snapshot files which do not have an offset\n+   * corresponding to one of the provided offsets in segmentBaseOffsets will be removed, except in the case that there\n+   * is a snapshot file at a higher offset than any offset in segmentBaseOffsets.\n+   *\n+   * The goal here is to remove any snapshot files which do not have an associated segment file, but not to remove", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyODA2Nw==", "bodyText": "Could we also make sure that the offset for latestStraySnapshot is > the largest offset in segmentBaseOffsets?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501328067", "createdAt": "2020-10-07T21:45:31Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -496,6 +491,53 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   // completed transactions whose markers are at offsets above the high watermark\n   private val unreplicatedTxns = new util.TreeMap[Long, TxnMetadata]\n \n+  /**\n+   * Load producer state snapshots by scanning the _logDir.\n+   */\n+  private def loadSnapshots(): ConcurrentSkipListMap[java.lang.Long, SnapshotFile] = {\n+    val tm = new ConcurrentSkipListMap[java.lang.Long, SnapshotFile]()\n+    for (f <- ProducerStateManager.listSnapshotFiles(_logDir)) {\n+      tm.put(f.offset, f)\n+    }\n+    tm\n+  }\n+\n+  /**\n+   * Scans the log directory, gathering all producer state snapshot files. Snapshot files which do not have an offset\n+   * corresponding to one of the provided offsets in segmentBaseOffsets will be removed, except in the case that there\n+   * is a snapshot file at a higher offset than any offset in segmentBaseOffsets.\n+   *\n+   * The goal here is to remove any snapshot files which do not have an associated segment file, but not to remove\n+   */\n+  private[log] def removeStraySnapshots(segmentBaseOffsets: Set[Long]): Unit = {\n+    var latestStraySnapshot: Option[SnapshotFile] = None\n+    val ss = loadSnapshots()\n+    for (snapshot <- ss.values().asScala) {\n+      val key = snapshot.offset\n+      latestStraySnapshot match {\n+        case Some(prev) =>\n+          if (!segmentBaseOffsets.contains(key)) {\n+            // this snapshot is now the largest stray snapshot.\n+            prev.deleteIfExists()\n+            ss.remove(prev.offset)\n+            latestStraySnapshot = Some(snapshot)\n+          }\n+        case None =>\n+          if (!segmentBaseOffsets.contains(key)) {\n+            latestStraySnapshot = Some(snapshot)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMzMjU2OQ==", "bodyText": "Hmm, why don't we need to delete snapshots before logStartOffset?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501332569", "createdAt": "2020-10-07T21:55:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/ProducerStateManager.scala", "diffHunk": "@@ -653,36 +697,44 @@ class ProducerStateManager(val topicPartition: TopicPartition,\n   def takeSnapshot(): Unit = {\n     // If not a new offset, then it is not worth taking another snapshot\n     if (lastMapOffset > lastSnapOffset) {\n-      val snapshotFile = Log.producerSnapshotFile(logDir, lastMapOffset)\n+      val snapshotFile = SnapshotFile(Log.producerSnapshotFile(_logDir, lastMapOffset))\n       info(s\"Writing producer snapshot at offset $lastMapOffset\")\n-      writeSnapshot(snapshotFile, producers)\n+      writeSnapshot(snapshotFile.file, producers)\n+      snapshots.put(snapshotFile.offset, snapshotFile)\n \n       // Update the last snap offset according to the serialized map\n       lastSnapOffset = lastMapOffset\n     }\n   }\n \n+  /**\n+   * Update the parentDir for this ProducerStateManager and all of the snapshot files which it manages.\n+   */\n+  def updateParentDir(parentDir: File): Unit ={\n+    _logDir = parentDir\n+    snapshots.forEach((_, s) => s.updateParentDir(parentDir))\n+  }\n+\n   /**\n    * Get the last offset (exclusive) of the latest snapshot file.\n    */\n-  def latestSnapshotOffset: Option[Long] = latestSnapshotFile.map(file => offsetFromFile(file))\n+  def latestSnapshotOffset: Option[Long] = latestSnapshotFile.map(_.offset)\n \n   /**\n    * Get the last offset (exclusive) of the oldest snapshot file.\n    */\n-  def oldestSnapshotOffset: Option[Long] = oldestSnapshotFile.map(file => offsetFromFile(file))\n+  def oldestSnapshotOffset: Option[Long] = oldestSnapshotFile.map(_.offset)\n \n   /**\n-   * When we remove the head of the log due to retention, we need to remove snapshots older than\n-   * the new log start offset.\n+   * Remove any unreplicated transactions lower than the provided logStartOffset and bring the lastMapOffset forward\n+   * if necessary.\n    */\n-  def truncateHead(logStartOffset: Long): Unit = {\n+  def onLogStartOffsetIncremented(logStartOffset: Long): Unit = {\n     removeUnreplicatedTransactions(logStartOffset)\n \n     if (lastMapOffset < logStartOffset)\n       lastMapOffset = logStartOffset\n \n-    deleteSnapshotsBefore(logStartOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2MjMzNQ==", "bodyText": "Hmm, why is orphanedSnapshotFile deleted since we keep the last stray snapshot?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501862335", "createdAt": "2020-10-08T16:42:19Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1226,6 +1225,104 @@ class LogTest {\n     assertEquals(retainedLastSeqOpt, reloadedLastSeqOpt)\n   }\n \n+  @Test\n+  def testRetentionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = 0, retentionMs = 1000 * 60, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+\n+    log.updateHighWatermark(log.logEndOffset)\n+\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+    // Sleep to breach the retention period\n+    mockTime.sleep(1000 * 60 + 1)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testLogStartOffsetMovementDeletesSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"b\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"c\".getBytes)), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Increment the log start offset to exclude the first two segments.\n+    log.maybeIncrementLogStartOffset(log.logEndOffset - 1, ClientRecordDeletion)\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expect a single producer state snapshot remaining\", 1, ProducerStateManager.listSnapshotFiles(logDir).size)\n+  }\n+\n+  @Test\n+  def testCompactionDeletesProducerStateSnapshots(): Unit = {\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, cleanupPolicy = LogConfig.Compact, fileDeleteDelayMs = 0)\n+    val log = createLog(logDir, logConfig)\n+    val pid1 = 1L\n+    val epoch = 0.toShort\n+    val cleaner = new Cleaner(id = 0,\n+      offsetMap = new FakeOffsetMap(Int.MaxValue),\n+      ioBufferSize = 64 * 1024,\n+      maxIoBufferSize = 64 * 1024,\n+      dupBufferLoadFactor = 0.75,\n+      throttler = new Throttler(Double.MaxValue, Long.MaxValue, false, time = mockTime),\n+      time = mockTime,\n+      checkDone = _ => {})\n+\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"a\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 0), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"b\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 1), leaderEpoch = 0)\n+    log.roll()\n+    log.appendAsLeader(TestUtils.records(List(new SimpleRecord(\"a\".getBytes, \"c\".getBytes())), producerId = pid1,\n+      producerEpoch = epoch, sequence = 2), leaderEpoch = 0)\n+    log.updateHighWatermark(log.logEndOffset)\n+    assertEquals(\"expected a snapshot file per segment base offset, except the first segment\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(logDir).size)\n+\n+    // Clean segments, this should delete everything except the active segment since there only\n+    // exists the key \"a\".\n+    cleaner.clean(LogToClean(log.topicPartition, log, 0, log.logEndOffset))\n+    log.deleteOldSegments()\n+    // Sleep to breach the file delete delay and run scheduled file deletion tasks\n+    mockTime.sleep(1)\n+    assertEquals(\"expected a snapshot file per segment base offset, excluding the first\", log.logSegments.map(_.baseOffset).toSeq.sorted.drop(1), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+  }\n+\n+  @Test\n+  def testLoadingLogCleansOrphanedProducerStateSnapshots(): Unit = {\n+    val orphanedSnapshotFile = Log.producerSnapshotFile(logDir, 42).toPath\n+    Files.createFile(orphanedSnapshotFile)\n+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5, retentionBytes = -1, fileDeleteDelayMs = 0)\n+    createLog(logDir, logConfig)\n+    assertEquals(\"expected orphaned producer state snapshot file to be cleaned up\", 0, ProducerStateManager.listSnapshotFiles(logDir).size)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2MzExMQ==", "bodyText": "mockTime.sleep() calls scheduler.tick() already.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501863111", "createdAt": "2020-10-08T16:43:37Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1661,14 +1753,17 @@ class LogTest {\n     log.roll()\n \n     assertEquals(2, log.activeProducersWithLastSequence.size)\n-    assertEquals(2, ProducerStateManager.listSnapshotFiles(log.producerStateManager.logDir).size)\n+    assertEquals(2, ProducerStateManager.listSnapshotFiles(log.dir).size)\n \n     log.updateHighWatermark(log.logEndOffset)\n     log.maybeIncrementLogStartOffset(2L, ClientRecordDeletion)\n+    log.deleteOldSegments() // force retention to kick in so that the snapshot files are cleaned up.\n+    mockTime.sleep(logConfig.fileDeleteDelayMs + 1000) // advance the clock so file deletion takes place\n+    mockTime.scheduler.tick()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg2NTkzMg==", "bodyText": "Below, the base offset of the largest segment equals to and doesn't exceed the offset of the largest stray snapshot.", "url": "https://github.com/apache/kafka/pull/7929#discussion_r501865932", "createdAt": "2020-10-08T16:48:13Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala", "diffHunk": "@@ -834,6 +834,40 @@ class ProducerStateManagerTest {\n     assertEquals(None, stateManager.lastEntry(producerId).get.currentTxnFirstOffset)\n   }\n \n+  @Test\n+  def testRemoveStraySnapshotsKeepCleanShutdownSnapshot(): Unit = {\n+    // Test that when stray snapshots are removed, the largest stray snapshot is kept around. This covers the case where\n+    // the broker shutdown cleanly and emitted a snapshot file larger than the base offset of the active segment.\n+\n+    // Create 3 snapshot files at different offsets.\n+    Log.producerSnapshotFile(logDir, 42).createNewFile()\n+    Log.producerSnapshotFile(logDir, 5).createNewFile()\n+    Log.producerSnapshotFile(logDir, 2).createNewFile()\n+\n+    // claim that we only have one segment with a base offset of 5\n+    stateManager.removeStraySnapshots(Set(5))\n+\n+    // The snapshot file at offset 2 should be considered a stray, but the snapshot at 42 should be kept\n+    // around because it is the largest snapshot.\n+    assertEquals(Some(42), stateManager.latestSnapshotOffset)\n+    assertEquals(Some(5), stateManager.oldestSnapshotOffset)\n+    assertEquals(Seq(5, 42), ProducerStateManager.listSnapshotFiles(logDir).map(_.offset).sorted)\n+  }\n+\n+  @Test\n+  def testRemoveAllStraySnapshots(): Unit = {\n+    // Test that when stray snapshots are removed, all stray snapshots are removed when the base offset of the largest\n+    // segment exceeds the offset of the largest stray snapshot.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1fd21b9e9bfaf5668f6cae3d5819d11c191bf359"}, "originalPosition": 27}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15bc15f15e7fe0cd92978cbad4699a3c956cab7b", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/15bc15f15e7fe0cd92978cbad4699a3c956cab7b", "committedDate": "2020-10-08T19:25:25Z", "message": "Address PR feedback\n\n- Clarify the purpose of the test which verifies truncation cleans up snapshots.\n\n- Add new test which verifies that we delete stray snapshots below the base offset of the active segment, but keep the largest stray snapshot within the log end offset."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa072aaeaeda278f92da51ac26f4f0c0f473a0a8", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/aa072aaeaeda278f92da51ac26f4f0c0f473a0a8", "committedDate": "2020-10-08T19:35:53Z", "message": "Address PR feedback\n\n- Directly remove the latestStraySnapshot if it is less than the largest segment base offset."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16f51f78f676c2dcaf2f75b0ec41d0847f277390", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/16f51f78f676c2dcaf2f75b0ec41d0847f277390", "committedDate": "2020-10-08T19:48:04Z", "message": "Address PR feedback\n\n- Reword some comments in ProducerStateManagerTest"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1MTgwMzE1", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-505180315", "createdAt": "2020-10-08T21:30:30Z", "commit": {"oid": "16f51f78f676c2dcaf2f75b0ec41d0847f277390"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozMDozMFrOHexG2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozMDozN1rOHexHFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAyMzg5OQ==", "bodyText": "Since deleteSnapshotsBefore() still exist, could we keep using it?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r502023899", "createdAt": "2020-10-08T21:30:30Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -782,7 +782,7 @@ class LogTest {\n     }\n \n     // Retain snapshots for the last 2 segments\n-    ProducerStateManager.deleteSnapshotsBefore(logDir, segmentOffsets(segmentOffsets.size - 2))\n+    ProducerStateManager.listSnapshotFiles(logDir).filter(_.offset < segmentOffsets(segmentOffsets.size - 2)).foreach(_.deleteIfExists())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16f51f78f676c2dcaf2f75b0ec41d0847f277390"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAyMzk1OA==", "bodyText": "Since deleteSnapshotsBefore() still exist, could we keep using it?", "url": "https://github.com/apache/kafka/pull/7929#discussion_r502023958", "createdAt": "2020-10-08T21:30:37Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -794,16 +794,12 @@ class LogTest {\n \n     // Only delete snapshots before the base offset of the recovery point segment (post KAFKA-5829 behaviour) to\n     // avoid reading all segments\n-    ProducerStateManager.deleteSnapshotsBefore(logDir, offsetForRecoveryPointSegment)\n+    ProducerStateManager.listSnapshotFiles(logDir).filter(_.offset < offsetForRecoveryPointSegment).foreach(_.deleteIfExists())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16f51f78f676c2dcaf2f75b0ec41d0847f277390"}, "originalPosition": 32}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56a68f566da4eecf7a88cd782967bfc4838808b9", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/56a68f566da4eecf7a88cd782967bfc4838808b9", "committedDate": "2020-10-08T23:29:34Z", "message": "Address PR feedback\n\n- Switch back to using `deleteSnapshotsBefore`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1MjQ3OTgy", "url": "https://github.com/apache/kafka/pull/7929#pullrequestreview-505247982", "createdAt": "2020-10-08T23:40:31Z", "commit": {"oid": "56a68f566da4eecf7a88cd782967bfc4838808b9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9de260d604a75d44b91e5063b5b6ff119b3c0c33", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/9de260d604a75d44b91e5063b5b6ff119b3c0c33", "committedDate": "2020-10-09T02:02:35Z", "message": "`maxOption` is not available in this version of Scala, use the explicit form instead."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae99b4ebf20875bf69ce4d992586cc5707c6a0ee", "author": {"user": {"login": "gardnervickers", "name": "Gardner Vickers"}}, "url": "https://github.com/apache/kafka/commit/ae99b4ebf20875bf69ce4d992586cc5707c6a0ee", "committedDate": "2020-10-09T13:21:33Z", "message": "Merge branch 'trunk' of github.com:apache/kafka into KAFKA-9393-2"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1943, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}