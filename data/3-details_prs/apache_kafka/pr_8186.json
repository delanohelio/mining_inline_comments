{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgxMDIyMDYx", "number": 8186, "title": "KAFKA-9618: Directory deletion failure leading to error task RocksDB open", "bodyText": "This PR tries to reorder the closing behavior by doing the state store cleanup first before releasing the lock.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-02-27T19:10:29Z", "url": "https://github.com/apache/kafka/pull/8186", "merged": true, "mergeCommit": {"oid": "c2ec974e81f1c65aa2f2e43f7f4dc820b1957bca"}, "closed": true, "closedAt": "2020-03-04T05:45:12Z", "author": {"login": "abbccdda"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcImnUegFqTM2NjEyOTY3MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcKPcC4ABqjMwOTQ5NTgwNTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2MTI5Njcw", "url": "https://github.com/apache/kafka/pull/8186#pullrequestreview-366129670", "createdAt": "2020-02-28T02:36:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQwMjozNjo0OVrOFvn3KQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQwMjozNjo0OVrOFvn3KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MDQ4OQ==", "bodyText": "Why do we now always wipe the state stores (even on clean close)? cc/ @guozhangwang", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385480489", "createdAt": "2020-02-28T02:36:49Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -83,8 +84,8 @@ static void wipeStateStores(final Logger log, final ProcessorStateManager stateM\n         try {\n             Utils.delete(stateMgr.baseDir());\n         } catch (final IOException fatalException) {\n-            // since it is only called under dirty close, we always swallow the exception", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2MTMxNzIz", "url": "https://github.com/apache/kafka/pull/8186#pullrequestreview-366131723", "createdAt": "2020-02-28T02:44:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQwMjo0NDoyNlrOFvn9sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQwMjo0NDoyNlrOFvn9sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MjE2Mg==", "bodyText": "Seems like we should retry, at least if this is a clean close...otherwise this thread still dies, and also kills any other threads that then get assigned this task.\nOr better yet, can we fix the state store to not blindly open non-existent files? If we successfully delete the checkpoint file but fail to delete the stores, Streams shouldn't attempt to open any store files.\nIf we fail at deleting the checkpoint, we're more at risk as the data may be corrupted (otherwise we wouldn't be trying to delete the task state to begin with). In that case it seems like the only safe thing to do would be to retry until we at least delete the checkpoint file", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385482162", "createdAt": "2020-02-28T02:44:26Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -83,8 +84,8 @@ static void wipeStateStores(final Logger log, final ProcessorStateManager stateM\n         try {\n             Utils.delete(stateMgr.baseDir());\n         } catch (final IOException fatalException) {\n-            // since it is only called under dirty close, we always swallow the exception\n-            log.warn(\"Failed to wiping state stores for task {}\", stateMgr.taskId());\n+            log.error(\"Failed to wiping state stores for task {} due to {}\", stateMgr.taskId(), fatalException);\n+            throw new StreamsException(fatalException);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY3NTQ2NTI3", "url": "https://github.com/apache/kafka/pull/8186#pullrequestreview-367546527", "createdAt": "2020-03-02T21:37:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQyMTozNzo0OFrOFwwMXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQyMTozNzo0OFrOFwwMXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjY2NTU2Nw==", "bodyText": "Is that okay that we wipe out the directory already and then closing state manager, in which we would flush the stores and then close them? Would any exception be thrown?", "url": "https://github.com/apache/kafka/pull/8186#discussion_r386665567", "createdAt": "2020-03-02T21:37:48Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -440,16 +440,16 @@ private void close(final boolean clean) {\n             }\n \n             if (state() == State.CLOSING) {\n-                // first close state manager (which is idempotent) then close the record collector (which could throw),\n-                // if the latter throws and we re-close dirty which would close the state manager again.\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);\n-\n                 // if EOS is enabled, we wipe out the whole state store for unclean close\n                 // since they are invalid to use anymore\n                 if (!clean && !eosDisabled) {\n                     StateManagerUtil.wipeStateStores(log, stateMgr);\n                 }\n \n+                // first close state manager (which is idempotent) then close the record collector (which could throw),\n+                // if the latter throws and we re-close dirty which would close the state manager again.\n+                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4MjU2OTA0", "url": "https://github.com/apache/kafka/pull/8186#pullrequestreview-368256904", "createdAt": "2020-03-03T19:22:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOToyMjoyN1rOFxTQ7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOToyMjo0N1rOFxTRjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI0MDE3NA==", "bodyText": "nit: use TaskType.ACTIVE.", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387240174", "createdAt": "2020-03-03T19:22:27Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -440,15 +442,14 @@ private void close(final boolean clean) {\n             }\n \n             if (state() == State.CLOSING) {\n-                // first close state manager (which is idempotent) then close the record collector (which could throw),\n-                // if the latter throws and we re-close dirty which would close the state manager again.\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);\n-\n                 // if EOS is enabled, we wipe out the whole state store for unclean close\n                 // since they are invalid to use anymore\n-                if (!clean && !eosDisabled) {\n-                    StateManagerUtil.wipeStateStores(log, stateMgr);\n-                }\n+                final boolean wipeStateStore = !clean && !eosDisabled;\n+\n+                // first close state manager (which is idempotent) then close the record collector (which could throw),\n+                // if the latter throws and we re-close dirty which would close the state manager again.\n+                StateManagerUtil.closeStateManager(log, logPrefix, clean,\n+                    wipeStateStore, stateMgr, stateDirectory, \"active\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI0MDMzNA==", "bodyText": "Use TaskType.STANDBY.", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387240334", "createdAt": "2020-03-03T19:22:47Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -179,7 +179,7 @@ private void close(final boolean clean) {\n             }\n \n             if (state() == State.CLOSING) {\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);\n+                StateManagerUtil.closeStateManager(log, logPrefix, clean, false, stateMgr, stateDirectory, \"standby\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4NDE0MDIx", "url": "https://github.com/apache/kafka/pull/8186#pullrequestreview-368414021", "createdAt": "2020-03-04T00:02:30Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowMjozMFrOFxbC2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNDowN1rOFxbEug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NzY0MQ==", "bodyText": "Why do we want to validate this case, where no exception would be thrown, and hence no handling logic should be triggered either?", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387367641", "createdAt": "2020-03-04T00:02:30Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.test.IntegrationTest;\n+\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateAfterTest;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateBeforeTest;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertFalse;\n+\n+/**\n+ * Test the unclean shutdown behavior around state store cleanup.\n+ */\n+@Category(IntegrationTest.class)\n+@RunWith(value = Parameterized.class)\n+public class EOSUncleanShutdownIntegrationTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(3);\n+\n+    @ClassRule\n+    public static final TemporaryFolder TEST_FOLDER = new TemporaryFolder(TestUtils.tempDirectory());\n+\n+    private static final Properties STREAMS_CONFIG = new Properties();\n+    private static final StringSerializer STRING_SERIALIZER = new StringSerializer();\n+    private static final Long COMMIT_INTERVAL = 100L;\n+\n+    private static final int RECORD_TOTAL = 3;\n+\n+    @Parameterized.Parameters(name = \"exception threshold\")\n+    public static Collection<Object[]> data() {\n+        final List<Object[]> values = new ArrayList<>();\n+        for (final int threshold : Arrays.asList(RECORD_TOTAL, RECORD_TOTAL + 1)) {\n+            values.add(new Object[]{threshold});\n+        }\n+        return values;\n+    }\n+\n+    private final int fatalExceptionThreshold;\n+\n+    public EOSUncleanShutdownIntegrationTest(final int fatalExceptionThreshold) {\n+        this.fatalExceptionThreshold = fatalExceptionThreshold;\n+    }\n+\n+    @BeforeClass\n+    public static void setupConfigsAndUtils() {\n+        STREAMS_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        STREAMS_CONFIG.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL);\n+\n+        STREAMS_CONFIG.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);\n+        STREAMS_CONFIG.put(StreamsConfig.STATE_DIR_CONFIG, TEST_FOLDER.getRoot().getPath());\n+    }\n+\n+    @Test\n+    public void shouldWorkWithUncleanShutdownWipeOutStateStore() throws InterruptedException {\n+        final String appId = getClass().getSimpleName().toLowerCase(Locale.getDefault()) + \"-test\";\n+        STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appId);\n+\n+        final String input = \"input-topic\";\n+        cleanStateBeforeTest(CLUSTER, input);\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final KStream<String, String> inputStream = builder.stream(input);\n+\n+        final AtomicInteger recordCount = new AtomicInteger(0);\n+\n+        final KTable<String, String> valueCounts = inputStream\n+                                                       .groupByKey()\n+                                                       .aggregate(() -> \"()\", (key, value, aggregate) -> aggregate + \",(\" + key + \": \" + value + \")\", Materialized.as(\"aggregated_value\"));\n+        valueCounts.toStream().peek((key, value) -> {\n+            if (recordCount.incrementAndGet() >= fatalExceptionThreshold) {\n+                throw new IllegalStateException(\"Crash on the \" + fatalExceptionThreshold + \" record\");\n+            }\n+        });\n+\n+        final Properties producerConfig = mkProperties(mkMap(\n+            mkEntry(ProducerConfig.CLIENT_ID_CONFIG, \"anything\"),\n+            mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers())\n+        ));\n+        final KafkaStreams driver = IntegrationTestUtils.getStartedStreams(STREAMS_CONFIG, builder, true);\n+\n+        final File stateDir = new File(\n+            String.join(\"/\", TEST_FOLDER.getRoot().getPath(), appId, \"0_0\"));\n+        try {\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                singletonList(new KeyValueTimestamp<>(\"k1\", \"v1\", 0L)));\n+\n+            TestUtils.waitForCondition(stateDir::exists,\n+                \"Failed awaiting CreateTopics first request failure\");\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                asList(new KeyValueTimestamp<>(\"k2\", \"v2\", 1L),\n+                    new KeyValueTimestamp<>(\"k3\", \"v3\", 2L)));\n+\n+            TestUtils.waitForCondition(() -> recordCount.get() == RECORD_TOTAL,\n+                \"Expected \" + RECORD_TOTAL + \" records processed but only got \" + recordCount.get());\n+        } finally {\n+            driver.close();\n+            if (fatalExceptionThreshold > RECORD_TOTAL) {\n+                assertTrue(stateDir.exists());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODEyMg==", "bodyText": "We can also add a validation that driver KafkaStreams's state is ERROR as we handle the error by shutting down the thread.", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387368122", "createdAt": "2020-03-04T00:04:07Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.test.IntegrationTest;\n+\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateAfterTest;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateBeforeTest;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertFalse;\n+\n+/**\n+ * Test the unclean shutdown behavior around state store cleanup.\n+ */\n+@Category(IntegrationTest.class)\n+@RunWith(value = Parameterized.class)\n+public class EOSUncleanShutdownIntegrationTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(3);\n+\n+    @ClassRule\n+    public static final TemporaryFolder TEST_FOLDER = new TemporaryFolder(TestUtils.tempDirectory());\n+\n+    private static final Properties STREAMS_CONFIG = new Properties();\n+    private static final StringSerializer STRING_SERIALIZER = new StringSerializer();\n+    private static final Long COMMIT_INTERVAL = 100L;\n+\n+    private static final int RECORD_TOTAL = 3;\n+\n+    @Parameterized.Parameters(name = \"exception threshold\")\n+    public static Collection<Object[]> data() {\n+        final List<Object[]> values = new ArrayList<>();\n+        for (final int threshold : Arrays.asList(RECORD_TOTAL, RECORD_TOTAL + 1)) {\n+            values.add(new Object[]{threshold});\n+        }\n+        return values;\n+    }\n+\n+    private final int fatalExceptionThreshold;\n+\n+    public EOSUncleanShutdownIntegrationTest(final int fatalExceptionThreshold) {\n+        this.fatalExceptionThreshold = fatalExceptionThreshold;\n+    }\n+\n+    @BeforeClass\n+    public static void setupConfigsAndUtils() {\n+        STREAMS_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        STREAMS_CONFIG.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL);\n+\n+        STREAMS_CONFIG.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);\n+        STREAMS_CONFIG.put(StreamsConfig.STATE_DIR_CONFIG, TEST_FOLDER.getRoot().getPath());\n+    }\n+\n+    @Test\n+    public void shouldWorkWithUncleanShutdownWipeOutStateStore() throws InterruptedException {\n+        final String appId = getClass().getSimpleName().toLowerCase(Locale.getDefault()) + \"-test\";\n+        STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appId);\n+\n+        final String input = \"input-topic\";\n+        cleanStateBeforeTest(CLUSTER, input);\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final KStream<String, String> inputStream = builder.stream(input);\n+\n+        final AtomicInteger recordCount = new AtomicInteger(0);\n+\n+        final KTable<String, String> valueCounts = inputStream\n+                                                       .groupByKey()\n+                                                       .aggregate(() -> \"()\", (key, value, aggregate) -> aggregate + \",(\" + key + \": \" + value + \")\", Materialized.as(\"aggregated_value\"));\n+        valueCounts.toStream().peek((key, value) -> {\n+            if (recordCount.incrementAndGet() >= fatalExceptionThreshold) {\n+                throw new IllegalStateException(\"Crash on the \" + fatalExceptionThreshold + \" record\");\n+            }\n+        });\n+\n+        final Properties producerConfig = mkProperties(mkMap(\n+            mkEntry(ProducerConfig.CLIENT_ID_CONFIG, \"anything\"),\n+            mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers())\n+        ));\n+        final KafkaStreams driver = IntegrationTestUtils.getStartedStreams(STREAMS_CONFIG, builder, true);\n+\n+        final File stateDir = new File(\n+            String.join(\"/\", TEST_FOLDER.getRoot().getPath(), appId, \"0_0\"));\n+        try {\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                singletonList(new KeyValueTimestamp<>(\"k1\", \"v1\", 0L)));\n+\n+            TestUtils.waitForCondition(stateDir::exists,\n+                \"Failed awaiting CreateTopics first request failure\");\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                asList(new KeyValueTimestamp<>(\"k2\", \"v2\", 1L),\n+                    new KeyValueTimestamp<>(\"k3\", \"v3\", 2L)));\n+\n+            TestUtils.waitForCondition(() -> recordCount.get() == RECORD_TOTAL,\n+                \"Expected \" + RECORD_TOTAL + \" records processed but only got \" + recordCount.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57dd73774114b79fdde271c21a3dae66ef25dc93", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/57dd73774114b79fdde271c21a3dae66ef25dc93", "committedDate": "2020-03-04T01:44:59Z", "message": "wipe out state store"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4NDYwMjE4", "url": "https://github.com/apache/kafka/pull/8186#pullrequestreview-368460218", "createdAt": "2020-03-04T02:12:17Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "45ffa590cadba452290ca6884059859d98682c77", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/45ffa590cadba452290ca6884059859d98682c77", "committedDate": "2020-03-04T04:44:18Z", "message": "fix unit test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "45ffa590cadba452290ca6884059859d98682c77", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/45ffa590cadba452290ca6884059859d98682c77", "committedDate": "2020-03-04T04:44:18Z", "message": "fix unit test"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1616, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}