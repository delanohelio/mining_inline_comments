{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3NjE5NTU1", "number": 9177, "title": "KAFKA-9924: Add RocksDB metric num-entries-active-mem-table", "bodyText": "This commit adds the first RocksDB metric that exposes RocksDB property num-entries-active-mem-table. More specifically it introduces\n\ncode in StreamsMetricsImpl that is shared by all such metrics,\nunit tests for the shared code\ncode that adds the metric\nunit tests and intergration tests for the metric\n\nThis commit only contains one metric to keep the PR at a reasonable size. All other RocksDB metrics described in KIP-607 will be added in other PRs.", "createdAt": "2020-08-13T20:33:59Z", "url": "https://github.com/apache/kafka/pull/9177", "merged": true, "mergeCommit": {"oid": "9da32b6bd014f1bdeeee5da8fcd00995a5916323"}, "closed": true, "closedAt": "2020-08-27T23:04:28Z", "author": {"login": "cadonna"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5paXIAH2gAyNDY3NjE5NTU1OmRkNjQ3NTM1MzM5YTlkMGNjNjAwM2NhYTVmMjk3MTk1ZmQ2NGQ2MTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdDHyb7AH2gAyNDY3NjE5NTU1OjhjZjIxNGU5NWI1Y2M5ZGY0OWUzNGUwNGE1NmY4MGVlMjJlMDk5NGI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "dd647535339a9d0cc6003caa5f297195fd64d615", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/dd647535339a9d0cc6003caa5f297195fd64d615", "committedDate": "2020-07-29T11:34:40Z", "message": "Add wrapper around BlockBasedTableConfig to make cache accessible"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "634d18b15ff400dced3b1af6b43c98630e115d8d", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/634d18b15ff400dced3b1af6b43c98630e115d8d", "committedDate": "2020-07-29T11:34:40Z", "message": "Refactor RocksDBMetricsRecorder and instantiation of RocksDBMetricsRecordingTrigger"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "committedDate": "2020-07-30T08:51:12Z", "message": "Add unit test when user specifies new table format config"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0afb35d8bba0d76adf9002bd9db14025cd0ca340", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/0afb35d8bba0d76adf9002bd9db14025cd0ca340", "committedDate": "2020-07-30T10:25:54Z", "message": "Make RocksDB recording trigger member variable final"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4331821d84ecc8873f814cd691e9bb13b7762fe2", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/4331821d84ecc8873f814cd691e9bb13b7762fe2", "committedDate": "2020-07-31T13:46:51Z", "message": "Make warning regarding RocksDB's table configuration clearer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34dac91f6249d809df5b366eaf82596dde3d5b3b", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/34dac91f6249d809df5b366eaf82596dde3d5b3b", "committedDate": "2020-07-31T13:50:02Z", "message": "Remove unused parameter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91b3430b9db78b0cff834d6197f509f65a639dcd", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/91b3430b9db78b0cff834d6197f509f65a639dcd", "committedDate": "2020-08-10T20:15:57Z", "message": "Throw exception instead of log a warning"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebab7af0bb7ea98af111aa626874f678d9fe3c56", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/ebab7af0bb7ea98af111aa626874f678d9fe3c56", "committedDate": "2020-08-11T19:41:48Z", "message": "Allow other table formats than block-based tables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c967a48033bcf92438e8fc419a6a8b4835ba665", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/5c967a48033bcf92438e8fc419a6a8b4835ba665", "committedDate": "2020-08-12T08:36:07Z", "message": "Improve statistics handling in metrics recorder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb15c3a5bf91f9a46d6914739b617ca2291f52da", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/fb15c3a5bf91f9a46d6914739b617ca2291f52da", "committedDate": "2020-08-12T15:27:07Z", "message": "Change guard to avoid recording when statistics are null"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "committedDate": "2020-08-12T15:31:38Z", "message": "Add methods to add gauge metrics on state store level"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86bcf00fc34a840160029178ca3b0ca77d4f9443", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/86bcf00fc34a840160029178ca3b0ca77d4f9443", "committedDate": "2020-08-12T15:31:38Z", "message": "Add metric num-entries-active-mem-table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a54a5053720a7f692d4b650ee6fa1e02fe436c75", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/a54a5053720a7f692d4b650ee6fa1e02fe436c75", "committedDate": "2020-08-13T19:55:24Z", "message": "Merge remote-tracking branch 'upstream/trunk' into AK9924-add-metrics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/1c733ffa84da60c9fb9b93f532c884beb7bd3063", "committedDate": "2020-08-13T20:13:15Z", "message": "Fix compile error"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4NTk3OTk3", "url": "https://github.com/apache/kafka/pull/9177#pullrequestreview-468597997", "createdAt": "2020-08-17T16:05:03Z", "commit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowNTowNFrOHBvHdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjoyNjoyMlrOHBv40A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MjU4MA==", "bodyText": "Should we check if (metrics.metric(metricName) == null) again after synchronizing?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471582580", "createdAt": "2020-08-17T16:05:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ==", "bodyText": "Should we make this all one method, and also synchronize both storeLevel collections on a single monitor?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471584025", "createdAt": "2020-08-17T16:07:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4ODA0OQ==", "bodyText": "It seems a little risky to use this in a multithreaded context. Why not just create a new short-lived buffer each time for the conversion?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471588049", "createdAt": "2020-08-17T16:14:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -56,6 +62,9 @@ public void maybeCloseStatistics() {\n         }\n     }\n \n+    private static final String ROCKSDB_PROPERTIES_PREFIX = \"rocksdb.\";\n+    private static final ByteBuffer CONVERSION_BUFFER = ByteBuffer.allocate(Long.BYTES);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU5NTIxNg==", "bodyText": "would it not be exactly 1?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471595216", "createdAt": "2020-08-17T16:26:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -609,6 +611,37 @@ public void shouldVerifyThatMetricsGetMeasurementsFromRocksDB() {\n         assertThat((double) bytesWrittenTotal.metricValue(), greaterThan(0d));\n     }\n \n+    @Test\n+    public void shouldVerifyThatMetricsRecordedFromPropertiesGetMeasurementsFromRocksDB() {\n+        final TaskId taskId = new TaskId(0, 0);\n+\n+        final Metrics metrics = new Metrics(new MetricConfig().recordLevel(RecordingLevel.INFO));\n+        final StreamsMetricsImpl streamsMetrics =\n+            new StreamsMetricsImpl(metrics, \"test-application\", StreamsConfig.METRICS_LATEST, time);\n+\n+        context = EasyMock.niceMock(InternalMockProcessorContext.class);\n+        EasyMock.expect(context.metrics()).andStubReturn(streamsMetrics);\n+        EasyMock.expect(context.taskId()).andStubReturn(taskId);\n+        EasyMock.expect(context.appConfigs())\n+                .andStubReturn(new StreamsConfig(StreamsTestUtils.getStreamsConfig()).originals());\n+        EasyMock.expect(context.stateDir()).andStubReturn(dir);\n+        EasyMock.replay(context);\n+\n+        rocksDBStore.init(context, rocksDBStore);\n+        final byte[] key = \"hello\".getBytes();\n+        final byte[] value = \"world\".getBytes();\n+        rocksDBStore.put(Bytes.wrap(key), value);\n+\n+        final Metric numberOfEntriesActiveMemTable = metrics.metric(new MetricName(\n+            \"num-entries-active-mem-table\",\n+            StreamsMetricsImpl.STATE_STORE_LEVEL_GROUP,\n+            \"description is not verified\",\n+            streamsMetrics.storeLevelTagMap(Thread.currentThread().getName(), taskId.toString(), METRICS_SCOPE, DB_NAME)\n+        ));\n+        assertThat(numberOfEntriesActiveMemTable, notNullValue());\n+        assertThat((BigInteger) numberOfEntriesActiveMemTable.metricValue(), greaterThan(BigInteger.valueOf(0)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 57}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f7e6c661557d2a91a9a6c736b00cf7dff01c0d89", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/f7e6c661557d2a91a9a6c736b00cf7dff01c0d89", "committedDate": "2020-08-18T13:34:44Z", "message": "Include feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f", "committedDate": "2020-08-18T15:48:28Z", "message": "Small refactoring"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NzQxMzg3", "url": "https://github.com/apache/kafka/pull/9177#pullrequestreview-469741387", "createdAt": "2020-08-18T19:02:50Z", "commit": {"oid": "f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOTowMjo1MFrOHCh_ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOToyNjo0MFrOHCiyVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxNjEzOA==", "bodyText": "This may be a bit paranoid, but when adding them, the order seem to be initSensors first and initGauges, while removing we call removeAllStoreLevelMetrics first and then the other. I know that today there should be not concurrent threads trying to init / removeAll concurrently, but just to be safe maybe we can make the call ordering to be sensors first and then gauges?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472416138", "createdAt": "2020-08-18T19:02:50Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQyOTE0MQ==", "bodyText": "nit: extra space.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472429141", "createdAt": "2020-08-18T19:26:40Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetrics.java", "diffHunk": "@@ -33,7 +35,7 @@\n import static org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.addSumMetricToSensor;\n import static org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.addValueMetricToSensor;\n \n-public class RocksDBMetrics {\n+public class  RocksDBMetrics {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f"}, "originalPosition": 18}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08299ba8e1e8ca0ee760f09db41395972ddb478a", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/08299ba8e1e8ca0ee760f09db41395972ddb478a", "committedDate": "2020-08-19T07:42:41Z", "message": "Include feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b1909dc3f9167ef3a16408dabd207059f794904", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/4b1909dc3f9167ef3a16408dabd207059f794904", "committedDate": "2020-08-19T09:08:36Z", "message": "Remove synchronized during adding and removing store level sensors and metrics"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1NjUwMzM2", "url": "https://github.com/apache/kafka/pull/9177#pullrequestreview-475650336", "createdAt": "2020-08-26T16:17:47Z", "commit": {"oid": "4b1909dc3f9167ef3a16408dabd207059f794904"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjoxNzo0N1rOHHTrwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjoxNzo0N1rOHHTrwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNDU3OA==", "bodyText": "I'm still mildly concerned about walking back the synchronization here, but I can't think of a realistic scenario in which we'd get a concurrency bug. Then again, the whole point of defaulting to less granular concurrency controls is that it's hard to imagine all the possible scenarios.\nIn this case, it really doesn't seem like there's a good reason to go for super granular concurrency control. Did we spend a lot of time blocked registering sensors before?\nActually, one condition comes to mind: LinkedList is not threadsafe, and accessing the ConcurrentHashMap value is only either a CAS or volatile read, so it doesn't create a memory barrier as synchronized does. Therefore, different threads will only be looking at their own locally cached list for each value in the map, although they'll all agree on the set of keys in the map.\nIf you want to push the current implementation style, then you should use a ConcurrentLinkedDeque instead of LinkedList, but I'd really prefer to see the synchronized blocks come back unless/until there's a compelling performance reason to drop them.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r477424578", "createdAt": "2020-08-26T16:17:47Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -396,34 +398,65 @@ private String cacheSensorPrefix(final String threadId, final String taskId, fin\n             + SENSOR_PREFIX_DELIMITER + SENSOR_CACHE_LABEL + SENSOR_PREFIX_DELIMITER + cacheName;\n     }\n \n-    public final Sensor storeLevelSensor(final String threadId,\n-                                         final String taskId,\n+    public final Sensor storeLevelSensor(final String taskId,\n                                          final String storeName,\n                                          final String sensorName,\n-                                         final Sensor.RecordingLevel recordingLevel,\n+                                         final RecordingLevel recordingLevel,\n                                          final Sensor... parents) {\n-        final String key = storeSensorPrefix(threadId, taskId, storeName);\n-        synchronized (storeLevelSensors) {\n-            final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n-            final Sensor sensor = metrics.getSensor(fullSensorName);\n-            if (sensor == null) {\n+        final String key = storeSensorPrefix(Thread.currentThread().getName(), taskId, storeName);\n+        final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n+        return Optional.ofNullable(metrics.getSensor(fullSensorName))\n+            .orElseGet(() -> {\n                 storeLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName);\n                 return metrics.sensor(fullSensorName, recordingLevel, parents);\n-            } else {\n-                return sensor;\n-            }\n+            });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b1909dc3f9167ef3a16408dabd207059f794904"}, "originalPosition": 59}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6773fca37bae28412562b903fdaac4579f3f664a", "author": {"user": {"login": "cadonna", "name": "Bruno Cadonna"}}, "url": "https://github.com/apache/kafka/commit/6773fca37bae28412562b903fdaac4579f3f664a", "committedDate": "2020-08-27T15:24:46Z", "message": "Add some clarifications"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTA0MTcw", "url": "https://github.com/apache/kafka/pull/9177#pullrequestreview-477104170", "createdAt": "2020-08-27T21:41:25Z", "commit": {"oid": "6773fca37bae28412562b903fdaac4579f3f664a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2d5ed0aedd5f8af2516d00b1cdbaf83b145e3c0", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/f2d5ed0aedd5f8af2516d00b1cdbaf83b145e3c0", "committedDate": "2020-08-27T21:48:14Z", "message": "Merge remote-tracking branch 'apache/trunk' into pull/9177"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cf214e95b5cc9df49e34e04a56f80ee22e0994b", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/8cf214e95b5cc9df49e34e04a56f80ee22e0994b", "committedDate": "2020-08-27T22:03:26Z", "message": "fix conflict with trunk"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 759, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}