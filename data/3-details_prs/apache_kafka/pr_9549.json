{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0ODM5OTEx", "number": 9549, "title": "KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader", "bodyText": "These SMTs were originally specified in KIP-145 but never implemented at the time.\nHeaderTo is not included since its original specification doesn't deal with the fact that there can be >1 header with the same name, but a field can only have a single value (while you could use an array, that doesn't work if the headers for the given name had different schemas).", "createdAt": "2020-11-03T16:09:55Z", "url": "https://github.com/apache/kafka/pull/9549", "merged": true, "mergeCommit": {"oid": "f8f1769256de2a1afa92794309ecee475cd80b79"}, "closed": true, "closedAt": "2021-04-16T14:11:25Z", "author": {"login": "tombentley"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABda6NOGAFqTUyNjU3MTgwMg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABeNnAFyABqjQ2MDM0ODIwNTY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NTcxODAy", "url": "https://github.com/apache/kafka/pull/9549#pullrequestreview-526571802", "createdAt": "2020-11-09T19:10:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToxMDowN1rOHv9nwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NzoxMVrOHv--UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1NDcyMA==", "bodyText": "Might make sense to refer to the MOVE.name and COPY.name fields declared in the Operation enum below instead of using the literal \"move\" and \"copy\" strings in this section.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520054720", "createdAt": "2020-11-09T19:10:07Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1NjY2Nw==", "bodyText": "Why duplicate headers here? According to the Header class's Javadocs, the collection should be mutable.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520056667", "createdAt": "2020-11-09T19:12:07Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1OTI1MQ==", "bodyText": "This looks fairly expensive to perform for every record. Do you think it might make sense to perform some caching, similarly to what's done in the ReplaceField transform?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520059251", "createdAt": "2020-11-09T19:16:34Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            String headerName = headers.get(i);\n+            Object fieldValue = value.get(fieldName);\n+            Schema fieldSchema = operatingSchema.field(fieldName).schema();\n+            updatedHeaders.add(headerName, fieldValue, fieldSchema);\n+        }\n+        return newRecord(record, updatedSchema, updatedValue, updatedHeaders);\n+    }\n+\n+    private Schema moveSchema(Schema operatingSchema) {\n+        final Schema updatedSchema;\n+        final SchemaBuilder builder = SchemaUtil.copySchemaBasics(operatingSchema, SchemaBuilder.struct());\n+        for (Field field : operatingSchema.fields()) {\n+            if (!fields.contains(field.name())) {\n+                builder.field(field.name(), field.schema());\n+            }\n+        }\n+        updatedSchema = builder.build();\n+        return updatedSchema;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1OTczOA==", "bodyText": "Probably want to refer to FIELDS_FIELD and HEADERS_FIELD here instead of the literal \"fields\" and \"headers\" strings.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520059738", "createdAt": "2020-11-09T19:17:24Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            String headerName = headers.get(i);\n+            Object fieldValue = value.get(fieldName);\n+            Schema fieldSchema = operatingSchema.field(fieldName).schema();\n+            updatedHeaders.add(headerName, fieldValue, fieldSchema);\n+        }\n+        return newRecord(record, updatedSchema, updatedValue, updatedHeaders);\n+    }\n+\n+    private Schema moveSchema(Schema operatingSchema) {\n+        final Schema updatedSchema;\n+        final SchemaBuilder builder = SchemaUtil.copySchemaBasics(operatingSchema, SchemaBuilder.struct());\n+        for (Field field : operatingSchema.fields()) {\n+            if (!fields.contains(field.name())) {\n+                builder.field(field.name(), field.schema());\n+            }\n+        }\n+        updatedSchema = builder.build();\n+        return updatedSchema;\n+    }\n+\n+    private R applySchemaless(R record, Object operatingValue) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Map<String, Object> value = Requirements.requireMap(operatingValue, \"header \" + operation);\n+        Map<String, Object> updatedValue = new HashMap<>(value);\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            Object fieldValue = value.get(fieldName);\n+            String headerName = headers.get(i);\n+            if (operation == Operation.MOVE) {\n+                updatedValue.remove(fieldName);\n+            }\n+            updatedHeaders.add(headerName, fieldValue, null);\n+        }\n+        return newRecord(record, null, updatedValue, updatedHeaders);\n+    }\n+\n+    protected abstract Object operatingValue(R record);\n+    protected abstract Schema operatingSchema(R record);\n+    protected abstract R newRecord(R record, Schema updatedSchema, Object updatedValue, Iterable<Header> updatedHeaders);\n+\n+    public static class Key<R extends ConnectRecord<R>> extends HeaderFrom<R> {\n+\n+        @Override\n+        public Object operatingValue(R record) {\n+            return record.key();\n+        }\n+\n+        @Override\n+        protected Schema operatingSchema(R record) {\n+            return record.keySchema();\n+        }\n+\n+        @Override\n+        protected R newRecord(R record, Schema updatedSchema, Object updatedValue, Iterable<Header> updatedHeaders) {\n+            return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue,\n+                    record.valueSchema(), record.value(), record.timestamp(), updatedHeaders);\n+        }\n+    }\n+\n+    public static class Value<R extends ConnectRecord<R>> extends HeaderFrom<R> {\n+\n+        @Override\n+        public Object operatingValue(R record) {\n+            return record.value();\n+        }\n+\n+        @Override\n+        protected Schema operatingSchema(R record) {\n+            return record.valueSchema();\n+        }\n+\n+        @Override\n+        protected R newRecord(R record, Schema updatedSchema, Object updatedValue, Iterable<Header> updatedHeaders) {\n+            return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+                    updatedSchema, updatedValue, record.timestamp(), updatedHeaders);\n+        }\n+    }\n+\n+    @Override\n+    public ConfigDef config() {\n+        return CONFIG_DEF;\n+    }\n+\n+    @Override\n+    public void close() {\n+\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> props) {\n+        final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);\n+        fields = config.getList(FIELDS_FIELD);\n+        headers = config.getList(HEADERS_FIELD);\n+        if (headers.size() != fields.size()) {\n+            throw new ConfigException(\"'fields' config must have the same number of elements as 'headers' config.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NDc2Ng==", "bodyText": "Worth noting that if there's no field with this name in the schema for the record, this will throw an exception and (unless it's configured with an error tolerance to ignore such issues), fail the task.\nThis isn't unreasonable behavior, especially given that that case isn't covered in the KIP, but it's different from what happens right now in the applySchemaless method, which is to silently add a null header instead.\nIt's probably best to ensure the same behavior happens in either case. I don't have a strong preference either way but I'm tentatively leaning towards silently adding null headers since it'll make working with heterogeneous topics easier.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520064766", "createdAt": "2020-11-09T19:25:51Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            String headerName = headers.get(i);\n+            Object fieldValue = value.get(fieldName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3MDMwMQ==", "bodyText": "Does this mean we'd be iterating over every field of every record? Not very performant \ud83d\ude41\nI think we can improve the efficiency here for the COPY case, if not the MOVE case, by reusing the existing value and not creating a new one.\nIn the MOVE case though, it might be the best we have to settle for until/unless there's a richer API added for mutating Connect records and their keys, values, schemas, etc.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520070301", "createdAt": "2020-11-09T19:35:35Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NDQyMA==", "bodyText": "Looks like this has two different names in the KIP; it's referenced as headers in the textual description but as header.names in the sample config.\nI think headers makes more sense here given that it would align with the properties defined in the InsertHeader and HeaderFrom transformations.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520074420", "createdAt": "2020-11-09T19:42:51Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"header.names\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NDc4Mg==", "bodyText": "Same question here RE: duplication", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520074782", "createdAt": "2020-11-09T19:43:32Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"header.names\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjQ3OA==", "bodyText": "I think this is supposed to be parsed using the Values class, which would allow users to specify integral, floating point, boolean, and even structured and nested types here that would then be correctly picked up by the framework and used in the resulting header.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520076478", "createdAt": "2020-11-09T19:46:29Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The literal value that is to be set as the header value on all records.\");\n+\n+    private String header;\n+\n+    private String literalValue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjU2Mg==", "bodyText": "Same question here RE: duplication", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520076562", "createdAt": "2020-11-09T19:46:38Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The literal value that is to be set as the header value on all records.\");\n+\n+    private String header;\n+\n+    private String literalValue;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3Njg4MA==", "bodyText": "If we use the Values class to parse the value literal, we can use the schema that it provides instead of hardcoding this to use a string schema.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520076880", "createdAt": "2020-11-09T19:47:11Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The literal value that is to be set as the header value on all records.\");\n+\n+    private String header;\n+\n+    private String literalValue;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        updatedHeaders.add(header, literalValue, Schema.STRING_SCHEMA);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMzI2Mzk5", "url": "https://github.com/apache/kafka/pull/9549#pullrequestreview-530326399", "createdAt": "2020-11-13T18:56:30Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo1NjozMFrOHy7PGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo1NjozMFrOHy7PGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MTM3MA==", "bodyText": "Little silly to run this for every iteration of the parameterized test \ud83d\ude1b\nI'm guessing there isn't an easy way to run this only once?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r523161370", "createdAt": "2020-11-13T18:56:30Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.ConnectHeaders;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.connect.data.Schema.STRING_SCHEMA;\n+import static org.junit.Assert.assertEquals;\n+\n+@RunWith(Parameterized.class)\n+public class HeaderFromTest {\n+\n+    private final boolean keyTransform;\n+\n+    static class RecordBuilder {\n+        private final List<String> fields = new ArrayList<>(2);\n+        private final List<Schema> fieldSchemas = new ArrayList<>(2);\n+        private final List<Object> fieldValues = new ArrayList<>(2);\n+        private final ConnectHeaders headers = new ConnectHeaders();\n+\n+        public RecordBuilder() {\n+        }\n+\n+        public RecordBuilder withField(String name, Schema schema, Object value) {\n+            fields.add(name);\n+            fieldSchemas.add(schema);\n+            fieldValues.add(value);\n+            return this;\n+        }\n+\n+        public RecordBuilder addHeader(String name, Schema schema, Object value) {\n+            headers.add(name, new SchemaAndValue(schema, value));\n+            return this;\n+        }\n+\n+        public SourceRecord schemaless(boolean keyTransform) {\n+            Map<String, Object> map = new HashMap<>();\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                map.put(fieldName, this.fieldValues.get(i));\n+\n+            }\n+            return sourceRecord(keyTransform, null, map);\n+        }\n+\n+        private Schema schema() {\n+            SchemaBuilder schemaBuilder = new SchemaBuilder(Schema.Type.STRUCT);\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                schemaBuilder.field(fieldName, this.fieldSchemas.get(i));\n+\n+            }\n+            return schemaBuilder.build();\n+        }\n+\n+        private Struct struct(Schema schema) {\n+            Struct struct = new Struct(schema);\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                struct.put(fieldName, this.fieldValues.get(i));\n+            }\n+            return struct;\n+        }\n+\n+        public SourceRecord withSchema(boolean keyTransform) {\n+            Schema schema = schema();\n+            Struct struct = struct(schema);\n+            return sourceRecord(keyTransform, schema, struct);\n+        }\n+\n+        private SourceRecord sourceRecord(boolean keyTransform, Schema keyOrValueSchema, Object keyOrValue) {\n+            Map<String, ?> sourcePartition = singletonMap(\"foo\", \"bar\");\n+            Map<String, ?> sourceOffset = singletonMap(\"baz\", \"quxx\");\n+            String topic = \"topic\";\n+            Integer partition = 0;\n+            Long timestamp = 0L;\n+\n+            ConnectHeaders headers = this.headers;\n+            if (keyOrValueSchema == null) {\n+                // When doing a schemaless transformation we don't expect the header to have a schema\n+                headers = new ConnectHeaders();\n+                for (Header header : this.headers) {\n+                    headers.add(header.key(), new SchemaAndValue(null, header.value()));\n+                }\n+            }\n+            return new SourceRecord(sourcePartition, sourceOffset, topic, partition,\n+                    keyTransform ? keyOrValueSchema : null,\n+                    keyTransform ? keyOrValue : \"key\",\n+                    !keyTransform ? keyOrValueSchema : null,\n+                    !keyTransform ? keyOrValue : \"value\",\n+                    timestamp, headers);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"RecordBuilder(\" +\n+                    \"fields=\" + fields +\n+                    \", fieldSchemas=\" + fieldSchemas +\n+                    \", fieldValues=\" + fieldValues +\n+                    \", headers=\" + headers +\n+                    ')';\n+        }\n+    }\n+\n+    @Parameterized.Parameters(name = \"{0}: testKey={1}, xformFields={3}, xformHeaders={4}, operation={5}\")\n+    public static Collection<Object[]> data() {\n+\n+        List<Object[]> result = new ArrayList<>();\n+\n+\n+\n+        for (Boolean testKeyTransform : asList(true, false)) {\n+            result.add(\n+                new Object[]{\n+                    \"basic copy\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"basic move\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"copy with preexisting header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"move with preexisting header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            Schema schema = new SchemaBuilder(Schema.Type.STRUCT).field(\"foo\", STRING_SCHEMA).build();\n+            Struct struct = new Struct(schema).put(\"foo\", \"foo-value\");\n+            result.add(\n+                new Object[]{\n+                    \"copy with struct value\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", schema, struct)\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"move with struct value\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", schema, struct)\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"two headers from same field\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    // two headers from the same field\n+                    asList(\"field1\", \"field1\"), asList(\"inserted1\", \"inserted2\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                        .addHeader(\"inserted2\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"two fields to same header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    // two headers from the same field\n+                    asList(\"field1\", \"field2\"), asList(\"inserted1\", \"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 and field2 got moved\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field2-value\")\n+                });\n+        }\n+        return result;\n+    }\n+\n+    private final HeaderFrom<SourceRecord> xform;\n+\n+    private final RecordBuilder originalRecordBuilder;\n+    private final RecordBuilder expectedRecordBuilder;\n+    private final List<String> transformFields;\n+    private final List<String> headers;\n+    private final HeaderFrom.Operation operation;\n+\n+    public HeaderFromTest(String description,\n+                          boolean keyTransform,\n+                          RecordBuilder originalBuilder,\n+                          List<String> transformFields, List<String> headers, HeaderFrom.Operation operation,\n+                          RecordBuilder expectedBuilder) {\n+        this.keyTransform = keyTransform;\n+        this.xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();\n+        this.originalRecordBuilder = originalBuilder;\n+        this.expectedRecordBuilder = expectedBuilder;\n+        this.transformFields = transformFields;\n+        this.headers = headers;\n+        this.operation = operation;\n+    }\n+\n+    private Map<String, Object> config() {\n+        Map<String, Object> result = new HashMap<>();\n+        result.put(HeaderFrom.HEADERS_FIELD, headers);\n+        result.put(HeaderFrom.FIELDS_FIELD, transformFields);\n+        result.put(HeaderFrom.OPERATION_FIELD, operation.toString());\n+        return result;\n+    }\n+\n+    @Test\n+    public void schemaless() {\n+        xform.configure(config());\n+        ConnectHeaders headers = new ConnectHeaders();\n+        headers.addString(\"existing\", \"existing-value\");\n+\n+        SourceRecord originalRecord = originalRecordBuilder.schemaless(keyTransform);\n+        SourceRecord expectedRecord = expectedRecordBuilder.schemaless(keyTransform);\n+        SourceRecord xformed = xform.apply(originalRecord);\n+        assertSameRecord(expectedRecord, xformed);\n+    }\n+\n+    @Test\n+    public void withSchema() {\n+        xform.configure(config());\n+        ConnectHeaders headers = new ConnectHeaders();\n+        headers.addString(\"existing\", \"existing-value\");\n+        Headers expect = headers.duplicate();\n+        for (int i = 0; i < this.headers.size(); i++) {\n+            expect.add(this.headers.get(i), originalRecordBuilder.fieldValues.get(i), originalRecordBuilder.fieldSchemas.get(i));\n+        }\n+\n+        SourceRecord originalRecord = originalRecordBuilder.withSchema(keyTransform);\n+        SourceRecord expectedRecord = expectedRecordBuilder.withSchema(keyTransform);\n+        SourceRecord xformed = xform.apply(originalRecord);\n+        assertSameRecord(expectedRecord, xformed);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void invalidConfig() {\n+        Map<String, Object> config = config();\n+        List<String> headers = new ArrayList<>(this.headers);\n+        headers.add(\"unexpected\");\n+        config.put(HeaderFrom.HEADERS_FIELD, headers);\n+        xform.configure(config);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 341}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMzMxMzMz", "url": "https://github.com/apache/kafka/pull/9549#pullrequestreview-530331333", "createdAt": "2020-11-13T19:03:55Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/1a79ffde1159c709dd1b7563c1ed6359a762685b", "committedDate": "2020-11-26T17:39:54Z", "message": "Docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjA1MDkxNzk3", "url": "https://github.com/apache/kafka/pull/9549#pullrequestreview-605091797", "createdAt": "2021-03-05T12:31:05Z", "commit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxMjozMTowNVrOIxAmzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDozMjo1MVrOIxFUaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODI2MTA3MA==", "bodyText": "Unused import", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588261070", "createdAt": "2021-03-05T12:31:05Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODI2Mjg4OA==", "bodyText": "We don't have this transformation!", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588262888", "createdAt": "2021-03-05T12:34:55Z", "author": {"login": "mimaison"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/tools/TransformationDoc.java", "diffHunk": "@@ -18,11 +18,15 @@\n \n import org.apache.kafka.common.config.ConfigDef;\n import org.apache.kafka.connect.transforms.Cast;\n+import org.apache.kafka.connect.transforms.DropHeaders;\n import org.apache.kafka.connect.transforms.ExtractField;\n import org.apache.kafka.connect.transforms.Filter;\n import org.apache.kafka.connect.transforms.Flatten;\n+import org.apache.kafka.connect.transforms.HeaderFrom;\n+import org.apache.kafka.connect.transforms.HeaderTo;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMyMTY1Mw==", "bodyText": "Because Headers is a LinkedList, remove() has to iterate the whole list each time. I wonder if we could instead start from an empty headers list and add the headers not being removed?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588321653", "createdAt": "2021-03-05T14:09:33Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"headers\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        for (String name : headers) {\n+            updatedHeaders.remove(name);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMyMTc0Mw==", "bodyText": "nit, extra line", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588321743", "createdAt": "2021-03-05T14:09:42Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"headers\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        for (String name : headers) {\n+            updatedHeaders.remove(name);\n+        }\n+        return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+                record.valueSchema(), record.value(), record.timestamp(), updatedHeaders);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMyMTc5Ng==", "bodyText": "nit, extra line", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588321796", "createdAt": "2021-03-05T14:09:48Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"headers\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        for (String name : headers) {\n+            updatedHeaders.remove(name);\n+        }\n+        return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+                record.valueSchema(), record.value(), record.timestamp(), updatedHeaders);\n+    }\n+\n+\n+    @Override\n+    public ConfigDef config() {\n+        return CONFIG_DEF;\n+    }\n+\n+    @Override\n+    public void close() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMzNjIyNw==", "bodyText": "Should we enforce these fields are not null?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588336227", "createdAt": "2021-03-05T14:29:55Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.Values;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMzODI4MA==", "bodyText": "Should we reuse MOVE_OPERATION and COPY_OPERATION here?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588338280", "createdAt": "2021-03-05T14:32:51Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.lang.String.format;\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+    private static final String MOVE_OPERATION = \"move\";\n+    private static final String COPY_OPERATION = \"copy\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 64}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/1a79ffde1159c709dd1b7563c1ed6359a762685b", "committedDate": "2020-11-26T17:39:54Z", "message": "Docs"}, "afterCommit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/a211f1e5003d31c87c678f50e6b56fb0ffc84388", "committedDate": "2021-04-07T11:21:46Z", "message": "Convert tests to junit5"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM2NDc2ODYw", "url": "https://github.com/apache/kafka/pull/9549#pullrequestreview-636476860", "createdAt": "2021-04-15T09:32:50Z", "commit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQwOTozMjo1MFrOJJeVLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQwOTozMjo1MFrOJJeVLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzkxMzkwMQ==", "bodyText": "Even though I don't think it's reachable by users, should we have a message here?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r613913901", "createdAt": "2021-04-15T09:32:50Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.NonEmptyListValidator;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.lang.String.format;\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+    private static final String MOVE_OPERATION = \"move\";\n+    private static final String COPY_OPERATION = \"copy\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(MOVE_OPERATION, COPY_OPERATION), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(MOVE_OPERATION),\n+        COPY(COPY_OPERATION);\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case MOVE_OPERATION:\n+                    return MOVE;\n+                case COPY_OPERATION:\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM2NTAxOTY4", "url": "https://github.com/apache/kafka/pull/9549#pullrequestreview-636501968", "createdAt": "2021-04-15T09:59:04Z", "commit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d1af2b4c8686b03225af7e62f29a9f53f46ae2d4", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/d1af2b4c8686b03225af7e62f29a9f53f46ae2d4", "committedDate": "2021-04-16T08:14:48Z", "message": "KIP-145: Add unimplementated SMTs, HeaderFrom, DropHeaders and InsertHeader\n\nThese SMTs were originally specified in KIP-145 but never implemented\nat the time.\n\nHeaderTo is not included since its original specification doesn't deal with\nthe fact that there can be >1 header with the same name, but a field can only\nhave a single value (which could be an array, but not if the headers for\nthe given name had different schemas)."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3233c5954d3a7dd0c3be50f3d02fc6f95acee799", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/3233c5954d3a7dd0c3be50f3d02fc6f95acee799", "committedDate": "2021-04-16T08:14:48Z", "message": "Review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e8ef0b30ff27329ed68650d5b0c899b8c6a0201", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/9e8ef0b30ff27329ed68650d5b0c899b8c6a0201", "committedDate": "2021-04-16T08:14:48Z", "message": "Docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e75b8cf48611d253ea513795ab6f7a79ffee069", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/6e75b8cf48611d253ea513795ab6f7a79ffee069", "committedDate": "2021-04-16T08:14:48Z", "message": "Review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c7ef2dc934db78512a5192b1450edb38f9e4461f", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/c7ef2dc934db78512a5192b1450edb38f9e4461f", "committedDate": "2021-04-16T08:14:48Z", "message": "Convert tests to junit5"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/a211f1e5003d31c87c678f50e6b56fb0ffc84388", "committedDate": "2021-04-07T11:21:46Z", "message": "Convert tests to junit5"}, "afterCommit": {"oid": "c7ef2dc934db78512a5192b1450edb38f9e4461f", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/c7ef2dc934db78512a5192b1450edb38f9e4461f", "committedDate": "2021-04-16T08:14:48Z", "message": "Convert tests to junit5"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2669, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}