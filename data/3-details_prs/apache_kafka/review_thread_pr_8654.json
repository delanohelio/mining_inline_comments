{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE2NzU3NTc4", "number": 8654, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowMTo0N1rOD87dDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDo0NjozOVrOD-eO5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE1MjQ1OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowMTo0N1rOGWK0ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowMTo0N1rOGWK0ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTE5NA==", "bodyText": "this check seems to replicate a bit what's happening in NewTopic. Should we just pass the value, given that we have validations too already?", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425899194", "createdAt": "2020-05-15T16:01:47Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -66,25 +71,55 @@\n         /**\n          * Specify the desired number of partitions for the topic.\n          *\n-         * @param numPartitions the desired number of partitions; must be positive\n+         * @param numPartitions the desired number of partitions; must be positive, or -1 to\n+         *                      signify using the broker's default\n          * @return this builder to allow methods to be chained; never null\n          */\n         public NewTopicBuilder partitions(int numPartitions) {\n+            if (numPartitions == NO_PARTITIONS) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE1MzAyOnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowMjowMFrOGWK1JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowMjowMFrOGWK1JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTMwMA==", "bodyText": "same question as above", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425899300", "createdAt": "2020-05-15T16:02:00Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -66,25 +71,55 @@\n         /**\n          * Specify the desired number of partitions for the topic.\n          *\n-         * @param numPartitions the desired number of partitions; must be positive\n+         * @param numPartitions the desired number of partitions; must be positive, or -1 to\n+         *                      signify using the broker's default\n          * @return this builder to allow methods to be chained; never null\n          */\n         public NewTopicBuilder partitions(int numPartitions) {\n+            if (numPartitions == NO_PARTITIONS) {\n+                return defaultPartitions();\n+            }\n             this.numPartitions = numPartitions;\n             return this;\n         }\n \n+        /**\n+         * Specify the topic's number of partition should be the broker configuration for\n+         * {@code num.partitions}.\n+         *\n+         * @return this builder to allow methods to be chained; never null\n+         */\n+        public NewTopicBuilder defaultPartitions() {\n+            this.numPartitions = null;\n+            return this;\n+        }\n+\n         /**\n          * Specify the desired replication factor for the topic.\n          *\n-         * @param replicationFactor the desired replication factor; must be positive\n+         * @param replicationFactor the desired replication factor; must be positive, or -1 to\n+         *                          signify using the broker's default\n          * @return this builder to allow methods to be chained; never null\n          */\n         public NewTopicBuilder replicationFactor(short replicationFactor) {\n+            if (replicationFactor == NO_REPLICATION_FACTOR) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE2MTE3OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowNDozM1rOGWK6hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQyMjowNDozM1rOGYfcdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMDY3Ng==", "bodyText": "did you notice that this matters? Not suggesting to remove necessarily, but I wonder whether you noticed it didn't work otherwise.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425900676", "createdAt": "2020-05-15T16:04:33Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")\n+                                                      .workerProps(workerProps)\n+                                                      .numWorkers(numWorkers)\n+                                                      .numBrokers(numBrokers)\n+                                                      .brokerProps(brokerProps)\n+                                                      .build();\n+\n+        // Start the Connect cluster\n+        connect.start();\n+        connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, \"Brokers did not start in time.\");\n+        connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, \"Worker did not start in time.\");\n+        log.info(\"Completed startup of {} Kafka brokers and {} Connect workers\", numBrokers, numWorkers);\n+\n+        // Check the topics\n+        log.info(\"Verifying the internal topics for Connect\");\n+        connect.assertions().assertTopicsExist(configTopic(), offsetTopic(), statusTopic());\n+        assertInternalTopicSettings();\n+\n+        // Remove the Connect worker\n+        log.info(\"Stopping the Connect worker\");\n+        connect.removeWorker();\n+\n+        // Sleep for a bit\n+        Thread.sleep(3000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzNDE5Ng==", "bodyText": "Good catch. I had put this in while debugging an issue, but it is not needed.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428334196", "createdAt": "2020-05-20T22:04:33Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")\n+                                                      .workerProps(workerProps)\n+                                                      .numWorkers(numWorkers)\n+                                                      .numBrokers(numBrokers)\n+                                                      .brokerProps(brokerProps)\n+                                                      .build();\n+\n+        // Start the Connect cluster\n+        connect.start();\n+        connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, \"Brokers did not start in time.\");\n+        connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, \"Worker did not start in time.\");\n+        log.info(\"Completed startup of {} Kafka brokers and {} Connect workers\", numBrokers, numWorkers);\n+\n+        // Check the topics\n+        log.info(\"Verifying the internal topics for Connect\");\n+        connect.assertions().assertTopicsExist(configTopic(), offsetTopic(), statusTopic());\n+        assertInternalTopicSettings();\n+\n+        // Remove the Connect worker\n+        log.info(\"Stopping the Connect worker\");\n+        connect.removeWorker();\n+\n+        // Sleep for a bit\n+        Thread.sleep(3000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMDY3Ng=="}, "originalCommit": null, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE3MTQ0OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowNzozMlrOGWLA2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDo1MDoxMVrOGYmFyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMjI5OQ==", "bodyText": "Is naming here to distinguish between runs of integration tests?\nI have a commit that will help us know the boundaries between ITs. I'll submit separately.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425902299", "createdAt": "2020-05-15T16:07:32Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzMzI2OA==", "bodyText": "I was running into other issues, and I never changed it. The names are not important, so I'll change them back to the same cluster name.\nI actually do prefer how each test fully sets up the cluster builder, in part because some of the information is changed and some isn't. I found it very error prone while I was working on this to reuse a builder and have each test only change some of the attributes, or to remember to set the worker props after changing the workerProps map.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428333268", "createdAt": "2020-05-20T22:02:11Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMjI5OQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ0MzA4MA==", "bodyText": "Makes sense.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428443080", "createdAt": "2020-05-21T04:50:11Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import org.apache.kafka.connect.runtime.distributed.DistributedConfig;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Integration test for the creation of internal topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class InternalTopicsIntegrationTest {\n+\n+    private static final Logger log = LoggerFactory.getLogger(InternalTopicsIntegrationTest.class);\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(1)\n+                .numBrokers(1)\n+                .brokerProps(brokerProps);\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateInternalTopicsWithDefaultSettings() throws InterruptedException {\n+        int numWorkers = 1;\n+        int numBrokers = 3;\n+        connect = new EmbeddedConnectCluster.Builder().name(\"connect-cluster-1\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMjI5OQ=="}, "originalCommit": null, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjI0MTc5OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedConfigTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjoyNzozN1rOGWLtrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjoyNzozN1rOGWLtrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkxMzc3Mw==", "bodyText": "topicSettings.forEach((k, v) -> {...}); is a better shorthand in cases like these here where you don't need to pass the entry between stream stages.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425913773", "createdAt": "2020-05-15T16:27:37Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedConfigTest.java", "diffHunk": "@@ -105,4 +106,204 @@ public void shouldValidateAllVerificationAlgorithms() {\n             algorithms.add(algorithms.remove(0));\n         }\n     }\n+\n+    @Test\n+    public void shouldAllowNegativeOneAndPositiveForPartitions() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, \"-1\");\n+        settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, \"-1\");\n+        new DistributedConfig(configs());\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+        settings.remove(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG);\n+\n+        for (int i = 1; i != 100; ++i) {\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+        }\n+    }\n+\n+    @Test\n+    public void shouldNotAllowZeroPartitions() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+        settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+    }\n+\n+    @Test\n+    public void shouldNotAllowNegativePartitionsLessThanNegativeOne() {\n+        Map<String, String> settings = configs();\n+        for (int i = -2; i > -100; --i) {\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldAllowNegativeOneAndPositiveForReplicationFactor() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, \"-1\");\n+        settings.put(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG, \"-1\");\n+        settings.put(DistributedConfig.STATUS_STORAGE_REPLICATION_FACTOR_CONFIG, \"-1\");\n+        new DistributedConfig(configs());\n+        settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+        settings.remove(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG);\n+\n+        for (int i = 1; i != 100; ++i) {\n+            settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+            settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_PARTITIONS_CONFIG, Integer.toString(i));\n+            new DistributedConfig(settings);\n+        }\n+    }\n+\n+    @Test\n+    public void shouldNotAllowZeroReplicationFactor() {\n+        Map<String, String> settings = configs();\n+        settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+        settings.put(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        settings.remove(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+        settings.put(DistributedConfig.STATUS_STORAGE_REPLICATION_FACTOR_CONFIG, \"0\");\n+        assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+    }\n+\n+    @Test\n+    public void shouldNotAllowNegativeReplicationFactorLessThanNegativeOne() {\n+        Map<String, String> settings = configs();\n+        for (int i = -2; i > -100; --i) {\n+            settings.put(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+            settings.remove(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+            settings.put(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+            settings.remove(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG);\n+\n+            settings.put(DistributedConfig.STATUS_STORAGE_REPLICATION_FACTOR_CONFIG, Integer.toString(i));\n+            assertThrows(ConfigException.class, () -> new DistributedConfig(settings));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldAllowSettingConfigTopicSettings() {\n+        Map<String, String> topicSettings = new HashMap<>();\n+        topicSettings.put(\"foo\", \"foo value\");\n+        topicSettings.put(\"bar\", \"bar value\");\n+        topicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {\n+            settings.put(DistributedConfig.CONFIG_STORAGE_PREFIX + e.getKey(), e.getValue());\n+        });\n+        DistributedConfig config = new DistributedConfig(settings);\n+        assertEquals(topicSettings, config.configStorageTopicSettings());\n+    }\n+\n+    @Test\n+    public void shouldAllowSettingOffsetTopicSettings() {\n+        Map<String, String> topicSettings = new HashMap<>();\n+        topicSettings.put(\"foo\", \"foo value\");\n+        topicSettings.put(\"bar\", \"bar value\");\n+        topicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {\n+            settings.put(DistributedConfig.OFFSET_STORAGE_PREFIX + e.getKey(), e.getValue());\n+        });\n+        DistributedConfig config = new DistributedConfig(settings);\n+        assertEquals(topicSettings, config.offsetStorageTopicSettings());\n+    }\n+\n+    @Test\n+    public void shouldAllowSettingStatusTopicSettings() {\n+        Map<String, String> topicSettings = new HashMap<>();\n+        topicSettings.put(\"foo\", \"foo value\");\n+        topicSettings.put(\"bar\", \"bar value\");\n+        topicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {\n+            settings.put(DistributedConfig.STATUS_STORAGE_PREFIX + e.getKey(), e.getValue());\n+        });\n+        DistributedConfig config = new DistributedConfig(settings);\n+        assertEquals(topicSettings, config.statusStorageTopicSettings());\n+    }\n+\n+    @Test\n+    public void shouldRemoveCompactionFromConfigTopicSettings() {\n+        Map<String, String> expectedTopicSettings = new HashMap<>();\n+        expectedTopicSettings.put(\"foo\", \"foo value\");\n+        expectedTopicSettings.put(\"bar\", \"bar value\");\n+        expectedTopicSettings.put(\"baz.bim\", \"100\");\n+        Map<String, String> topicSettings = new HashMap<>(expectedTopicSettings);\n+        topicSettings.put(\"cleanup.policy\", \"something-else\");\n+        topicSettings.put(\"partitions\", \"3\");\n+\n+        Map<String, String> settings = configs();\n+        topicSettings.entrySet().forEach(e -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjI2MjQ2OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjozMjo0M1rOGWL6-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjozMjo0M1rOGWL6-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkxNzE3OA==", "bodyText": "I didn't comment elsewhere, but that's where != has the potential of stackoverflow vs <=. I understand we trust ourselves to set maxDefaultRf to be >0 but that's a couple indirections away.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r425917178", "createdAt": "2020-05-15T16:32:43Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java", "diffHunk": "@@ -97,12 +103,65 @@ public void shouldNotCreateTopicWhenItAlreadyExists() {\n     }\n \n     @Test\n-    public void shouldCreateTopicWhenItDoesNotExist() {\n-        NewTopic newTopic = TopicAdmin.defineTopic(\"myTopic\").partitions(1).compacted().build();\n-        Cluster cluster = createCluster(1);\n-        try (MockAdminClient mockAdminClient = new MockAdminClient(cluster.nodes(), cluster.nodeById(0))) {\n-            TopicAdmin admin = new TopicAdmin(null, mockAdminClient);\n-            assertTrue(admin.createTopic(newTopic));\n+    public void shouldCreateTopicWithPartitionsWhenItDoesNotExist() {\n+        for (int numBrokers = 1; numBrokers < 10; ++numBrokers) {\n+            int expectedReplicas = Math.min(3, numBrokers);\n+            int maxDefaultRf = Math.min(numBrokers, 5);\n+            for (int numPartitions = 1; numPartitions != 30; ++numPartitions) {\n+                NewTopic newTopic = TopicAdmin.defineTopic(\"myTopic\").partitions(numPartitions).compacted().build();\n+\n+                // Try clusters with no default replication factor or default partitions\n+                assertTopicCreation(numBrokers, newTopic, null, null, expectedReplicas, numPartitions);\n+\n+                // Try clusters with different default partitions\n+                for (int defaultPartitions = 1; defaultPartitions != 20; ++defaultPartitions) {\n+                    assertTopicCreation(numBrokers, newTopic, defaultPartitions, null, expectedReplicas, numPartitions);\n+                }\n+\n+                // Try clusters with different default replication factors\n+                for (int defaultRF = 1; defaultRF != maxDefaultRf; ++defaultRF) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODMyMzE0OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDozNzo1M1rOGYl7Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDozNzo1M1rOGYl7Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ0MDM3OA==", "bodyText": "It's one more import statement, but I don't see a reason not to get the actual config from:\nTopicConfig.CLEANUP_POLICY_CONFIG. I think you'll agree that a variable is better than a string, but just a nit.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428440378", "createdAt": "2020-05-21T04:37:53Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java", "diffHunk": "@@ -400,6 +424,33 @@ public KeyGenerator getInternalRequestKeyGenerator() {\n         }\n     }\n \n+    private Map<String, Object> topicSettings(String prefix) {\n+        Map<String, Object> result = originalsWithPrefix(prefix);\n+        if (CONFIG_STORAGE_PREFIX.equals(prefix) && result.containsKey(PARTITIONS_SUFFIX)) {\n+            log.warn(\"Ignoring '{}{}={}' setting, since config topic partitions is always 1\", prefix, PARTITIONS_SUFFIX, result.get(\"partitions\"));\n+        }\n+        Object removedPolicy = result.remove(\"cleanup.policy\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODMzNjM3OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDo0NjozOVrOGYmDCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDo0NjozOVrOGYmDCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ0MjM3OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Map<String, Object> topicSettings = null;\n          \n          \n            \n                    if (config instanceof DistributedConfig) {\n          \n          \n            \n                        topicSettings = ((DistributedConfig) config).configStorageTopicSettings();\n          \n          \n            \n                    }\n          \n          \n            \n                    Map<String, Object> topicSettings = config instanceof DistributedConfig\n          \n          \n            \n                            ? ((DistributedConfig) config).configStorageTopicSettings()\n          \n          \n            \n                            : Collections.emptyMap();\n          \n      \n    \n    \n  \n\nI know that TopicAdmin#defineTopic checks for null, but I think using null with collections is better to do when such optimization matters. Wdyt?\n(btw you don't have to use the ternary operator, I just added it to make the suggestion clear).\nAlso, if you change here, please change in the other files too.", "url": "https://github.com/apache/kafka/pull/8654#discussion_r428442379", "createdAt": "2020-05-21T04:46:39Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java", "diffHunk": "@@ -453,11 +453,17 @@ public void putSessionKey(SessionKey sessionKey) {\n         consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n \n         Map<String, Object> adminProps = new HashMap<>(originals);\n-        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n-                compacted().\n-                partitions(1).\n-                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n-                build();\n+\n+        Map<String, Object> topicSettings = null;\n+        if (config instanceof DistributedConfig) {\n+            topicSettings = ((DistributedConfig) config).configStorageTopicSettings();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2576, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}