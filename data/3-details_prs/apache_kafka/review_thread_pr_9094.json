{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4MTQ3NTE4", "number": 9094, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMTo1M1rOETELrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMjoyOTowOFrOEX0kVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDI2OTI0OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMTo1M1rOG4l22A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMTo1M1rOG4l22A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5MzY4OA==", "bodyText": "It's kind of a bummer that we can't just add the new TRACE level for Streams only; we have to add it to all the clients that Streams passes its configs down to. We could check for the new TRACE level and strip it off before passing the configs on to the clients, but that just seems like asking for trouble.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461993688", "createdAt": "2020-07-29T02:01:53Z", "author": {"login": "ableegoldman"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java", "diffHunk": "@@ -198,7 +198,7 @@\n                                 .define(METRICS_RECORDING_LEVEL_CONFIG,\n                                         Type.STRING,\n                                         Sensor.RecordingLevel.INFO.toString(),\n-                                        in(Sensor.RecordingLevel.INFO.toString(), Sensor.RecordingLevel.DEBUG.toString()),\n+                                        in(Sensor.RecordingLevel.INFO.toString(), Sensor.RecordingLevel.DEBUG.toString(), Sensor.RecordingLevel.TRACE.toString()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDI3MTM3OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMzowM1rOG4l4Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMzowM1rOG4l4Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5Mzk4Ng==", "bodyText": "Moved the common descriptions to StreamsMetricsImpl", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461993986", "createdAt": "2020-07-29T02:03:03Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java", "diffHunk": "@@ -88,13 +92,6 @@ private TaskMetrics() {}\n     private static final String NUM_BUFFERED_RECORDS_DESCRIPTION = \"The count of buffered records that are polled \" +\n         \"from consumer and not yet processed for this active task\";\n \n-    private static final String RECORD_E2E_LATENCY = \"record-e2e-latency\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDI3NTg2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowNTowOVrOG4l6Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowNTowOVrOG4l6Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5NDU5OQ==", "bodyText": "For KV stores, we just compare the current time with the current record's timestamp", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461994599", "createdAt": "2020-07-29T02:05:09Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -227,6 +234,14 @@ protected Bytes keyBytes(final K key) {\n         return byteEntries;\n     }\n \n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDMwNTA3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyMDozOFrOG4mK3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyMDoyM1rOG46x7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5ODgxMg==", "bodyText": "For session and window stores, we also just compare the current time with the current record's timestamp when put is called. This can mean the e2e latency is measured several times on the same record, for example in a windowed aggregation.\nAt first I thought that didn't make sense, but now I think it's actually exactly what we want. First of all, it means we can actually account for the latency between calls to put within a processor. For simple point inserts this might not be a huge increase on the scale of ms, but more complex processing may benefit from seeing this granularity of information. If they don't want it, well, that's why we introduced TRACE\nSecond, while it might seem like we're over-weighting some records by measuring the e2e latency on them more than others, I'm starting to think this actually makes more sense than not: the big picture benefit/use case for the e2e latency metric is less \"how long for this record to get sent downstream\" and more \"how long for this record to be reflected in the state store/IQ results\". Given that, each record should be weighted by its actual proportion of the state store. You aren't querying individual records (in a window store), you're querying the windows themselves\nI toyed around with the idea of measuring the e2e latency relative to the window time, instead of the record timestamp, but ultimately couldn't find any sense in that.\nThoughts?", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461998812", "createdAt": "2020-07-29T02:20:38Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java", "diffHunk": "@@ -248,4 +253,12 @@ public void close() {\n     private Bytes keyBytes(final K key) {\n         return Bytes.wrap(serdes.rawKey(key));\n     }\n+\n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMzNjQ5Mw==", "bodyText": "Your approach makes sense to me. I agree that the latency should refer to the update in the state store and not to record itself. If a record updates the state more than once then latency should be measured each time.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462336493", "createdAt": "2020-07-29T14:20:23Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java", "diffHunk": "@@ -248,4 +253,12 @@ public void close() {\n     private Bytes keyBytes(final K key) {\n         return Bytes.wrap(serdes.rawKey(key));\n     }\n+\n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5ODgxMg=="}, "originalCommit": null, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDMxOTU0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyOToyMFrOG4mTVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxNzo1NjoyNFrOG6RHHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA==", "bodyText": "Ok there's something I'm not understanding about this test and/or the built-in metrics version. For some reason, the KV-store metrics are 0 when METRICS_0100_TO_24 is used, and 1 (as expected) when the latest version in used. I feel like this is wrong, and it should always be 1, but I need some clarify on how this config is supposed to be used\nWhat makes me pretty sure there's something actually wrong here is that for the window/session store metrics, they are actually always at 1. But I can't figure out why the KV store metrics would be any different than the others. Any ideas @cadonna ?", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462000980", "createdAt": "2020-07-29T02:29:20Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3MzY5NQ==", "bodyText": "I agree with you, it should always be 1. It is the group of the metrics. See my comment in StateStoreMetrics. I am glad this test served its purpose, because I did not notice this in the unit tests!", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462373695", "createdAt": "2020-07-29T15:08:31Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA=="}, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg4MTkzMw==", "bodyText": "Sorry, I did a mistake here. We should not give new metrics old groups. I think to fix this test you need to adapt the filter on line 618 to let all metrics with groups that relate to KV state stores pass. See checkWindowStoreAndSuppressionBufferMetrics() for an example.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462881933", "createdAt": "2020-07-30T09:48:46Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA=="}, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI5MTI2OA==", "bodyText": "Hm, if I let additional metrics through (by allowing all STATE_STORE_LEVEL_GROUP metrics regardless of buildInMetricsVersion) then the test fails much earlier. For example\njava.lang.AssertionError: Size of metrics of type:'put-latency-avg' must be equal to 1 but it's equal to 3 \nI think I could just do the check for the e2e latency before filtering, but I'm worried there's something else wrong here. Why do we get 3 put-latency-avg metrics if we let all store metrics through in checkKeyValueStoreMetrics but not when we let all metrics through in checkWindowStoreAndSuppressionBufferMetrics?", "url": "https://github.com/apache/kafka/pull/9094#discussion_r463291268", "createdAt": "2020-07-30T21:49:41Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA=="}, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzUxNDE3OQ==", "bodyText": "You can use the following to get it right without the need to do the check for the e2e latency before filtering\n.filter(m -> m.metricName().tags().containsKey(tagKey) && \n    (m.metricName().group().equals(group0100To24) || m.metricName().group().equals(STATE_STORE_LEVEL_GROUP))\n).collect(Collectors.toList());\n\n\nThe reason for the difference between the KV store and the window store is that they are used in different tests with different number of state stores.\nThe test that uses the KV stores tests three different types of KV stores, namely in-memory, rocksdb, and in-memory-lru-cache. For each of this types the old group name changes. That is also the reason we need to pass the parameter group0100To24 to checkKeyValueStoreMetrics().\nIn checkWindowStoreAndSuppressionBufferMetrics() we need to filter for four groups, because the corresponding test uses suppression and window state store. Suppression buffers had their own group in the old version. In the new version they moved into the state store group. Those groups are BUFFER_LEVEL_GROUP_0100_TO_24 and STATE_STORE_LEVEL_GROUP. The window state store had their own group in the old version, i.e., STATE_STORE_LEVEL_GROUP_ROCKSDB_WINDOW_STORE_0100_TO_24 (we are only using RocksDB-based window stores in the test). Finally, during the implementation of KIP-444, we discovered that we named a group incorrectly. That's why we filter also for group stream-rocksdb-window-metrics.\nSo to sum up, it is hard to compare the verifications for KV stores and window stores, because they are used in different tests. Sorry, I should have been clearer on that before.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r463514179", "createdAt": "2020-07-31T09:46:14Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA=="}, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc1MDk0MQ==", "bodyText": "I see, thanks for the explanation.  The suggestion worked", "url": "https://github.com/apache/kafka/pull/9094#discussion_r463750941", "createdAt": "2020-07-31T17:56:24Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA=="}, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjQyNjcwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDowODozNFrOG46NCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMTozMToxNVrOG50jPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMyNzA1MA==", "bodyText": "I think, you do not need to check for metrics with e2eLatencySensor.hasMetrics(). There should always be metrics within this sensor.\nhasMetrics() is used in StreamsMetricsImpl#maybeMeasureLatency() because some sensors may not contain any metrics due to the built-in metrics version. For instance, the destroy sensor exists for built-in metrics version 0.10.0-2.4 but not for latest. To avoid version checks in the record processing code, we just create an empty sensor and call record on it effectively not recording any metrics for this sensor for version latest.\nWe do not hide newly added metrics if the built-in version is set to an older version.\nSame applies to the other uses of hasMetrics() introduced in this PR.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462327050", "createdAt": "2020-07-29T14:08:34Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -227,6 +234,14 @@ protected Bytes keyBytes(final K key) {\n         return byteEntries;\n     }\n \n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI4MzAwNA==", "bodyText": "Ah, ok that makes sense. Thanks for clarifying", "url": "https://github.com/apache/kafka/pull/9094#discussion_r463283004", "createdAt": "2020-07-30T21:31:15Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -227,6 +234,14 @@ protected Bytes keyBytes(final K key) {\n         return byteEntries;\n     }\n \n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMyNzA1MA=="}, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjU0NjE3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDozMjowNFrOG47WUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo0NDoxOFrOG52Vtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM0NTgxMQ==", "bodyText": "Could you test maybeRecordE2ELatency() through process() and forward()? Although you test maybeRecordE2ELatency(), you do not test if the recording is done during processing, but that is the crucial thing, IMO.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462345811", "createdAt": "2020-07-29T14:32:04Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -468,38 +468,44 @@ public void shouldRecordE2ELatencyOnProcessForSourceNodes() {\n     }\n \n     @Test\n-    public void shouldRecordE2ELatencyMinAndMax() {\n+    public void shouldRecordE2ELatencyAvgAndMinAndMax() {\n         time = new MockTime(0L, 0L, 0L);\n         metrics = new Metrics(new MetricConfig().recordLevel(Sensor.RecordingLevel.INFO), time);\n         task = createStatelessTask(createConfig(false, \"0\"), StreamsConfig.METRICS_LATEST);\n \n         final String sourceNode = source1.name();\n \n-        final Metric maxMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-max\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n+        final Metric avgMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-avg\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n         final Metric minMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-min\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n+        final Metric maxMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-max\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n \n+        assertThat(avgMetric.metricValue(), equalTo(Double.NaN));\n         assertThat(minMetric.metricValue(), equalTo(Double.NaN));\n         assertThat(maxMetric.metricValue(), equalTo(Double.NaN));\n \n         // e2e latency = 10\n         task.maybeRecordE2ELatency(0L, 10L, sourceNode);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxMjMxMA==", "bodyText": "Ack", "url": "https://github.com/apache/kafka/pull/9094#discussion_r463312310", "createdAt": "2020-07-30T22:44:18Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -468,38 +468,44 @@ public void shouldRecordE2ELatencyOnProcessForSourceNodes() {\n     }\n \n     @Test\n-    public void shouldRecordE2ELatencyMinAndMax() {\n+    public void shouldRecordE2ELatencyAvgAndMinAndMax() {\n         time = new MockTime(0L, 0L, 0L);\n         metrics = new Metrics(new MetricConfig().recordLevel(Sensor.RecordingLevel.INFO), time);\n         task = createStatelessTask(createConfig(false, \"0\"), StreamsConfig.METRICS_LATEST);\n \n         final String sourceNode = source1.name();\n \n-        final Metric maxMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-max\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n+        final Metric avgMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-avg\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n         final Metric minMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-min\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n+        final Metric maxMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-max\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n \n+        assertThat(avgMetric.metricValue(), equalTo(Double.NaN));\n         assertThat(minMetric.metricValue(), equalTo(Double.NaN));\n         assertThat(maxMetric.metricValue(), equalTo(Double.NaN));\n \n         // e2e latency = 10\n         task.maybeRecordE2ELatency(0L, 10L, sourceNode);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM0NTgxMQ=="}, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjY5OTAyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNTowNDowMFrOG482JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwOTo1MDozNVrOG5cIbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3MDM0MA==", "bodyText": "You need to use the stateStoreLevelGroup() here instead of STATE_STORE_LEVEL_GROUP because the group name depends on the version and the store type.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462370340", "createdAt": "2020-07-29T15:04:00Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -443,6 +447,25 @@ public static Sensor suppressionBufferSizeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor e2ELatencySensor(final String threadId,\n+                                          final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(threadId, taskId, storeName, RECORD_E2E_LATENCY, RecordingLevel.TRACE);\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(threadId, taskId, storeType, storeName);\n+        addAvgAndMinAndMaxToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg4MjkyNg==", "bodyText": "I just realized that we should not put new metrics into old groups. Your code is fine. Do not use stateStoreLevelGroup()! Sorry for the confusion.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462882926", "createdAt": "2020-07-30T09:50:35Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -443,6 +447,25 @@ public static Sensor suppressionBufferSizeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor e2ELatencySensor(final String threadId,\n+                                          final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(threadId, taskId, storeName, RECORD_E2E_LATENCY, RecordingLevel.TRACE);\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(threadId, taskId, storeType, storeName);\n+        addAvgAndMinAndMaxToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3MDM0MA=="}, "originalCommit": null, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4Njc0MjYzOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNToxMzoyM1rOG49Raw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwOTo1MToxN1rOG5cKEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3NzMyMw==", "bodyText": "You need to make this dependent on the built-in metrics version by using instance variable storeLevelGroup.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462377323", "createdAt": "2020-07-29T15:13:23Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -327,6 +327,38 @@ public void shouldGetExpiredWindowRecordDropSensor() {\n         assertThat(sensor, is(expectedSensor));\n     }\n \n+    @Test\n+    public void shouldGetRecordE2ELatencySensor() {\n+        final String metricName = \"record-e2e-latency\";\n+\n+        final String e2eLatencyDescription =\n+            \"end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+                + \"system time when it has been fully processed by the node\";\n+        final String descriptionOfAvg = \"The average \" + e2eLatencyDescription;\n+        final String descriptionOfMin = \"The minimum \" + e2eLatencyDescription;\n+        final String descriptionOfMax = \"The maximum \" + e2eLatencyDescription;\n+\n+        expect(streamsMetrics.storeLevelSensor(THREAD_ID, TASK_ID, STORE_NAME, metricName, RecordingLevel.TRACE))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(THREAD_ID, TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+        StreamsMetricsImpl.addAvgAndMinAndMaxToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg4MzM0Nw==", "bodyText": "I just realized that we should not put new metrics into old groups. Your code is fine. Do not use instance variable storeLevelGroup! Sorry for the confusion.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462883347", "createdAt": "2020-07-30T09:51:17Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -327,6 +327,38 @@ public void shouldGetExpiredWindowRecordDropSensor() {\n         assertThat(sensor, is(expectedSensor));\n     }\n \n+    @Test\n+    public void shouldGetRecordE2ELatencySensor() {\n+        final String metricName = \"record-e2e-latency\";\n+\n+        final String e2eLatencyDescription =\n+            \"end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+                + \"system time when it has been fully processed by the node\";\n+        final String descriptionOfAvg = \"The average \" + e2eLatencyDescription;\n+        final String descriptionOfMin = \"The minimum \" + e2eLatencyDescription;\n+        final String descriptionOfMax = \"The maximum \" + e2eLatencyDescription;\n+\n+        expect(streamsMetrics.storeLevelSensor(THREAD_ID, TASK_ID, STORE_NAME, metricName, RecordingLevel.TRACE))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(THREAD_ID, TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+        StreamsMetricsImpl.addAvgAndMinAndMaxToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3NzMyMw=="}, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMTQ3NDEyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNDowOTo1MVrOG8ikqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMzoyODozMlrOG_Qfcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEzNDE4Ng==", "bodyText": "This is not introduced by this PR, but I'm wondering why we record the maybeRecordE2ELatency twice, once before the record is being processed and once when it reaches the sink node?", "url": "https://github.com/apache/kafka/pull/9094#discussion_r466134186", "createdAt": "2020-08-06T04:09:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -154,15 +153,15 @@ public StreamTask(final TaskId id,\n         for (final String terminalNodeName : topology.terminalNodes()) {\n             e2eLatencySensors.put(\n                 terminalNodeName,\n-                TaskMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, RecordingLevel.INFO, streamsMetrics)\n+                TaskMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, streamsMetrics)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MzY2Nw==", "bodyText": "NVM, I realized it is at the per-node metrics level, we just have two types of nodes (source and termination node) for each task.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r468983667", "createdAt": "2020-08-12T03:28:32Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -154,15 +153,15 @@ public StreamTask(final TaskId id,\n         for (final String terminalNodeName : topology.terminalNodes()) {\n             e2eLatencySensors.put(\n                 terminalNodeName,\n-                TaskMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, RecordingLevel.INFO, streamsMetrics)\n+                TaskMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, streamsMetrics)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEzNDE4Ng=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDEzOTcyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMjoyOTowOFrOG_1BzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1Mjo0OVrOHAXEPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU4MjI4NA==", "bodyText": "There's a slight hiccup with moving the INFO metrics from task to node level:\nWe get the current sensor from StreamsMetrics#taskLevelSensor which computes the fullSensorName with the #taskSensorPrefix\nIf we instead use StreamsMetrics#nodeLevelSensor then the fullSensorName is constructed from the #nodeSensorPrefix, which is obviously different. So moving this to a \u201ctrue\u201d node level sensor would be a breaking change, IIUC\nI think the best we can do is just move this from TaskMetrics to ProcessorNodeMetrics, but still leave it as a taskLevelSensor. Let me know if I'm missing something though cc @guozhangwang @cadonna", "url": "https://github.com/apache/kafka/pull/9094#discussion_r469582284", "createdAt": "2020-08-12T22:29:08Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java", "diffHunk": "@@ -289,6 +294,25 @@ public static Sensor processorAtSourceSensorOrForwardSensor(final String threadI\n         return processAtSourceSensor(threadId, taskId, processorNodeId, streamsMetrics);\n     }\n \n+    public static Sensor e2ELatencySensor(final String threadId,\n+                                          final String taskId,\n+                                          final String processorNodeId,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+        final String sensorName = processorNodeId + \"-\" + RECORD_E2E_LATENCY;\n+        final Sensor sensor = streamsMetrics.taskLevelSensor(threadId, taskId, sensorName, RecordingLevel.INFO);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc4MTMxOQ==", "bodyText": "I do not think that using StreamsMetrics#nodeLevelSensor() is a breaking change. Sensors are not exposed publicly. They are merely containers for metrics. Metrics are exposed publicy. The full sensor name is just the key to retrieve sensors in the metrics map. It is also stored in StreamsMetricsImpl in the *LevelSensor data structures to know what sensors need to be removed from the metrics map when removeAll*LevelSensors() is called. So, the full sensor name is used for internal house keeping and hidden from the users. I think it is fine to change the fullSensorName.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r469781319", "createdAt": "2020-08-13T08:23:50Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java", "diffHunk": "@@ -289,6 +294,25 @@ public static Sensor processorAtSourceSensorOrForwardSensor(final String threadI\n         return processAtSourceSensor(threadId, taskId, processorNodeId, streamsMetrics);\n     }\n \n+    public static Sensor e2ELatencySensor(final String threadId,\n+                                          final String taskId,\n+                                          final String processorNodeId,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+        final String sensorName = processorNodeId + \"-\" + RECORD_E2E_LATENCY;\n+        final Sensor sensor = streamsMetrics.taskLevelSensor(threadId, taskId, sensorName, RecordingLevel.INFO);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU4MjI4NA=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzOTk2Nw==", "bodyText": "Oh, awesome, that's what I was hoping to hear! I'll move it to use the nodeLevelSensor then", "url": "https://github.com/apache/kafka/pull/9094#discussion_r470139967", "createdAt": "2020-08-13T17:52:49Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java", "diffHunk": "@@ -289,6 +294,25 @@ public static Sensor processorAtSourceSensorOrForwardSensor(final String threadI\n         return processAtSourceSensor(threadId, taskId, processorNodeId, streamsMetrics);\n     }\n \n+    public static Sensor e2ELatencySensor(final String threadId,\n+                                          final String taskId,\n+                                          final String processorNodeId,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+        final String sensorName = processorNodeId + \"-\" + RECORD_E2E_LATENCY;\n+        final Sensor sensor = streamsMetrics.taskLevelSensor(threadId, taskId, sensorName, RecordingLevel.INFO);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU4MjI4NA=="}, "originalCommit": null, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2031, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}