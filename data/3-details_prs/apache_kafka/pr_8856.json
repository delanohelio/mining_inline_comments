{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzNDAyOTk3", "number": 8856, "title": "KAFKA-10150: task state transitions/management and committing cleanup", "bodyText": "KAFKA-10150:\n\nalways transition to SUSPENDED during suspend, no matter the current state\nonly call prepareCommit before closing if task.commitNeeded is true\n\n\nDon't commit any consumed offsets during handleAssignment -- revoked active tasks (and any others that need committing) will be committed during handleRevocation so we only need to worry about cleaning them up in handleAssignment\nKAFKA-10152: when recycling a task we should always commit consumed offsets (if any), but don't need to write the checkpoint (since changelog offsets are preserved across task transitions)\nMake sure we close all tasks during shutdown, even if an exception is thrown during commit\nIn-flight records were skipped when we saved the checkpointableOffsets before flushing in prepareCommit (This was the root cause of KAFKA-10151 -- we now don't save the offsets at all during prepareCommit and just write the current offsets during postCommit\n\nMust be cherry-picked to 2.6", "createdAt": "2020-06-12T00:36:09Z", "url": "https://github.com/apache/kafka/pull/8856", "merged": true, "mergeCommit": {"oid": "2239004907b29e00811fee9ded5a790172701a03"}, "closed": true, "closedAt": "2020-06-16T23:30:37Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcqky8uAFqTQyOTg0MjE2NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcr7HNdAFqTQzMTg1ODU2Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5ODQyMTY0", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-429842164", "createdAt": "2020-06-12T15:37:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNTozNzowN1rOGjIiYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNTozOTowMVrOGjImnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzIxOQ==", "bodyText": "When closing-clean a standby task, we would checkpoint the file and close the state store which would also flush it as well, so I think we do not need to call\ntask.prepareCommit();\ntask.postCommit();\n\nwhich is just to flush the stores and write checkpoint files, right?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439493219", "createdAt": "2020-06-12T15:37:07Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzkyNA==", "bodyText": "nit: Add in the above javadoc that we should only revoke active tasks here?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439493924", "createdAt": "2020-06-12T15:38:20Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -440,41 +402,35 @@ boolean tryToCompleteRestoration() {\n      * @throws TaskMigratedException if the task producer got fenced (EOS only)\n      */\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n-        final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n+        final Set<TopicPartition> remainingRevokedPartitions = new HashSet<>(revokedPartitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ==", "bodyText": "nit: add a comment above task.suspend() that for active it should always be an no-op?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439494301", "createdAt": "2020-06-12T15:39:01Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5ODk5MzI5", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-429899329", "createdAt": "2020-06-12T16:58:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjo1ODoxOVrOGjLNtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzowODo1N1rOGjLh-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNzA3Ng==", "bodyText": "Can we update the comment with the state transitions above, too?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439537076", "createdAt": "2020-06-12T16:58:19Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3\n+        CLOSED(0);                // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzODE5Nw==", "bodyText": "and there is no new commit needed -> this seem to be miss leading because the commitNeeded flag is not really a guard for this case. -- Also, if the previous commit has complete is something we don't really know here.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439538197", "createdAt": "2020-06-12T17:00:23Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -528,7 +521,8 @@ private void maybeScheduleCheckpoint() {\n \n     private void writeCheckpointIfNeed() {\n         if (commitNeeded) {\n-            throw new IllegalStateException(\"A checkpoint should only be written if no commit is needed.\");\n+            throw new IllegalStateException(\"A checkpoint should only be written if the previous commit has completed\"\n+                                                + \" and there is no new commit needed.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzOTA3Ng==", "bodyText": "Seems we need to transit from RESTORING to SUSPENDED now, before closing, and never directly from RESTORING to CLOSED?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439539076", "createdAt": "2020-06-12T17:02:18Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MDc0OA==", "bodyText": "Not sure if I can follow -- if it's a no-op, why do we call it? Or do you say, we need to tall if for standbies as we don't suspend them presiously?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439540748", "createdAt": "2020-06-12T17:05:45Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MjI2NA==", "bodyText": "Above, we call suspend() blindly and have a comment that for active it's a no-op. -- Might be good to align both cases to use the same pattern (I don't care which one we pick)?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439542264", "createdAt": "2020-06-12T17:08:57Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -239,54 +240,15 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                }\n-\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-\n-                for (final Task task : additionalTasksForCommitting) {\n-                    task.postCommit();\n-                }\n-            } catch (final RuntimeException e) {\n-                log.error(\"Failed to batch commit tasks, \" +\n-                    \"will close all tasks involved in this commit as dirty by the end\", e);\n-                dirtyTasks.addAll(additionalTasksForCommitting);\n-                dirtyTasks.addAll(tasksToClose);\n-\n-                tasksToClose.clear();\n-                // Just add first taskId to re-throw by the end.\n-                taskCloseExceptions.put(consumedOffsetsAndMetadataPerTask.keySet().iterator().next(), e);\n-            }\n-        }\n-\n-        for (final Task task : tasksToClose) {\n-            try {\n-                completeTaskCloseClean(task);\n-                cleanUpTaskProducer(task, taskCloseExceptions);\n-                tasks.remove(task.id());\n-            } catch (final RuntimeException e) {\n-                final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n-                log.error(uncleanMessage, e);\n-                taskCloseExceptions.put(task.id(), e);\n-                // We've already recorded the exception (which is the point of clean).\n-                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                dirtyTasks.add(task);\n-            }\n-        }\n-\n         for (final Task oldTask : tasksToRecycle) {\n             final Task newTask;\n             try {\n                 if (oldTask.isActive()) {\n+                    // If active, the task should have already been suspended and committed during handleRevocation", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 104}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5ODkyOTcz", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-429892973", "createdAt": "2020-06-12T16:48:29Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjo0ODoyOVrOGjK6pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjo1NDo0MFrOGjLGYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzMjE5Nw==", "bodyText": "I get that standbys should never really be in RESTORING state, but it still doesn't seem like it's philosophically any more illegal to suspend from RESTORING than it is from RUNNING. I'd vote to legalize RESTORING here. It does seem like a useful sanity check for CLOSED to be illegal, though.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439532197", "createdAt": "2020-06-12T16:48:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -112,10 +112,19 @@ public void completeRestoration() {\n     @Override\n     public void suspend() {\n         log.trace(\"No-op suspend with state {}\", state());\n-        if (state() == State.RUNNING) {\n-            transitionTo(State.SUSPENDED);\n-        } else if (state() == State.RESTORING) {\n-            throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending standby task \" + id);\n+        switch (state()) {\n+            case CREATED:\n+            case RUNNING:\n+            case SUSPENDED:\n+                transitionTo(State.SUSPENDED);\n+                break;\n+\n+            case RESTORING:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNDI5NQ==", "bodyText": "It looks like your changes in the tasks have prohibited any state from transitioning to CLOSED except SUSPENDED. Should we update the state machine to reflect this?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439534295", "createdAt": "2020-06-12T16:52:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3\n+        CLOSED(0);                // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNTIwMQ==", "bodyText": "I think we previously followed the \"loop over iterator and remove during iteration\" pattern, and we got away from it because it was too confusing. Do we really need to re-introduce it now?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439535201", "createdAt": "2020-06-12T16:54:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -219,13 +214,19 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 tasksToRecycle.add(task);\n             } else {\n                 try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    task.suspend(); // Should be a no-op for all active tasks, unless we hit an exception during handleRevocation\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);\n+                    cleanUpTaskProducer(task, taskCloseExceptions);\n+                    taskIter.remove();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5OTkwODMw", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-429990830", "createdAt": "2020-06-12T19:32:56Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxOTozMjo1N1rOGjPemw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxOTozMjo1N1rOGjPemw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYwNjkzOQ==", "bodyText": "This was a test I added a little while back in response to a bugfix, but it no longer makes sense in the current context (in fact it's currently not really testing anything at all, since the original point was to make sure the changelog reader partitions were cleaned up but that's not even the responsibility of the TaskManager anymore)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439606939", "createdAt": "2020-06-12T19:32:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -952,58 +951,11 @@ public void shouldNotCommitOnHandleAssignmentIfOnlyStandbyTaskClosed() {\n     }\n \n     @Test\n-    public void shouldCleanupAnyTasksClosedAsDirtyAfterCommitException() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "a5f47cda67e0876be6877c7aec3ec770cc46891b", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/a5f47cda67e0876be6877c7aec3ec770cc46891b", "committedDate": "2020-06-12T21:52:50Z", "message": "always close all tasks during shutdown"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMTAwNjUz", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-430100653", "createdAt": "2020-06-13T01:12:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxMjoyMVrOGjU3Aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMTozOTowN1rOGjVAEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTEwNw==", "bodyText": "So far, we did not allow idempotent state transitions in the state machine itself, but handle it caller side. -- It seem inconsistent to allow SUSPENDED -> SUSPEND but not CREATE -> CREATED etc.\nI would recommend to keep the current pattern and avoid calling transiteState() if the task is already in the target state. -- I would also be happy to change it, but for this case, we should change it for all cases. However, this would enlarge the scope of this PR and I think it better not to do it in this PR.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695107", "createdAt": "2020-06-13T01:12:21Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -56,21 +56,21 @@\n      *          |            |              |     |\n      *          |            v              |     |\n      *          |     +------+--------+     |     |\n-     *          |     | Suspended (3) | <---+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n-     *          |     +------+--------+           |\n-     *          |            |                    |\n-     *          |            v                    |\n-     *          |      +-----+-------+            |\n-     *          +----> | Closed (4)  | -----------+\n+     *          +---->| Suspended (3) | ----+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n+     *                +------+--------+           |\n+     *                       |                    |\n+     *                       v                    |\n+     *                 +-----+-------+            |\n+     *                 | Closed (4)  | -----------+\n      *                 +-------------+\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3),            // 0\n+        RESTORING(2, 3),          // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTE5NA==", "bodyText": "The SUSPEND case should be no-op IMHO and not call transiteTo() (compare my other comment in Task.java).", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695194", "createdAt": "2020-06-13T01:13:08Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -112,10 +112,19 @@ public void completeRestoration() {\n     @Override\n     public void suspend() {\n         log.trace(\"No-op suspend with state {}\", state());\n-        if (state() == State.RUNNING) {\n-            transitionTo(State.SUSPENDED);\n-        } else if (state() == State.RESTORING) {\n-            throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending standby task \" + id);\n+        switch (state()) {\n+            case CREATED:\n+            case RUNNING:\n+            case SUSPENDED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTMxNA==", "bodyText": "IMHO, we should keep the SUSPENDED case for consistency reasons. Only merge CREATED and RESTORING (cf. my other comment on Task.java)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695314", "createdAt": "2020-06-13T01:14:37Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -250,14 +250,10 @@ public void completeRestoration() {\n     public void suspend() {\n         switch (state()) {\n             case CREATED:\n-            case SUSPENDED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTUxMg==", "bodyText": "Is the comment necessary? Seem the code is self-explaining?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695512", "createdAt": "2020-06-13T01:16:21Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -474,20 +470,17 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        suspend();\n-        prepareCommit();\n-        writeCheckpointIfNeed();\n-\n+        // Stream tasks should have already been suspended and their consumed offsets committed before recycling", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTcwMA==", "bodyText": "Why remove the < arrow? We can still transit from RESTORING to SUSPENDED.\nSuper-nit: +---->| Suspended -> +---> | Suspended", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695700", "createdAt": "2020-06-13T01:18:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -56,21 +56,21 @@\n      *          |            |              |     |\n      *          |            v              |     |\n      *          |     +------+--------+     |     |\n-     *          |     | Suspended (3) | <---+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n-     *          |     +------+--------+           |\n-     *          |            |                    |\n-     *          |            v                    |\n-     *          |      +-----+-------+            |\n-     *          +----> | Closed (4)  | -----------+\n+     *          +---->| Suspended (3) | ----+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTg0Ng==", "bodyText": "Nit: Why LinkedList<Task> tasksToClose? Should we only declare it as List?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695846", "createdAt": "2020-06-13T01:20:02Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg==", "bodyText": "If we hit an exception in handleRevocation why would we continue here? Are we still in a \"clean enough\" state to actually continue?\nBelow we call completeTaskCloseClean(task) what seem incorrect for this case as it might close clean task even if we did not successfully commit before.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439696982", "createdAt": "2020-06-13T01:32:51Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg==", "bodyText": "Why do we do the try-catch as outer-layer? In an exception occurs, we should stop looping through the tasks to call postCommit() -- is this intended? If yes, why?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439697426", "createdAt": "2020-06-13T01:39:07Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 235}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMjM3NzA2", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-430237706", "createdAt": "2020-06-14T17:35:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNzozNTowN1rOGjeaYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNzo0NjoyN1rOGjeeFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MTYxNg==", "bodyText": "Seems we would never commit and checkpoint state manager any more.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439851616", "createdAt": "2020-06-14T17:35:07Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -539,19 +537,18 @@ private void writeCheckpointIfNeed() {\n     /**\n      * <pre>\n      * the following order must be followed:\n-     *  1. checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n+     *  1. commit/checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MTkwNA==", "bodyText": "I re-read the current code structure and got some questions:\n\nwe collect checkpoint from prepareCommit and check if it is not null in postCommit, but the actual checkpoint value itself is always collectable post the commit, and hence what's only required to that we need to know if we need to write a checkpoint file or not. Previously this needs to be decided since we may transit the state in between but now from the source code it seems to me that we would only call prepare/post before suspend / close ever, so this is no longer required actually, i.e. we can decide whether we need to checkpoint and then collect the checkpoint map and write the file if needed in a single call. Is that right?\n\n\nI think I agree with you that it is cleaner to make sure in handleRevocation, we still transit those revoked partition's corresponding tasks to suspended even if some of their commit call failed.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439851904", "createdAt": "2020-06-14T17:38:50Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MjU2NA==", "bodyText": "Yeah I think if the actual consumer.commit call failed, then we should not trigger postCommit for any one.\nAs for postCommit, I think it should never fail (we swallow the IO exception happened, because for non-EOS it is just fine, for EOS we would bootstrap from scratch).", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439852564", "createdAt": "2020-06-14T17:46:27Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg=="}, "originalCommit": null, "originalPosition": 235}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMDI2NzE3", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431026717", "createdAt": "2020-06-15T22:07:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMjowNzowMVrOGkEYfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMzowMjowNVrOGkFjzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3MzcyNg==", "bodyText": "ack", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440473726", "createdAt": "2020-06-15T22:07:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -539,19 +537,18 @@ private void writeCheckpointIfNeed() {\n     /**\n      * <pre>\n      * the following order must be followed:\n-     *  1. checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n+     *  1. commit/checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MTYxNg=="}, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAwNQ==", "bodyText": "I think you're right, we don't need to keep track of the current checkpoint offsets at all and can just write the current checkpointableOffsets in postCommit\ndone", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440493005", "createdAt": "2020-06-15T23:02:05Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNzQwODM5", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431740839", "createdAt": "2020-06-16T17:37:03Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozNzowNFrOGkmHtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzo1MDo0OVrOGkmnAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNjQ4NQ==", "bodyText": "nit: was an active scheduled checkpoint -> there was a pending uncommitted data.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441026485", "createdAt": "2020-06-16T17:37:04Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -502,56 +494,24 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    private void maybeScheduleCheckpoint() {\n-        switch (state()) {\n-            case RESTORING:\n-            case SUSPENDED:\n-                this.checkpoint = checkpointableOffsets();\n-\n-                break;\n-\n-            case RUNNING:\n-                if (!eosEnabled) {\n-                    this.checkpoint = checkpointableOffsets();\n-                }\n-\n-                break;\n-\n-            case CREATED:\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-        }\n-    }\n-\n-    private void writeCheckpointIfNeed() {\n+    private void maybeWriteCheckpoint() {\n         if (commitNeeded) {\n+            log.error(\"Tried to write a checkpoint with pending uncommitted data, should complete the commit first.\");\n             throw new IllegalStateException(\"A checkpoint should only be written if no commit is needed.\");\n         }\n-        if (checkpoint != null) {\n-            stateMgr.checkpoint(checkpoint);\n-            checkpoint = null;\n-        }\n+        stateMgr.checkpoint(checkpointableOffsets());\n     }\n \n     /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     *  2. then if we are closing on EOS and dirty, wipe out the state store directory\n-     *  3. finally release the state manager lock\n-     * </pre>\n+     * You must commit a task and checkpoint the state manager before closing as this will release the state dir lock\n      */\n     private void close(final boolean clean) {\n-        if (clean) {\n-            executeAndMaybeSwallow(true, this::writeCheckpointIfNeed, \"state manager checkpoint\", log);\n+        if (clean && commitNeeded) {\n+            log.debug(\"Tried to close clean but there was an active scheduled checkpoint, this means we failed to\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAzNDQ5Nw==", "bodyText": "Sounds good, in that case the nested try-catch would be necessary.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441034497", "createdAt": "2020-06-16T17:50:49Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg=="}, "originalCommit": null, "originalPosition": 235}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNzY3MjE1", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431767215", "createdAt": "2020-06-16T18:10:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMDoyN1rOGknUCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMDoyN1rOGknUCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA==", "bodyText": "I think we still need to make this call -- in eager rebalancing, we suspend a task when we get a partition revoked. For this case, we \"forget\" the current offset within the consumer and thus need to clear the partition grouper. Otherwise, we might read the data a second time, if the partition is reassigned (what would violate EOS).", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441046024", "createdAt": "2020-06-16T18:10:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNzY5ODY1", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431769865", "createdAt": "2020-06-16T18:12:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMjo1MVrOGknZQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMjo1MVrOGknZQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NzM2MA==", "bodyText": "As we always suspend a task before closing (even for unclean closing), I think we can actually remove this call? (We only needed it before, because SUSPEND could be skipped.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441047360", "createdAt": "2020-06-16T18:12:51Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -474,26 +468,24 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        suspend();\n-        prepareCommit();\n-        writeCheckpointIfNeed();\n-\n         switch (state()) {\n-            case CREATED:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n \n                 break;\n \n-            case RESTORING: // we should have transitioned to `SUSPENDED` already\n-            case RUNNING: // we should have transitioned to `SUSPENDED` already\n+            case CREATED:\n+            case RESTORING:\n+            case RUNNING:\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n         }\n \n+        // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n+        // because otherwise we loose the partition-time information\n         partitionGroup.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNzc0NjE5", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431774619", "createdAt": "2020-06-16T18:16:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxNjowOFrOGkngcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxNjowOFrOGkngcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0OTIwMA==", "bodyText": "Seems we call stateMgr.checkpoint unconditionally now. Should we rename this this writeCheckpoint ? Or even remove it all together as we if (commitNeeded) check is \"just\" a guard and the method is a single liner now?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441049200", "createdAt": "2020-06-16T18:16:08Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -502,56 +494,24 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    private void maybeScheduleCheckpoint() {\n-        switch (state()) {\n-            case RESTORING:\n-            case SUSPENDED:\n-                this.checkpoint = checkpointableOffsets();\n-\n-                break;\n-\n-            case RUNNING:\n-                if (!eosEnabled) {\n-                    this.checkpoint = checkpointableOffsets();\n-                }\n-\n-                break;\n-\n-            case CREATED:\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-        }\n-    }\n-\n-    private void writeCheckpointIfNeed() {\n+    private void maybeWriteCheckpoint() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 147}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNzgwODgx", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431780881", "createdAt": "2020-06-16T18:24:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoyNDoyN1rOGkn7lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoyNDoyN1rOGkn7lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1NjE0OQ==", "bodyText": "Is this necessarily a warning? A wall-clock-time punctuation could have set commitNeeded to true?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441056149", "createdAt": "2020-06-16T18:24:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -465,44 +429,82 @@ boolean tryToCompleteRestoration() {\n     }\n \n     /**\n+     * Handle the revoked partitions and prepare for closing the associated tasks in {@link #handleAssignment(Map, Map)}\n+     * We should commit the revoked tasks now as we will not officially own them anymore when {@link #handleAssignment(Map, Map)}\n+     * is called. Note that only active task partitions are passed in from the rebalance listener, so we only need to\n+     * consider/commit active tasks here\n+     *\n+     * If eos-beta is used, we must commit ALL tasks. Otherwise, we can just commit those (active) tasks which are revoked\n+     *\n      * @throws TaskMigratedException if the task producer got fenced (EOS only)\n      */\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n-        final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n+        final Set<TopicPartition> remainingRevokedPartitions = new HashSet<>(revokedPartitions);\n \n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        for (final Task task : tasks.values()) {\n-            if (remainingPartitions.containsAll(task.inputPartitions())) {\n-                task.suspend();\n-                final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+        final Set<Task> tasksToCommit = new HashSet<>();\n+        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n \n-                if (!committableOffsets.isEmpty()) {\n-                    consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+        for (final Task task : activeTaskIterable()) {\n+            if (remainingRevokedPartitions.containsAll(task.inputPartitions())) {\n+                try {\n+                    task.suspend();\n+                    if (task.commitNeeded()) {\n+                        tasksToCommit.add(task);\n+                    }\n+                } catch (final RuntimeException e) {\n+                    log.error(\"Caught the following exception while trying to suspend revoked task \" + task.id(), e);\n+                    firstException.compareAndSet(null, new StreamsException(\"Failed to suspend \" + task.id(), e));\n                 }\n-            } else if (task.isActive() && task.commitNeeded()) {\n-                final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+            } else if (task.commitNeeded()) {\n+                additionalTasksForCommitting.add(task);\n+            }\n+            remainingRevokedPartitions.removeAll(task.inputPartitions());\n+        }\n \n-                if (!committableOffsets.isEmpty()) {\n-                    consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                }\n+        if (!remainingRevokedPartitions.isEmpty()) {\n+            log.warn(\"The following partitions {} are missing from the task partitions. It could potentially \" +\n+                         \"due to race condition of consumer detecting the heartbeat failure, or the tasks \" +\n+                         \"have been cleaned up by the handleAssignment callback.\", remainingRevokedPartitions);\n+        }\n+\n+        final RuntimeException suspendException = firstException.get();\n+        if (suspendException != null) {\n+            throw suspendException;\n+        }\n+\n+        // If using eos-beta, if we must commit any task then we must commit all of them\n+        // TODO: when KAFKA-9450 is done this will be less expensive, and we can simplify by always committing everything\n+        if (processingMode ==  EXACTLY_ONCE_BETA && !tasksToCommit.isEmpty()) {\n+            tasksToCommit.addAll(additionalTasksForCommitting);\n+        }\n+\n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+        for (final Task task : tasksToCommit) {\n+            final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+            if (!committableOffsets.isEmpty()) {\n+                consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+            } else {\n+                log.warn(\"Task {} claimed to need a commit but had no committable consumed offsets\", task.id());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 192}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNzg0ODE4", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431784818", "createdAt": "2020-06-16T18:29:37Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoyOTozN1rOGkoHHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoyOTozN1rOGkoHHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1OTEwMA==", "bodyText": "Thinking about punctuation, should we actually call commitOffsetsOrTransaction() unconditionally (ie, not consider if consumedOffsetsAndMetadataPerTask is empty or not?\nWe can still move the check inside consumedOffsetsAndMetadataPerTask, but for EOS there might pending writes from punctuation that we still need to commit?\nThis would apply to all calls of commitOffsetsOrTransaction ?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441059100", "createdAt": "2020-06-16T18:29:37Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +717,26 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 249}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3789a7c1dc71b69ec4d6af621a3415210d731fa8", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/3789a7c1dc71b69ec4d6af621a3415210d731fa8", "committedDate": "2020-06-16T20:00:35Z", "message": "enforce SUSPENDED and check commitNeeded"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f621c3a02d673a57bcfb32a653a257bfc4d48241", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/f621c3a02d673a57bcfb32a653a257bfc4d48241", "committedDate": "2020-06-16T20:00:35Z", "message": "check if commit needed in TM"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c3c080bf6c6007979404dcfaf45e75399711332", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/0c3c080bf6c6007979404dcfaf45e75399711332", "committedDate": "2020-06-16T20:00:35Z", "message": "just suspend in closeandRecycleState"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6805838f5c3fbbade7eaa82330b14d88e52f783", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/a6805838f5c3fbbade7eaa82330b14d88e52f783", "committedDate": "2020-06-16T20:00:36Z", "message": "clean up handleRevocation and handleAssignment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb64b9787153ae1da96db9694008139f4d299a4d", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/fb64b9787153ae1da96db9694008139f4d299a4d", "committedDate": "2020-06-16T20:00:36Z", "message": "Task and TaskManager tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b08e28d940ae822b05da54460064c261b56e0ed", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/7b08e28d940ae822b05da54460064c261b56e0ed", "committedDate": "2020-06-16T20:00:36Z", "message": "always close all tasks during shutdown"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd03646b9c8c01602219d7afb25730df5663a7d0", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/dd03646b9c8c01602219d7afb25730df5663a7d0", "committedDate": "2020-06-16T20:00:36Z", "message": "only commit all for eos-beta"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f989fc78ffa6b89cf50775767e563a64120460a", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/1f989fc78ffa6b89cf50775767e563a64120460a", "committedDate": "2020-06-16T20:00:36Z", "message": "addressing CR feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf21fb5d42e1a5b1521ae58b7f57839195a4791b", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cf21fb5d42e1a5b1521ae58b7f57839195a4791b", "committedDate": "2020-06-16T20:00:36Z", "message": "get rid of checkpoint member"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2ba0df1befe929672850755d1610427c6234e57", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/d2ba0df1befe929672850755d1610427c6234e57", "committedDate": "2020-06-16T20:00:36Z", "message": "fixing up last few TM tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bd032c97d515c9dfff9ed940ae795105b2c466c7", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/bd032c97d515c9dfff9ed940ae795105b2c466c7", "committedDate": "2020-06-16T20:00:36Z", "message": "log before transition"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0349663e2836129908a73e9e587ece9afb3bc04f", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/0349663e2836129908a73e9e587ece9afb3bc04f", "committedDate": "2020-06-16T20:00:36Z", "message": "attempt to postCommit all tasks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ff79ea351cd20c4021b053133061984c0bbd202", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/3ff79ea351cd20c4021b053133061984c0bbd202", "committedDate": "2020-06-16T20:00:36Z", "message": "always commit when commitNeeded"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "3ff79ea351cd20c4021b053133061984c0bbd202", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/3ff79ea351cd20c4021b053133061984c0bbd202", "committedDate": "2020-06-16T20:00:36Z", "message": "always commit when commitNeeded"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2529fb91b156a520ceb4178665ab313051f68cfd", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/2529fb91b156a520ceb4178665ab313051f68cfd", "committedDate": "2020-06-16T20:13:05Z", "message": "only commit active tasks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxODU4NTYy", "url": "https://github.com/apache/kafka/pull/8856#pullrequestreview-431858562", "createdAt": "2020-06-16T20:17:06Z", "commit": {"oid": "2529fb91b156a520ceb4178665ab313051f68cfd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMDoxNzowNlrOGkrl7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMDoxNzowNlrOGkrl7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTExNjE0MQ==", "bodyText": "As @mjsax pointed out, we should still commit even if there are no consumed offsets. However, we should not commit the offsets/transaction if there are no active tasks that need committing", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441116141", "createdAt": "2020-06-16T20:17:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -690,18 +686,21 @@ void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n         final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n         for (final Task task : tasks.values()) {\n             if (clean) {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        tasksToCommit.add(task);\n+                        final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        if (task.isActive()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2529fb91b156a520ceb4178665ab313051f68cfd"}, "originalPosition": 238}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 606, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}