{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcwNjY1NzEx", "number": 8037, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQwODoxMDowN1rODc40-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0MzoyMFrODdEXmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNjE3Nzg2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQwODoxMDowN1rOFlLGIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzozMzo0N1rOFldC5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDUyMzQyNA==", "bodyText": "Is there a case that recoveryPoint gets smaller than logStartOffset? If yes, is it ok to just move the recoveryPoint without flush? If not, is it worthy to throw exception or log this weird case?", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374523424", "createdAt": "2020-02-04T08:10:07Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -721,14 +721,30 @@ class Log(@volatile var dir: File,\n     }\n   }\n \n-  private def updateLogEndOffset(messageOffset: Long): Unit = {\n-    nextOffsetMetadata = LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size)\n+  private def updateLogEndOffset(offset: Long): Unit = {\n+    nextOffsetMetadata = LogOffsetMetadata(offset, activeSegment.baseOffset, activeSegment.size)\n \n     // Update the high watermark in case it has gotten ahead of the log end offset following a truncation\n     // or if a new segment has been rolled and the offset metadata needs to be updated.\n-    if (highWatermark >= messageOffset) {\n+    if (highWatermark >= offset) {\n       updateHighWatermarkMetadata(nextOffsetMetadata)\n     }\n+\n+    if (this.recoveryPoint > offset) {\n+      this.recoveryPoint = offset\n+    }\n+  }\n+\n+  private def updateLogStartOffset(offset: Long): Unit = {\n+    logStartOffset = offset\n+\n+    if (highWatermark < offset) {\n+      updateHighWatermark(offset)\n+    }\n+\n+    if (this.recoveryPoint < offset) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxMjEyMw==", "bodyText": "Is a recovery point lower than the log start offset useful? All data below the log start offset is subject to deletion.", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374812123", "createdAt": "2020-02-04T17:23:36Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -721,14 +721,30 @@ class Log(@volatile var dir: File,\n     }\n   }\n \n-  private def updateLogEndOffset(messageOffset: Long): Unit = {\n-    nextOffsetMetadata = LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size)\n+  private def updateLogEndOffset(offset: Long): Unit = {\n+    nextOffsetMetadata = LogOffsetMetadata(offset, activeSegment.baseOffset, activeSegment.size)\n \n     // Update the high watermark in case it has gotten ahead of the log end offset following a truncation\n     // or if a new segment has been rolled and the offset metadata needs to be updated.\n-    if (highWatermark >= messageOffset) {\n+    if (highWatermark >= offset) {\n       updateHighWatermarkMetadata(nextOffsetMetadata)\n     }\n+\n+    if (this.recoveryPoint > offset) {\n+      this.recoveryPoint = offset\n+    }\n+  }\n+\n+  private def updateLogStartOffset(offset: Long): Unit = {\n+    logStartOffset = offset\n+\n+    if (highWatermark < offset) {\n+      updateHighWatermark(offset)\n+    }\n+\n+    if (this.recoveryPoint < offset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDUyMzQyNA=="}, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxNzUxMA==", "bodyText": "U are right :)", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374817510", "createdAt": "2020-02-04T17:33:47Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -721,14 +721,30 @@ class Log(@volatile var dir: File,\n     }\n   }\n \n-  private def updateLogEndOffset(messageOffset: Long): Unit = {\n-    nextOffsetMetadata = LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size)\n+  private def updateLogEndOffset(offset: Long): Unit = {\n+    nextOffsetMetadata = LogOffsetMetadata(offset, activeSegment.baseOffset, activeSegment.size)\n \n     // Update the high watermark in case it has gotten ahead of the log end offset following a truncation\n     // or if a new segment has been rolled and the offset metadata needs to be updated.\n-    if (highWatermark >= messageOffset) {\n+    if (highWatermark >= offset) {\n       updateHighWatermarkMetadata(nextOffsetMetadata)\n     }\n+\n+    if (this.recoveryPoint > offset) {\n+      this.recoveryPoint = offset\n+    }\n+  }\n+\n+  private def updateLogStartOffset(offset: Long): Unit = {\n+    logStartOffset = offset\n+\n+    if (highWatermark < offset) {\n+      updateHighWatermark(offset)\n+    }\n+\n+    if (this.recoveryPoint < offset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDUyMzQyNA=="}, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNzM5MTg4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDo0ODozNlrOFlWuYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzoyMzo1M1rOFlcudQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDcxMzk1Mw==", "bodyText": "resetLastCaughtUpTime takes curLeaderLogEndOffset, but we are now passing leaderEpochStartOffset. Do we need to update the parameter name of that method?", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374713953", "createdAt": "2020-02-04T14:48:36Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -506,12 +507,11 @@ class Partition(val topicPartition: TopicPartition,\n       leaderLog.maybeAssignEpochStartOffset(leaderEpoch, leaderEpochStartOffset)\n \n       val isNewLeader = !isLeader\n-      val curLeaderLogEndOffset = leaderLog.logEndOffset\n       val curTimeMs = time.milliseconds\n       // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.\n       remoteReplicas.foreach { replica =>\n         val lastCaughtUpTimeMs = if (inSyncReplicaIds.contains(replica.brokerId)) curTimeMs else 0L\n-        replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)\n+        replica.resetLastCaughtUpTime(leaderEpochStartOffset, curTimeMs, lastCaughtUpTimeMs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgwNDkyNA==", "bodyText": "They are the same thing. I just got rid of a redundant variable.", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374804924", "createdAt": "2020-02-04T17:10:38Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -506,12 +507,11 @@ class Partition(val topicPartition: TopicPartition,\n       leaderLog.maybeAssignEpochStartOffset(leaderEpoch, leaderEpochStartOffset)\n \n       val isNewLeader = !isLeader\n-      val curLeaderLogEndOffset = leaderLog.logEndOffset\n       val curTimeMs = time.milliseconds\n       // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.\n       remoteReplicas.foreach { replica =>\n         val lastCaughtUpTimeMs = if (inSyncReplicaIds.contains(replica.brokerId)) curTimeMs else 0L\n-        replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)\n+        replica.resetLastCaughtUpTime(leaderEpochStartOffset, curTimeMs, lastCaughtUpTimeMs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDcxMzk1Mw=="}, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxMjI3Nw==", "bodyText": "Ok, I see.", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374812277", "createdAt": "2020-02-04T17:23:53Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -506,12 +507,11 @@ class Partition(val topicPartition: TopicPartition,\n       leaderLog.maybeAssignEpochStartOffset(leaderEpoch, leaderEpochStartOffset)\n \n       val isNewLeader = !isLeader\n-      val curLeaderLogEndOffset = leaderLog.logEndOffset\n       val curTimeMs = time.milliseconds\n       // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.\n       remoteReplicas.foreach { replica =>\n         val lastCaughtUpTimeMs = if (inSyncReplicaIds.contains(replica.brokerId)) curTimeMs else 0L\n-        replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)\n+        replica.resetLastCaughtUpTime(leaderEpochStartOffset, curTimeMs, lastCaughtUpTimeMs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDcxMzk1Mw=="}, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA2ODczOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0MzoyMFrOFldVNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxODo1MTowM1rOFlfdUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMjE5OQ==", "bodyText": "Previous behavior is recoveryPoint = math.min(newOffset, recoveryPoint)) but this patch changes it to\nif (this.recoveryPoint < offset) {\n  this.recoveryPoint = offset\n}\n\nwhich is equal to recoveryPoint = math.max(newOffset, recoveryPoint)). Is it a bug?", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374822199", "createdAt": "2020-02-04T17:43:20Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2081,9 +2096,7 @@ class Log(@volatile var dir: File,\n         producerStateManager.truncate()\n         producerStateManager.updateMapEndOffset(newOffset)\n         maybeIncrementFirstUnstableOffset()\n-\n-        this.recoveryPoint = math.min(newOffset, this.recoveryPoint)\n-        this.logStartOffset = newOffset\n+        updateLogStartOffset(newOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NTgzNw==", "bodyText": "I have also updated updateLogEndOffset to set the recovery point. In truncateFully where we delete all segments and set the log start offset to be equal to the log end offset, this ensures recovery point is also set consistently.", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374845837", "createdAt": "2020-02-04T18:29:30Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2081,9 +2096,7 @@ class Log(@volatile var dir: File,\n         producerStateManager.truncate()\n         producerStateManager.updateMapEndOffset(newOffset)\n         maybeIncrementFirstUnstableOffset()\n-\n-        this.recoveryPoint = math.min(newOffset, this.recoveryPoint)\n-        this.logStartOffset = newOffset\n+        updateLogStartOffset(newOffset)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMjE5OQ=="}, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg1NzA0Mg==", "bodyText": "Thanks for the explanation!", "url": "https://github.com/apache/kafka/pull/8037#discussion_r374857042", "createdAt": "2020-02-04T18:51:03Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2081,9 +2096,7 @@ class Log(@volatile var dir: File,\n         producerStateManager.truncate()\n         producerStateManager.updateMapEndOffset(newOffset)\n         maybeIncrementFirstUnstableOffset()\n-\n-        this.recoveryPoint = math.min(newOffset, this.recoveryPoint)\n-        this.logStartOffset = newOffset\n+        updateLogStartOffset(newOffset)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMjE5OQ=="}, "originalCommit": {"oid": "8c5b977813084661c071ad823e3e7e5555c0ca5a"}, "originalPosition": 69}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4185, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}