{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0ODM5OTEx", "number": 9549, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToxMDowN1rOE26cDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQwOTozMjo1MFrOFyaZxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDE2MDEyOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToxMDowN1rOHv9nwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTowODo1NVrOHxEcaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1NDcyMA==", "bodyText": "Might make sense to refer to the MOVE.name and COPY.name fields declared in the Operation enum below instead of using the literal \"move\" and \"copy\" strings in this section.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520054720", "createdAt": "2020-11-09T19:10:07Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIxNTA4MQ==", "bodyText": "MOVE.name is not a compile time constant (at least the compiler doesn't see it that way). I factored out constants to avoid the duplication of literals.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r521215081", "createdAt": "2020-11-11T09:08:55Z", "author": {"login": "tombentley"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1NDcyMA=="}, "originalCommit": null, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDE3MTgyOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToxMjowN1rOHv9vWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODozMjo1NlrOHy6ahQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1NjY2Nw==", "bodyText": "Why duplicate headers here? According to the Header class's Javadocs, the collection should be mutable.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520056667", "createdAt": "2020-11-09T19:12:07Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIxNTE0MA==", "bodyText": "I wasn't completely sure whether a transformation was allowed to mutate headers, since it has to create a new ConnectRecord. Take this code from WorkerSourceTask as an example:\n            final SourceRecord record = transformationChain.apply(preTransformRecord);\n            final ProducerRecord<byte[], byte[]> producerRecord = convertTransformedRecord(record);\n            if (producerRecord == null || retryWithToleranceOperator.failed()) {\n                counter.skipRecord();\n                commitTaskRecord(preTransformRecord, null);\n                continue;\n            }\nSee how preTransformRecord can be used after the transformation chain has been applied? I think it would be incorrect for commitTaskRecord to commit the original record but with headers which had been mutated by transformations, right?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r521215140", "createdAt": "2020-11-11T09:09:02Z", "author": {"login": "tombentley"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1NjY2Nw=="}, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE0NzkwOQ==", "bodyText": "Ah, that's fair. Does seem to be the pattern followed by the other out-of-the-box transformations as well; probably best to continue to follow that pattern.\nI'm a little unnerved by this though, since as far as I can tell it's not publicly documented and so it's possible people writing their own transformations may be violating this implicit rule.\nOut of scope, so I've filed KAFKA-10720 to track the need for possible documentation improvements.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r523147909", "createdAt": "2020-11-13T18:32:56Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1NjY2Nw=="}, "originalCommit": null, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDE4NzgyOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToxNjozNFrOHv95cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTowOTowNlrOHxEc5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1OTI1MQ==", "bodyText": "This looks fairly expensive to perform for every record. Do you think it might make sense to perform some caching, similarly to what's done in the ReplaceField transform?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520059251", "createdAt": "2020-11-09T19:16:34Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            String headerName = headers.get(i);\n+            Object fieldValue = value.get(fieldName);\n+            Schema fieldSchema = operatingSchema.field(fieldName).schema();\n+            updatedHeaders.add(headerName, fieldValue, fieldSchema);\n+        }\n+        return newRecord(record, updatedSchema, updatedValue, updatedHeaders);\n+    }\n+\n+    private Schema moveSchema(Schema operatingSchema) {\n+        final Schema updatedSchema;\n+        final SchemaBuilder builder = SchemaUtil.copySchemaBasics(operatingSchema, SchemaBuilder.struct());\n+        for (Field field : operatingSchema.fields()) {\n+            if (!fields.contains(field.name())) {\n+                builder.field(field.name(), field.schema());\n+            }\n+        }\n+        updatedSchema = builder.build();\n+        return updatedSchema;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIxNTIwNQ==", "bodyText": "Good idea.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r521215205", "createdAt": "2020-11-11T09:09:06Z", "author": {"login": "tombentley"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            String headerName = headers.get(i);\n+            Object fieldValue = value.get(fieldName);\n+            Schema fieldSchema = operatingSchema.field(fieldName).schema();\n+            updatedHeaders.add(headerName, fieldValue, fieldSchema);\n+        }\n+        return newRecord(record, updatedSchema, updatedValue, updatedHeaders);\n+    }\n+\n+    private Schema moveSchema(Schema operatingSchema) {\n+        final Schema updatedSchema;\n+        final SchemaBuilder builder = SchemaUtil.copySchemaBasics(operatingSchema, SchemaBuilder.struct());\n+        for (Field field : operatingSchema.fields()) {\n+            if (!fields.contains(field.name())) {\n+                builder.field(field.name(), field.schema());\n+            }\n+        }\n+        updatedSchema = builder.build();\n+        return updatedSchema;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1OTI1MQ=="}, "originalCommit": null, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDE5MTEyOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToxNzoyNFrOHv97Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToxNzoyNFrOHv97Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA1OTczOA==", "bodyText": "Probably want to refer to FIELDS_FIELD and HEADERS_FIELD here instead of the literal \"fields\" and \"headers\" strings.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520059738", "createdAt": "2020-11-09T19:17:24Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            String headerName = headers.get(i);\n+            Object fieldValue = value.get(fieldName);\n+            Schema fieldSchema = operatingSchema.field(fieldName).schema();\n+            updatedHeaders.add(headerName, fieldValue, fieldSchema);\n+        }\n+        return newRecord(record, updatedSchema, updatedValue, updatedHeaders);\n+    }\n+\n+    private Schema moveSchema(Schema operatingSchema) {\n+        final Schema updatedSchema;\n+        final SchemaBuilder builder = SchemaUtil.copySchemaBasics(operatingSchema, SchemaBuilder.struct());\n+        for (Field field : operatingSchema.fields()) {\n+            if (!fields.contains(field.name())) {\n+                builder.field(field.name(), field.schema());\n+            }\n+        }\n+        updatedSchema = builder.build();\n+        return updatedSchema;\n+    }\n+\n+    private R applySchemaless(R record, Object operatingValue) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Map<String, Object> value = Requirements.requireMap(operatingValue, \"header \" + operation);\n+        Map<String, Object> updatedValue = new HashMap<>(value);\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            Object fieldValue = value.get(fieldName);\n+            String headerName = headers.get(i);\n+            if (operation == Operation.MOVE) {\n+                updatedValue.remove(fieldName);\n+            }\n+            updatedHeaders.add(headerName, fieldValue, null);\n+        }\n+        return newRecord(record, null, updatedValue, updatedHeaders);\n+    }\n+\n+    protected abstract Object operatingValue(R record);\n+    protected abstract Schema operatingSchema(R record);\n+    protected abstract R newRecord(R record, Schema updatedSchema, Object updatedValue, Iterable<Header> updatedHeaders);\n+\n+    public static class Key<R extends ConnectRecord<R>> extends HeaderFrom<R> {\n+\n+        @Override\n+        public Object operatingValue(R record) {\n+            return record.key();\n+        }\n+\n+        @Override\n+        protected Schema operatingSchema(R record) {\n+            return record.keySchema();\n+        }\n+\n+        @Override\n+        protected R newRecord(R record, Schema updatedSchema, Object updatedValue, Iterable<Header> updatedHeaders) {\n+            return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue,\n+                    record.valueSchema(), record.value(), record.timestamp(), updatedHeaders);\n+        }\n+    }\n+\n+    public static class Value<R extends ConnectRecord<R>> extends HeaderFrom<R> {\n+\n+        @Override\n+        public Object operatingValue(R record) {\n+            return record.value();\n+        }\n+\n+        @Override\n+        protected Schema operatingSchema(R record) {\n+            return record.valueSchema();\n+        }\n+\n+        @Override\n+        protected R newRecord(R record, Schema updatedSchema, Object updatedValue, Iterable<Header> updatedHeaders) {\n+            return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+                    updatedSchema, updatedValue, record.timestamp(), updatedHeaders);\n+        }\n+    }\n+\n+    @Override\n+    public ConfigDef config() {\n+        return CONFIG_DEF;\n+    }\n+\n+    @Override\n+    public void close() {\n+\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> props) {\n+        final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);\n+        fields = config.getList(FIELDS_FIELD);\n+        headers = config.getList(HEADERS_FIELD);\n+        if (headers.size() != fields.size()) {\n+            throw new ConfigException(\"'fields' config must have the same number of elements as 'headers' config.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDIyMzQ2OnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNTo1MVrOHv-O_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNTo1MVrOHv-O_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NDc2Ng==", "bodyText": "Worth noting that if there's no field with this name in the schema for the record, this will throw an exception and (unless it's configured with an error tolerance to ignore such issues), fail the task.\nThis isn't unreasonable behavior, especially given that that case isn't covered in the KIP, but it's different from what happens right now in the applySchemaless method, which is to silently add a null header instead.\nIt's probably best to ensure the same behavior happens in either case. I don't have a strong preference either way but I'm tentatively leaning towards silently adding null headers since it'll make working with heterogeneous topics easier.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520064766", "createdAt": "2020-11-09T19:25:51Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }\n+        for (int i = 0; i < fields.size(); i++) {\n+            String fieldName = fields.get(i);\n+            String headerName = headers.get(i);\n+            Object fieldValue = value.get(fieldName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDI1ODc5OnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTozNTozNVrOHv-knQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTozNTozNVrOHv-knQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3MDMwMQ==", "bodyText": "Does this mean we'd be iterating over every field of every record? Not very performant \ud83d\ude41\nI think we can improve the efficiency here for the COPY case, if not the MOVE case, by reusing the existing value and not creating a new one.\nIn the MOVE case though, it might be the best we have to settle for until/unless there's a richer API added for mutating Connect records and their keys, values, schemas, etc.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520070301", "createdAt": "2020-11-09T19:35:35Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(\"move\"),\n+        COPY(\"copy\");\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case \"move\":\n+                    return MOVE;\n+                case \"copy\":\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();\n+            }\n+        }\n+\n+        public String toString() {\n+            return name;\n+        }\n+    }\n+\n+    private List<String> fields;\n+\n+    private List<String> headers;\n+\n+    private Operation operation;\n+\n+    @Override\n+    public R apply(R record) {\n+        Object operatingValue = operatingValue(record);\n+        Schema operatingSchema = operatingSchema(record);\n+\n+        if (operatingSchema == null) {\n+            return applySchemaless(record, operatingValue);\n+        } else {\n+            return applyWithSchema(record, operatingValue, operatingSchema);\n+        }\n+    }\n+\n+    private R applyWithSchema(R record, Object operatingValue, Schema operatingSchema) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        Struct value = Requirements.requireStruct(operatingValue, \"header \" + operation);\n+        final Schema updatedSchema;\n+        if (operation == Operation.MOVE) {\n+            updatedSchema = moveSchema(operatingSchema);\n+        } else {\n+            updatedSchema = operatingSchema;\n+        }\n+        final Struct updatedValue = new Struct(updatedSchema);\n+        for (Field field : updatedSchema.fields()) {\n+            updatedValue.put(field, value.get(field.name()));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDI4NTY2OnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0Mjo1MVrOHv-0tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0Mjo1MVrOHv-0tA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NDQyMA==", "bodyText": "Looks like this has two different names in the KIP; it's referenced as headers in the textual description but as header.names in the sample config.\nI think headers makes more sense here given that it would align with the properties defined in the InsertHeader and HeaderFrom transformations.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520074420", "createdAt": "2020-11-09T19:42:51Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"header.names\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDI4ODAwOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0MzozMlrOHv-2Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0MzozMlrOHv-2Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NDc4Mg==", "bodyText": "Same question here RE: duplication", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520074782", "createdAt": "2020-11-09T19:43:32Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"header.names\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDI5ODQzOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NjoyOVrOHv-8vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NjoyOVrOHv-8vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjQ3OA==", "bodyText": "I think this is supposed to be parsed using the Values class, which would allow users to specify integral, floating point, boolean, and even structured and nested types here that would then be correctly picked up by the framework and used in the resulting header.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520076478", "createdAt": "2020-11-09T19:46:29Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The literal value that is to be set as the header value on all records.\");\n+\n+    private String header;\n+\n+    private String literalValue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDI5OTA5OnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NjozOFrOHv-9Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NjozOFrOHv-9Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjU2Mg==", "bodyText": "Same question here RE: duplication", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520076562", "createdAt": "2020-11-09T19:46:38Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The literal value that is to be set as the header value on all records.\");\n+\n+    private String header;\n+\n+    private String literalValue;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDMwMTExOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NzoxMVrOHv--UA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NzoxMVrOHv--UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3Njg4MA==", "bodyText": "If we use the Values class to parse the value literal, we can use the schema that it provides instead of hardcoding this to use a string schema.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r520076880", "createdAt": "2020-11-09T19:47:11Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The literal value that is to be set as the header value on all records.\");\n+\n+    private String header;\n+\n+    private String literalValue;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        updatedHeaders.add(header, literalValue, Schema.STRING_SCHEMA);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3OTg3MDY4OnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo1NjozMFrOHy7PGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwOToyNDoxNFrOHzva3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MTM3MA==", "bodyText": "Little silly to run this for every iteration of the parameterized test \ud83d\ude1b\nI'm guessing there isn't an easy way to run this only once?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r523161370", "createdAt": "2020-11-13T18:56:30Z", "author": {"login": "C0urante"}, "path": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.ConnectHeaders;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.connect.data.Schema.STRING_SCHEMA;\n+import static org.junit.Assert.assertEquals;\n+\n+@RunWith(Parameterized.class)\n+public class HeaderFromTest {\n+\n+    private final boolean keyTransform;\n+\n+    static class RecordBuilder {\n+        private final List<String> fields = new ArrayList<>(2);\n+        private final List<Schema> fieldSchemas = new ArrayList<>(2);\n+        private final List<Object> fieldValues = new ArrayList<>(2);\n+        private final ConnectHeaders headers = new ConnectHeaders();\n+\n+        public RecordBuilder() {\n+        }\n+\n+        public RecordBuilder withField(String name, Schema schema, Object value) {\n+            fields.add(name);\n+            fieldSchemas.add(schema);\n+            fieldValues.add(value);\n+            return this;\n+        }\n+\n+        public RecordBuilder addHeader(String name, Schema schema, Object value) {\n+            headers.add(name, new SchemaAndValue(schema, value));\n+            return this;\n+        }\n+\n+        public SourceRecord schemaless(boolean keyTransform) {\n+            Map<String, Object> map = new HashMap<>();\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                map.put(fieldName, this.fieldValues.get(i));\n+\n+            }\n+            return sourceRecord(keyTransform, null, map);\n+        }\n+\n+        private Schema schema() {\n+            SchemaBuilder schemaBuilder = new SchemaBuilder(Schema.Type.STRUCT);\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                schemaBuilder.field(fieldName, this.fieldSchemas.get(i));\n+\n+            }\n+            return schemaBuilder.build();\n+        }\n+\n+        private Struct struct(Schema schema) {\n+            Struct struct = new Struct(schema);\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                struct.put(fieldName, this.fieldValues.get(i));\n+            }\n+            return struct;\n+        }\n+\n+        public SourceRecord withSchema(boolean keyTransform) {\n+            Schema schema = schema();\n+            Struct struct = struct(schema);\n+            return sourceRecord(keyTransform, schema, struct);\n+        }\n+\n+        private SourceRecord sourceRecord(boolean keyTransform, Schema keyOrValueSchema, Object keyOrValue) {\n+            Map<String, ?> sourcePartition = singletonMap(\"foo\", \"bar\");\n+            Map<String, ?> sourceOffset = singletonMap(\"baz\", \"quxx\");\n+            String topic = \"topic\";\n+            Integer partition = 0;\n+            Long timestamp = 0L;\n+\n+            ConnectHeaders headers = this.headers;\n+            if (keyOrValueSchema == null) {\n+                // When doing a schemaless transformation we don't expect the header to have a schema\n+                headers = new ConnectHeaders();\n+                for (Header header : this.headers) {\n+                    headers.add(header.key(), new SchemaAndValue(null, header.value()));\n+                }\n+            }\n+            return new SourceRecord(sourcePartition, sourceOffset, topic, partition,\n+                    keyTransform ? keyOrValueSchema : null,\n+                    keyTransform ? keyOrValue : \"key\",\n+                    !keyTransform ? keyOrValueSchema : null,\n+                    !keyTransform ? keyOrValue : \"value\",\n+                    timestamp, headers);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"RecordBuilder(\" +\n+                    \"fields=\" + fields +\n+                    \", fieldSchemas=\" + fieldSchemas +\n+                    \", fieldValues=\" + fieldValues +\n+                    \", headers=\" + headers +\n+                    ')';\n+        }\n+    }\n+\n+    @Parameterized.Parameters(name = \"{0}: testKey={1}, xformFields={3}, xformHeaders={4}, operation={5}\")\n+    public static Collection<Object[]> data() {\n+\n+        List<Object[]> result = new ArrayList<>();\n+\n+\n+\n+        for (Boolean testKeyTransform : asList(true, false)) {\n+            result.add(\n+                new Object[]{\n+                    \"basic copy\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"basic move\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"copy with preexisting header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"move with preexisting header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            Schema schema = new SchemaBuilder(Schema.Type.STRUCT).field(\"foo\", STRING_SCHEMA).build();\n+            Struct struct = new Struct(schema).put(\"foo\", \"foo-value\");\n+            result.add(\n+                new Object[]{\n+                    \"copy with struct value\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", schema, struct)\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"move with struct value\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", schema, struct)\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"two headers from same field\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    // two headers from the same field\n+                    asList(\"field1\", \"field1\"), asList(\"inserted1\", \"inserted2\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                        .addHeader(\"inserted2\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"two fields to same header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    // two headers from the same field\n+                    asList(\"field1\", \"field2\"), asList(\"inserted1\", \"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 and field2 got moved\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field2-value\")\n+                });\n+        }\n+        return result;\n+    }\n+\n+    private final HeaderFrom<SourceRecord> xform;\n+\n+    private final RecordBuilder originalRecordBuilder;\n+    private final RecordBuilder expectedRecordBuilder;\n+    private final List<String> transformFields;\n+    private final List<String> headers;\n+    private final HeaderFrom.Operation operation;\n+\n+    public HeaderFromTest(String description,\n+                          boolean keyTransform,\n+                          RecordBuilder originalBuilder,\n+                          List<String> transformFields, List<String> headers, HeaderFrom.Operation operation,\n+                          RecordBuilder expectedBuilder) {\n+        this.keyTransform = keyTransform;\n+        this.xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();\n+        this.originalRecordBuilder = originalBuilder;\n+        this.expectedRecordBuilder = expectedBuilder;\n+        this.transformFields = transformFields;\n+        this.headers = headers;\n+        this.operation = operation;\n+    }\n+\n+    private Map<String, Object> config() {\n+        Map<String, Object> result = new HashMap<>();\n+        result.put(HeaderFrom.HEADERS_FIELD, headers);\n+        result.put(HeaderFrom.FIELDS_FIELD, transformFields);\n+        result.put(HeaderFrom.OPERATION_FIELD, operation.toString());\n+        return result;\n+    }\n+\n+    @Test\n+    public void schemaless() {\n+        xform.configure(config());\n+        ConnectHeaders headers = new ConnectHeaders();\n+        headers.addString(\"existing\", \"existing-value\");\n+\n+        SourceRecord originalRecord = originalRecordBuilder.schemaless(keyTransform);\n+        SourceRecord expectedRecord = expectedRecordBuilder.schemaless(keyTransform);\n+        SourceRecord xformed = xform.apply(originalRecord);\n+        assertSameRecord(expectedRecord, xformed);\n+    }\n+\n+    @Test\n+    public void withSchema() {\n+        xform.configure(config());\n+        ConnectHeaders headers = new ConnectHeaders();\n+        headers.addString(\"existing\", \"existing-value\");\n+        Headers expect = headers.duplicate();\n+        for (int i = 0; i < this.headers.size(); i++) {\n+            expect.add(this.headers.get(i), originalRecordBuilder.fieldValues.get(i), originalRecordBuilder.fieldSchemas.get(i));\n+        }\n+\n+        SourceRecord originalRecord = originalRecordBuilder.withSchema(keyTransform);\n+        SourceRecord expectedRecord = expectedRecordBuilder.withSchema(keyTransform);\n+        SourceRecord xformed = xform.apply(originalRecord);\n+        assertSameRecord(expectedRecord, xformed);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void invalidConfig() {\n+        Map<String, Object> config = config();\n+        List<String> headers = new ArrayList<>(this.headers);\n+        headers.add(\"unexpected\");\n+        config.put(HeaderFrom.HEADERS_FIELD, headers);\n+        xform.configure(config);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 341}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDAxNjM0OQ==", "bodyText": "Yeah, parameterized tests do have their faults. AFAIK the only way to handle this using Junit 4 is to have two tests. You can arrange to have both tests in the same source file so both parameterised and non-parameterised tests are still easily discoverable (e.g. using Enclosed), but I've never seen that done in practice and it looks rather fiddly. Since these tests execute pretty quickly, I'm inclined to think we should just live with it. The real solution might just be to adopt Junit 5.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r524016349", "createdAt": "2020-11-16T09:24:14Z", "author": {"login": "tombentley"}, "path": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.ConnectHeaders;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.connect.data.Schema.STRING_SCHEMA;\n+import static org.junit.Assert.assertEquals;\n+\n+@RunWith(Parameterized.class)\n+public class HeaderFromTest {\n+\n+    private final boolean keyTransform;\n+\n+    static class RecordBuilder {\n+        private final List<String> fields = new ArrayList<>(2);\n+        private final List<Schema> fieldSchemas = new ArrayList<>(2);\n+        private final List<Object> fieldValues = new ArrayList<>(2);\n+        private final ConnectHeaders headers = new ConnectHeaders();\n+\n+        public RecordBuilder() {\n+        }\n+\n+        public RecordBuilder withField(String name, Schema schema, Object value) {\n+            fields.add(name);\n+            fieldSchemas.add(schema);\n+            fieldValues.add(value);\n+            return this;\n+        }\n+\n+        public RecordBuilder addHeader(String name, Schema schema, Object value) {\n+            headers.add(name, new SchemaAndValue(schema, value));\n+            return this;\n+        }\n+\n+        public SourceRecord schemaless(boolean keyTransform) {\n+            Map<String, Object> map = new HashMap<>();\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                map.put(fieldName, this.fieldValues.get(i));\n+\n+            }\n+            return sourceRecord(keyTransform, null, map);\n+        }\n+\n+        private Schema schema() {\n+            SchemaBuilder schemaBuilder = new SchemaBuilder(Schema.Type.STRUCT);\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                schemaBuilder.field(fieldName, this.fieldSchemas.get(i));\n+\n+            }\n+            return schemaBuilder.build();\n+        }\n+\n+        private Struct struct(Schema schema) {\n+            Struct struct = new Struct(schema);\n+            for (int i = 0; i < this.fields.size(); i++) {\n+                String fieldName = this.fields.get(i);\n+                struct.put(fieldName, this.fieldValues.get(i));\n+            }\n+            return struct;\n+        }\n+\n+        public SourceRecord withSchema(boolean keyTransform) {\n+            Schema schema = schema();\n+            Struct struct = struct(schema);\n+            return sourceRecord(keyTransform, schema, struct);\n+        }\n+\n+        private SourceRecord sourceRecord(boolean keyTransform, Schema keyOrValueSchema, Object keyOrValue) {\n+            Map<String, ?> sourcePartition = singletonMap(\"foo\", \"bar\");\n+            Map<String, ?> sourceOffset = singletonMap(\"baz\", \"quxx\");\n+            String topic = \"topic\";\n+            Integer partition = 0;\n+            Long timestamp = 0L;\n+\n+            ConnectHeaders headers = this.headers;\n+            if (keyOrValueSchema == null) {\n+                // When doing a schemaless transformation we don't expect the header to have a schema\n+                headers = new ConnectHeaders();\n+                for (Header header : this.headers) {\n+                    headers.add(header.key(), new SchemaAndValue(null, header.value()));\n+                }\n+            }\n+            return new SourceRecord(sourcePartition, sourceOffset, topic, partition,\n+                    keyTransform ? keyOrValueSchema : null,\n+                    keyTransform ? keyOrValue : \"key\",\n+                    !keyTransform ? keyOrValueSchema : null,\n+                    !keyTransform ? keyOrValue : \"value\",\n+                    timestamp, headers);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"RecordBuilder(\" +\n+                    \"fields=\" + fields +\n+                    \", fieldSchemas=\" + fieldSchemas +\n+                    \", fieldValues=\" + fieldValues +\n+                    \", headers=\" + headers +\n+                    ')';\n+        }\n+    }\n+\n+    @Parameterized.Parameters(name = \"{0}: testKey={1}, xformFields={3}, xformHeaders={4}, operation={5}\")\n+    public static Collection<Object[]> data() {\n+\n+        List<Object[]> result = new ArrayList<>();\n+\n+\n+\n+        for (Boolean testKeyTransform : asList(true, false)) {\n+            result.add(\n+                new Object[]{\n+                    \"basic copy\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"basic move\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"copy with preexisting header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"move with preexisting header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            Schema schema = new SchemaBuilder(Schema.Type.STRUCT).field(\"foo\", STRING_SCHEMA).build();\n+            Struct struct = new Struct(schema).put(\"foo\", \"foo-value\");\n+            result.add(\n+                new Object[]{\n+                    \"copy with struct value\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.COPY,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", schema, struct)\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"move with struct value\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", schema, struct)\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    singletonList(\"field1\"), singletonList(\"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", schema, struct)\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"two headers from same field\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    // two headers from the same field\n+                    asList(\"field1\", \"field1\"), asList(\"inserted1\", \"inserted2\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 got moved\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                        .addHeader(\"inserted2\", STRING_SCHEMA, \"field1-value\")\n+                });\n+            result.add(\n+                new Object[]{\n+                    \"two fields to same header\",\n+                    testKeyTransform,\n+                    new RecordBuilder()\n+                        .withField(\"field1\", STRING_SCHEMA, \"field1-value\")\n+                        .withField(\"field2\", STRING_SCHEMA, \"field2-value\")\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\"),\n+                    // two headers from the same field\n+                    asList(\"field1\", \"field2\"), asList(\"inserted1\", \"inserted1\"), HeaderFrom.Operation.MOVE,\n+                    new RecordBuilder()\n+                        // field1 and field2 got moved\n+                        .addHeader(\"header1\", STRING_SCHEMA, \"existing-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field1-value\")\n+                        .addHeader(\"inserted1\", STRING_SCHEMA, \"field2-value\")\n+                });\n+        }\n+        return result;\n+    }\n+\n+    private final HeaderFrom<SourceRecord> xform;\n+\n+    private final RecordBuilder originalRecordBuilder;\n+    private final RecordBuilder expectedRecordBuilder;\n+    private final List<String> transformFields;\n+    private final List<String> headers;\n+    private final HeaderFrom.Operation operation;\n+\n+    public HeaderFromTest(String description,\n+                          boolean keyTransform,\n+                          RecordBuilder originalBuilder,\n+                          List<String> transformFields, List<String> headers, HeaderFrom.Operation operation,\n+                          RecordBuilder expectedBuilder) {\n+        this.keyTransform = keyTransform;\n+        this.xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();\n+        this.originalRecordBuilder = originalBuilder;\n+        this.expectedRecordBuilder = expectedBuilder;\n+        this.transformFields = transformFields;\n+        this.headers = headers;\n+        this.operation = operation;\n+    }\n+\n+    private Map<String, Object> config() {\n+        Map<String, Object> result = new HashMap<>();\n+        result.put(HeaderFrom.HEADERS_FIELD, headers);\n+        result.put(HeaderFrom.FIELDS_FIELD, transformFields);\n+        result.put(HeaderFrom.OPERATION_FIELD, operation.toString());\n+        return result;\n+    }\n+\n+    @Test\n+    public void schemaless() {\n+        xform.configure(config());\n+        ConnectHeaders headers = new ConnectHeaders();\n+        headers.addString(\"existing\", \"existing-value\");\n+\n+        SourceRecord originalRecord = originalRecordBuilder.schemaless(keyTransform);\n+        SourceRecord expectedRecord = expectedRecordBuilder.schemaless(keyTransform);\n+        SourceRecord xformed = xform.apply(originalRecord);\n+        assertSameRecord(expectedRecord, xformed);\n+    }\n+\n+    @Test\n+    public void withSchema() {\n+        xform.configure(config());\n+        ConnectHeaders headers = new ConnectHeaders();\n+        headers.addString(\"existing\", \"existing-value\");\n+        Headers expect = headers.duplicate();\n+        for (int i = 0; i < this.headers.size(); i++) {\n+            expect.add(this.headers.get(i), originalRecordBuilder.fieldValues.get(i), originalRecordBuilder.fieldSchemas.get(i));\n+        }\n+\n+        SourceRecord originalRecord = originalRecordBuilder.withSchema(keyTransform);\n+        SourceRecord expectedRecord = expectedRecordBuilder.withSchema(keyTransform);\n+        SourceRecord xformed = xform.apply(originalRecord);\n+        assertSameRecord(expectedRecord, xformed);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void invalidConfig() {\n+        Map<String, Object> config = config();\n+        List<String> headers = new ArrayList<>(this.headers);\n+        headers.add(\"unexpected\");\n+        config.put(HeaderFrom.HEADERS_FIELD, headers);\n+        xform.configure(config);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MTM3MA=="}, "originalCommit": null, "originalPosition": 341}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxMjM1MDkzOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxMjozMTowNVrOIxAmzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxMjozMTowNVrOIxAmzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODI2MTA3MA==", "bodyText": "Unused import", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588261070", "createdAt": "2021-03-05T12:31:05Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxMjM2MjM5OnYy", "diffSide": "RIGHT", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/tools/TransformationDoc.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxMjozNDo1NVrOIxAt6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxMjozNDo1NVrOIxAt6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODI2Mjg4OA==", "bodyText": "We don't have this transformation!", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588262888", "createdAt": "2021-03-05T12:34:55Z", "author": {"login": "mimaison"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/tools/TransformationDoc.java", "diffHunk": "@@ -18,11 +18,15 @@\n \n import org.apache.kafka.common.config.ConfigDef;\n import org.apache.kafka.connect.transforms.Cast;\n+import org.apache.kafka.connect.transforms.DropHeaders;\n import org.apache.kafka.connect.transforms.ExtractField;\n import org.apache.kafka.connect.transforms.Filter;\n import org.apache.kafka.connect.transforms.Flatten;\n+import org.apache.kafka.connect.transforms.HeaderFrom;\n+import org.apache.kafka.connect.transforms.HeaderTo;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxMjc0Njg1OnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDowOTozM1rOIxETdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDowOTozM1rOIxETdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMyMTY1Mw==", "bodyText": "Because Headers is a LinkedList, remove() has to iterate the whole list each time. I wonder if we could instead start from an empty headers list and add the headers not being removed?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588321653", "createdAt": "2021-03-05T14:09:33Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"headers\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        for (String name : headers) {\n+            updatedHeaders.remove(name);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxMjc0NzUzOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDowOTo0MlrOIxETzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDowOTo0MlrOIxETzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMyMTc0Mw==", "bodyText": "nit, extra line", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588321743", "createdAt": "2021-03-05T14:09:42Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"headers\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        for (String name : headers) {\n+            updatedHeaders.remove(name);\n+        }\n+        return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+                record.valueSchema(), record.value(), record.timestamp(), updatedHeaders);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxMjc0Nzg1OnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDowOTo0OFrOIxEUBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDowOTo0OFrOIxEUBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMyMTc5Ng==", "bodyText": "nit, extra line", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588321796", "createdAt": "2021-03-05T14:09:48Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class DropHeaders<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Removes one or more headers from each record.\";\n+\n+    public static final String HEADERS_FIELD = \"headers\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"The name of the headers to be removed.\");\n+\n+    private List<String> headers;\n+\n+    @Override\n+    public R apply(R record) {\n+        Headers updatedHeaders = record.headers().duplicate();\n+        for (String name : headers) {\n+            updatedHeaders.remove(name);\n+        }\n+        return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+                record.valueSchema(), record.value(), record.timestamp(), updatedHeaders);\n+    }\n+\n+\n+    @Override\n+    public ConfigDef config() {\n+        return CONFIG_DEF;\n+    }\n+\n+    @Override\n+    public void close() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxMjgzOTUwOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDoyOTo1NVrOIxFMYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0wN1QxMDo1MToyMFrOJEWtqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMzNjIyNw==", "bodyText": "Should we enforce these fields are not null?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588336227", "createdAt": "2021-03-05T14:29:55Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.Values;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwODU0NjIxNw==", "bodyText": "I guess that makes sense @mimaison, but then for consistency shouldn't we make DropHeaders.headers, HeaderFrom.headers and HeaderFrom.fields reject empty lists?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r608546217", "createdAt": "2021-04-07T10:51:20Z", "author": {"login": "tombentley"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.Values;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.Map;\n+\n+public class InsertHeader<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Add a header to each record.\";\n+\n+    public static final String HEADER_FIELD = \"header\";\n+    public static final String VALUE_LITERAL_FIELD = \"value.literal\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(HEADER_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n+                    \"The name of the header.\")\n+            .define(VALUE_LITERAL_FIELD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMzNjIyNw=="}, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxMjg1MjMyOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDozMjo1MVrOIxFUaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQxNDozMjo1MVrOIxFUaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODMzODI4MA==", "bodyText": "Should we reuse MOVE_OPERATION and COPY_OPERATION here?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r588338280", "createdAt": "2021-03-05T14:32:51Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.lang.String.format;\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+    private static final String MOVE_OPERATION = \"move\";\n+    private static final String COPY_OPERATION = \"copy\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(\"move\", \"copy\"), ConfigDef.Importance.HIGH,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a79ffde1159c709dd1b7563c1ed6359a762685b"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NDA1NzAzOnYy", "diffSide": "RIGHT", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQwOTozMjo1MFrOJJeVLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQwOTo1Nzo1NVrOJJfbsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzkxMzkwMQ==", "bodyText": "Even though I don't think it's reachable by users, should we have a message here?", "url": "https://github.com/apache/kafka/pull/9549#discussion_r613913901", "createdAt": "2021-04-15T09:32:50Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.NonEmptyListValidator;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.lang.String.format;\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+    private static final String MOVE_OPERATION = \"move\";\n+    private static final String COPY_OPERATION = \"copy\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(MOVE_OPERATION, COPY_OPERATION), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(MOVE_OPERATION),\n+        COPY(COPY_OPERATION);\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case MOVE_OPERATION:\n+                    return MOVE;\n+                case COPY_OPERATION:\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzkyNjEwNw==", "bodyText": "It is impossible due to the ConfigDef.ValidString.in(MOVE_OPERATION, COPY_OPERATION), so this is really an assertion failure. The line number in the stacktrace would be enough to track it down if it ever did happen due to a later refactoring, so imho an error message is of no value. But I'm happy to add one if you like.", "url": "https://github.com/apache/kafka/pull/9549#discussion_r613926107", "createdAt": "2021-04-15T09:49:38Z", "author": {"login": "tombentley"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.NonEmptyListValidator;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.lang.String.format;\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+    private static final String MOVE_OPERATION = \"move\";\n+    private static final String COPY_OPERATION = \"copy\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(MOVE_OPERATION, COPY_OPERATION), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(MOVE_OPERATION),\n+        COPY(COPY_OPERATION);\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case MOVE_OPERATION:\n+                    return MOVE;\n+                case COPY_OPERATION:\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzkxMzkwMQ=="}, "originalCommit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzkzMTk1NQ==", "bodyText": "Ok, that's fair enough. Thanks", "url": "https://github.com/apache/kafka/pull/9549#discussion_r613931955", "createdAt": "2021-04-15T09:57:55Z", "author": {"login": "mimaison"}, "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.header.Headers;\n+import org.apache.kafka.connect.transforms.util.NonEmptyListValidator;\n+import org.apache.kafka.connect.transforms.util.Requirements;\n+import org.apache.kafka.connect.transforms.util.SchemaUtil;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.lang.String.format;\n+import static org.apache.kafka.common.config.ConfigDef.NO_DEFAULT_VALUE;\n+\n+public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    public static final String FIELDS_FIELD = \"fields\";\n+    public static final String HEADERS_FIELD = \"headers\";\n+    public static final String OPERATION_FIELD = \"operation\";\n+    private static final String MOVE_OPERATION = \"move\";\n+    private static final String COPY_OPERATION = \"copy\";\n+\n+    public static final String OVERVIEW_DOC =\n+            \"Moves or copies fields in the key/value of a record into that record's headers. \" +\n+                    \"Corresponding elements of <code>\" + FIELDS_FIELD + \"</code> and \" +\n+                    \"<code>\" + HEADERS_FIELD + \"</code> together identify a field and the header it should be \" +\n+                    \"moved or copied to. \" +\n+                    \"Use the concrete transformation type designed for the record \" +\n+                    \"key (<code>\" + Key.class.getName() + \"</code>) or value (<code>\" + Value.class.getName() + \"</code>).\";\n+\n+    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n+            .define(FIELDS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Field names in the record whose values are to be copied or moved to headers.\")\n+            .define(HEADERS_FIELD, ConfigDef.Type.LIST,\n+                    NO_DEFAULT_VALUE, new NonEmptyListValidator(),\n+                    ConfigDef.Importance.HIGH,\n+                    \"Header names, in the same order as the field names listed in the fields configuration property.\")\n+            .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,\n+                    ConfigDef.ValidString.in(MOVE_OPERATION, COPY_OPERATION), ConfigDef.Importance.HIGH,\n+                    \"Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), \" +\n+                            \"or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).\");\n+\n+    enum Operation {\n+        MOVE(MOVE_OPERATION),\n+        COPY(COPY_OPERATION);\n+\n+        private final String name;\n+\n+        Operation(String name) {\n+            this.name = name;\n+        }\n+\n+        static Operation fromName(String name) {\n+            switch (name) {\n+                case MOVE_OPERATION:\n+                    return MOVE;\n+                case COPY_OPERATION:\n+                    return COPY;\n+                default:\n+                    throw new IllegalArgumentException();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzkxMzkwMQ=="}, "originalCommit": {"oid": "a211f1e5003d31c87c678f50e6b56fb0ffc84388"}, "originalPosition": 90}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3784, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}