{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDczMzYxOTk2", "number": 9219, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDozNTo0MlrOEcWCaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDozNTo0MlrOEcWCaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MTU2NjQ5OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogSegmentTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDozNTo0MlrOHG0pHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDo0OTowMVrOHG1MUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkxNTk5Nw==", "bodyText": "nit: use arg names consistently?", "url": "https://github.com/apache/kafka/pull/9219#discussion_r476915997", "createdAt": "2020-08-26T00:35:42Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/log/LogSegmentTest.scala", "diffHunk": "@@ -367,6 +371,45 @@ class LogSegmentTest {\n     assertEquals(100L, abortedTxn.lastStableOffset)\n   }\n \n+  /**\n+   * Create a segment with some data, then recover the segment.\n+   * The epoch cache entries should reflect the segment.\n+   */\n+  @Test\n+  def testRecoveryRebuildsEpochCache(): Unit = {\n+    val seg = createSegment(0)\n+\n+    val checkpoint: LeaderEpochCheckpoint = new LeaderEpochCheckpoint {\n+      private var epochs = Seq.empty[EpochEntry]\n+\n+      override def write(epochs: Seq[EpochEntry]): Unit = {\n+        this.epochs = epochs.toVector\n+      }\n+\n+      override def read(): Seq[EpochEntry] = this.epochs\n+    }\n+\n+    val cache = new LeaderEpochFileCache(topicPartition, () => seg.readNextOffset, checkpoint)\n+    seg.append(largestOffset = 105L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 104L, MemoryRecords.withRecords(104L, CompressionType.NONE, 0,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.append(largestOffset = 107L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 106L, MemoryRecords.withRecords(106L, CompressionType.NONE, 1,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.append(largestOffset = 109L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 108L, MemoryRecords.withRecords(108L, CompressionType.NONE, 1,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.append(largestOffset = 111L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 110, MemoryRecords.withRecords(110L, CompressionType.NONE, 2,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.recover(new ProducerStateManager(topicPartition, logDir), Some(cache))\n+    assertEquals(ArrayBuffer(EpochEntry(0, 104L), EpochEntry(epoch=1, startOffset=106), EpochEntry(epoch=2, startOffset=110)), cache.epochEntries)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9743bc03653a1fbb42795a7beeca4a9e1fae5919"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNTAwOA==", "bodyText": "Fixed", "url": "https://github.com/apache/kafka/pull/9219#discussion_r476925008", "createdAt": "2020-08-26T00:49:01Z", "author": {"login": "lbradstreet"}, "path": "core/src/test/scala/unit/kafka/log/LogSegmentTest.scala", "diffHunk": "@@ -367,6 +371,45 @@ class LogSegmentTest {\n     assertEquals(100L, abortedTxn.lastStableOffset)\n   }\n \n+  /**\n+   * Create a segment with some data, then recover the segment.\n+   * The epoch cache entries should reflect the segment.\n+   */\n+  @Test\n+  def testRecoveryRebuildsEpochCache(): Unit = {\n+    val seg = createSegment(0)\n+\n+    val checkpoint: LeaderEpochCheckpoint = new LeaderEpochCheckpoint {\n+      private var epochs = Seq.empty[EpochEntry]\n+\n+      override def write(epochs: Seq[EpochEntry]): Unit = {\n+        this.epochs = epochs.toVector\n+      }\n+\n+      override def read(): Seq[EpochEntry] = this.epochs\n+    }\n+\n+    val cache = new LeaderEpochFileCache(topicPartition, () => seg.readNextOffset, checkpoint)\n+    seg.append(largestOffset = 105L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 104L, MemoryRecords.withRecords(104L, CompressionType.NONE, 0,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.append(largestOffset = 107L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 106L, MemoryRecords.withRecords(106L, CompressionType.NONE, 1,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.append(largestOffset = 109L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 108L, MemoryRecords.withRecords(108L, CompressionType.NONE, 1,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.append(largestOffset = 111L, largestTimestamp = RecordBatch.NO_TIMESTAMP,\n+      shallowOffsetOfMaxTimestamp = 110, MemoryRecords.withRecords(110L, CompressionType.NONE, 2,\n+        new SimpleRecord(\"a\".getBytes), new SimpleRecord(\"b\".getBytes)))\n+\n+    seg.recover(new ProducerStateManager(topicPartition, logDir), Some(cache))\n+    assertEquals(ArrayBuffer(EpochEntry(0, 104L), EpochEntry(epoch=1, startOffset=106), EpochEntry(epoch=2, startOffset=110)), cache.epochEntries)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkxNTk5Nw=="}, "originalCommit": {"oid": "9743bc03653a1fbb42795a7beeca4a9e1fae5919"}, "originalPosition": 58}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1952, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}