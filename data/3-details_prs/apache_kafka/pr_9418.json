{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxODY3Mjcy", "number": 9418, "title": "KAFKA-10601; Add support for append linger to Raft implementation", "bodyText": "The main purpose of this patch is to add quorum.linger.ms behavior to the raft implementation. This gives users a powerful knob to tune the impact of fsync.  When an append is accepted from the state machine, it is held in an accumulator (similar to the producer) until the configured linger time is exceeded. This allows the implementation to amortize fsync overhead at the expense of some write latency.\nThe patch also improves our methodology for testing performance. Up to now, we have relied on the producer performance test, but it is difficult to simulate expected controller loads because producer performance is limited by other factors such as the number of producer clients and head-of-line blocking. Instead, this patch adds a workload generator which runs on the leader after election.\nFinally, this patch brings us nearer to the write semantics expected by the KIP-500 controller. It makes the following changes:\n\nIntroduce RecordSerde<T> interface which abstracts the underlying log implementation from RaftClient. The generic type is carried over to RaftClient<T> and is exposed through the read/write APIs.\nRaftClient.append is changed to RaftClient.scheduleAppend and returns the last offset of the expected log append.\nRaftClient.scheduleAppend accepts a list of records and ensures that the full set are included in a single batch.\nIntroduce RaftClient.Listener with a single handleCommit API which will eventually replace RaftClient.read in order to surface committed data to the controller state machine. Currently handleCommit is only used for records appended by the leader.\n\n(Note that this patch piggybacks on top of #9352. I will rebase as soon as this PR has been merged.)", "createdAt": "2020-10-12T23:06:10Z", "url": "https://github.com/apache/kafka/pull/9418", "merged": true, "mergeCommit": {"oid": "927edfece3db8aab7d01850955f9a65e5c110da5"}, "closed": true, "closedAt": "2020-10-27T19:10:14Z", "author": {"login": "hachikuji"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdR8b0LABqjM4Njg2MjUzOTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdYGmSUAFqTUyMTE4NzUxMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3OTI5NDYx", "url": "https://github.com/apache/kafka/pull/9418#pullrequestreview-507929461", "createdAt": "2020-10-14T01:13:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMToxMzo0OFrOHg-uQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMToxMzo0OFrOHg-uQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NDEyOA==", "bodyText": "fix typo", "url": "https://github.com/apache/kafka/pull/9418#discussion_r504344128", "createdAt": "2020-10-14T01:13:48Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -19,33 +19,51 @@\n import org.apache.kafka.common.record.Records;\n \n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked when records written through {@link #scheduleAppend(int, List)}\n+         * become committed.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch passed to {@link #handleCommit(int, long, List)}.\n+         *\n+         * @param epoch the epoch in which the write was accepted\n+         * @param lastOffset the last offset of the recor", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4ODIyODE3", "url": "https://github.com/apache/kafka/pull/9418#pullrequestreview-508822817", "createdAt": "2020-10-14T22:45:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMjo0NToyMlrOHho9Hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMjo1MDoxMVrOHhpLPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTAzNjA2Mw==", "bodyText": "How about either\nOptionalLong scheduleAppend(int epoch, List<T> records);\n\nor\nvoid scheduleAppend(int epoch, List<T> records) throws BusyException;\n\nI am okay with either solution but I am wondering why did you decide to return a null for this case instead of throwing some exception?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r505036063", "createdAt": "2020-10-14T22:45:22Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -19,33 +19,51 @@\n import org.apache.kafka.common.record.Records;\n \n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked when records written through {@link #scheduleAppend(int, List)}\n+         * become committed.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch passed to {@link #handleCommit(int, long, List)}.\n+         *\n+         * @param epoch the epoch in which the write was accepted\n+         * @param lastOffset the offset of the last record in the record list\n+         * @param records the set of records that were committed\n+         */\n+        void handleCommit(int epoch, long lastOffset, List<T> records);\n+    }\n \n     /**\n      * Initialize the client. This should only be called once and it must be\n      * called before any of the other APIs can be invoked.\n      *\n      * @throws IOException For any IO errors during initialization\n      */\n-    void initialize() throws IOException;\n+    void initialize(Listener<T> listener) throws IOException;\n \n     /**\n-     * Append a new entry to the log. The client must be in the leader state to\n-     * accept an append: it is up to the state machine implementation\n-     * to ensure this using {@link #currentLeaderAndEpoch()}.\n-     *\n-     * TODO: One improvement we can make here is to allow the caller to specify\n-     * the current leader epoch in the record set. That would ensure that each\n-     * leader change must be \"observed\" by the state machine before new appends\n-     * are accepted.\n+     * Append a list of records to the log. The write will be scheduled for some time\n+     * in the future. There is no guarantee that appended records will be written to\n+     * the log and eventually committed. However, it is guaranteed that if any of the\n+     * records become committed, then all of them will be.\n      *\n-     * @param records The records to append to the log\n-     * @param timeoutMs Maximum time to wait for the append to complete\n-     * @return A future containing the last offset and epoch of the appended records (if successful)\n+     * @param epoch the current leader epoch\n+     * @param records the list of records to append\n+     * @return the offset within the current epoch that the log entries will be appended,\n+     *         or null if the leader was unable to accept the write (e.g. due to memory\n+     *         being reached).\n      */\n-    CompletableFuture<OffsetAndEpoch> append(Records records, AckMode ackMode, long timeoutMs);\n+    Long scheduleAppend(int epoch, List<T> records);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTAzOTY3Ng==", "bodyText": "I vote yes for this.\nI think if we use this for writing snapshot from the state machine, then minimum size is a more interesting metrics for flushing to disk vs lingerMs.\nIf we implement this so that either one has to be true then the client can set the lingerMs or minSize to MAX_VALUE if it wants to ignore those values.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r505039676", "createdAt": "2020-10-14T22:50:11Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzMjE3MzMz", "url": "https://github.com/apache/kafka/pull/9418#pullrequestreview-513217333", "createdAt": "2020-10-20T23:59:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMzo1OTo0NVrOHlVc2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwMDo1MDo1MlrOHlWVFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMDgxMA==", "bodyText": "nit: not clear why we want an intermediate class instead of just having a DataOutputStreamWritable class and its constructor parameter declared as DataOutputStream?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508910810", "createdAt": "2020-10-20T23:59:45Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataOutputWritable.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.utils.ByteUtils;\n+import org.apache.kafka.common.utils.Utils;\n+\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class DataOutputWritable implements Writable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMTYxOQ==", "bodyText": "I vaguely remember @rajinisivaram has another PR which may be overlapping with this fix, will let her to check and confirm if it is fine.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508911619", "createdAt": "2020-10-21T00:02:16Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/common/RecordValidationException.scala", "diffHunk": "@@ -23,5 +23,6 @@ import org.apache.kafka.common.requests.ProduceResponse.RecordError\n import scala.collection.Seq\n \n class RecordValidationException(val invalidException: ApiException,\n-                                val recordErrors: Seq[RecordError]) extends RuntimeException {\n+                                val recordErrors: Seq[RecordError])\n+  extends RuntimeException(invalidException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw==", "bodyText": "nit: maybe add a comment indicating this function is for testing only (and hence we do not care about the correlation id).", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508913087", "createdAt": "2020-10-21T00:07:48Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala", "diffHunk": "@@ -216,11 +216,9 @@ class KafkaNetworkChannel(time: Time,\n     endpoints.put(id, node)\n   }\n \n-  def postInboundRequest(header: RequestHeader,\n-                         request: AbstractRequest,\n-                         onResponseReceived: ResponseHandler): Unit = {\n+  def postInboundRequest(request: AbstractRequest, onResponseReceived: ResponseHandler): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzk1MQ==", "bodyText": "Could you elaborate a bit why we would not expect these api keys in the test handler any more?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508913951", "createdAt": "2020-10-21T00:10:43Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/tools/TestRaftRequestHandler.scala", "diffHunk": "@@ -56,73 +47,8 @@ class TestRaftRequestHandler(\n              | ApiKeys.END_QUORUM_EPOCH\n              | ApiKeys.FETCH =>\n           val requestBody = request.body[AbstractRequest]\n-          networkChannel.postInboundRequest(\n-            request.header,\n-            requestBody,\n-            response => sendResponse(request, Some(response)))\n-\n-        case ApiKeys.API_VERSIONS =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNTIzNg==", "bodyText": "If offset == Long.MaxValue would sleeping save us anything? Should we terminate the testing server then?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508915236", "createdAt": "2020-10-21T00:15:11Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -272,7 +291,79 @@ class TestRaftServer(val config: KafkaConfig) extends Logging {\n     )\n   }\n \n-  class RaftIoThread(client: KafkaRaftClient) extends ShutdownableThread(\"raft-io-thread\") {\n+  class RaftWorkloadGenerator(\n+    client: KafkaRaftClient[Array[Byte]],\n+    time: Time,\n+    brokerId: Int,\n+    recordsPerSec: Int,\n+    recordSize: Int\n+  ) extends ShutdownableThread(name = \"raft-workload-generator\") with RaftClient.Listener[Array[Byte]] {\n+\n+    private val stats = new WriteStats(time, printIntervalMs = 5000)\n+    private val payload = new Array[Byte](recordSize)\n+    private val pendingAppends = new util.ArrayDeque[PendingAppend]()\n+\n+    private var latestLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty, 0)\n+    private var isLeader = false\n+    private var throttler: ThroughputThrottler = _\n+    private var recordCount = 0\n+\n+    override def doWork(): Unit = {\n+      if (latestLeaderAndEpoch != client.currentLeaderAndEpoch()) {\n+        latestLeaderAndEpoch = client.currentLeaderAndEpoch()\n+        isLeader = latestLeaderAndEpoch.leaderId.orElse(-1) == brokerId\n+        if (isLeader) {\n+          pendingAppends.clear()\n+          throttler = new ThroughputThrottler(time, recordsPerSec)\n+          recordCount = 0\n+        }\n+      }\n+\n+      if (isLeader) {\n+        recordCount += 1\n+\n+        val startTimeMs = time.milliseconds()\n+        val sendTimeMs = if (throttler.maybeThrottle(recordCount, startTimeMs)) {\n+          time.milliseconds()\n+        } else {\n+          startTimeMs\n+        }\n+\n+        val offset = client.scheduleAppend(latestLeaderAndEpoch.epoch, Collections.singletonList(payload))\n+        if (offset == null || offset == Long.MaxValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNjAwMw==", "bodyText": "Should this ever happen? If not, then we could shutdown the server after logging the error.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508916003", "createdAt": "2020-10-21T00:17:55Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -272,7 +291,79 @@ class TestRaftServer(val config: KafkaConfig) extends Logging {\n     )\n   }\n \n-  class RaftIoThread(client: KafkaRaftClient) extends ShutdownableThread(\"raft-io-thread\") {\n+  class RaftWorkloadGenerator(\n+    client: KafkaRaftClient[Array[Byte]],\n+    time: Time,\n+    brokerId: Int,\n+    recordsPerSec: Int,\n+    recordSize: Int\n+  ) extends ShutdownableThread(name = \"raft-workload-generator\") with RaftClient.Listener[Array[Byte]] {\n+\n+    private val stats = new WriteStats(time, printIntervalMs = 5000)\n+    private val payload = new Array[Byte](recordSize)\n+    private val pendingAppends = new util.ArrayDeque[PendingAppend]()\n+\n+    private var latestLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty, 0)\n+    private var isLeader = false\n+    private var throttler: ThroughputThrottler = _\n+    private var recordCount = 0\n+\n+    override def doWork(): Unit = {\n+      if (latestLeaderAndEpoch != client.currentLeaderAndEpoch()) {\n+        latestLeaderAndEpoch = client.currentLeaderAndEpoch()\n+        isLeader = latestLeaderAndEpoch.leaderId.orElse(-1) == brokerId\n+        if (isLeader) {\n+          pendingAppends.clear()\n+          throttler = new ThroughputThrottler(time, recordsPerSec)\n+          recordCount = 0\n+        }\n+      }\n+\n+      if (isLeader) {\n+        recordCount += 1\n+\n+        val startTimeMs = time.milliseconds()\n+        val sendTimeMs = if (throttler.maybeThrottle(recordCount, startTimeMs)) {\n+          time.milliseconds()\n+        } else {\n+          startTimeMs\n+        }\n+\n+        val offset = client.scheduleAppend(latestLeaderAndEpoch.epoch, Collections.singletonList(payload))\n+        if (offset == null || offset == Long.MaxValue) {\n+          time.sleep(10)\n+        } else {\n+          pendingAppends.offer(PendingAppend(latestLeaderAndEpoch.epoch, offset, sendTimeMs))\n+        }\n+      } else {\n+        time.sleep(500)\n+      }\n+    }\n+\n+    override def handleCommit(epoch: Int, lastOffset: Long, records: util.List[Array[Byte]]): Unit = {\n+      var offset = lastOffset - records.size() + 1\n+      val currentTimeMs = time.milliseconds()\n+\n+      for (record <- records.asScala) {\n+        val pendingAppend = pendingAppends.poll()\n+        if (pendingAppend.epoch != epoch || pendingAppend.offset!= offset) {\n+          warn(s\"Expected next commit at offset ${pendingAppend.offset}, \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNjY1Nw==", "bodyText": "Why we need to import the inner classes?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508916657", "createdAt": "2020-10-21T00:20:11Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -19,37 +19,44 @@ package kafka.tools\n \n import java.io.File\n import java.nio.file.Files\n-import java.util.concurrent.CountDownLatch\n-import java.util.{Properties, Random}\n+import java.util\n+import java.util.concurrent.{CountDownLatch, TimeUnit}\n+import java.util.{Collections, OptionalInt, Random}\n \n-import joptsimple.OptionParser\n+import com.yammer.metrics.core.MetricName\n+import joptsimple.OptionException\n import kafka.log.{Log, LogConfig, LogManager}\n import kafka.network.SocketServer\n import kafka.raft.{KafkaFuturePurgatory, KafkaMetadataLog, KafkaNetworkChannel}\n import kafka.security.CredentialProvider\n import kafka.server.{BrokerTopicStats, KafkaConfig, KafkaRequestHandlerPool, KafkaServer, LogDirFailureChannel}\n+import kafka.tools.TestRaftServer.{ByteArraySerde, PendingAppend, ThroughputThrottler, WriteStats}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxODE5MA==", "bodyText": "Even if the leader has changed back, previously pending appends may still get committed right? I think it is sufficient to just poll-and-drop them in handleCommit when leader changes and not clearing the queue here.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508918190", "createdAt": "2020-10-21T00:25:44Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -272,7 +291,79 @@ class TestRaftServer(val config: KafkaConfig) extends Logging {\n     )\n   }\n \n-  class RaftIoThread(client: KafkaRaftClient) extends ShutdownableThread(\"raft-io-thread\") {\n+  class RaftWorkloadGenerator(\n+    client: KafkaRaftClient[Array[Byte]],\n+    time: Time,\n+    brokerId: Int,\n+    recordsPerSec: Int,\n+    recordSize: Int\n+  ) extends ShutdownableThread(name = \"raft-workload-generator\") with RaftClient.Listener[Array[Byte]] {\n+\n+    private val stats = new WriteStats(time, printIntervalMs = 5000)\n+    private val payload = new Array[Byte](recordSize)\n+    private val pendingAppends = new util.ArrayDeque[PendingAppend]()\n+\n+    private var latestLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty, 0)\n+    private var isLeader = false\n+    private var throttler: ThroughputThrottler = _\n+    private var recordCount = 0\n+\n+    override def doWork(): Unit = {\n+      if (latestLeaderAndEpoch != client.currentLeaderAndEpoch()) {\n+        latestLeaderAndEpoch = client.currentLeaderAndEpoch()\n+        isLeader = latestLeaderAndEpoch.leaderId.orElse(-1) == brokerId\n+        if (isLeader) {\n+          pendingAppends.clear()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxOTMzNg==", "bodyText": "nit: appendLingerMs?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508919336", "createdAt": "2020-10-21T00:29:49Z", "author": {"login": "guozhangwang"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -148,36 +162,44 @@ public KafkaRaftClient(RaftConfig raftConfig,\n             raftConfig.retryBackoffMs(),\n             raftConfig.requestTimeoutMs(),\n             1000,\n+            raftConfig.lingerMs(),\n             logContext,\n             new Random());\n     }\n \n-    public KafkaRaftClient(NetworkChannel channel,\n-                           ReplicatedLog log,\n-                           QuorumState quorum,\n-                           Time time,\n-                           Metrics metrics,\n-                           FuturePurgatory<LogOffset> fetchPurgatory,\n-                           FuturePurgatory<LogOffset> appendPurgatory,\n-                           Map<Integer, InetSocketAddress> voterAddresses,\n-                           int electionBackoffMaxMs,\n-                           int retryBackoffMs,\n-                           int requestTimeoutMs,\n-                           int fetchMaxWaitMs,\n-                           LogContext logContext,\n-                           Random random) {\n+    public KafkaRaftClient(\n+        RecordSerde<T> serde,\n+        NetworkChannel channel,\n+        ReplicatedLog log,\n+        QuorumState quorum,\n+        MemoryPool memoryPool,\n+        Time time,\n+        Metrics metrics,\n+        FuturePurgatory<LogOffset> fetchPurgatory,\n+        FuturePurgatory<LogOffset> appendPurgatory,\n+        Map<Integer, InetSocketAddress> voterAddresses,\n+        int electionBackoffMaxMs,\n+        int retryBackoffMs,\n+        int requestTimeoutMs,\n+        int fetchMaxWaitMs,\n+        int lingerMs,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMTQ2NQ==", "bodyText": "For now since we only have a single thread processing all incoming req/resp, this is okay; but when we multi-thread processing requests this would no longer be safe, since it is possible that some batches gets replicated and committed while not being flushed locally yet.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508921465", "createdAt": "2020-10-21T00:37:10Z", "author": {"login": "guozhangwang"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1443,15 +1485,79 @@ private void pollShutdown(GracefulShutdown shutdown) throws IOException {\n         }\n     }\n \n+    private void appendBatch(\n+        LeaderState state,\n+        BatchAccumulator.CompletedBatch<T> batch,\n+        long appendTimeMs\n+    ) {\n+        try {\n+            List<T> records = batch.records;\n+            int epoch = state.epoch();\n+\n+            LogAppendInfo info = appendAsLeader(batch.data);\n+            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n+            CompletableFuture<Long> future = appendPurgatory.await(\n+                LogOffset.awaitCommitted(offsetAndEpoch.offset),\n+                Integer.MAX_VALUE\n+            );\n+\n+            future.whenComplete((commitTimeMs, exception) -> {\n+                int numRecords = batch.records.size();\n+                if (exception != null) {\n+                    logger.debug(\"Failed to commit {} records at {}\", numRecords, offsetAndEpoch, exception);\n+                } else {\n+                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n+                    double elapsedTimePerRecord = (double) elapsedTime / numRecords;\n+                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n+                    logger.debug(\"Completed commit of {} records at {}\", numRecords, offsetAndEpoch);\n+                    listener.handleCommit(epoch, info.lastOffset, records);\n+                }\n+            });\n+        } finally {\n+            batch.release();\n+        }\n+    }\n+\n+    private long maybeAppendBatches(\n+        LeaderState state,\n+        long currentTimeMs\n+    ) {\n+        long timeUnitFlush = accumulator.timeUntilFlush(currentTimeMs);\n+        if (timeUnitFlush <= 0) {\n+            List<BatchAccumulator.CompletedBatch<T>> batches = accumulator.flush();\n+            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n+\n+            try {\n+                while (iterator.hasNext()) {\n+                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n+                    appendBatch(state, batch, currentTimeMs);\n+                }\n+                flushLeaderLog(state, currentTimeMs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMjA2NA==", "bodyText": "Why return MAX_VALUE instead of null here? If we want to use null to indicate memory full and use MAX_VALUE to indicate not leader, the javadoc should reflecting this.\nAnyways, I think returning sth like a combo(Offset, ErrorCode, backoffMs) would be preferred in the end state.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508922064", "createdAt": "2020-10-21T00:39:31Z", "author": {"login": "guozhangwang"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1605,100 +1708,18 @@ public void poll() throws IOException {\n         }\n     }\n \n-    private void failPendingAppends(KafkaException exception) {\n-        for (UnwrittenAppend unwrittenAppend : unwrittenAppends) {\n-            unwrittenAppend.fail(exception);\n-        }\n-        unwrittenAppends.clear();\n-    }\n-\n-    private void pollPendingAppends(LeaderState state, long currentTimeMs) {\n-        int numAppends = 0;\n-        int maxNumAppends = unwrittenAppends.size();\n-\n-        while (!unwrittenAppends.isEmpty() && numAppends < maxNumAppends) {\n-            final UnwrittenAppend unwrittenAppend = unwrittenAppends.poll();\n-\n-            if (unwrittenAppend.future.isDone())\n-                continue;\n-\n-            if (unwrittenAppend.isTimedOut(currentTimeMs)) {\n-                unwrittenAppend.fail(new TimeoutException(\"Request timeout \" + unwrittenAppend.requestTimeoutMs\n-                    + \" expired before the records could be appended to the log\"));\n-            } else {\n-                int epoch = quorum.epoch();\n-                LogAppendInfo info = appendAsLeader(unwrittenAppend.records);\n-                OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n-                long numRecords = info.lastOffset - info.firstOffset + 1;\n-                logger.debug(\"Completed write of {} records at {}\", numRecords, offsetAndEpoch);\n-\n-                if (unwrittenAppend.ackMode == AckMode.LEADER) {\n-                    unwrittenAppend.complete(offsetAndEpoch);\n-                } else if (unwrittenAppend.ackMode == AckMode.QUORUM) {\n-                    CompletableFuture<Long> future = appendPurgatory.await(\n-                        LogOffset.awaitCommitted(offsetAndEpoch.offset),\n-                        unwrittenAppend.requestTimeoutMs);\n-\n-                    future.whenComplete((completionTimeMs, exception) -> {\n-                        if (exception != null) {\n-                            logger.error(\"Failed to commit append at {} due to {}\", offsetAndEpoch, exception);\n-\n-                            unwrittenAppend.fail(exception);\n-                        } else {\n-                            long elapsedTime = Math.max(0, completionTimeMs - currentTimeMs);\n-                            double elapsedTimePerRecord = (double) elapsedTime / numRecords;\n-                            kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, currentTimeMs);\n-                            unwrittenAppend.complete(offsetAndEpoch);\n-\n-                            logger.debug(\"Completed commit of {} records at {}\", numRecords, offsetAndEpoch);\n-                        }\n-                    });\n-                }\n-            }\n-\n-            numAppends++;\n-        }\n-\n-        if (numAppends > 0) {\n-            flushLeaderLog(state, currentTimeMs);\n-        }\n-    }\n-\n-    /**\n-     * Append a set of records to the log. Successful completion of the future indicates a success of\n-     * the append, with the uncommitted base offset and epoch.\n-     *\n-     * @param records The records to write to the log\n-     * @param ackMode The commit mode for the appended records\n-     * @param timeoutMs The maximum time to wait for the append operation to complete (including\n-     *                  any time needed for replication)\n-     * @return The uncommitted base offset and epoch of the appended records\n-     */\n     @Override\n-    public CompletableFuture<OffsetAndEpoch> append(\n-        Records records,\n-        AckMode ackMode,\n-        long timeoutMs\n-    ) {\n-        if (records.sizeInBytes() == 0)\n-            throw new IllegalArgumentException(\"Attempt to append empty record set\");\n-\n-        if (shutdown.get() != null)\n-            throw new IllegalStateException(\"Cannot append records while we are shutting down\");\n-\n-        if (quorum.isObserver())\n-            throw new IllegalStateException(\"Illegal attempt to write to an observer\");\n-\n-        CompletableFuture<OffsetAndEpoch> future = new CompletableFuture<>();\n-        UnwrittenAppend unwrittenAppend = new UnwrittenAppend(\n-            records, time.milliseconds(), timeoutMs, ackMode, future);\n+    public Long scheduleAppend(int epoch, List<T> records) {\n+        BatchAccumulator<T> accumulator = this.accumulator;\n+        if (accumulator == null) {\n+            return Long.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 424}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMjY5NA==", "bodyText": "Should we handle null / max-value here?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508922694", "createdAt": "2020-10-21T00:41:31Z", "author": {"login": "guozhangwang"}, "path": "raft/src/main/java/org/apache/kafka/raft/ReplicatedCounter.java", "diffHunk": "@@ -104,29 +62,17 @@ public synchronized boolean isWritable() {\n     public synchronized void increment() {\n         if (!isWritable())\n             throw new KafkaException(\"Counter is not currently writable\");\n-        int initialValue = uncommitted.get();\n-        int incrementedValue = uncommitted.incrementAndGet();\n-        Records records = MemoryRecords.withRecords(CompressionType.NONE, serialize(incrementedValue));\n-        client.append(records, AckMode.LEADER, Integer.MAX_VALUE).whenComplete((offsetAndEpoch, throwable) -> {\n-            if (offsetAndEpoch != null) {\n-                log.debug(\"Appended increment at offset {}: {} -> {}\",\n-                    offsetAndEpoch.offset, initialValue, incrementedValue);\n-            } else {\n-                uncommitted.set(initialValue);\n-                log.debug(\"Failed append of increment: {} -> {}\", initialValue, incrementedValue, throwable);\n-            }\n-        });\n-    }\n-\n-    private SimpleRecord serialize(int value) {\n-        ByteBuffer buffer = ByteBuffer.allocate(4);\n-        Type.INT32.write(buffer, value);\n-        buffer.flip();\n-        return new SimpleRecord(buffer);\n+        uncommitted += 1;\n+        long offset = client.scheduleAppend(currentLeaderAndEpoch.epoch, Collections.singletonList(uncommitted));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNDg1OQ==", "bodyText": "Is this going to be used for non-testing code in the future? If it is only going to be for metrics purposes maybe we can allow it to be non thread-safe just to not blocking on other critical paths.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508924859", "createdAt": "2020-10-21T00:49:26Z", "author": {"login": "guozhangwang"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match\n+     */\n+    public Long append(int epoch, List<T> records) {\n+        if (epoch != this.epoch) {\n+            // If the epoch does not match, then the state machine probably\n+            // has not gotten the notification about the latest epoch change.\n+            // In this case, ignore the append and return a large offset value\n+            // which will never be committed.\n+            return Long.MAX_VALUE;\n+        }\n+\n+        Object serdeContext = serde.newWriteContext();\n+        int batchSize = 0;\n+        for (T record : records) {\n+            batchSize += serde.recordSize(record, serdeContext);\n+        }\n+\n+        if (batchSize > maxBatchSize) {\n+            throw new IllegalArgumentException(\"The total size of \" + records + \" is \" + batchSize +\n+                \", which exceeds the maximum allowed batch size of \" + maxBatchSize);\n+        }\n+\n+        lock.lock();\n+        try {\n+            BatchBuilder<T> batch = maybeAllocateBatch(batchSize);\n+            if (batch == null) {\n+                return null;\n+            }\n+\n+            if (isEmpty()) {\n+                lingerTimer.update();\n+                lingerTimer.reset(lingerMs);\n+            }\n+\n+            for (T record : records) {\n+                batch.appendRecord(record, serdeContext);\n+                nextOffset += 1;\n+            }\n+\n+            return nextOffset - 1;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    private BatchBuilder<T> maybeAllocateBatch(int batchSize) {\n+        if (currentBatch == null) {\n+            startNewBatch();\n+        } else if (!currentBatch.hasRoomFor(batchSize)) {\n+            completeCurrentBatch();\n+        }\n+        return currentBatch;\n+    }\n+\n+    private void completeCurrentBatch() {\n+        MemoryRecords data = currentBatch.build();\n+        completed.add(new CompletedBatch<>(\n+            currentBatch.baseOffset(),\n+            currentBatch.records(),\n+            data,\n+            memoryPool,\n+            currentBatch.initialBuffer()\n+        ));\n+        currentBatch = null;\n+        startNewBatch();\n+    }\n+\n+    private void startNewBatch() {\n+        ByteBuffer buffer = memoryPool.tryAllocate(maxBatchSize);\n+        if (buffer != null) {\n+            currentBatch = new BatchBuilder<>(\n+                buffer,\n+                serde,\n+                compressionType,\n+                nextOffset,\n+                time.milliseconds(),\n+                false,\n+                RecordBatch.NO_PARTITION_LEADER_EPOCH,\n+                maxBatchSize\n+            );\n+        }\n+    }\n+\n+    /**\n+     * Check whether there are any batches which need flushing now.\n+     *\n+     * @param currentTimeMs current time in milliseconds\n+     * @return true if there are batches ready to flush, false otherwise\n+     */\n+    public boolean needsFlush(long currentTimeMs) {\n+        return timeUntilFlush(currentTimeMs) <= 0;\n+    }\n+\n+    /**\n+     * Check the time remaining until the next needed flush. If the accumulator\n+     * is empty, then {@link Long#MAX_VALUE} will be returned.\n+     *\n+     * @param currentTimeMs current time in milliseconds\n+     * @return the delay in milliseconds before the next expected flush\n+     */\n+    public long timeUntilFlush(long currentTimeMs) {\n+        lock.lock();\n+        try {\n+            lingerTimer.update(currentTimeMs);\n+            if (isEmpty()) {\n+                return Long.MAX_VALUE;\n+            } else {\n+                return lingerTimer.remainingMs();\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    private boolean isEmpty() {\n+        lock.lock();\n+        try {\n+            if (currentBatch != null && currentBatch.nonEmpty()) {\n+                return false;\n+            } else {\n+                return completed.isEmpty();\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Get the leader epoch, which is constant for each instance.\n+     *\n+     * @return the leader epoch\n+     */\n+    public int epoch() {\n+        return epoch;\n+    }\n+\n+    /**\n+     * Flush completed batches. The caller is expected to first check whether\n+     * a flush is expected using {@link #needsFlush(long)} in order to avoid\n+     * unnecessary flushing.\n+     *\n+     * @return the list of completed batches\n+     */\n+    public List<CompletedBatch<T>> flush() {\n+        lock.lock();\n+        try {\n+            if (currentBatch != null && currentBatch.nonEmpty()) {\n+                completeCurrentBatch();\n+            }\n+\n+            List<CompletedBatch<T>> res = completed;\n+            this.completed = new ArrayList<>();\n+            return res;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Get the number of batches including the one that is currently being\n+     * written to (if it exists).\n+     *\n+     * @return\n+     */\n+    public int count() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 246}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNTIwNw==", "bodyText": "nit: can we consolidate this class with the existing one (of course, it needs to be generalized on T record) than creating a new class? Ditto for BatchAccumulator and BatchMemoryPool.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508925207", "createdAt": "2020-10-21T00:50:52Z", "author": {"login": "guozhangwang"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.protocol.DataOutputStreamWritable;\n+import org.apache.kafka.common.record.AbstractRecords;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.DefaultRecord;\n+import org.apache.kafka.common.record.DefaultRecordBatch;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.utils.ByteBufferOutputStream;\n+import org.apache.kafka.common.utils.ByteUtils;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.DataOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class BatchBuilder<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTMwNzQ4", "url": "https://github.com/apache/kafka/pull/9418#pullrequestreview-515930748", "createdAt": "2020-10-23T18:57:32Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxODo1NzozMlrOHnaJnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDoxMTo1NFrOHndHag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA4NDk1Ng==", "bodyText": "Currently, postInboundRequest in only used by TestRaftRequestHandler and KafkaNetworkChannelTest. Are you thinking that we will use the same or similar method when integrating with the broker code?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511084956", "createdAt": "2020-10-23T18:57:32Z", "author": {"login": "jsancio"}, "path": "core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala", "diffHunk": "@@ -216,11 +216,9 @@ class KafkaNetworkChannel(time: Time,\n     endpoints.put(id, node)\n   }\n \n-  def postInboundRequest(header: RequestHeader,\n-                         request: AbstractRequest,\n-                         onResponseReceived: ResponseHandler): Unit = {\n+  def postInboundRequest(request: AbstractRequest, onResponseReceived: ResponseHandler): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA4NTY3OA==", "bodyText": "Unrelated with this change but when reading the code. I was confused by the name pollInboundResponses. That function returns both pending request and responses.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511085678", "createdAt": "2020-10-23T18:59:09Z", "author": {"login": "jsancio"}, "path": "core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala", "diffHunk": "@@ -216,11 +216,9 @@ class KafkaNetworkChannel(time: Time,\n     endpoints.put(id, node)\n   }\n \n-  def postInboundRequest(header: RequestHeader,\n-                         request: AbstractRequest,\n-                         onResponseReceived: ResponseHandler): Unit = {\n+  def postInboundRequest(request: AbstractRequest, onResponseReceived: ResponseHandler): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEwMTU4MQ==", "bodyText": "What are you trying to protect with this check? For example, the signature could be public long append(List<T> records) with the accumulator writing the correct epoch.\nIn #9482 you implemented handleClaim in to only fire when the Listener's \"acknowledged\" offset + 1 is >= to the leader's epoch start offset.\nThinking through the code's behaviour, I see this check catching the case the the raft replica lost leadership and won leadership before the Listener was able to asynchronously process handleResign and handleClaim.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511101581", "createdAt": "2020-10-23T19:22:25Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match\n+     */\n+    public Long append(int epoch, List<T> records) {\n+        if (epoch != this.epoch) {\n+            // If the epoch does not match, then the state machine probably\n+            // has not gotten the notification about the latest epoch change.\n+            // In this case, ignore the append and return a large offset value\n+            // which will never be committed.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEwMzMzNg==", "bodyText": "We should document this returns null and what it means when null is returned.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511103336", "createdAt": "2020-10-23T19:24:34Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTExMjI4MQ==", "bodyText": "Throughout this type we use flush to mean \"collect the current batch\" and start a new one. When I started reading this code I was assuming that it meant flush to disk/IO. Should we use a different verb for this? I think Java tends to use \"collect\" for this type of operation. What do you think?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511112281", "createdAt": "2020-10-23T19:35:56Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match\n+     */\n+    public Long append(int epoch, List<T> records) {\n+        if (epoch != this.epoch) {\n+            // If the epoch does not match, then the state machine probably\n+            // has not gotten the notification about the latest epoch change.\n+            // In this case, ignore the append and return a large offset value\n+            // which will never be committed.\n+            return Long.MAX_VALUE;\n+        }\n+\n+        Object serdeContext = serde.newWriteContext();\n+        int batchSize = 0;\n+        for (T record : records) {\n+            batchSize += serde.recordSize(record, serdeContext);\n+        }\n+\n+        if (batchSize > maxBatchSize) {\n+            throw new IllegalArgumentException(\"The total size of \" + records + \" is \" + batchSize +\n+                \", which exceeds the maximum allowed batch size of \" + maxBatchSize);\n+        }\n+\n+        lock.lock();\n+        try {\n+            BatchBuilder<T> batch = maybeAllocateBatch(batchSize);\n+            if (batch == null) {\n+                return null;\n+            }\n+\n+            if (isEmpty()) {\n+                lingerTimer.update();\n+                lingerTimer.reset(lingerMs);\n+            }\n+\n+            for (T record : records) {\n+                batch.appendRecord(record, serdeContext);\n+                nextOffset += 1;\n+            }\n+\n+            return nextOffset - 1;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    private BatchBuilder<T> maybeAllocateBatch(int batchSize) {\n+        if (currentBatch == null) {\n+            startNewBatch();\n+        } else if (!currentBatch.hasRoomFor(batchSize)) {\n+            completeCurrentBatch();\n+        }\n+        return currentBatch;\n+    }\n+\n+    private void completeCurrentBatch() {\n+        MemoryRecords data = currentBatch.build();\n+        completed.add(new CompletedBatch<>(\n+            currentBatch.baseOffset(),\n+            currentBatch.records(),\n+            data,\n+            memoryPool,\n+            currentBatch.initialBuffer()\n+        ));\n+        currentBatch = null;\n+        startNewBatch();\n+    }\n+\n+    private void startNewBatch() {\n+        ByteBuffer buffer = memoryPool.tryAllocate(maxBatchSize);\n+        if (buffer != null) {\n+            currentBatch = new BatchBuilder<>(\n+                buffer,\n+                serde,\n+                compressionType,\n+                nextOffset,\n+                time.milliseconds(),\n+                false,\n+                RecordBatch.NO_PARTITION_LEADER_EPOCH,\n+                maxBatchSize\n+            );\n+        }\n+    }\n+\n+    /**\n+     * Check whether there are any batches which need flushing now.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEzMjEwMQ==", "bodyText": "I think it would be good to have a description of what this type is doing and how to use it. Reading the code and seeing how the buffer is shared, I am under the impression that this type should not be reused after the build method is called. Is that correct?\nIf this is correct, I think all of the public methods should check that they are not being called after build is called.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511132101", "createdAt": "2020-10-23T20:08:20Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.protocol.DataOutputStreamWritable;\n+import org.apache.kafka.common.record.AbstractRecords;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.DefaultRecord;\n+import org.apache.kafka.common.record.DefaultRecordBatch;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.utils.ByteBufferOutputStream;\n+import org.apache.kafka.common.utils.ByteUtils;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.DataOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class BatchBuilder<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEzMzU0Ng==", "bodyText": "Let's check that previouslyAllocated's capacity is batchSize else buffer.limit(sizeBytes) is going to throw with a less useful stacktrace.", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511133546", "createdAt": "2020-10-23T20:11:54Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchMemoryPool.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * Simple memory pool which maintains a limited number of fixed-size buffers.\n+ */\n+public class BatchMemoryPool implements MemoryPool {\n+    private final ReentrantLock lock;\n+    private final Deque<ByteBuffer> free;\n+    private final int maxBatches;\n+    private final int batchSize;\n+\n+    private int numAllocatedBatches = 0;\n+\n+    public BatchMemoryPool(int maxBatches, int batchSize) {\n+        this.maxBatches = maxBatches;\n+        this.batchSize = batchSize;\n+        this.free = new ArrayDeque<>(maxBatches);\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    @Override\n+    public ByteBuffer tryAllocate(int sizeBytes) {\n+        if (sizeBytes > batchSize) {\n+            throw new IllegalArgumentException(\"Cannot allocate buffers larger than max \" +\n+                \"batch size of \" + batchSize);\n+        }\n+\n+        lock.lock();\n+        try {\n+            ByteBuffer buffer = free.poll();\n+            if (buffer == null && numAllocatedBatches < maxBatches) {\n+                buffer = ByteBuffer.allocate(batchSize);\n+                numAllocatedBatches += 1;\n+            }\n+\n+            if (buffer != null) {\n+                buffer.clear();\n+                buffer.limit(sizeBytes);\n+            }\n+            return buffer;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    @Override\n+    public void release(ByteBuffer previouslyAllocated) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e194616b3ad241d0375feecd51a11dac89926444", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/e194616b3ad241d0375feecd51a11dac89926444", "committedDate": "2020-10-23T23:45:36Z", "message": "KAFKA-10601; Add support for append linger to Raft implementation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2446f4defac4b640e2fc5b7424a67f43dd79e54", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/b2446f4defac4b640e2fc5b7424a67f43dd79e54", "committedDate": "2020-10-23T23:45:36Z", "message": "Remove unused class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7fb4b0e7f88f2149bf42a9af37b947c3dea07a9", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/b7fb4b0e7f88f2149bf42a9af37b947c3dea07a9", "committedDate": "2020-10-23T23:45:37Z", "message": "Remove unused `MetadataRecordSerde`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9993b602edde63f44d4bb986ecc2d33767287d1", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/d9993b602edde63f44d4bb986ecc2d33767287d1", "committedDate": "2020-10-23T23:45:37Z", "message": "Remove unused `DataOutputWritable`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "00fac83174380afeb68b5a9fe979a5fd0774c23c", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/00fac83174380afeb68b5a9fe979a5fd0774c23c", "committedDate": "2020-10-23T23:45:37Z", "message": "Clarify handling when the epoch in `scheduleAppend` does not match"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5a769c5a1b3d10a410e0fdc3c8f2fccaf7782b0", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/c5a769c5a1b3d10a410e0fdc3c8f2fccaf7782b0", "committedDate": "2020-10-23T23:45:37Z", "message": "Rename linger time to `append.linger.ms`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf381c0fff222263dacb1edc97e38a6b1fd723b8", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/cf381c0fff222263dacb1edc97e38a6b1fd723b8", "committedDate": "2020-10-23T23:45:37Z", "message": "Throw an error if `handleCommit` includes unexpected append"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3dabc1178f9dcb665c677362f8a3bb487fa4fc35", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/3dabc1178f9dcb665c677362f8a3bb487fa4fc35", "committedDate": "2020-10-23T23:45:37Z", "message": "`ReplicatedCounter` should check return of `scheduleAppend`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae3c2dc21230619220d74133f9e76195f957e6da", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/ae3c2dc21230619220d74133f9e76195f957e6da", "committedDate": "2020-10-23T23:45:37Z", "message": "Fix jdk8 compilation error"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "ae3c2dc21230619220d74133f9e76195f957e6da", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/ae3c2dc21230619220d74133f9e76195f957e6da", "committedDate": "2020-10-23T23:45:37Z", "message": "Fix jdk8 compilation error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0490965ad582fbae717edbec413abd29a170691a", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/0490965ad582fbae717edbec413abd29a170691a", "committedDate": "2020-10-26T20:12:55Z", "message": "Address various comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "fac5c0a9507f9f40f99dac017242d12500a97d5d", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/fac5c0a9507f9f40f99dac017242d12500a97d5d", "committedDate": "2020-10-27T03:03:07Z", "message": "Do not block IO thread in order to drain accumulator"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "fac5c0a9507f9f40f99dac017242d12500a97d5d", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/fac5c0a9507f9f40f99dac017242d12500a97d5d", "committedDate": "2020-10-27T03:03:07Z", "message": "Do not block IO thread in order to drain accumulator"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMTg3NTEw", "url": "https://github.com/apache/kafka/pull/9418#pullrequestreview-521187510", "createdAt": "2020-11-01T02:32:40Z", "commit": {"oid": "fac5c0a9507f9f40f99dac017242d12500a97d5d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQwMjozMjo0MFrOHrrqTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQwMjozMjo0MFrOHrrqTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU2NjE1OA==", "bodyText": "@hachikuji Should we move this state to LeaderState? We can delegate maybeCloseAccumulator to the transitionTo... functions.\nI have a similar requirements for snapshot. I am currently adding a SnapshotWriter to FollowerState to track this state. I haven't implemented it yet but I am thinking of making FollowerState Closeable which will handle the cleanup. The transitionTo... functions have the additional responsibility of calling close if necessary.\nWhat do you think?", "url": "https://github.com/apache/kafka/pull/9418#discussion_r515566158", "createdAt": "2020-11-01T02:32:40Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -276,6 +299,17 @@ private void onBecomeLeader(long currentTimeMs) {\n         resetConnections();\n \n         kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n+\n+        accumulator = new BatchAccumulator<>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fac5c0a9507f9f40f99dac017242d12500a97d5d"}, "originalPosition": 178}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 519, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}