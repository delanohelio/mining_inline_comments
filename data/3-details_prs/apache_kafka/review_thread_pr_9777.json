{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQzOTI3OTA5", "number": 9777, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yOVQyMTo1MToyMlrOFUr9Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMVQxMzo0ODowMVrOFVM4CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MjM2MDI3OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yOVQyMTo1MToyMlrOIc12rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMVQwODo0OTowOVrOIdWnEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzM4OQ==", "bodyText": "Can we use server.metadataCache?", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567113389", "createdAt": "2021-01-29T21:51:22Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -885,6 +891,24 @@ object TestUtils extends Logging {\n     ), \"Timed out waiting for broker metadata to propagate to all servers\", timeout)\n   }\n \n+  /**\n+   * Wait until the expected number of partitions is in the metadata cache in each broker.\n+   *\n+   * @param servers The list of servers that the metadata should reach to\n+   * @param topic The topic name\n+   * @param expectedNumPartitions The expected number of partitions\n+   */\n+  def waitUntilMetadataIsPropagatedWithExpectedSize(servers: Seq[KafkaServer], topic: String, expectedNumPartitions: Int): Unit = {\n+    waitUntilTrue(\n+      () => servers.forall { server =>\n+        server.dataPlaneRequestProcessor.metadataCache.numPartitions(topic) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b3f528ba538d4e70369409fe26dfae16760f3e"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzY1MDA2NQ==", "bodyText": "Good suggestion. Updated.", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567650065", "createdAt": "2021-02-01T08:49:09Z", "author": {"login": "showuon"}, "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -885,6 +891,24 @@ object TestUtils extends Logging {\n     ), \"Timed out waiting for broker metadata to propagate to all servers\", timeout)\n   }\n \n+  /**\n+   * Wait until the expected number of partitions is in the metadata cache in each broker.\n+   *\n+   * @param servers The list of servers that the metadata should reach to\n+   * @param topic The topic name\n+   * @param expectedNumPartitions The expected number of partitions\n+   */\n+  def waitUntilMetadataIsPropagatedWithExpectedSize(servers: Seq[KafkaServer], topic: String, expectedNumPartitions: Int): Unit = {\n+    waitUntilTrue(\n+      () => servers.forall { server =>\n+        server.dataPlaneRequestProcessor.metadataCache.numPartitions(topic) match {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzM4OQ=="}, "originalCommit": {"oid": "f7b3f528ba538d4e70369409fe26dfae16760f3e"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MjM2MTc5OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yOVQyMTo1MjowNlrOIc13rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMVQwODo0OToyNVrOIdWnsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzY0NQ==", "bodyText": "What do you think about more concise names?\nwaitUntilMetadataIsPropagatedWithExpectedSize -> waitForAllPartitionMetadata\nwaitUntilMetadataIsPropagated -> waitForPartitionMetadata\nI wonder if this would be more useful if we return the partition metadata: Map[TopicPartition, UpdateMetadataPartitionState. Then we could probably skip the calls to waitUntilMetadataIsPropagated and waitUntilLeaderIsElectedOrChanged above.", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567113645", "createdAt": "2021-01-29T21:52:06Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -885,6 +891,24 @@ object TestUtils extends Logging {\n     ), \"Timed out waiting for broker metadata to propagate to all servers\", timeout)\n   }\n \n+  /**\n+   * Wait until the expected number of partitions is in the metadata cache in each broker.\n+   *\n+   * @param servers The list of servers that the metadata should reach to\n+   * @param topic The topic name\n+   * @param expectedNumPartitions The expected number of partitions\n+   */\n+  def waitUntilMetadataIsPropagatedWithExpectedSize(servers: Seq[KafkaServer], topic: String, expectedNumPartitions: Int): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b3f528ba538d4e70369409fe26dfae16760f3e"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzY1MDIyNQ==", "bodyText": "Good suggestion. Updated.", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567650225", "createdAt": "2021-02-01T08:49:25Z", "author": {"login": "showuon"}, "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -885,6 +891,24 @@ object TestUtils extends Logging {\n     ), \"Timed out waiting for broker metadata to propagate to all servers\", timeout)\n   }\n \n+  /**\n+   * Wait until the expected number of partitions is in the metadata cache in each broker.\n+   *\n+   * @param servers The list of servers that the metadata should reach to\n+   * @param topic The topic name\n+   * @param expectedNumPartitions The expected number of partitions\n+   */\n+  def waitUntilMetadataIsPropagatedWithExpectedSize(servers: Seq[KafkaServer], topic: String, expectedNumPartitions: Int): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzY0NQ=="}, "originalCommit": {"oid": "f7b3f528ba538d4e70369409fe26dfae16760f3e"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MjM4NjQ1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yOVQyMjowMDo1MVrOIc2GVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yOVQyMjowMDo1MVrOIc2GVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExNzM5Nw==", "bodyText": "As mentioned in the other comment, this is probably good enough. The extra validations below seem like overkill.", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567117397", "createdAt": "2021-01-29T22:00:51Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -349,10 +349,13 @@ object TestUtils extends Logging {\n       !hasSessionExpirationException},\n       s\"Can't create topic $topic\")\n \n+    // wait until we've got the expected partition size\n+    waitUntilMetadataIsPropagatedWithExpectedSize(servers, topic, numPartitions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b3f528ba538d4e70369409fe26dfae16760f3e"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3Nzc1MzY5OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/admin/AddPartitionsTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMVQxMzo0ODowMVrOIdiCnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMVQxMzo0ODowMVrOIdiCnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzgzNzM0Mw==", "bodyText": "Don't replace them with waitForAllPartitionsMetadata because I'm afraid it'll break the original testing purposes. And same as other places.", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567837343", "createdAt": "2021-02-01T13:48:01Z", "author": {"login": "showuon"}, "path": "core/src/test/scala/unit/kafka/admin/AddPartitionsTest.scala", "diffHunk": "@@ -137,12 +137,12 @@ class AddPartitionsTest extends BaseRequestTest {\n     adminZkClient.addPartitions(topic3, topic3Assignment, adminZkClient.getBrokerMetadatas(), 7)\n \n     // read metadata from a broker and verify the new topic partitions exist\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 1)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 2)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 3)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 4)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 5)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 6)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 1)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 2)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 3)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 4)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 5)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 6)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73d9cfc120cc92a25babb62eccd61ac73619dee9"}, "originalPosition": 37}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3546, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}