{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE4MjAwNjAy", "number": 9582, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDoyNTowNlrOE3d0qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxOTo0MTo1MlrOE4znnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTk1NzU0OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDoyNTowNlrOHw01OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxOToyNToxN1rOHy8HFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1OTI4OA==", "bodyText": "Saw this and at first I thought it was broken because it only considers pattern-subscribed topics that happened to explicitly configure an offset reset policy. Unless I'm missing something here, that makes no sense and we should consider all  source patterns and whether they overlap.\nBut then I started thinking, why does it matter if they overlap? Just because one pattern is a substring of another does not mean that they'll match the same topics. So I think that we should actually just remove this restriction altogether. Am I missing anything here?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r520959288", "createdAt": "2020-11-11T00:25:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -410,18 +410,6 @@ public final void addSource(final Topology.AutoOffsetReset offsetReset,\n             }\n         }\n \n-        for (final Pattern otherPattern : earliestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-\n-        for (final Pattern otherPattern : latestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwNDkyMA==", "bodyText": "I agree on the first part.\nRegarding the second part, I had similiar thoughts when I wrote my comment in mergeDuplicateSourceNodes().\nBut I might also be missing something here.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522104920", "createdAt": "2020-11-12T13:28:56Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -410,18 +410,6 @@ public final void addSource(final Topology.AutoOffsetReset offsetReset,\n             }\n         }\n \n-        for (final Pattern otherPattern : earliestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-\n-        for (final Pattern otherPattern : latestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1OTI4OA=="}, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NDMxOA==", "bodyText": "I might also be missing something, but what's the scenario where one pattern is a substring of another and they dont match the same topics? If you take Bruno's example from earlier of topic* and topi*, topi* would be considered a substring of topic* and they would both match topic A, right? I guess the other scenario is if we have a topic topia A, that would match topi* and not topic*. So I guess it seems like it isn't always true that they'll overlap, but we would want to check if they do, right?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r523044318", "createdAt": "2020-11-13T16:00:44Z", "author": {"login": "lct45"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -410,18 +410,6 @@ public final void addSource(final Topology.AutoOffsetReset offsetReset,\n             }\n         }\n \n-        for (final Pattern otherPattern : earliestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-\n-        for (final Pattern otherPattern : latestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1OTI4OA=="}, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA2MTY5Ng==", "bodyText": "Pattern topic* is contained in pattern topic*A. However, topic*A matches only a subset of topic*. So, they do not match exactly the same topics. But matching exactly the same topics is a pre-requisite for merging the source nodes.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r523061696", "createdAt": "2020-11-13T16:28:30Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -410,18 +410,6 @@ public final void addSource(final Topology.AutoOffsetReset offsetReset,\n             }\n         }\n \n-        for (final Pattern otherPattern : earliestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-\n-        for (final Pattern otherPattern : latestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1OTI4OA=="}, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE3NTcwMw==", "bodyText": "I think in this case we were matching whether the pattern's string was a literal substring of another pattern's string, not whether the regexes themselves are substrings. So topi* would not be a substring of topic* because topi* is not contained literally within the string topic*. It's not doing a smart regex-matching, just a dumb  literal string comparison", "url": "https://github.com/apache/kafka/pull/9582#discussion_r523175703", "createdAt": "2020-11-13T19:25:17Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -410,18 +410,6 @@ public final void addSource(final Topology.AutoOffsetReset offsetReset,\n             }\n         }\n \n-        for (final Pattern otherPattern : earliestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-\n-        for (final Pattern otherPattern : latestResetPatterns) {\n-            if (topicPattern.pattern().contains(otherPattern.pattern()) || otherPattern.pattern().contains(topicPattern.pattern())) {\n-                throw new TopologyException(\"Pattern \" + topicPattern + \" will overlap with another pattern \" + otherPattern + \" already been registered by another source\");\n-            }\n-        }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1OTI4OA=="}, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjQ2NzA0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMDowMjo0OVrOHxzUPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMDowMjo0OVrOHxzUPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTk4MzAzOQ==", "bodyText": "Could you please add a try-catch clause to better document the test?\nFor example:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void shouldAllowReadingFromSameTopic() {\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    builder.build();\n          \n          \n            \n                }\n          \n          \n            \n                public void shouldAllowReadingFromSameTopic() {\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    builder.stream(\"topic\");\n          \n          \n            \n                    \n          \n          \n            \n                    try {\n          \n          \n            \n                        builder.build();\n          \n          \n            \n                    } catch (final TopologyException topologyException) {\n          \n          \n            \n                        fail(\"TopologyException not expected\");\n          \n          \n            \n                    }\n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nThis applies also to the other tests.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r521983039", "createdAt": "2020-11-12T10:02:49Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjU4MTYwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMDozMDo1N1rOHx0bOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxNTowMzoxMFrOHyxzhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAwMTIwOQ==", "bodyText": "We could avoid the instanceof and the casting if we introduce a RootGraphNode with a method sourceNodes(). Since a root can only have source nodes and state stores as children, we could make the topology code in general a bit more type safe. As far as I can see that would need some additional changes outside the scope of this PR. So, feel free to not consider this comment for this PR and we can do another PR for that.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522001209", "createdAt": "2020-11-12T10:30:57Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));\n+\n+        for (final StreamsGraphNode graphNode : root.children()) {\n+            if (graphNode instanceof StreamSourceNode) {\n+                final StreamSourceNode<?, ?> currentSourceNode = (StreamSourceNode<?, ?>) graphNode;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUyNTA2OA==", "bodyText": "Yeah I think that's a fair point but I would prefer to keep the scope of this PR as small as possible for now. Maybe @lct45 could pick this up on the side once this is merged?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522525068", "createdAt": "2020-11-13T00:24:41Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));\n+\n+        for (final StreamsGraphNode graphNode : root.children()) {\n+            if (graphNode instanceof StreamSourceNode) {\n+                final StreamSourceNode<?, ?> currentSourceNode = (StreamSourceNode<?, ?>) graphNode;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAwMTIwOQ=="}, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAwNjg1NQ==", "bodyText": "SGTM", "url": "https://github.com/apache/kafka/pull/9582#discussion_r523006855", "createdAt": "2020-11-13T15:03:10Z", "author": {"login": "lct45"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));\n+\n+        for (final StreamsGraphNode graphNode : root.children()) {\n+            if (graphNode instanceof StreamSourceNode) {\n+                final StreamSourceNode<?, ?> currentSourceNode = (StreamSourceNode<?, ?>) graphNode;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAwMTIwOQ=="}, "originalCommit": null, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3Mjc1MzgzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMToxNDozN1rOHx2Dpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoxNDowOVrOHyjrig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAyNzk0Mg==", "bodyText": "Just to be clear. This improves the situation but it is not a complete solution, right? Assume we have a topic topicA. Patterns topic* and topi* both  match topicA but they are different when compared with this comparator. In that case a TopologyException would be thrown in the InternalTopologyBuilder, right?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522027942", "createdAt": "2020-11-12T11:14:37Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU0OTY4NQ==", "bodyText": "Yes to all of that: this PR improves some situations, but not all. Specifically you would still get a TopologyException if  (a) subscribing to overlapping but not equal collection of topics, (b) subscribing to a topic and to a pattern that matches said topic, and (c) subscribing to two (or more) patterns that match the same topic(s).\nCase (c) is what you described, I just wanted to list them all here for completion. Here's my take on what we can/should reasonably try to tackle:\n(a) this case is easily detected, easily worked around, and easy for us to fix. It results in a \"compile time\" exception (meaning when the topology is compiled, not the program) which users can quickly detect and work around if need be by rewriting the topology themselves. Fix is relatively straightforward but very low priority, so I plan to just file a followup ticket for this for now\n(b) is easily detected (you get a compile time exception) and possible to work around, but difficult to solve. I think in all cases a user could find a way around this issue by some combination of topology rewriting and Pattern manipulation or topic renaming, depending on what exactly they're trying to achieve. Of course there's no way for us to detect what an arbitrary user is trying to do in this case, so I don't see any path forwarding to making this case possible. No plans to file a followup ticket\n(c) is difficult to detect, might be possible to work around, and probably very complicated to actually fix. Unfortunately, in this case you only get a run-time exception, since there's no way of knowing which topics will or will not be created ahead of time. And I'm thinking that determining whether two regexes will both match any possible string may be unsolvable...so, no followup ticket planned for this.\nWDYT?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522549685", "createdAt": "2020-11-13T01:23:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAyNzk0Mg=="}, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc3NTQzNA==", "bodyText": "Thank you for the list of issues. I agree in all points.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522775434", "createdAt": "2020-11-13T08:14:09Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAyNzk0Mg=="}, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzE4MjEwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzoxMjo1NlrOHx6KIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxOTo1NDowNFrOHyLp6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA5NTEzOA==", "bodyText": "Do we really need this comment and the comment on line 81. We get the same information when we navigate to the call and to the implementation of the methods with the difference that comments can start to be outdated without us noticing it.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522095138", "createdAt": "2020-11-12T13:12:56Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,21 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times\n+    public void merge(final StreamSourceNode<?, ?> other) {\n+        final AutoOffsetReset resetPolicy = consumedInternal.offsetResetPolicy();\n+        if (resetPolicy != null && !resetPolicy.equals(other.consumedInternal().offsetResetPolicy())) {\n+            log.error(\"Tried to merge source nodes {} and {} which are subscribed to the same topic/pattern, but \"\n+                          + \"the offset reset policies do not match\", this, other);\n+            throw new TopologyException(\"Can't configure different offset reset policies on the same input topic(s)\");\n+        }\n+        for (final StreamsGraphNode otherChild : other.children()) {\n+            // Move children from other to this, these calls take care of resetting the child's parents to this", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM4MTgwMw==", "bodyText": "I'll remove it", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522381803", "createdAt": "2020-11-12T19:54:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,21 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times\n+    public void merge(final StreamSourceNode<?, ?> other) {\n+        final AutoOffsetReset resetPolicy = consumedInternal.offsetResetPolicy();\n+        if (resetPolicy != null && !resetPolicy.equals(other.consumedInternal().offsetResetPolicy())) {\n+            log.error(\"Tried to merge source nodes {} and {} which are subscribed to the same topic/pattern, but \"\n+                          + \"the offset reset policies do not match\", this, other);\n+            throw new TopologyException(\"Can't configure different offset reset policies on the same input topic(s)\");\n+        }\n+        for (final StreamsGraphNode otherChild : other.children()) {\n+            // Move children from other to this, these calls take care of resetting the child's parents to this", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA5NTEzOA=="}, "originalCommit": null, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzI1MTYxOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzozMDozN1rOHx60ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzozMDozN1rOHx60ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwNjA0Mg==", "bodyText": "Please use a collection with at least two topics to test the loop over the collections.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522106042", "createdAt": "2020-11-12T13:30:37Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowSubscribingToSamePattern() {\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowReadingFromSameCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(Collections.singletonList(\"topic\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzI2NDU3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzozMzo1NVrOHx68jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxOTo1OTowMVrOHyL1Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwODA0Nw==", "bodyText": "What should happen if this reset policy is null and the other is not null? I guess we should also throw in that case, don't we?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522108047", "createdAt": "2020-11-12T13:33:55Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,21 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times\n+    public void merge(final StreamSourceNode<?, ?> other) {\n+        final AutoOffsetReset resetPolicy = consumedInternal.offsetResetPolicy();\n+        if (resetPolicy != null && !resetPolicy.equals(other.consumedInternal().offsetResetPolicy())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM4NDY5MA==", "bodyText": "Ack, good catch", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522384690", "createdAt": "2020-11-12T19:59:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,21 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times\n+    public void merge(final StreamSourceNode<?, ?> other) {\n+        final AutoOffsetReset resetPolicy = consumedInternal.offsetResetPolicy();\n+        if (resetPolicy != null && !resetPolicy.equals(other.consumedInternal().offsetResetPolicy())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwODA0Nw=="}, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzI3NjI3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzozNjo1M1rOHx7D3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMjozNzoxNVrOHyRbXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwOTkxNg==", "bodyText": "What should happen in this case? See also my comment in merge().\n    @Test\n    public void shouldThrowWhenSubscribedToATopicWithSetAndUnsetResetPolicies() {\n        builder.stream(\"topic\");\n        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n        assertThrows(TopologyException.class, builder::build);\n    }", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522109916", "createdAt": "2020-11-12T13:36:53Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowSubscribingToSamePattern() {\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowReadingFromSameCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldNotAllowReadingFromOverlappingAndUnequalCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(asList(\"topic\", \"anotherTopic\"));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithDifferentResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.LATEST));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithSetAndUnsetResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\");\n+        assertThrows(TopologyException.class, builder::build);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ3NjM4Mg==", "bodyText": "Yep that was just an oversight in the condition in merge(). I fixed that and added another unit test for the case", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522476382", "createdAt": "2020-11-12T22:37:15Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowSubscribingToSamePattern() {\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowReadingFromSameCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldNotAllowReadingFromOverlappingAndUnequalCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(asList(\"topic\", \"anotherTopic\"));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithDifferentResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.LATEST));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithSetAndUnsetResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\");\n+        assertThrows(TopologyException.class, builder::build);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEwOTkxNg=="}, "originalCommit": null, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzI4OTQ1OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzo0MDoxMFrOHx7L8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMzo0MDoxMFrOHx7L8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjExMTk4NA==", "bodyText": "Could you also add a test with two patterns with the same string but one with a set reset policy and one with unset reset policy like you did for the non-pattern case. Just to make it clear it should also throw in that case.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522111984", "createdAt": "2020-11-12T13:40:10Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +898,55 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowReadingFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowSubscribingToSamePattern() {\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.stream(Pattern.compile(\"some-regex\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldAllowReadingFromSameCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.build();\n+    }\n+\n+    @Test\n+    public void shouldNotAllowReadingFromOverlappingAndUnequalCollectionOfTopics() {\n+        builder.stream(Collections.singletonList(\"topic\"));\n+        builder.stream(asList(\"topic\", \"anotherTopic\"));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithDifferentResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.LATEST));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToATopicWithSetAndUnsetResetPolicies() {\n+        builder.stream(\"topic\", Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(\"topic\");\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+\n+    @Test\n+    public void shouldThrowWhenSubscribedToAPatternWithDifferentResetPolicies() {\n+        builder.stream(Pattern.compile(\"some-regex\"), Consumed.with(AutoOffsetReset.EARLIEST));\n+        builder.stream(Pattern.compile(\"some-regex\"), Consumed.with(AutoOffsetReset.LATEST));\n+        assertThrows(TopologyException.class, builder::build);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NzQzNTI3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoxNjozM1rOHyj1mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoxNjozM1rOHyj1mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc3ODAxMA==", "bodyText": "I would also remove this comment.", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522778010", "createdAt": "2020-11-13T08:16:33Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java", "diffHunk": "@@ -71,6 +78,22 @@ public Pattern topicPattern() {\n         return consumedInternal.valueSerde();\n     }\n \n+    // We \"merge\" source nodes into a single node under the hood if a user tries to read in a source topic multiple times", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NzQ1OTc4OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoyMDo0MVrOHykGWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwODoyMDo0MVrOHykGWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc4MjI5OQ==", "bodyText": "Could you please extract this part in its own method since we use it also in a couple of other tests?", "url": "https://github.com/apache/kafka/pull/9582#discussion_r522782299", "createdAt": "2020-11-13T08:20:41Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "diffHunk": "@@ -895,6 +899,103 @@ public void shouldUseSpecifiedNameForAggregateOperationGivenTable() {\n             STREAM_OPERATION_NAME);\n     }\n \n+    @Test\n+    public void shouldAllowStreamsFromSameTopic() {\n+        builder.stream(\"topic\");\n+        builder.stream(\"topic\");\n+        try {\n+            builder.build();\n+        } catch (final TopologyException topologyException) {\n+            fail(\"TopologyException not expected\");\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4MDAxNDM3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxOTo0MTo1MlrOHy8mzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxOTo0MTo1MlrOHy8mzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzgyMQ==", "bodyText": "Filed https://issues.apache.org/jira/browse/KAFKA-10721", "url": "https://github.com/apache/kafka/pull/9582#discussion_r523183821", "createdAt": "2020-11-13T19:41:52Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "diffHunk": "@@ -314,6 +317,50 @@ public void buildAndOptimizeTopology(final Properties props) {\n         internalTopologyBuilder.validateCopartition();\n     }\n \n+    private void mergeDuplicateSourceNodes() {\n+        final Map<String, StreamSourceNode<?, ?>> topicsToSourceNodes = new HashMap<>();\n+\n+        // We don't really care about the order here, but since Pattern does not implement equals() we can't rely on\n+        // a regular HashMap and containsKey(Pattern). But for our purposes it's sufficient to compare the compiled\n+        // string and flags to determine if two pattern subscriptions can be merged into a single source node\n+        final Map<Pattern, StreamSourceNode<?, ?>> patternsToSourceNodes =\n+            new TreeMap<>(Comparator.comparing(Pattern::pattern).thenComparing(Pattern::flags));\n+\n+        for (final StreamsGraphNode graphNode : root.children()) {\n+            if (graphNode instanceof StreamSourceNode) {\n+                final StreamSourceNode<?, ?> currentSourceNode = (StreamSourceNode<?, ?>) graphNode;\n+\n+                if (currentSourceNode.topicPattern() != null) {\n+                    if (!patternsToSourceNodes.containsKey(currentSourceNode.topicPattern())) {\n+                        patternsToSourceNodes.put(currentSourceNode.topicPattern(), currentSourceNode);\n+                    } else {\n+                        final StreamSourceNode<?, ?> mainSourceNode = patternsToSourceNodes.get(currentSourceNode.topicPattern());\n+                        mainSourceNode.merge(currentSourceNode);\n+                        root.removeChild(graphNode);\n+                    }\n+                } else {\n+                    for (final String topic : currentSourceNode.topicNames()) {\n+                        if (!topicsToSourceNodes.containsKey(topic)) {\n+                            topicsToSourceNodes.put(topic, currentSourceNode);\n+                        } else {\n+                            final StreamSourceNode<?, ?> mainSourceNode = topicsToSourceNodes.get(topic);\n+                            // TODO we only merge source nodes if the subscribed topic(s) are an exact match, so it's still not\n+                            // possible to subscribe to topicA in one KStream and topicA + topicB in another. We could achieve\n+                            // this by splitting these source nodes into one topic per node and routing to the subscribed children", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf32d6925b2ad8506179c2694595067cc26e1b43"}, "originalPosition": 54}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3841, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}