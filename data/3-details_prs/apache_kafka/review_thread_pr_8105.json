{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc0NjQ0ODQz", "number": 8105, "reviewThreads": {"totalCount": 47, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzowNjo1OFrODfZ0_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowNDozOFrODg4qvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU1NjEzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzowNjo1OFrOFpFuRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzowNjo1OFrOFpFuRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYyOTcwMA==", "bodyText": "This allow us to simplify RecordCollectorImplTest significantly -- we can register a exception that we want to throw on the next invocation of the corresponding method.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378629700", "createdAt": "2020-02-13T03:06:58Z", "author": {"login": "mjsax"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "diffHunk": "@@ -72,6 +72,16 @@\n     private long commitCount = 0L;\n     private Map<MetricName, Metric> mockMetrics;\n \n+    public RuntimeException initTransactionException = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU1Nzc5OnYy", "diffSide": "LEFT", "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzowODowMFrOFpFvKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxNzo1Njo1MVrOFqP9Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYyOTkyOQ==", "bodyText": "I think we should do the regular state check first before we throw any test ingested exception.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378629929", "createdAt": "2020-02-13T03:08:00Z", "author": {"login": "mjsax"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "diffHunk": "@@ -182,14 +202,17 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs\n \n     @Override\n     public void commitTransaction() throws ProducerFencedException {\n-        if (producerFencedOnCommitTxn) {\n-            throw new ProducerFencedException(\"Producer is fenced\");\n-        }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA1MTY1NA==", "bodyText": "That depends on the purpose of this class. According to the meta comment:\nBy default this mock will synchronously complete each send call successfully. However it can be configured to allow the user to control the completion of the call and supply an optional error for the producer to throw.\nWe are definitely not concerned too much about the internal correctness of MockProducer, as long as it is reproducing the error we injected IMHO.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379051654", "createdAt": "2020-02-13T18:49:44Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "diffHunk": "@@ -182,14 +202,17 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs\n \n     @Override\n     public void commitTransaction() throws ProducerFencedException {\n-        if (producerFencedOnCommitTxn) {\n-            throw new ProducerFencedException(\"Producer is fenced\");\n-        }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYyOTkyOQ=="}, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTg0NTkzMQ==", "bodyText": "We just replaced this with the commitTransactionException -- a user can just set a ProducerFencedException to get the same behavior", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379845931", "createdAt": "2020-02-15T17:56:51Z", "author": {"login": "mjsax"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "diffHunk": "@@ -182,14 +202,17 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs\n \n     @Override\n     public void commitTransaction() throws ProducerFencedException {\n-        if (producerFencedOnCommitTxn) {\n-            throw new ProducerFencedException(\"Producer is fenced\");\n-        }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYyOTkyOQ=="}, "originalCommit": null, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU2MDgwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxMDowN1rOFpFw1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxMDowN1rOFpFw1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMDM1Ng==", "bodyText": "We use StreamsProducer now that encapsulates the actually producer, does transaction state handling, and error handling. RecordCollectorImpl is now agnostic to transactions as desired.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378630356", "createdAt": "2020-02-13T03:10:07Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -61,117 +59,48 @@\n \n     private final Logger log;\n     private final TaskId taskId;\n-    private final boolean eosEnabled;\n-    private final String applicationId;\n+    private final Consumer<byte[], byte[]> mainConsumer;\n+    private final StreamsProducer streamProducer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU2MjUzOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxMTowMlrOFpFxuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxMTowMlrOFpFxuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMDU4NQ==", "bodyText": "Note that we don't even need to call a \"maybeInitTx\" on StreamsProducer -- it embeds the corresponding logic internally and fully automatic.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378630585", "createdAt": "2020-02-13T03:11:02Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -61,117 +59,48 @@\n \n     private final Logger log;\n     private final TaskId taskId;\n-    private final boolean eosEnabled;\n-    private final String applicationId;\n+    private final Consumer<byte[], byte[]> mainConsumer;\n+    private final StreamsProducer streamProducer;\n+    private final ProductionExceptionHandler productionExceptionHandler;\n     private final Sensor droppedRecordsSensor;\n+    private final String applicationId;\n+    private final boolean eosEnabled;\n     private final Map<TopicPartition, Long> offsets;\n-    private final Consumer<byte[], byte[]> consumer;\n-    private final ProductionExceptionHandler productionExceptionHandler;\n \n-    // used when eosEnabled is true only\n-    private boolean transactionInFlight = false;\n-    private Producer<byte[], byte[]> producer;\n     private volatile KafkaException sendException;\n \n     /**\n      * @throws StreamsException fatal error that should cause the thread to die (from producer.initTxn)\n      */\n-    public RecordCollectorImpl(final TaskId taskId,\n-                               final StreamsConfig config,\n-                               final LogContext logContext,\n-                               final StreamsMetricsImpl streamsMetrics,\n-                               final Consumer<byte[], byte[]> consumer,\n-                               final StreamThread.ProducerSupplier producerSupplier) {\n-        this.taskId = taskId;\n-        this.consumer = consumer;\n-        this.offsets = new HashMap<>();\n+    public RecordCollectorImpl(final LogContext logContext,\n+                               final TaskId taskId,\n+                               final Consumer<byte[], byte[]> mainConsumer,\n+                               final StreamsProducer producer,\n+                               final ProductionExceptionHandler productionExceptionHandler,\n+                               final String applicationId,\n+                               final boolean eosAlphaEnabled,\n+                               final StreamsMetricsImpl streamsMetrics) {\n         this.log = logContext.logger(getClass());\n-\n-        this.applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n-        this.productionExceptionHandler = config.defaultProductionExceptionHandler();\n-        this.eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+        this.taskId = taskId;\n+        this.mainConsumer = mainConsumer;\n+        this.streamProducer = producer;\n+        this.productionExceptionHandler = productionExceptionHandler;\n+        this.applicationId = applicationId;\n+        this.eosEnabled = eosAlphaEnabled;\n \n         final String threadId = Thread.currentThread().getName();\n         this.droppedRecordsSensor = TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor(threadId, taskId.toString(), streamsMetrics);\n \n-        producer = producerSupplier.get(taskId);\n-\n-        maybeInitTxns();\n-    }\n-\n-    private void maybeInitTxns() {\n-        if (eosEnabled) {\n-            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n-            // completed yet; do not start the first transaction until the topology has been initialized later\n-            try {\n-                producer.initTransactions();\n-            } catch (final TimeoutException exception) {\n-                final String errorMessage = \"Timeout exception caught when initializing transactions for task \" + taskId + \". \" +\n-                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n-                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n-                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n-\n-                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n-                throw new StreamsException(errorMessage, exception);\n-            } catch (final KafkaException exception) {\n-                throw new StreamsException(\"Error encountered while initializing transactions for task \" + taskId, exception);\n-            }\n-        }\n-    }\n-\n-    private void maybeBeginTxn() {\n-        if (eosEnabled && !transactionInFlight) {\n-            try {\n-                producer.beginTransaction();\n-            } catch (final ProducerFencedException error) {\n-                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction\", error);\n-            }\n-            transactionInFlight = true;\n-        }\n-    }\n-\n-    private void maybeAbortTxn() {\n-        if (eosEnabled && transactionInFlight) {\n-            try {\n-                producer.abortTransaction();\n-            } catch (final ProducerFencedException ignore) {\n-                /* TODO\n-                 * this should actually never happen atm as we guard the call to #abortTransaction\n-                 * -> the reason for the guard is a \"bug\" in the Producer -- it throws IllegalStateException\n-                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got\n-                 * fixed and fall-back to this catch-and-swallow code\n-                 */\n-\n-                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Producer encounter unexpected error trying to abort the transaction\", error);\n-            }\n-            transactionInFlight = false;\n-        }\n+        this.offsets = new HashMap<>();\n     }\n \n     public void commit(final Map<TopicPartition, OffsetAndMetadata> offsets) {\n         if (eosEnabled) {\n-            maybeBeginTxn();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU2MzcxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxMTo1OFrOFpFyZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODo1NjoxNVrOFqAb2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMDc1OA==", "bodyText": "This comment is unclear to me, that is why I added the text -- can you explain what it means @guozhangwang ?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378630758", "createdAt": "2020-02-13T03:11:58Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -98,7 +100,7 @@\n \n         private long totalRestored;\n \n-        // only for active restoring tasks (for standby changelog it is null)\n+        // only for active restoring tasks (for standby changelog it is null) WHAT IS NULL?", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5MTY0MA==", "bodyText": "These first line should be removed -- I did it in my current on-going PR.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379591640", "createdAt": "2020-02-14T18:56:15Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -98,7 +100,7 @@\n \n         private long totalRestored;\n \n-        // only for active restoring tasks (for standby changelog it is null)\n+        // only for active restoring tasks (for standby changelog it is null) WHAT IS NULL?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMDc1OA=="}, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU2NDAwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxMjoxNFrOFpFynQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwNDo1MzoyN1rOFpG_dA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMDgxMw==", "bodyText": "Just some side cleanup :)", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378630813", "createdAt": "2020-02-13T03:12:14Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -171,7 +175,7 @@ int bufferedLimitIndex() {\n         }\n     }\n \n-    private final static long DEFAULT_OFFSET_UPDATE_MS = 5 * 60 * 1000; // five minutes\n+    private final static long DEFAULT_OFFSET_UPDATE_MS = Duration.ofMinutes(5L).toMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY1MDQ4NA==", "bodyText": "This is nice!", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378650484", "createdAt": "2020-02-13T04:53:27Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -171,7 +175,7 @@ int bufferedLimitIndex() {\n         }\n     }\n \n-    private final static long DEFAULT_OFFSET_UPDATE_MS = 5 * 60 * 1000; // five minutes\n+    private final static long DEFAULT_OFFSET_UPDATE_MS = Duration.ofMinutes(5L).toMillis();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMDgxMw=="}, "originalCommit": null, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU2NzU4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxNDoxNVrOFpF0lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxNDoxNVrOFpF0lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMTMxOA==", "bodyText": "I think we could also set to null here -- not sure which one would be better?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378631318", "createdAt": "2020-02-13T03:14:15Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -336,25 +335,26 @@ void close() {}\n                 time,\n                 log);\n \n-            final boolean eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+            final boolean eosEnabled = EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n             if (!eosEnabled) {\n                 final Map<String, Object> producerConfigs = config.getProducerConfigs(getThreadProducerClientId(threadId));\n                 log.info(\"Creating thread producer client\");\n-                threadProducer = clientSupplier.getProducer(producerConfigs);\n+                this.threadProducer = clientSupplier.getProducer(producerConfigs);\n+                this.taskProducer = Collections.emptyMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU3Mzc3OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxODoyMlrOFpF4DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxODoyMlrOFpF4DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMjIwNQ==", "bodyText": "Seems the comment does not make sense any longer? Only side cleanup in this class", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378632205", "createdAt": "2020-02-13T03:18:22Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -50,26 +50,23 @@\n import static org.apache.kafka.streams.processor.internals.Task.State.RESTORING;\n \n public class TaskManager {\n-    // initialize the task list\n-    // activeTasks needs to be concurrent as it can be accessed\n-    // by QueryableState", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU3NTU2OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxOTo0MVrOFpF5GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoxOTo0MVrOFpF5GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMjQ3Mg==", "bodyText": "To avoid redundant code, we setup everything for non-eos and eos case upfront (even if not every test needs everything).", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378632472", "createdAt": "2020-02-13T03:19:41Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -93,15 +94,38 @@\n \n     private final StringSerializer stringSerializer = new StringSerializer();\n     private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n-    private final MockProducer<byte[], byte[]> producer = new MockProducer<>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n-    private final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final StreamsProducer streamsProducer = new StreamsProducer(mockProducer, null);\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final StreamsProducer eosStreamsProducer = new StreamsProducer(eosMockProducer, taskId);\n+    private final MockConsumer<byte[], byte[]> mockConsumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU3NjgwOnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyMDozOVrOFpF51w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyMDozOVrOFpF51w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMjY2Mw==", "bodyText": "We can avoid to create \"collectors\" in almost all tests now reducing the overall complexity of the test", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378632663", "createdAt": "2020-02-13T03:20:39Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -185,27 +209,18 @@ public void shouldSendWithNoPartition() {\n         assertEquals(3L, (long) offsets.get(new TopicPartition(topic, 0)));\n         assertEquals(2L, (long) offsets.get(new TopicPartition(topic, 1)));\n         assertEquals(1L, (long) offsets.get(new TopicPartition(topic, 2)));\n-        assertEquals(9, producer.history().size());\n+        assertEquals(9, mockProducer.history().size());\n     }\n \n     @Test\n     public void shouldUpdateOffsetsUponCompletion() {\n-        final RecordCollector collector = new RecordCollectorImpl(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU3NzM4OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyMToxMVrOFpF6Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyMToxMVrOFpF6Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMjc2Mw==", "bodyText": "I added stricter verification to all tests, to make sure the right exception was triggered.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378632763", "createdAt": "2020-02-13T03:21:11Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -217,193 +232,195 @@ public void shouldUpdateOffsetsUponCompletion() {\n \n     @Test\n     public void shouldThrowStreamsExceptionOnSendFatalException() {\n-        final KafkaException exception = new KafkaException();\n-        final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            streamsConfig,\n-            logContext,\n-            streamsMetrics,\n-            consumer,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {\n-                    throw exception;\n-                }\n-            }\n-        );\n+        mockProducer.sendException  = new KafkaException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class, () ->\n             collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n         );\n-        assertEquals(exception, thrown.getCause());\n+\n+        assertEquals(mockProducer.sendException, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task null due to:\\norg.apache.kafka.common.KafkaException: KABOOM!\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU3OTQwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyMjo0MFrOFpF7cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyMjo0MFrOFpF7cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMzA3NA==", "bodyText": "MockProducer does not support the correct handling of callback exceptions, hence, we need to do it manually. (Did not think it worth to extend MockProducer -- let me know what you think)", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378633074", "createdAt": "2020-02-13T03:22:40Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -217,193 +232,195 @@ public void shouldUpdateOffsetsUponCompletion() {\n \n     @Test\n     public void shouldThrowStreamsExceptionOnSendFatalException() {\n-        final KafkaException exception = new KafkaException();\n-        final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            streamsConfig,\n-            logContext,\n-            streamsMetrics,\n-            consumer,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {\n-                    throw exception;\n-                }\n-            }\n-        );\n+        mockProducer.sendException  = new KafkaException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class, () ->\n             collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n         );\n-        assertEquals(exception, thrown.getCause());\n+\n+        assertEquals(mockProducer.sendException, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task null due to:\\norg.apache.kafka.common.KafkaException: KABOOM!\"));\n     }\n \n     @Test\n     public void shouldThrowTaskMigratedExceptionOnProducerFencedException() {\n-        final KafkaException exception = new ProducerFencedException(\"KABOOM!\");\n-        final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            streamsConfig,\n-            logContext,\n-            streamsMetrics,\n-            consumer,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {\n-                    throw new KafkaException(exception);\n-                }\n-            }\n-        );\n+        // cannot use `eosMockProducer.fenceProducer()` because this would already trigger in `beginTransaction()`\n+        final ProducerFencedException exception = new ProducerFencedException(\"KABOOM!\");\n+        // we need to mimic that `send()` always wraps error in a KafkaException\n+        eosMockProducer.sendException = new KafkaException(exception);\n \n-        final TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () ->\n-            collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n+        final TaskMigratedException thrown = assertThrows(\n+            TaskMigratedException.class,\n+            () -> eosCollector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n         );\n+\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Producer cannot send records anymore since it got fenced\"));\n+\n     }\n \n     @Test\n     public void shouldThrowTaskMigratedExceptionOnUnknownProducerIdException() {\n-        final KafkaException exception = new UnknownProducerIdException(\"KABOOM!\");\n-        final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            streamsConfig,\n-            logContext,\n-            streamsMetrics,\n-            consumer,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {\n-                    throw new KafkaException(exception);\n-                }\n-            }\n-        );\n+        final UnknownProducerIdException exception = new UnknownProducerIdException(\"KABOOM!\");\n+        // we need to mimic that `send()` always wraps error in a KafkaException\n+        eosMockProducer.sendException = new KafkaException(exception);\n \n-        final TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () ->\n-            collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n+        final TaskMigratedException thrown = assertThrows(\n+            TaskMigratedException.class,\n+            () -> eosCollector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n         );\n+\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Producer cannot send records anymore since it got fenced\"));\n     }\n \n     @Test\n     public void shouldThrowTaskMigratedExceptionOnSubsequentCallWhenProducerFencedInCallback() {\n         final KafkaException exception = new ProducerFencedException(\"KABOOM!\");\n         final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            streamsConfig,\n             logContext,\n-            streamsMetrics,\n-            null,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {\n-                    callback.onCompletion(null, exception);\n-                    return null;\n-                }\n-            }\n+            taskId,\n+            mockConsumer,\n+            new StreamsProducer(\n+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n+                    @Override\n+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {\n+                        callback.onCompletion(null, exception);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 260}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU4NDQ4OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyNjo0OFrOFpF-kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxNDoyOTowNFrOFrqPdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMzg3NA==", "bodyText": "The name and the test does not really match as it throws in beginTransaction. In any case, we have test for both cases already and thus I removed this duplicate test (please verify that I did not reduce test coverage)", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378633874", "createdAt": "2020-02-13T03:26:48Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -422,440 +439,268 @@ public void shouldNotThrowStreamsExceptionOnSubsequentCallIfASendFailsWithContin\n     @Test\n     public void shouldThrowStreamsExceptionOnSubsequentCallIfFatalEvenWithContinueExceptionHandler() {\n         final KafkaException exception = new AuthenticationException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, AlwaysContinueProductionExceptionHandler.class.getName());\n         final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            new StreamsConfig(props),\n             logContext,\n-            streamsMetrics,\n-            null,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {\n-                    callback.onCompletion(null, exception);\n-                    return null;\n-                }\n-            }\n+            taskId,\n+            mockConsumer,\n+            new StreamsProducer(\n+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n+                    @Override\n+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {\n+                        callback.onCompletion(null, exception);\n+                        return null;\n+                    }\n+                },\n+                null),\n+            new AlwaysContinueProductionExceptionHandler(),\n+            \"appId\",\n+            false,\n+            streamsMetrics\n         );\n \n         collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner);\n \n-        StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n+        StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n         );\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n \n         thrown = assertThrows(StreamsException.class, collector::flush);\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n \n         thrown = assertThrows(StreamsException.class, collector::close);\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n     }\n \n     @Test\n     public void shouldThrowStreamsExceptionOnEOSInitializeTimeout() {\n-        final KafkaException exception = new TimeoutException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init\n+        mockProducer.initTransactionException = new TimeoutException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            new RecordCollectorImpl(\n-                taskId,\n-                new StreamsConfig(props),\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> new RecordCollectorImpl(\n                 logContext,\n-                streamsMetrics,\n-                null,\n-                id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                    @Override\n-                    public void initTransactions() {\n-                        throw exception;\n-                    }\n-                }\n+                taskId,\n+                mockConsumer,\n+                new StreamsProducer(mockProducer, taskId),\n+                productionExceptionHandler,\n+                \"appId\",\n+                true,\n+                streamsMetrics\n             )\n         );\n-        assertEquals(exception, thrown.getCause());\n+\n+        assertEquals(mockProducer.initTransactionException, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Timeout exception caught when initializing transactions for task 0_0. \\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, or the connection to broker was interrupted sending the request or receiving the response. \\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\"));\n     }\n \n     @Test\n     public void shouldThrowStreamsExceptionOnEOSInitializeError() {\n-        final KafkaException exception = new KafkaException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init\n+        mockProducer.initTransactionException = new KafkaException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            new RecordCollectorImpl(\n-                taskId,\n-                new StreamsConfig(props),\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> new RecordCollectorImpl(\n                 logContext,\n-                streamsMetrics,\n-                null,\n-                id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                    @Override\n-                    public void initTransactions() {\n-                        throw exception;\n-                    }\n-                }\n+                taskId,\n+                mockConsumer,\n+                new StreamsProducer(mockProducer, taskId),\n+                productionExceptionHandler,\n+                \"appId\",\n+                true,\n+                streamsMetrics\n             )\n         );\n-        assertEquals(exception, thrown.getCause());\n-    }\n \n-    @Test\n-    public void shouldThrowMigrateExceptionOnEOSFirstSendProducerFenced() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 572}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NDY2OA==", "bodyText": "The two cases are differ that one throwing KafkaException (fatal) and the other throwing ProducerFencedException (task-migrated).", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380984668", "createdAt": "2020-02-18T22:54:26Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -422,440 +439,268 @@ public void shouldNotThrowStreamsExceptionOnSubsequentCallIfASendFailsWithContin\n     @Test\n     public void shouldThrowStreamsExceptionOnSubsequentCallIfFatalEvenWithContinueExceptionHandler() {\n         final KafkaException exception = new AuthenticationException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, AlwaysContinueProductionExceptionHandler.class.getName());\n         final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            new StreamsConfig(props),\n             logContext,\n-            streamsMetrics,\n-            null,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {\n-                    callback.onCompletion(null, exception);\n-                    return null;\n-                }\n-            }\n+            taskId,\n+            mockConsumer,\n+            new StreamsProducer(\n+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n+                    @Override\n+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {\n+                        callback.onCompletion(null, exception);\n+                        return null;\n+                    }\n+                },\n+                null),\n+            new AlwaysContinueProductionExceptionHandler(),\n+            \"appId\",\n+            false,\n+            streamsMetrics\n         );\n \n         collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner);\n \n-        StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n+        StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n         );\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n \n         thrown = assertThrows(StreamsException.class, collector::flush);\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n \n         thrown = assertThrows(StreamsException.class, collector::close);\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n     }\n \n     @Test\n     public void shouldThrowStreamsExceptionOnEOSInitializeTimeout() {\n-        final KafkaException exception = new TimeoutException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init\n+        mockProducer.initTransactionException = new TimeoutException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            new RecordCollectorImpl(\n-                taskId,\n-                new StreamsConfig(props),\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> new RecordCollectorImpl(\n                 logContext,\n-                streamsMetrics,\n-                null,\n-                id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                    @Override\n-                    public void initTransactions() {\n-                        throw exception;\n-                    }\n-                }\n+                taskId,\n+                mockConsumer,\n+                new StreamsProducer(mockProducer, taskId),\n+                productionExceptionHandler,\n+                \"appId\",\n+                true,\n+                streamsMetrics\n             )\n         );\n-        assertEquals(exception, thrown.getCause());\n+\n+        assertEquals(mockProducer.initTransactionException, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Timeout exception caught when initializing transactions for task 0_0. \\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, or the connection to broker was interrupted sending the request or receiving the response. \\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\"));\n     }\n \n     @Test\n     public void shouldThrowStreamsExceptionOnEOSInitializeError() {\n-        final KafkaException exception = new KafkaException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init\n+        mockProducer.initTransactionException = new KafkaException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            new RecordCollectorImpl(\n-                taskId,\n-                new StreamsConfig(props),\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> new RecordCollectorImpl(\n                 logContext,\n-                streamsMetrics,\n-                null,\n-                id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                    @Override\n-                    public void initTransactions() {\n-                        throw exception;\n-                    }\n-                }\n+                taskId,\n+                mockConsumer,\n+                new StreamsProducer(mockProducer, taskId),\n+                productionExceptionHandler,\n+                \"appId\",\n+                true,\n+                streamsMetrics\n             )\n         );\n-        assertEquals(exception, thrown.getCause());\n-    }\n \n-    @Test\n-    public void shouldThrowMigrateExceptionOnEOSFirstSendProducerFenced() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMzg3NA=="}, "originalCommit": null, "originalPosition": 572}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTMyNTE3Mg==", "bodyText": "My argument was that the old test was named \"send\" but did throw in \"beginTx\" (and there was already a corresponding \"beginTx\" test in the old code -> hence \"redundant\" / or at least need a fix). However, the new StreamsProducerTest should cover all cases for \"send\" as well as \"beginTx\":\nBeginTx:\n\nshouldThrowTaskMigrateExceptionOnEosBeginTxnFenced\nshouldThrowTaskMigrateExceptionOnEosBeginTxnError\nshouldFailOnEosBeginTxnFatal\n\nsend:\n\nshouldThrowTaskMigratedExceptionOnEosSendFenced\nshouldThrowTaskMigratedExceptionOnEosSendUnknownPid\nshouldThrowTaskMigrateExceptionOnEosSendOffsetFenced\nshouldThrowStreamsExceptionOnEosSendOffsetError\nshouldFailOnEosSendOffsetFatal", "url": "https://github.com/apache/kafka/pull/8105#discussion_r381325172", "createdAt": "2020-02-19T14:29:04Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -422,440 +439,268 @@ public void shouldNotThrowStreamsExceptionOnSubsequentCallIfASendFailsWithContin\n     @Test\n     public void shouldThrowStreamsExceptionOnSubsequentCallIfFatalEvenWithContinueExceptionHandler() {\n         final KafkaException exception = new AuthenticationException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, AlwaysContinueProductionExceptionHandler.class.getName());\n         final RecordCollector collector = new RecordCollectorImpl(\n-            taskId,\n-            new StreamsConfig(props),\n             logContext,\n-            streamsMetrics,\n-            null,\n-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                @Override\n-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {\n-                    callback.onCompletion(null, exception);\n-                    return null;\n-                }\n-            }\n+            taskId,\n+            mockConsumer,\n+            new StreamsProducer(\n+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n+                    @Override\n+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {\n+                        callback.onCompletion(null, exception);\n+                        return null;\n+                    }\n+                },\n+                null),\n+            new AlwaysContinueProductionExceptionHandler(),\n+            \"appId\",\n+            false,\n+            streamsMetrics\n         );\n \n         collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner);\n \n-        StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n+        StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner)\n         );\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n \n         thrown = assertThrows(StreamsException.class, collector::flush);\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n \n         thrown = assertThrows(StreamsException.class, collector::close);\n         assertEquals(exception, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic for task 0_0 due to:\\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.\"));\n     }\n \n     @Test\n     public void shouldThrowStreamsExceptionOnEOSInitializeTimeout() {\n-        final KafkaException exception = new TimeoutException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init\n+        mockProducer.initTransactionException = new TimeoutException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            new RecordCollectorImpl(\n-                taskId,\n-                new StreamsConfig(props),\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> new RecordCollectorImpl(\n                 logContext,\n-                streamsMetrics,\n-                null,\n-                id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                    @Override\n-                    public void initTransactions() {\n-                        throw exception;\n-                    }\n-                }\n+                taskId,\n+                mockConsumer,\n+                new StreamsProducer(mockProducer, taskId),\n+                productionExceptionHandler,\n+                \"appId\",\n+                true,\n+                streamsMetrics\n             )\n         );\n-        assertEquals(exception, thrown.getCause());\n+\n+        assertEquals(mockProducer.initTransactionException, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Timeout exception caught when initializing transactions for task 0_0. \\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, or the connection to broker was interrupted sending the request or receiving the response. \\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\"));\n     }\n \n     @Test\n     public void shouldThrowStreamsExceptionOnEOSInitializeError() {\n-        final KafkaException exception = new KafkaException(\"KABOOM!\");\n-        final Properties props = StreamsTestUtils.getStreamsConfig(\"test\");\n-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init\n+        mockProducer.initTransactionException = new KafkaException(\"KABOOM!\");\n \n-        final StreamsException thrown = assertThrows(StreamsException.class, () ->\n-            new RecordCollectorImpl(\n-                taskId,\n-                new StreamsConfig(props),\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> new RecordCollectorImpl(\n                 logContext,\n-                streamsMetrics,\n-                null,\n-                id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {\n-                    @Override\n-                    public void initTransactions() {\n-                        throw exception;\n-                    }\n-                }\n+                taskId,\n+                mockConsumer,\n+                new StreamsProducer(mockProducer, taskId),\n+                productionExceptionHandler,\n+                \"appId\",\n+                true,\n+                streamsMetrics\n             )\n         );\n-        assertEquals(exception, thrown.getCause());\n-    }\n \n-    @Test\n-    public void shouldThrowMigrateExceptionOnEOSFirstSendProducerFenced() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzMzg3NA=="}, "originalCommit": null, "originalPosition": 572}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU4NzE3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyODozN1rOFpGAGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMjozMDo1MFrOFslb-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzNDI2Nw==", "bodyText": "This is actually an incorrect test -- on commit, we would never do anything with StandbyTasks because they always return false on commitNeeded() -- the used StateMachineTask got a corresponding fix.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378634267", "createdAt": "2020-02-13T03:28:37Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -428,7 +428,7 @@ public void shouldCommitActiveAndStandbyTasks() {\n         task00.setCommitNeeded();\n         task01.setCommitNeeded();\n \n-        assertThat(taskManager.commitAll(), equalTo(2));\n+        assertThat(taskManager.commitAll(), equalTo(1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NjY2Mg==", "bodyText": "Hmm... maybe that's correct, but that would also mean that the standby tasks are not committed  at any occasions except closing which sounds like a lurking bug to me?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380986662", "createdAt": "2020-02-18T22:59:12Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -428,7 +428,7 @@ public void shouldCommitActiveAndStandbyTasks() {\n         task00.setCommitNeeded();\n         task01.setCommitNeeded();\n \n-        assertThat(taskManager.commitAll(), equalTo(2));\n+        assertThat(taskManager.commitAll(), equalTo(1));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzNDI2Nw=="}, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTMyODcyNQ==", "bodyText": "Sounds correct -- I was double checking this code and this issue was introduced in the refactoring via 4090f9a\nTo not block this PR further, I would suggest to keep this change as-is and do a follow up PR to fix the issue.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r381328725", "createdAt": "2020-02-19T14:34:06Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -428,7 +428,7 @@ public void shouldCommitActiveAndStandbyTasks() {\n         task00.setCommitNeeded();\n         task01.setCommitNeeded();\n \n-        assertThat(taskManager.commitAll(), equalTo(2));\n+        assertThat(taskManager.commitAll(), equalTo(1));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzNDI2Nw=="}, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI5NTAzMg==", "bodyText": "I will make a separate PR to fix this.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r382295032", "createdAt": "2020-02-20T22:30:50Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -428,7 +428,7 @@ public void shouldCommitActiveAndStandbyTasks() {\n         task00.setCommitNeeded();\n         task01.setCommitNeeded();\n \n-        assertThat(taskManager.commitAll(), equalTo(2));\n+        assertThat(taskManager.commitAll(), equalTo(1));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzNDI2Nw=="}, "originalCommit": null, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU4NzU2OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyODo1NVrOFpGAVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyODo1NVrOFpGAVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzNDMyNA==", "bodyText": "Removed this test for the same reason as above.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378634324", "createdAt": "2020-02-13T03:28:55Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -462,37 +462,6 @@ public void commit() {\n         }\n     }\n \n-    @Test\n-    public void shouldPropagateExceptionFromStandbyCommit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU4ODUwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyOToyOVrOFpGA0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzoyOToyOVrOFpGA0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzNDQ0OA==", "bodyText": "This is the fix for StateMachineTask -- it should behave the same way as an actual active or standby task", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378634448", "createdAt": "2020-02-13T03:29:29Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -756,7 +725,7 @@ public void setCommitNeeded() {\n \n         @Override\n         public boolean commitNeeded() {\n-            return commitNeeded;\n+            return active && commitNeeded;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MjU4OTU1OnYy", "diffSide": "LEFT", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzozMDoyMFrOFpGBbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMzozMDoyMFrOFpGBbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYzNDYwNA==", "bodyText": "After John's TDD fix, this is not used any longer (just cleanup to remove it).", "url": "https://github.com/apache/kafka/pull/8105#discussion_r378634604", "createdAt": "2020-02-13T03:30:20Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -212,7 +214,6 @@\n \n     private final MockProducer<byte[], byte[]> producer;\n \n-    private final Set<String> internalTopics = new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NTE4MTk0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozMzoxMFrOFpe8cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozMzoxMFrOFpe8cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA0MjkzMA==", "bodyText": "nit: could be simplified as eosEnabled", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379042930", "createdAt": "2020-02-13T18:33:10Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -61,117 +59,48 @@\n \n     private final Logger log;\n     private final TaskId taskId;\n-    private final boolean eosEnabled;\n-    private final String applicationId;\n+    private final Consumer<byte[], byte[]> mainConsumer;\n+    private final StreamsProducer streamProducer;\n+    private final ProductionExceptionHandler productionExceptionHandler;\n     private final Sensor droppedRecordsSensor;\n+    private final String applicationId;\n+    private final boolean eosEnabled;\n     private final Map<TopicPartition, Long> offsets;\n-    private final Consumer<byte[], byte[]> consumer;\n-    private final ProductionExceptionHandler productionExceptionHandler;\n \n-    // used when eosEnabled is true only\n-    private boolean transactionInFlight = false;\n-    private Producer<byte[], byte[]> producer;\n     private volatile KafkaException sendException;\n \n     /**\n      * @throws StreamsException fatal error that should cause the thread to die (from producer.initTxn)\n      */\n-    public RecordCollectorImpl(final TaskId taskId,\n-                               final StreamsConfig config,\n-                               final LogContext logContext,\n-                               final StreamsMetricsImpl streamsMetrics,\n-                               final Consumer<byte[], byte[]> consumer,\n-                               final StreamThread.ProducerSupplier producerSupplier) {\n-        this.taskId = taskId;\n-        this.consumer = consumer;\n-        this.offsets = new HashMap<>();\n+    public RecordCollectorImpl(final LogContext logContext,\n+                               final TaskId taskId,\n+                               final Consumer<byte[], byte[]> mainConsumer,\n+                               final StreamsProducer producer,\n+                               final ProductionExceptionHandler productionExceptionHandler,\n+                               final String applicationId,\n+                               final boolean eosAlphaEnabled,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjE1MDQxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo0NDoyNVrOFpobQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo0NDoyNVrOFpobQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE5ODI3NA==", "bodyText": "This is not used.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379198274", "createdAt": "2020-02-14T00:44:25Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -61,117 +59,47 @@\n \n     private final Logger log;\n     private final TaskId taskId;\n-    private final boolean eosEnabled;\n-    private final String applicationId;\n+    private final Consumer<byte[], byte[]> mainConsumer;\n+    private final TransactionManager streamProducer;\n+    private final ProductionExceptionHandler productionExceptionHandler;\n     private final Sensor droppedRecordsSensor;\n+    private final String applicationId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjE1NzA1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo0ODoxOFrOFpofPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo0ODoxOFrOFpofPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE5OTI5NA==", "bodyText": "\"Task \" + taskId + \" could not get partition information for topic \" + topic", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379199294", "createdAt": "2020-02-14T00:48:18Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -246,11 +174,11 @@ private void recordSendError(final String topic, final Exception exception, fina\n \n         // TODO K9113: we need to decide how to handle exceptions from partitionsFor\n         if (partitioner != null) {\n-            final List<PartitionInfo> partitions = producer.partitionsFor(topic);\n+            final List<PartitionInfo> partitions = streamProducer.partitionsFor(topic);\n             if (partitions.size() > 0) {\n                 partition = partitioner.partition(topic, key, value, partitions.size());\n             } else {\n-                throw new StreamsException(\"Could not get partition information for topic '\" + topic + \"'  for task \" + taskId +\n+                throw new StreamsException(\"Could not get partition information for topic \" + topic + \" for task \" + taskId +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjE2NzU5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo1NDowMlrOFpolWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo1NDowMlrOFpolWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIwMDg1OA==", "bodyText": "s/productionExceptionIsFatal/isFatalException?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379200858", "createdAt": "2020-02-14T00:54:02Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -229,94 +186,37 @@ private void recordSendError(final String topic, final Exception exception, fina\n         log.error(errorMessage);\n     }\n \n-    /**\n-     * @throws StreamsException fatal error that should cause the thread to die\n-     * @throws TaskMigratedException recoverable error that would cause the task to be removed\n-     */\n-    @Override\n-    public <K, V> void send(final String topic,\n-                            final K key,\n-                            final V value,\n-                            final Headers headers,\n-                            final Long timestamp,\n-                            final Serializer<K> keySerializer,\n-                            final Serializer<V> valueSerializer,\n-                            final StreamPartitioner<? super K, ? super V> partitioner) {\n-        final Integer partition;\n+    private boolean productionExceptionIsFatal(final Exception exception) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 244}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjE3MjEyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo1NjoxNFrOFpon5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMDo1NjoxNFrOFpon5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIwMTUwOQ==", "bodyText": "A meta comment is useful to distinguish from producer TransactionManager", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379201509", "createdAt": "2020-02-14T00:56:14Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODYyNzM3OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODo0Mzo1NlrOFqAHOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODo0Mzo1NlrOFqAHOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU4NjM2Mg==", "bodyText": "nit: can we consolidate producerFencedOnCommitTxn to the more-general commitTransactionException? I.e. if you want to fence on commit, you just register the commitTransactionException as a ProducerFencedException", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379586362", "createdAt": "2020-02-14T18:43:56Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java", "diffHunk": "@@ -182,14 +206,17 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs\n \n     @Override\n     public void commitTransaction() throws ProducerFencedException {\n-        if (producerFencedOnCommitTxn) {\n-            throw new ProducerFencedException(\"Producer is fenced\");\n-        }\n-\n         verifyProducerState();\n         verifyTransactionsInitialized();\n         verifyNoTransactionInFlight();\n \n+        if (this.commitTransactionException != null) {\n+            throw this.commitTransactionException;\n+        }\n+        if (this.producerFencedOnCommitTxn) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODY1NTMyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODo1NDo1MlrOFqAZXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODo1NDo1MlrOFqAZXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5MTAwNQ==", "bodyText": "The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379591005", "createdAt": "2020-02-14T18:54:52Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -61,142 +59,101 @@\n \n     private final Logger log;\n     private final TaskId taskId;\n-    private final boolean eosEnabled;\n-    private final String applicationId;\n+    private final Consumer<byte[], byte[]> mainConsumer;\n+    private final TransactionManager transactionManager;\n+    private final ProductionExceptionHandler productionExceptionHandler;\n     private final Sensor droppedRecordsSensor;\n+    private final boolean eosEnabled;\n     private final Map<TopicPartition, Long> offsets;\n-    private final Consumer<byte[], byte[]> consumer;\n-    private final ProductionExceptionHandler productionExceptionHandler;\n \n-    // used when eosEnabled is true only\n-    private boolean transactionInFlight = false;\n-    private Producer<byte[], byte[]> producer;\n     private volatile KafkaException sendException;\n \n     /**\n      * @throws StreamsException fatal error that should cause the thread to die (from producer.initTxn)\n      */\n-    public RecordCollectorImpl(final TaskId taskId,\n-                               final StreamsConfig config,\n-                               final LogContext logContext,\n-                               final StreamsMetricsImpl streamsMetrics,\n-                               final Consumer<byte[], byte[]> consumer,\n-                               final StreamThread.ProducerSupplier producerSupplier) {\n-        this.taskId = taskId;\n-        this.consumer = consumer;\n-        this.offsets = new HashMap<>();\n+    public RecordCollectorImpl(final LogContext logContext,\n+                               final TaskId taskId,\n+                               final Consumer<byte[], byte[]> mainConsumer,\n+                               final TransactionManager transactionManager,\n+                               final ProductionExceptionHandler productionExceptionHandler,\n+                               final boolean eosAlphaEnabled,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODY2NzIwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODo1OToxNFrOFqAhMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODo1OToxNFrOFqAhMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5MzAxMA==", "bodyText": "nit: taskProducers", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379593010", "createdAt": "2020-02-14T18:59:14Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -314,7 +312,7 @@ void close() {}\n         private final ThreadCache cache;\n         private final Producer<byte[], byte[]> threadProducer;\n         private final KafkaClientSupplier clientSupplier;\n-        private final ProducerSupplier producerSupplier;\n+        final Map<TaskId, Producer<byte[], byte[]>> taskProducer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODY3MzUwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOTowMTo0NlrOFqAlVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOTozNzozMVrOFqBhjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5NDA2OQ==", "bodyText": "When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379594069", "createdAt": "2020-02-14T19:01:46Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -374,19 +373,30 @@ StreamTask createTask(final Consumer<byte[], byte[]> consumer,\n                 storeChangelogReader,\n                 logContext);\n \n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(getTaskProducerClientId(threadId, taskId));\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducer.put(taskId, clientSupplier.getProducer(producerConfigs));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwOTQ4NA==", "bodyText": "The TaskManager does this in handleAssignment() and handleLostAll().", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379609484", "createdAt": "2020-02-14T19:37:31Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -374,19 +373,30 @@ StreamTask createTask(final Consumer<byte[], byte[]> consumer,\n                 storeChangelogReader,\n                 logContext);\n \n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(getTaskProducerClientId(threadId, taskId));\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducer.put(taskId, clientSupplier.getProducer(producerConfigs));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5NDA2OQ=="}, "originalCommit": null, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODY4ODk3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOTowNzozNlrOFqAu-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo0MzozN1rOFrVKzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5NjUzOQ==", "bodyText": "Regret to see this ... I guess in order to allow mocking the task creation inside threadTest we'd have to do this, but it also makes the code less readable. Maybe we can augment the createTask interface to pass in all parameters so that the preparation logic can then be pushed into the StreamThread?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379596539", "createdAt": "2020-02-14T19:07:36Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -558,6 +559,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n             cache,\n             time,\n             clientSupplier,\n+            taskProducer,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5OTU0OQ==", "bodyText": "Also the taskProducers map is reference in three different classes: 1) creator to populate it, 2) task-manager to remove from it, 3) thread to get metrics from it (this is to be fixed).\nI think we can let it to be purely owned by the task-manager, after we fixed 3) (cc @vvcephei ), as for 1) if we can augment the createTask above, we can then push the addition logic into TaskManager and creates the record-collector with the passed-in producer, and the only pass-in the record collector into createTask interface.\nWDYT?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379599549", "createdAt": "2020-02-14T19:14:28Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -558,6 +559,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n             cache,\n             time,\n             clientSupplier,\n+            taskProducer,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5NjUzOQ=="}, "originalCommit": null, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxMTAxMA==", "bodyText": "I did not want to do more refactoring this PR is already quite big -- it would be best to push TaskCreator stuff completely into TaskManager.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379611010", "createdAt": "2020-02-14T19:41:09Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -558,6 +559,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n             cache,\n             time,\n             clientSupplier,\n+            taskProducer,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5NjUzOQ=="}, "originalCommit": null, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3OTkxOQ==", "bodyText": "Fair enough :) could you add a TODO marker here?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380979919", "createdAt": "2020-02-18T22:43:37Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -558,6 +559,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n             cache,\n             time,\n             clientSupplier,\n+            taskProducer,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5NjUzOQ=="}, "originalCommit": null, "originalPosition": 175}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODcxNzY4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToxODoyNlrOFqBBTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOTo0MzozMVrOFqBrYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwMTIyOA==", "bodyText": "nit: in KIP-447 we would not only use applicationId but also consumer's whole ConsumerGroupMetadata, so for future-uses I'm thinking maybe we can just pass in the consumer object here, and then before KIP-447 we can just get the applicationId from its consumer.metadata.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379601228", "createdAt": "2020-02-14T19:18:26Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxMjAwMg==", "bodyText": "It's for sure an intermediate state -- will get changed in follow up PRs.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379612002", "createdAt": "2020-02-14T19:43:31Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwMTIyOA=="}, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODczODUxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToyNTo1OVrOFqBOFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToyNTo1OVrOFqBOFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwNDUwMw==", "bodyText": "The passed in task-id are used for two purposes: 1) define eosAlphaEnabled, 2) use in logging (btw there are still some place where we do not check taskId == null). And in #8058 we no longer log the task-id in TaskMigrated since when it happens we would migrate all the tasks anyways. So we do not need it in the TaskMigrated exception.\nSo I'd suggest the following:\na) we keep a String for taskIds which would be a single task-id if the passed in taskId != null or default to \"all owned active tasks\" for logging purposes only.\nb) we keep the eosEnabled checking the taskId as is.\nAnd moving forward as we remove the alpha we would replace this parameter with a boolean only since it would always be all owned active tasks.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379604503", "createdAt": "2020-02-14T19:25:59Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODc0MDk2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToyNjo1MlrOFqBPmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToyNjo1MlrOFqBPmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwNDg5MQ==", "bodyText": "Ditto here: even later when we introduce the eosBeta the logic here would be the same as we wrap the logic in TransactionProducer right? In that case we can just name this boolean as eosEnabled.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379604891", "createdAt": "2020-02-14T19:26:52Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODc0NDEyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToyODowN1rOFqBRsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxNDoxMDo0MlrOFrphXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwNTQyNg==", "bodyText": "nit: rename to isProducerFenced?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379605426", "createdAt": "2020-02-14T19:28:07Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        eosAlphaEnabled = taskId != null;\n+\n+        initTx();\n+    }\n+\n+    private void initTx() {\n+        if (eosAlphaEnabled) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+            } catch (final TimeoutException exception) {\n+                final String errorMessage = \"Timeout exception caught when initializing transactions for task \" + taskId + \". \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n+\n+                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n+                throw new StreamsException(errorMessage, exception);\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for task \" + taskId, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosAlphaEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for task \" + taskId, error);\n+            }\n+            transactionInFlight = true;\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" for task \" + taskId,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNjk4NA==", "bodyText": "An UnknownProducerIdException does not imply fencing? Does it?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379616984", "createdAt": "2020-02-14T19:55:29Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        eosAlphaEnabled = taskId != null;\n+\n+        initTx();\n+    }\n+\n+    private void initTx() {\n+        if (eosAlphaEnabled) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+            } catch (final TimeoutException exception) {\n+                final String errorMessage = \"Timeout exception caught when initializing transactions for task \" + taskId + \". \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n+\n+                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n+                throw new StreamsException(errorMessage, exception);\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for task \" + taskId, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosAlphaEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for task \" + taskId, error);\n+            }\n+            transactionInFlight = true;\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" for task \" + taskId,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwNTQyNg=="}, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3NDM3MQ==", "bodyText": "Well yes and no: it is not fenced by txn coordinator, but by broker :) Anyways I'm just a feeling a bit awkward that in the caller we say if it is recoverable, we always handle it as task-migrated --- it is a very nit so your call", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380974371", "createdAt": "2020-02-18T22:30:09Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        eosAlphaEnabled = taskId != null;\n+\n+        initTx();\n+    }\n+\n+    private void initTx() {\n+        if (eosAlphaEnabled) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+            } catch (final TimeoutException exception) {\n+                final String errorMessage = \"Timeout exception caught when initializing transactions for task \" + taskId + \". \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n+\n+                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n+                throw new StreamsException(errorMessage, exception);\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for task \" + taskId, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosAlphaEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for task \" + taskId, error);\n+            }\n+            transactionInFlight = true;\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" for task \" + taskId,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwNTQyNg=="}, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTMxMzM3Mg==", "bodyText": "It was called isRecoverable() before -- think I just leave it.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r381313372", "createdAt": "2020-02-19T14:10:42Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final boolean eosAlphaEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        eosAlphaEnabled = taskId != null;\n+\n+        initTx();\n+    }\n+\n+    private void initTx() {\n+        if (eosAlphaEnabled) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+            } catch (final TimeoutException exception) {\n+                final String errorMessage = \"Timeout exception caught when initializing transactions for task \" + taskId + \". \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n+\n+                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n+                throw new StreamsException(errorMessage, exception);\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for task \" + taskId, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosAlphaEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for task \" + taskId, error);\n+            }\n+            transactionInFlight = true;\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" for task \" + taskId,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYwNTQyNg=="}, "originalCommit": null, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0OTQxNzE1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQwMTo0OTo0N1rOFqHlqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxODowMzo1NVrOFqP-xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcwODg0MQ==", "bodyText": "It looks weird to let transactionManager to send record when the application is not on EOS. What's the issue with name StreamProducer?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379708841", "createdAt": "2020-02-15T01:49:47Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -61,142 +59,101 @@\n \n     private final Logger log;\n     private final TaskId taskId;\n-    private final boolean eosEnabled;\n-    private final String applicationId;\n+    private final Consumer<byte[], byte[]> mainConsumer;\n+    private final TransactionManager transactionManager;\n+    private final ProductionExceptionHandler productionExceptionHandler;\n     private final Sensor droppedRecordsSensor;\n+    private final boolean eosEnabled;\n     private final Map<TopicPartition, Long> offsets;\n-    private final Consumer<byte[], byte[]> consumer;\n-    private final ProductionExceptionHandler productionExceptionHandler;\n \n-    // used when eosEnabled is true only\n-    private boolean transactionInFlight = false;\n-    private Producer<byte[], byte[]> producer;\n     private volatile KafkaException sendException;\n \n     /**\n      * @throws StreamsException fatal error that should cause the thread to die (from producer.initTxn)\n      */\n-    public RecordCollectorImpl(final TaskId taskId,\n-                               final StreamsConfig config,\n-                               final LogContext logContext,\n-                               final StreamsMetricsImpl streamsMetrics,\n-                               final Consumer<byte[], byte[]> consumer,\n-                               final StreamThread.ProducerSupplier producerSupplier) {\n-        this.taskId = taskId;\n-        this.consumer = consumer;\n-        this.offsets = new HashMap<>();\n+    public RecordCollectorImpl(final LogContext logContext,\n+                               final TaskId taskId,\n+                               final Consumer<byte[], byte[]> mainConsumer,\n+                               final TransactionManager transactionManager,\n+                               final ProductionExceptionHandler productionExceptionHandler,\n+                               final boolean eosEnabled,\n+                               final StreamsMetricsImpl streamsMetrics) {\n         this.log = logContext.logger(getClass());\n-\n-        this.applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n-        this.productionExceptionHandler = config.defaultProductionExceptionHandler();\n-        this.eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+        this.taskId = taskId;\n+        this.mainConsumer = mainConsumer;\n+        this.transactionManager = transactionManager;\n+        this.productionExceptionHandler = productionExceptionHandler;\n+        this.eosEnabled = eosEnabled;\n \n         final String threadId = Thread.currentThread().getName();\n         this.droppedRecordsSensor = TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor(threadId, taskId.toString(), streamsMetrics);\n \n-        producer = producerSupplier.get(taskId);\n-\n-        maybeInitTxns();\n+        this.offsets = new HashMap<>();\n     }\n \n-    private void maybeInitTxns() {\n-        if (eosEnabled) {\n-            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n-            // completed yet; do not start the first transaction until the topology has been initialized later\n-            try {\n-                producer.initTransactions();\n-            } catch (final TimeoutException exception) {\n-                final String errorMessage = \"Timeout exception caught when initializing transactions for task \" + taskId + \". \" +\n-                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n-                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n-                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n-\n-                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n-                throw new StreamsException(errorMessage, exception);\n-            } catch (final KafkaException exception) {\n-                throw new StreamsException(\"Error encountered while initializing transactions for task \" + taskId, exception);\n-            }\n-        }\n-    }\n+    /**\n+     * @throws StreamsException fatal error that should cause the thread to die\n+     * @throws TaskMigratedException recoverable error that would cause the task to be removed\n+     */\n+    @Override\n+    public <K, V> void send(final String topic,\n+                            final K key,\n+                            final V value,\n+                            final Headers headers,\n+                            final Long timestamp,\n+                            final Serializer<K> keySerializer,\n+                            final Serializer<V> valueSerializer,\n+                            final StreamPartitioner<? super K, ? super V> partitioner) {\n+        final Integer partition;\n \n-    private void maybeBeginTxn() {\n-        if (eosEnabled && !transactionInFlight) {\n-            try {\n-                producer.beginTransaction();\n-            } catch (final ProducerFencedException error) {\n-                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction\", error);\n+        // TODO K9113: we need to decide how to handle exceptions from partitionsFor\n+        if (partitioner != null) {\n+            final List<PartitionInfo> partitions = transactionManager.partitionsFor(topic);\n+            if (partitions.size() > 0) {\n+                partition = partitioner.partition(topic, key, value, partitions.size());\n+            } else {\n+                throw new StreamsException(\"Could not get partition information for topic \" + topic + \" for task \" + taskId +\n+                    \". This can happen if the topic does not exist.\");\n             }\n-            transactionInFlight = true;\n+        } else {\n+            partition = null;\n         }\n-    }\n \n-    private void maybeAbortTxn() {\n-        if (eosEnabled && transactionInFlight) {\n-            try {\n-                producer.abortTransaction();\n-            } catch (final ProducerFencedException ignore) {\n-                /* TODO\n-                 * this should actually never happen atm as we guard the call to #abortTransaction\n-                 * -> the reason for the guard is a \"bug\" in the Producer -- it throws IllegalStateException\n-                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got\n-                 * fixed and fall-back to this catch-and-swallow code\n-                 */\n-\n-                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Producer encounter unexpected error trying to abort the transaction\", error);\n-            }\n-            transactionInFlight = false;\n-        }\n+        send(topic, key, value, headers, partition, timestamp, keySerializer, valueSerializer);\n     }\n \n-    public void commit(final Map<TopicPartition, OffsetAndMetadata> offsets) {\n-        if (eosEnabled) {\n-            maybeBeginTxn();\n+    @Override\n+    public <K, V> void send(final String topic,\n+                            final K key,\n+                            final V value,\n+                            final Headers headers,\n+                            final Integer partition,\n+                            final Long timestamp,\n+                            final Serializer<K> keySerializer,\n+                            final Serializer<V> valueSerializer) {\n+        checkForException();\n \n-            try {\n-                producer.sendOffsetsToTransaction(offsets, applicationId);\n-                producer.commitTransaction();\n-                transactionInFlight = false;\n-            } catch (final ProducerFencedException error) {\n-                throw new TaskMigratedException(taskId, \"Producer get fenced trying to commit a transaction\", error);\n-            } catch (final TimeoutException error) {\n-                // TODO K9113: currently handle timeout exception as a fatal error, should discuss whether we want to handle it\n-                throw new StreamsException(\"Timed out while committing transaction via producer for task \" + taskId, error);\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Error encountered sending offsets and committing transaction \" +\n-                    \"via producer for task \" + taskId, error);\n-            }\n-        } else {\n-            try {\n-                consumer.commitSync(offsets);\n-            } catch (final CommitFailedException error) {\n-                throw new TaskMigratedException(taskId, \"Consumer committing offsets failed, \" +\n-                    \"indicating the corresponding thread is no longer part of the group.\", error);\n-            } catch (final TimeoutException error) {\n-                // TODO K9113: currently handle timeout exception as a fatal error\n-                throw new StreamsException(\"Timed out while committing offsets via consumer for task \" + taskId, error);\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Error encountered committing offsets via consumer for task \" + taskId, error);\n-            }\n-        }\n+        final byte[] keyBytes = keySerializer.serialize(topic, headers, key);\n+        final byte[] valBytes = valueSerializer.serialize(topic, headers, value);\n \n-    }\n+        final ProducerRecord<byte[], byte[]> serializedRecord = new ProducerRecord<>(topic, partition, timestamp, keyBytes, valBytes, headers);\n \n-    private boolean productionExceptionIsFatal(final Exception exception) {\n-        final boolean securityException = exception instanceof AuthenticationException ||\n-            exception instanceof AuthorizationException ||\n-            exception instanceof SecurityDisabledException;\n+        transactionManager.send(serializedRecord, (metadata, exception) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTg0NjM0MA==", "bodyText": "We can also go back to StreamProducer -- I just had the impression that TransactionManager is the better name. You are correct, that if EOS is disabled, the TM has nothing to manage :) -- I don't feel strong about the name either way.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379846340", "createdAt": "2020-02-15T18:03:55Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -61,142 +59,101 @@\n \n     private final Logger log;\n     private final TaskId taskId;\n-    private final boolean eosEnabled;\n-    private final String applicationId;\n+    private final Consumer<byte[], byte[]> mainConsumer;\n+    private final TransactionManager transactionManager;\n+    private final ProductionExceptionHandler productionExceptionHandler;\n     private final Sensor droppedRecordsSensor;\n+    private final boolean eosEnabled;\n     private final Map<TopicPartition, Long> offsets;\n-    private final Consumer<byte[], byte[]> consumer;\n-    private final ProductionExceptionHandler productionExceptionHandler;\n \n-    // used when eosEnabled is true only\n-    private boolean transactionInFlight = false;\n-    private Producer<byte[], byte[]> producer;\n     private volatile KafkaException sendException;\n \n     /**\n      * @throws StreamsException fatal error that should cause the thread to die (from producer.initTxn)\n      */\n-    public RecordCollectorImpl(final TaskId taskId,\n-                               final StreamsConfig config,\n-                               final LogContext logContext,\n-                               final StreamsMetricsImpl streamsMetrics,\n-                               final Consumer<byte[], byte[]> consumer,\n-                               final StreamThread.ProducerSupplier producerSupplier) {\n-        this.taskId = taskId;\n-        this.consumer = consumer;\n-        this.offsets = new HashMap<>();\n+    public RecordCollectorImpl(final LogContext logContext,\n+                               final TaskId taskId,\n+                               final Consumer<byte[], byte[]> mainConsumer,\n+                               final TransactionManager transactionManager,\n+                               final ProductionExceptionHandler productionExceptionHandler,\n+                               final boolean eosEnabled,\n+                               final StreamsMetricsImpl streamsMetrics) {\n         this.log = logContext.logger(getClass());\n-\n-        this.applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n-        this.productionExceptionHandler = config.defaultProductionExceptionHandler();\n-        this.eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+        this.taskId = taskId;\n+        this.mainConsumer = mainConsumer;\n+        this.transactionManager = transactionManager;\n+        this.productionExceptionHandler = productionExceptionHandler;\n+        this.eosEnabled = eosEnabled;\n \n         final String threadId = Thread.currentThread().getName();\n         this.droppedRecordsSensor = TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor(threadId, taskId.toString(), streamsMetrics);\n \n-        producer = producerSupplier.get(taskId);\n-\n-        maybeInitTxns();\n+        this.offsets = new HashMap<>();\n     }\n \n-    private void maybeInitTxns() {\n-        if (eosEnabled) {\n-            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n-            // completed yet; do not start the first transaction until the topology has been initialized later\n-            try {\n-                producer.initTransactions();\n-            } catch (final TimeoutException exception) {\n-                final String errorMessage = \"Timeout exception caught when initializing transactions for task \" + taskId + \". \" +\n-                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n-                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n-                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n-\n-                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n-                throw new StreamsException(errorMessage, exception);\n-            } catch (final KafkaException exception) {\n-                throw new StreamsException(\"Error encountered while initializing transactions for task \" + taskId, exception);\n-            }\n-        }\n-    }\n+    /**\n+     * @throws StreamsException fatal error that should cause the thread to die\n+     * @throws TaskMigratedException recoverable error that would cause the task to be removed\n+     */\n+    @Override\n+    public <K, V> void send(final String topic,\n+                            final K key,\n+                            final V value,\n+                            final Headers headers,\n+                            final Long timestamp,\n+                            final Serializer<K> keySerializer,\n+                            final Serializer<V> valueSerializer,\n+                            final StreamPartitioner<? super K, ? super V> partitioner) {\n+        final Integer partition;\n \n-    private void maybeBeginTxn() {\n-        if (eosEnabled && !transactionInFlight) {\n-            try {\n-                producer.beginTransaction();\n-            } catch (final ProducerFencedException error) {\n-                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction\", error);\n+        // TODO K9113: we need to decide how to handle exceptions from partitionsFor\n+        if (partitioner != null) {\n+            final List<PartitionInfo> partitions = transactionManager.partitionsFor(topic);\n+            if (partitions.size() > 0) {\n+                partition = partitioner.partition(topic, key, value, partitions.size());\n+            } else {\n+                throw new StreamsException(\"Could not get partition information for topic \" + topic + \" for task \" + taskId +\n+                    \". This can happen if the topic does not exist.\");\n             }\n-            transactionInFlight = true;\n+        } else {\n+            partition = null;\n         }\n-    }\n \n-    private void maybeAbortTxn() {\n-        if (eosEnabled && transactionInFlight) {\n-            try {\n-                producer.abortTransaction();\n-            } catch (final ProducerFencedException ignore) {\n-                /* TODO\n-                 * this should actually never happen atm as we guard the call to #abortTransaction\n-                 * -> the reason for the guard is a \"bug\" in the Producer -- it throws IllegalStateException\n-                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got\n-                 * fixed and fall-back to this catch-and-swallow code\n-                 */\n-\n-                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Producer encounter unexpected error trying to abort the transaction\", error);\n-            }\n-            transactionInFlight = false;\n-        }\n+        send(topic, key, value, headers, partition, timestamp, keySerializer, valueSerializer);\n     }\n \n-    public void commit(final Map<TopicPartition, OffsetAndMetadata> offsets) {\n-        if (eosEnabled) {\n-            maybeBeginTxn();\n+    @Override\n+    public <K, V> void send(final String topic,\n+                            final K key,\n+                            final V value,\n+                            final Headers headers,\n+                            final Integer partition,\n+                            final Long timestamp,\n+                            final Serializer<K> keySerializer,\n+                            final Serializer<V> valueSerializer) {\n+        checkForException();\n \n-            try {\n-                producer.sendOffsetsToTransaction(offsets, applicationId);\n-                producer.commitTransaction();\n-                transactionInFlight = false;\n-            } catch (final ProducerFencedException error) {\n-                throw new TaskMigratedException(taskId, \"Producer get fenced trying to commit a transaction\", error);\n-            } catch (final TimeoutException error) {\n-                // TODO K9113: currently handle timeout exception as a fatal error, should discuss whether we want to handle it\n-                throw new StreamsException(\"Timed out while committing transaction via producer for task \" + taskId, error);\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Error encountered sending offsets and committing transaction \" +\n-                    \"via producer for task \" + taskId, error);\n-            }\n-        } else {\n-            try {\n-                consumer.commitSync(offsets);\n-            } catch (final CommitFailedException error) {\n-                throw new TaskMigratedException(taskId, \"Consumer committing offsets failed, \" +\n-                    \"indicating the corresponding thread is no longer part of the group.\", error);\n-            } catch (final TimeoutException error) {\n-                // TODO K9113: currently handle timeout exception as a fatal error\n-                throw new StreamsException(\"Timed out while committing offsets via consumer for task \" + taskId, error);\n-            } catch (final KafkaException error) {\n-                throw new StreamsException(\"Error encountered committing offsets via consumer for task \" + taskId, error);\n-            }\n-        }\n+        final byte[] keyBytes = keySerializer.serialize(topic, headers, key);\n+        final byte[] valBytes = valueSerializer.serialize(topic, headers, value);\n \n-    }\n+        final ProducerRecord<byte[], byte[]> serializedRecord = new ProducerRecord<>(topic, partition, timestamp, keyBytes, valBytes, headers);\n \n-    private boolean productionExceptionIsFatal(final Exception exception) {\n-        final boolean securityException = exception instanceof AuthenticationException ||\n-            exception instanceof AuthorizationException ||\n-            exception instanceof SecurityDisabledException;\n+        transactionManager.send(serializedRecord, (metadata, exception) -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcwODg0MQ=="}, "originalCommit": null, "originalPosition": 201}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0OTQyMDc4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQwMTo1NzoxMVrOFqHn6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxODowNzowOVrOFqP_ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcwOTQxNw==", "bodyText": "should be abortTransactionIfOngoing or maybeAbortTransaction as there may not be one on-going txn.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379709417", "createdAt": "2020-02-15T01:57:11Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        if (taskId != null) {\n+            logMessage = \"task \" + taskId.toString();\n+            eosEnabled = true;\n+        } else {\n+            logMessage = \"all owned active tasks\";\n+            eosEnabled = false;\n+        }\n+\n+        initTx();\n+    }\n+\n+    private void initTx() {\n+        if (eosEnabled) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+            } catch (final TimeoutException exception) {\n+                final String errorMessage = \"Timeout exception caught when initializing transactions for \" + logMessage + \". \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n+\n+                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n+                throw new StreamsException(errorMessage, exception);\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for \" + logMessage, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for \" + logMessage, error);\n+            }\n+            transactionInFlight = true;\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" \" + logMessage,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {\n+        return uncaughtException.getCause() instanceof ProducerFencedException ||\n+            uncaughtException.getCause() instanceof UnknownProducerIdException;\n+    }\n+\n+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        maybeBeginTransaction();\n+        try {\n+            producer.sendOffsetsToTransaction(offsets, applicationId);\n+            producer.commitTransaction();\n+        } catch (final ProducerFencedException error) {\n+            throw new TaskMigratedException(taskId, \"Producer get fenced trying to commit a transaction\", error);\n+        } catch (final TimeoutException error) {\n+            // TODO K9113: currently handle timeout exception as a fatal error, should discuss whether we want to handle it\n+            throw new StreamsException(\"Timed out while committing a transaction for \" + logMessage, error);\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to commit a transaction for \" + logMessage, error);\n+        }\n+        transactionInFlight = false;\n+    }\n+\n+    public void abortTransaction() throws ProducerFencedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTg0NjUwMQ==", "bodyText": "Well, the idea of the TM is \"hide\" those details -- as a caller, I don't want to think about it, I just say, something was wrong, please abort. That the call might be a no-op should be an implementation detail to the caller.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379846501", "createdAt": "2020-02-15T18:07:09Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        if (taskId != null) {\n+            logMessage = \"task \" + taskId.toString();\n+            eosEnabled = true;\n+        } else {\n+            logMessage = \"all owned active tasks\";\n+            eosEnabled = false;\n+        }\n+\n+        initTx();\n+    }\n+\n+    private void initTx() {\n+        if (eosEnabled) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+            } catch (final TimeoutException exception) {\n+                final String errorMessage = \"Timeout exception caught when initializing transactions for \" + logMessage + \". \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"\\n Consider overwriting `max.block.ms` to a larger value to avoid timeout errors\";\n+\n+                // TODO K9113: we do NOT try to handle timeout exception here but throw it as a fatal error\n+                throw new StreamsException(errorMessage, exception);\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for \" + logMessage, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for \" + logMessage, error);\n+            }\n+            transactionInFlight = true;\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" \" + logMessage,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {\n+        return uncaughtException.getCause() instanceof ProducerFencedException ||\n+            uncaughtException.getCause() instanceof UnknownProducerIdException;\n+    }\n+\n+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        maybeBeginTransaction();\n+        try {\n+            producer.sendOffsetsToTransaction(offsets, applicationId);\n+            producer.commitTransaction();\n+        } catch (final ProducerFencedException error) {\n+            throw new TaskMigratedException(taskId, \"Producer get fenced trying to commit a transaction\", error);\n+        } catch (final TimeoutException error) {\n+            // TODO K9113: currently handle timeout exception as a fatal error, should discuss whether we want to handle it\n+            throw new StreamsException(\"Timed out while committing a transaction for \" + logMessage, error);\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to commit a transaction for \" + logMessage, error);\n+        }\n+        transactionInFlight = false;\n+    }\n+\n+    public void abortTransaction() throws ProducerFencedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcwOTQxNw=="}, "originalCommit": null, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0OTQyMzc5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQwMjowMzo1MFrOFqHpgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxODowNDo0MlrOFqP-5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcwOTgyNA==", "bodyText": "I don't think the comment is necessary", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379709824", "createdAt": "2020-02-15T02:03:50Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    // used when eosEnabled is true only", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTg0NjM3NQ==", "bodyText": "Agree -- copied this from RecrodCollectorImpl...", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379846375", "createdAt": "2020-02-15T18:04:42Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    // used when eosEnabled is true only", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcwOTgyNA=="}, "originalCommit": null, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0OTQyNjE5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQwMjowOTo0MFrOFqHq0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxODowOTowM1rOFqP_yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcxMDE2MQ==", "bodyText": "On stream this could only be consumer, so we could use consumerGroupId", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379710161", "createdAt": "2020-02-15T02:09:40Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTg0NjYwMw==", "bodyText": "appid == groupid and in KS we always use appid.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379846603", "createdAt": "2020-02-15T18:09:03Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcxMDE2MQ=="}, "originalCommit": null, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0OTQyNjc4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQwMjoxMTowNFrOFqHrJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQwMjoxMTowNFrOFqHrJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTcxMDI0NQ==", "bodyText": "nit: as we are using full names elsewhere, we could also name it maybeInitTransaction", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379710245", "createdAt": "2020-02-15T02:11:04Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+public class TransactionManager {\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    // used when eosEnabled is true only\n+    private boolean transactionInFlight = false;\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer) {\n+        this(producer, null, null);\n+    }\n+\n+    public TransactionManager(final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        if (taskId != null) {\n+            logMessage = \"task \" + taskId.toString();\n+            eosEnabled = true;\n+        } else {\n+            logMessage = \"all owned active tasks\";\n+            eosEnabled = false;\n+        }\n+\n+        initTx();\n+    }\n+\n+    private void initTx() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1MDYwODUyOnYy", "diffSide": "RIGHT", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxODozNjowNlrOFqQFyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxODozNjowNlrOFqQFyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTg0ODEzNg==", "bodyText": "Needed to change the constructor to make it shorter (checkstyle failed). Hence, some variables cannot be final any longer.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r379848136", "createdAt": "2020-02-15T18:36:06Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -197,22 +199,24 @@\n     private static final Logger log = LoggerFactory.getLogger(TopologyTestDriver.class);\n \n     private final Time mockWallClockTime;\n-    private final InternalTopologyBuilder internalTopologyBuilder;\n+    private InternalTopologyBuilder internalTopologyBuilder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1Nzk3ODUxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjoyMTowOFrOFrUmvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjoyMTowOFrOFrUmvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3MDY4NA==", "bodyText": "We should remove K9113 here since in the scope of KAFKA-9113 we are going to treat this as fatal (which is already incorporated now). If we want to use the buffer-and-retry logic then it would be out of 9113's scope.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380970684", "createdAt": "2020-02-18T22:21:08Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -252,10 +113,11 @@ private void recordSendError(final String topic, final Exception exception, fina\n                             final StreamPartitioner<? super K, ? super V> partitioner) {\n         final Integer partition;\n \n+        // TODO K9113: we need to decide how to handle exceptions from partitionsFor", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1Nzk5MzYzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjoyNjo1MVrOFrUv-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjoyNjo1MVrOFrUv-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3MzA1MA==", "bodyText": "I think each transaction manager only manages one producer right?\nAlso nit: I feel more inclined to the name of StreamProducer since it is used for both eos and non-eos, @abbccdda @mjsax wdyt?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380973050", "createdAt": "2020-02-18T22:26:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.slf4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+/**\n+ * {@code TransactionManager} manages the producers within a Kafka Streams application.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODAwNjA1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjozMTozOVrOFrU3jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMjoyOTo1NVrOFslagw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3NDk4OA==", "bodyText": "nit: If we rename it to StreamProducer then maybe this can be renamed to embeddedProducer or sth.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380974988", "createdAt": "2020-02-18T22:31:39Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.slf4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+/**\n+ * {@code TransactionManager} manages the producers within a Kafka Streams application.\n+ * <p>\n+ * If EOS is enabled, it is responsible to init and begin transactions if necessary.\n+ * It also tracks the transaction status, ie, if a transaction is in-fight.\n+ * <p>\n+ * For non-EOS, the user should not call transaction related methods.\n+ */\n+public class TransactionManager {\n+    private final Logger log;\n+\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    private boolean transactionInFlight = false;\n+    private boolean transactionInitialized = false;\n+\n+    public TransactionManager(final LogContext logContext,\n+                              final Producer<byte[], byte[]> producer) {\n+        this(logContext, producer, null, null);\n+    }\n+\n+    public TransactionManager(final LogContext logContext,\n+                              final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.log = logContext.logger(getClass());\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        if (taskId != null) {\n+            logMessage = \"task \" + taskId.toString();\n+            eosEnabled = true;\n+        } else {\n+            logMessage = \"all owned active tasks\";\n+            eosEnabled = false;\n+        }\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     */\n+    public void initTransaction() {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        if (!transactionInitialized) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+                transactionInitialized = true;\n+            } catch (final TimeoutException exception) {\n+                log.warn(\"Timeout exception caught when initializing transactions for {}. \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"Will retry initializing the task in the next loop. \" +\n+                    \"\\nConsider overwriting {} to a larger value to avoid timeout errors\",\n+                    logMessage,\n+                    ProducerConfig.MAX_BLOCK_MS_CONFIG);\n+\n+                throw exception;\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for \" + logMessage, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+                transactionInFlight = true;\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for \" + logMessage, error);\n+            }\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" \" + logMessage,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {\n+        return uncaughtException.getCause() instanceof ProducerFencedException ||\n+            uncaughtException.getCause() instanceof UnknownProducerIdException;\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     * @throws TaskMigratedException\n+     */\n+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        maybeBeginTransaction();\n+        try {\n+            producer.sendOffsetsToTransaction(offsets, applicationId);\n+            producer.commitTransaction();\n+            transactionInFlight = false;\n+        } catch (final ProducerFencedException error) {\n+            throw new TaskMigratedException(taskId, \"Producer get fenced trying to commit a transaction\", error);\n+        } catch (final TimeoutException error) {\n+            // TODO KIP-447: we can consider treating it as non-fatal and retry on the thread level\n+            throw new StreamsException(\"Timed out while committing a transaction for \" + logMessage, error);\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to commit a transaction for \" + logMessage, error);\n+        }\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     */\n+    public void abortTransaction() throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        if (transactionInFlight) {\n+            try {\n+                producer.abortTransaction();\n+            } catch (final ProducerFencedException ignore) {\n+                /* TODO\n+                 * this should actually never happen atm as we guard the call to #abortTransaction\n+                 * -> the reason for the guard is a \"bug\" in the Producer -- it throws IllegalStateException\n+                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got\n+                 * fixed and fall-back to this catch-and-swallow code\n+                 */\n+\n+                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to abort a transaction for \" + logMessage, error);\n+            }\n+            transactionInFlight = false;\n+        }\n+    }\n+\n+    public List<PartitionInfo> partitionsFor(final String topic) throws TimeoutException {\n+        return producer.partitionsFor(topic);\n+    }\n+\n+    public void flush() {\n+        producer.flush();\n+    }\n+\n+    public void close() {\n+        try {\n+            producer.close();\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to close\" + (taskId == null ? \"\" : \" \" + logMessage), error);\n+        }\n+    }\n+\n+    // for testing only\n+    Producer<byte[], byte[]> producer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTAyODgwMw==", "bodyText": "Are you fine with kafkaProducer() ?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r381028803", "createdAt": "2020-02-19T01:21:22Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.slf4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+/**\n+ * {@code TransactionManager} manages the producers within a Kafka Streams application.\n+ * <p>\n+ * If EOS is enabled, it is responsible to init and begin transactions if necessary.\n+ * It also tracks the transaction status, ie, if a transaction is in-fight.\n+ * <p>\n+ * For non-EOS, the user should not call transaction related methods.\n+ */\n+public class TransactionManager {\n+    private final Logger log;\n+\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    private boolean transactionInFlight = false;\n+    private boolean transactionInitialized = false;\n+\n+    public TransactionManager(final LogContext logContext,\n+                              final Producer<byte[], byte[]> producer) {\n+        this(logContext, producer, null, null);\n+    }\n+\n+    public TransactionManager(final LogContext logContext,\n+                              final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.log = logContext.logger(getClass());\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        if (taskId != null) {\n+            logMessage = \"task \" + taskId.toString();\n+            eosEnabled = true;\n+        } else {\n+            logMessage = \"all owned active tasks\";\n+            eosEnabled = false;\n+        }\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     */\n+    public void initTransaction() {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        if (!transactionInitialized) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+                transactionInitialized = true;\n+            } catch (final TimeoutException exception) {\n+                log.warn(\"Timeout exception caught when initializing transactions for {}. \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"Will retry initializing the task in the next loop. \" +\n+                    \"\\nConsider overwriting {} to a larger value to avoid timeout errors\",\n+                    logMessage,\n+                    ProducerConfig.MAX_BLOCK_MS_CONFIG);\n+\n+                throw exception;\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for \" + logMessage, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+                transactionInFlight = true;\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for \" + logMessage, error);\n+            }\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" \" + logMessage,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {\n+        return uncaughtException.getCause() instanceof ProducerFencedException ||\n+            uncaughtException.getCause() instanceof UnknownProducerIdException;\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     * @throws TaskMigratedException\n+     */\n+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        maybeBeginTransaction();\n+        try {\n+            producer.sendOffsetsToTransaction(offsets, applicationId);\n+            producer.commitTransaction();\n+            transactionInFlight = false;\n+        } catch (final ProducerFencedException error) {\n+            throw new TaskMigratedException(taskId, \"Producer get fenced trying to commit a transaction\", error);\n+        } catch (final TimeoutException error) {\n+            // TODO KIP-447: we can consider treating it as non-fatal and retry on the thread level\n+            throw new StreamsException(\"Timed out while committing a transaction for \" + logMessage, error);\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to commit a transaction for \" + logMessage, error);\n+        }\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     */\n+    public void abortTransaction() throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        if (transactionInFlight) {\n+            try {\n+                producer.abortTransaction();\n+            } catch (final ProducerFencedException ignore) {\n+                /* TODO\n+                 * this should actually never happen atm as we guard the call to #abortTransaction\n+                 * -> the reason for the guard is a \"bug\" in the Producer -- it throws IllegalStateException\n+                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got\n+                 * fixed and fall-back to this catch-and-swallow code\n+                 */\n+\n+                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to abort a transaction for \" + logMessage, error);\n+            }\n+            transactionInFlight = false;\n+        }\n+    }\n+\n+    public List<PartitionInfo> partitionsFor(final String topic) throws TimeoutException {\n+        return producer.partitionsFor(topic);\n+    }\n+\n+    public void flush() {\n+        producer.flush();\n+    }\n+\n+    public void close() {\n+        try {\n+            producer.close();\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to close\" + (taskId == null ? \"\" : \" \" + logMessage), error);\n+        }\n+    }\n+\n+    // for testing only\n+    Producer<byte[], byte[]> producer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3NDk4OA=="}, "originalCommit": null, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI5NDY1OQ==", "bodyText": "Yup.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r382294659", "createdAt": "2020-02-20T22:29:55Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TransactionManager.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.slf4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Future;\n+\n+/**\n+ * {@code TransactionManager} manages the producers within a Kafka Streams application.\n+ * <p>\n+ * If EOS is enabled, it is responsible to init and begin transactions if necessary.\n+ * It also tracks the transaction status, ie, if a transaction is in-fight.\n+ * <p>\n+ * For non-EOS, the user should not call transaction related methods.\n+ */\n+public class TransactionManager {\n+    private final Logger log;\n+\n+    private final Producer<byte[], byte[]> producer;\n+    private final String applicationId;\n+    private final TaskId taskId;\n+    private final String logMessage;\n+    private final boolean eosEnabled;\n+\n+    private boolean transactionInFlight = false;\n+    private boolean transactionInitialized = false;\n+\n+    public TransactionManager(final LogContext logContext,\n+                              final Producer<byte[], byte[]> producer) {\n+        this(logContext, producer, null, null);\n+    }\n+\n+    public TransactionManager(final LogContext logContext,\n+                              final Producer<byte[], byte[]> producer,\n+                              final String applicationId,\n+                              final TaskId taskId) {\n+        if ((applicationId != null && taskId == null) ||\n+            (applicationId == null && taskId != null)) {\n+            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n+        }\n+\n+        this.log = logContext.logger(getClass());\n+\n+        this.producer = Objects.requireNonNull(producer, \"producer cannot be null\");\n+        this.applicationId = applicationId;\n+        this.taskId = taskId;\n+        if (taskId != null) {\n+            logMessage = \"task \" + taskId.toString();\n+            eosEnabled = true;\n+        } else {\n+            logMessage = \"all owned active tasks\";\n+            eosEnabled = false;\n+        }\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     */\n+    public void initTransaction() {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        if (!transactionInitialized) {\n+            // initialize transactions if eos is turned on, which will block if the previous transaction has not\n+            // completed yet; do not start the first transaction until the topology has been initialized later\n+            try {\n+                producer.initTransactions();\n+                transactionInitialized = true;\n+            } catch (final TimeoutException exception) {\n+                log.warn(\"Timeout exception caught when initializing transactions for {}. \" +\n+                    \"\\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, \" +\n+                    \"or the connection to broker was interrupted sending the request or receiving the response. \" +\n+                    \"Will retry initializing the task in the next loop. \" +\n+                    \"\\nConsider overwriting {} to a larger value to avoid timeout errors\",\n+                    logMessage,\n+                    ProducerConfig.MAX_BLOCK_MS_CONFIG);\n+\n+                throw exception;\n+            } catch (final KafkaException exception) {\n+                throw new StreamsException(\"Error encountered while initializing transactions for \" + logMessage, exception);\n+            }\n+        }\n+    }\n+\n+    private void maybeBeginTransaction() throws ProducerFencedException {\n+        if (eosEnabled && !transactionInFlight) {\n+            try {\n+                producer.beginTransaction();\n+                transactionInFlight = true;\n+            } catch (final ProducerFencedException error) {\n+                throw new TaskMigratedException(taskId, \"Producer get fenced trying to begin a new transaction\", error);\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to begin a new transaction for \" + logMessage, error);\n+            }\n+        }\n+    }\n+\n+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,\n+                                       final Callback callback) {\n+        maybeBeginTransaction();\n+        try {\n+            return producer.send(record, callback);\n+        } catch (final KafkaException uncaughtException) {\n+            if (isRecoverable(uncaughtException)) {\n+                // producer.send() call may throw a KafkaException which wraps a FencedException,\n+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n+                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+            } else {\n+                final String errorMessage = String.format(\n+                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n+                    record.topic(),\n+                    taskId == null ? \"\" : \" \" + logMessage,\n+                    uncaughtException.toString());\n+                throw new StreamsException(errorMessage, uncaughtException);\n+            }\n+        }\n+    }\n+\n+    private static boolean isRecoverable(final KafkaException uncaughtException) {\n+        return uncaughtException.getCause() instanceof ProducerFencedException ||\n+            uncaughtException.getCause() instanceof UnknownProducerIdException;\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     * @throws TaskMigratedException\n+     */\n+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        maybeBeginTransaction();\n+        try {\n+            producer.sendOffsetsToTransaction(offsets, applicationId);\n+            producer.commitTransaction();\n+            transactionInFlight = false;\n+        } catch (final ProducerFencedException error) {\n+            throw new TaskMigratedException(taskId, \"Producer get fenced trying to commit a transaction\", error);\n+        } catch (final TimeoutException error) {\n+            // TODO KIP-447: we can consider treating it as non-fatal and retry on the thread level\n+            throw new StreamsException(\"Timed out while committing a transaction for \" + logMessage, error);\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to commit a transaction for \" + logMessage, error);\n+        }\n+    }\n+\n+    /**\n+     * @throws IllegalStateException if EOS is disabled\n+     */\n+    public void abortTransaction() throws ProducerFencedException {\n+        if (!eosEnabled) {\n+            throw new IllegalStateException(\"EOS is disabled\");\n+        }\n+        if (transactionInFlight) {\n+            try {\n+                producer.abortTransaction();\n+            } catch (final ProducerFencedException ignore) {\n+                /* TODO\n+                 * this should actually never happen atm as we guard the call to #abortTransaction\n+                 * -> the reason for the guard is a \"bug\" in the Producer -- it throws IllegalStateException\n+                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got\n+                 * fixed and fall-back to this catch-and-swallow code\n+                 */\n+\n+                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens\n+            } catch (final KafkaException error) {\n+                throw new StreamsException(\"Producer encounter unexpected error trying to abort a transaction for \" + logMessage, error);\n+            }\n+            transactionInFlight = false;\n+        }\n+    }\n+\n+    public List<PartitionInfo> partitionsFor(final String topic) throws TimeoutException {\n+        return producer.partitionsFor(topic);\n+    }\n+\n+    public void flush() {\n+        producer.flush();\n+    }\n+\n+    public void close() {\n+        try {\n+            producer.close();\n+        } catch (final KafkaException error) {\n+            throw new StreamsException(\"Producer encounter unexpected error trying to close\" + (taskId == null ? \"\" : \" \" + logMessage), error);\n+        }\n+    }\n+\n+    // for testing only\n+    Producer<byte[], byte[]> producer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3NDk4OA=="}, "originalCommit": null, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODAzNDUxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo0MjozOVrOFrVJKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo0MjozOVrOFrVJKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3OTQ5Nw==", "bodyText": "If the serialization failed we wrap it as a StreamsException -- with this PR it seems a regression that we do not do it any more.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380979497", "createdAt": "2020-02-18T22:42:39Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -286,52 +148,90 @@ private void recordSendError(final String topic, final Exception exception, fina\n                             final Serializer<V> valueSerializer) {\n         checkForException();\n \n-        maybeBeginTxn();\n-\n-        try {\n-            final byte[] keyBytes = keySerializer.serialize(topic, headers, key);\n-            final byte[] valBytes = valueSerializer.serialize(topic, headers, value);\n-\n-            final ProducerRecord<byte[], byte[]> serializedRecord = new ProducerRecord<>(topic, partition, timestamp, keyBytes, valBytes, headers);\n-\n-            producer.send(serializedRecord, (metadata, exception) -> {\n-                // if there's already an exception record, skip logging offsets or new exceptions\n-                if (sendException != null) {\n-                    return;\n-                }\n-\n-                if (exception == null) {\n-                    final TopicPartition tp = new TopicPartition(metadata.topic(), metadata.partition());\n-                    offsets.put(tp, metadata.offset());\n-                } else {\n-                    recordSendError(topic, exception, serializedRecord);\n-\n-                    // KAFKA-7510 only put message key and value in TRACE level log so we don't leak data by default\n-                    log.trace(\"Failed record: (key {} value {} timestamp {}) topic=[{}] partition=[{}]\", key, value, timestamp, topic, partition);\n-                }\n-            });\n-        } catch (final RuntimeException uncaughtException) {\n-            if (isRecoverable(uncaughtException)) {\n-                // producer.send() call may throw a KafkaException which wraps a FencedException,\n-                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException\n-                throw new TaskMigratedException(taskId, \"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n+        final byte[] keyBytes = keySerializer.serialize(topic, headers, key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 275}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODA0MDAxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo0NDozMVrOFrVMdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo0NDozMVrOFrVMdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4MDM0MA==", "bodyText": "nit: taskProducers?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380980340", "createdAt": "2020-02-18T22:44:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -518,9 +511,10 @@ StandbyTask createTask(final Consumer<byte[], byte[]> consumer,\n \n     // package-private for testing\n     final ConsumerRebalanceListener rebalanceListener;\n+    final Consumer<byte[], byte[]> mainConsumer;\n     final Consumer<byte[], byte[]> restoreConsumer;\n-    final Consumer<byte[], byte[]> consumer;\n-    final Producer<byte[], byte[]> producer;\n+    final Producer<byte[], byte[]> threadProducer;\n+    final Map<TaskId, Producer<byte[], byte[]>> taskProducer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODA4NjE5OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowMTowMFrOFrVnwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxNDozNToyM1rOFrqgiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NzMyOA==", "bodyText": "Brackets looks unnecessary here.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380987328", "createdAt": "2020-02-18T23:01:00Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "diffHunk": "@@ -0,0 +1,638 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.MockProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.mock;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.verify;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransactionManagerTest {\n+\n+    private final LogContext logContext = new LogContext(\"test \");\n+    private final String topic = \"topic\";\n+    private final Cluster cluster = new Cluster(\n+        \"cluster\",\n+        Collections.singletonList(Node.noNode()),\n+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),\n+        Collections.emptySet(),\n+        Collections.emptySet()\n+    );\n+    private final TaskId taskId = new TaskId(0, 0);\n+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(\n+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))\n+    );\n+\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager transactionManager = new TransactionManager(logContext, mockProducer);\n+\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager eosTransactionManager = new TransactionManager(logContext, eosMockProducer, \"appId\", taskId);\n+\n+    private final ProducerRecord<byte[], byte[]> record =\n+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());\n+\n+    @Before\n+    public void before() {\n+        eosTransactionManager.initTransaction();\n+    }\n+\n+    @Test\n+    public void shouldFailIfProducerIsNull() {\n+        {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTMyOTU0NQ==", "bodyText": "I reuse the variable name thrown twice and want to make it final -- hence the braces", "url": "https://github.com/apache/kafka/pull/8105#discussion_r381329545", "createdAt": "2020-02-19T14:35:23Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "diffHunk": "@@ -0,0 +1,638 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.MockProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.mock;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.verify;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransactionManagerTest {\n+\n+    private final LogContext logContext = new LogContext(\"test \");\n+    private final String topic = \"topic\";\n+    private final Cluster cluster = new Cluster(\n+        \"cluster\",\n+        Collections.singletonList(Node.noNode()),\n+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),\n+        Collections.emptySet(),\n+        Collections.emptySet()\n+    );\n+    private final TaskId taskId = new TaskId(0, 0);\n+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(\n+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))\n+    );\n+\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager transactionManager = new TransactionManager(logContext, mockProducer);\n+\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager eosTransactionManager = new TransactionManager(logContext, eosMockProducer, \"appId\", taskId);\n+\n+    private final ProducerRecord<byte[], byte[]> record =\n+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());\n+\n+    @Before\n+    public void before() {\n+        eosTransactionManager.initTransaction();\n+    }\n+\n+    @Test\n+    public void shouldFailIfProducerIsNull() {\n+        {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NzMyOA=="}, "originalCommit": null, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODA4NjU3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowMToxM1rOFrVoAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMjoyOTozMFrOFslZ1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NzM5NA==", "bodyText": "qq: not sure what does this mean.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380987394", "createdAt": "2020-02-18T23:01:13Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "diffHunk": "@@ -0,0 +1,638 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.MockProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.mock;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.verify;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransactionManagerTest {\n+\n+    private final LogContext logContext = new LogContext(\"test \");\n+    private final String topic = \"topic\";\n+    private final Cluster cluster = new Cluster(\n+        \"cluster\",\n+        Collections.singletonList(Node.noNode()),\n+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),\n+        Collections.emptySet(),\n+        Collections.emptySet()\n+    );\n+    private final TaskId taskId = new TaskId(0, 0);\n+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(\n+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))\n+    );\n+\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager transactionManager = new TransactionManager(logContext, mockProducer);\n+\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager eosTransactionManager = new TransactionManager(logContext, eosMockProducer, \"appId\", taskId);\n+\n+    private final ProducerRecord<byte[], byte[]> record =\n+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());\n+\n+    @Before\n+    public void before() {\n+        eosTransactionManager.initTransaction();\n+    }\n+\n+    @Test\n+    public void shouldFailIfProducerIsNull() {\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null, \"appId\", taskId)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowIfIncorrectlyInitialized() {\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, null, taskId)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, \"appId\", null)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+    }\n+\n+    // non-eos tests\n+\n+    // positive tests", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTAyODA5MQ==", "bodyText": "It means functional test, ie, we that something should work (instead of negative test that test that something should not work/throw an error). Are you fine with this or do you have a suggestion on how to rephrase it?", "url": "https://github.com/apache/kafka/pull/8105#discussion_r381028091", "createdAt": "2020-02-19T01:18:36Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "diffHunk": "@@ -0,0 +1,638 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.MockProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.mock;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.verify;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransactionManagerTest {\n+\n+    private final LogContext logContext = new LogContext(\"test \");\n+    private final String topic = \"topic\";\n+    private final Cluster cluster = new Cluster(\n+        \"cluster\",\n+        Collections.singletonList(Node.noNode()),\n+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),\n+        Collections.emptySet(),\n+        Collections.emptySet()\n+    );\n+    private final TaskId taskId = new TaskId(0, 0);\n+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(\n+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))\n+    );\n+\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager transactionManager = new TransactionManager(logContext, mockProducer);\n+\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager eosTransactionManager = new TransactionManager(logContext, eosMockProducer, \"appId\", taskId);\n+\n+    private final ProducerRecord<byte[], byte[]> record =\n+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());\n+\n+    @Before\n+    public void before() {\n+        eosTransactionManager.initTransaction();\n+    }\n+\n+    @Test\n+    public void shouldFailIfProducerIsNull() {\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null, \"appId\", taskId)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowIfIncorrectlyInitialized() {\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, null, taskId)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, \"appId\", null)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+    }\n+\n+    // non-eos tests\n+\n+    // positive tests", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NzM5NA=="}, "originalCommit": null, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI5NDQ4NA==", "bodyText": "Yeah this is fine.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r382294484", "createdAt": "2020-02-20T22:29:30Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "diffHunk": "@@ -0,0 +1,638 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.MockProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.mock;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.verify;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransactionManagerTest {\n+\n+    private final LogContext logContext = new LogContext(\"test \");\n+    private final String topic = \"topic\";\n+    private final Cluster cluster = new Cluster(\n+        \"cluster\",\n+        Collections.singletonList(Node.noNode()),\n+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),\n+        Collections.emptySet(),\n+        Collections.emptySet()\n+    );\n+    private final TaskId taskId = new TaskId(0, 0);\n+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(\n+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))\n+    );\n+\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager transactionManager = new TransactionManager(logContext, mockProducer);\n+\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager eosTransactionManager = new TransactionManager(logContext, eosMockProducer, \"appId\", taskId);\n+\n+    private final ProducerRecord<byte[], byte[]> record =\n+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());\n+\n+    @Before\n+    public void before() {\n+        eosTransactionManager.initTransaction();\n+    }\n+\n+    @Test\n+    public void shouldFailIfProducerIsNull() {\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null, \"appId\", taskId)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowIfIncorrectlyInitialized() {\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, null, taskId)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, \"appId\", null)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+    }\n+\n+    // non-eos tests\n+\n+    // positive tests", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NzM5NA=="}, "originalCommit": null, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODA4OTEzOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowMjoxMVrOFrVpjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowMjoxMVrOFrVpjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4Nzc4OQ==", "bodyText": "nit: shouldFailOnInitTxnWhenEOSDisabled.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380987789", "createdAt": "2020-02-18T23:02:11Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "diffHunk": "@@ -0,0 +1,638 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.MockProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.mock;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.verify;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransactionManagerTest {\n+\n+    private final LogContext logContext = new LogContext(\"test \");\n+    private final String topic = \"topic\";\n+    private final Cluster cluster = new Cluster(\n+        \"cluster\",\n+        Collections.singletonList(Node.noNode()),\n+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),\n+        Collections.emptySet(),\n+        Collections.emptySet()\n+    );\n+    private final TaskId taskId = new TaskId(0, 0);\n+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(\n+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))\n+    );\n+\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager transactionManager = new TransactionManager(logContext, mockProducer);\n+\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager eosTransactionManager = new TransactionManager(logContext, eosMockProducer, \"appId\", taskId);\n+\n+    private final ProducerRecord<byte[], byte[]> record =\n+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());\n+\n+    @Before\n+    public void before() {\n+        eosTransactionManager.initTransaction();\n+    }\n+\n+    @Test\n+    public void shouldFailIfProducerIsNull() {\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null, \"appId\", taskId)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowIfIncorrectlyInitialized() {\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, null, taskId)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, \"appId\", null)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+    }\n+\n+    // non-eos tests\n+\n+    // positive tests\n+\n+    @Test\n+    public void shouldNotInitTxIfEosDisable() {\n+        assertFalse(mockProducer.transactionInitialized());\n+    }\n+\n+    @Test\n+    public void shouldNotBeginTxOnSendIfEosDisable() {\n+        transactionManager.send(record, null);\n+        assertFalse(mockProducer.transactionInFlight());\n+    }\n+\n+    @Test\n+    public void shouldForwardRecordOnSend() {\n+        transactionManager.send(record, null);\n+        assertThat(mockProducer.history().size(), equalTo(1));\n+        assertThat(mockProducer.history().get(0), equalTo(record));\n+    }\n+\n+    @Test\n+    public void shouldForwardCallToPartitionsFor() {\n+        final Producer<byte[], byte[]> producer = mock(Producer.class);\n+\n+        final List<PartitionInfo> expectedPartitionInfo = Collections.emptyList();\n+        expect(producer.partitionsFor(\"topic\")).andReturn(expectedPartitionInfo);\n+        replay(producer);\n+\n+        final TransactionManager transactionManager = new TransactionManager(logContext, producer);\n+\n+        final List<PartitionInfo> partitionInfo = transactionManager.partitionsFor(topic);\n+\n+        assertSame(expectedPartitionInfo, partitionInfo);\n+        verify(producer);\n+    }\n+\n+    @Test\n+    public void shouldForwardCallToFlush() {\n+        final Producer<byte[], byte[]> producer = mock(Producer.class);\n+\n+        producer.flush();\n+        expectLastCall();\n+        replay(producer);\n+\n+        final TransactionManager transactionManager = new TransactionManager(logContext, producer);\n+\n+        transactionManager.flush();\n+\n+        verify(producer);\n+    }\n+\n+    @Test\n+    public void shouldCloseProducerOnClose() {\n+        transactionManager.close();\n+        assertTrue(mockProducer.closed());\n+    }\n+\n+    // error handling tests\n+\n+    @Test\n+    public void shouldFail() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 195}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODA5MDc2OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowMjo1NVrOFrVqgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowMjo1NVrOFrVqgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4ODAzMg==", "bodyText": "Ditto here. Not sure what does positive mean.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380988032", "createdAt": "2020-02-18T23:02:55Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TransactionManagerTest.java", "diffHunk": "@@ -0,0 +1,638 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.MockProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.UnknownProducerIdException;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.errors.TaskMigratedException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.mock;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.verify;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransactionManagerTest {\n+\n+    private final LogContext logContext = new LogContext(\"test \");\n+    private final String topic = \"topic\";\n+    private final Cluster cluster = new Cluster(\n+        \"cluster\",\n+        Collections.singletonList(Node.noNode()),\n+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),\n+        Collections.emptySet(),\n+        Collections.emptySet()\n+    );\n+    private final TaskId taskId = new TaskId(0, 0);\n+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();\n+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(\n+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))\n+    );\n+\n+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager transactionManager = new TransactionManager(logContext, mockProducer);\n+\n+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(\n+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);\n+    private final TransactionManager eosTransactionManager = new TransactionManager(logContext, eosMockProducer, \"appId\", taskId);\n+\n+    private final ProducerRecord<byte[], byte[]> record =\n+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());\n+\n+    @Before\n+    public void before() {\n+        eosTransactionManager.initTransaction();\n+    }\n+\n+    @Test\n+    public void shouldFailIfProducerIsNull() {\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+\n+        {\n+            final NullPointerException thrown = assertThrows(\n+                NullPointerException.class,\n+                () -> new TransactionManager(logContext, null, \"appId\", taskId)\n+            );\n+\n+            assertThat(thrown.getMessage(), equalTo(\"producer cannot be null\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowIfIncorrectlyInitialized() {\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, null, taskId)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+\n+        {\n+            final IllegalArgumentException thrown = assertThrows(\n+                IllegalArgumentException.class,\n+                () -> new TransactionManager(logContext, mockProducer, \"appId\", null)\n+            );\n+            assertThat(thrown.getMessage(), equalTo(\"applicationId and taskId must either be both null or both be not null\"));\n+        }\n+    }\n+\n+    // non-eos tests\n+\n+    // positive tests\n+\n+    @Test\n+    public void shouldNotInitTxIfEosDisable() {\n+        assertFalse(mockProducer.transactionInitialized());\n+    }\n+\n+    @Test\n+    public void shouldNotBeginTxOnSendIfEosDisable() {\n+        transactionManager.send(record, null);\n+        assertFalse(mockProducer.transactionInFlight());\n+    }\n+\n+    @Test\n+    public void shouldForwardRecordOnSend() {\n+        transactionManager.send(record, null);\n+        assertThat(mockProducer.history().size(), equalTo(1));\n+        assertThat(mockProducer.history().get(0), equalTo(record));\n+    }\n+\n+    @Test\n+    public void shouldForwardCallToPartitionsFor() {\n+        final Producer<byte[], byte[]> producer = mock(Producer.class);\n+\n+        final List<PartitionInfo> expectedPartitionInfo = Collections.emptyList();\n+        expect(producer.partitionsFor(\"topic\")).andReturn(expectedPartitionInfo);\n+        replay(producer);\n+\n+        final TransactionManager transactionManager = new TransactionManager(logContext, producer);\n+\n+        final List<PartitionInfo> partitionInfo = transactionManager.partitionsFor(topic);\n+\n+        assertSame(expectedPartitionInfo, partitionInfo);\n+        verify(producer);\n+    }\n+\n+    @Test\n+    public void shouldForwardCallToFlush() {\n+        final Producer<byte[], byte[]> producer = mock(Producer.class);\n+\n+        producer.flush();\n+        expectLastCall();\n+        replay(producer);\n+\n+        final TransactionManager transactionManager = new TransactionManager(logContext, producer);\n+\n+        transactionManager.flush();\n+\n+        verify(producer);\n+    }\n+\n+    @Test\n+    public void shouldCloseProducerOnClose() {\n+        transactionManager.close();\n+        assertTrue(mockProducer.closed());\n+    }\n+\n+    // error handling tests\n+\n+    @Test\n+    public void shouldFail() {\n+        final IllegalStateException thrown = assertThrows(\n+            IllegalStateException.class,\n+            transactionManager::initTransaction\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"EOS is disabled\"));\n+    }\n+\n+    @Test\n+    public void shouldThrowStreamsExceptionOnSendError() {\n+        mockProducer.sendException  = new KafkaException(\"KABOOM!\");\n+\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            () -> transactionManager.send(record, null)\n+        );\n+\n+        assertEquals(mockProducer.sendException, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Error encountered sending record to topic topic due to:\\norg.apache.kafka.common.KafkaException: KABOOM!\"));\n+    }\n+\n+    @Test\n+    public void shouldFailOnSendFatal() {\n+        mockProducer.sendException = new RuntimeException(\"KABOOM!\");\n+\n+        final RuntimeException thrown = assertThrows(\n+            RuntimeException.class,\n+            () -> transactionManager.send(record, null)\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"KABOOM!\"));\n+    }\n+\n+    @Test\n+    public void shouldFailOnCommitIfEosDisabled() {\n+        final IllegalStateException thrown = assertThrows(\n+            IllegalStateException.class,\n+            () -> transactionManager.commitTransaction(null)\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"EOS is disabled\"));\n+    }\n+\n+    @Test\n+    public void shouldFailOnAbortIfEosDisabled() {\n+        final IllegalStateException thrown = assertThrows(\n+            IllegalStateException.class,\n+            transactionManager::abortTransaction\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"EOS is disabled\"));\n+    }\n+\n+    @Test\n+    public void shouldThrowStreamsExceptionOnCloseError() {\n+        mockProducer.closeException = new KafkaException(\"KABOOM!\");\n+\n+        final StreamsException thrown = assertThrows(\n+            StreamsException.class,\n+            transactionManager::close\n+        );\n+\n+        assertEquals(mockProducer.closeException, thrown.getCause());\n+        assertThat(thrown.getMessage(), equalTo(\"Producer encounter unexpected error trying to close\"));\n+    }\n+\n+    @Test\n+    public void shouldFailOnCloseFatal() {\n+        mockProducer.closeException = new RuntimeException(\"KABOOM!\");\n+\n+        final RuntimeException thrown = assertThrows(\n+            RuntimeException.class,\n+            transactionManager::close\n+        );\n+\n+        assertThat(thrown.getMessage(), equalTo(\"KABOOM!\"));\n+    }\n+\n+    // EOS tests\n+\n+    // positive tests", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 276}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODA5NDcxOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowNDozOFrOFrVs7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowNDozOFrOFrVs7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4ODY1NQ==", "bodyText": "nit: eosEnabled.", "url": "https://github.com/apache/kafka/pull/8105#discussion_r380988655", "createdAt": "2020-02-18T23:04:38Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java", "diffHunk": "@@ -324,13 +325,21 @@ private StreamTask createStreamsTask(final StreamsConfig streamsConfig,\n                 clientSupplier.restoreConsumer,\n                 new MockStateRestoreListener()),\n             logContext);\n+        final boolean eosAlphaEnabled = StreamsConfig.EXACTLY_ONCE.equals(streamsConfig.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4282, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}