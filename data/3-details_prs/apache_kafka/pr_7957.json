{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYyNjQ4NDA2", "number": 7957, "title": "KAFKA-8768: DeleteRecords request/response automated protocol", "bodyText": "Also add version 2 to make use of flexible versions, per KIP-482.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-01-14T13:55:40Z", "url": "https://github.com/apache/kafka/pull/7957", "merged": true, "mergeCommit": {"oid": "f869e33ab240008050edb9a390892294344f13de"}, "closed": true, "closedAt": "2020-03-13T17:48:48Z", "author": {"login": "tombentley"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb6Rbx-ABqjI5NDcxNjc3MDI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcNRmyZgFqTM3NDM2ODI0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0Mzc1NjMx", "url": "https://github.com/apache/kafka/pull/7957#pullrequestreview-354375631", "createdAt": "2020-02-06T11:20:22Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxMToyMDoyMlrOFmXrWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxMzo0MDowMVrOFmbVIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTc3ODEzOA==", "bodyText": "Why are we importing these classes? Same below", "url": "https://github.com/apache/kafka/pull/7957#discussion_r375778138", "createdAt": "2020-02-06T11:20:22Z", "author": {"login": "mimaison"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/ApiKeys.java", "diffHunk": "@@ -102,8 +104,14 @@\n import org.apache.kafka.common.requests.AlterConfigsResponse;\n import org.apache.kafka.common.requests.AlterReplicaLogDirsRequest;\n import org.apache.kafka.common.requests.AlterReplicaLogDirsResponse;\n+import org.apache.kafka.common.requests.CreateAclsRequest;\n+import org.apache.kafka.common.requests.CreateAclsResponse;\n+import org.apache.kafka.common.requests.DeleteAclsRequest;\n+import org.apache.kafka.common.requests.DeleteAclsResponse;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTc3OTUyNQ==", "bodyText": "Can we do without this field? AbstractRequest already has it.\nWe can use super(ApiKeys.DELETE_RECORDS, version) ion the constructor and then use version()", "url": "https://github.com/apache/kafka/pull/7957#discussion_r375779525", "createdAt": "2020-02-06T11:23:39Z", "author": {"login": "mimaison"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsRequest.java", "diffHunk": "@@ -17,153 +17,78 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.DeleteRecordsRequestData;\n+import org.apache.kafka.common.message.DeleteRecordsRequestData.DeleteRecordsTopic;\n+import org.apache.kafka.common.message.DeleteRecordsResponseData;\n+import org.apache.kafka.common.message.DeleteRecordsResponseData.DeleteRecordsTopicResult;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.INT64;\n \n public class DeleteRecordsRequest extends AbstractRequest {\n \n     public static final long HIGH_WATERMARK = -1L;\n \n-    // request level key names\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-    private static final String TIMEOUT_KEY_NAME = \"timeout\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    // partition level key names\n-    private static final String OFFSET_KEY_NAME = \"offset\";\n-\n-\n-    private static final Schema DELETE_RECORDS_REQUEST_PARTITION_V0 = new Schema(\n-            PARTITION_ID,\n-            new Field(OFFSET_KEY_NAME, INT64, \"The offset before which the messages will be deleted. -1 means high-watermark for the partition.\"));\n-\n-    private static final Schema DELETE_RECORDS_REQUEST_TOPIC_V0 = new Schema(\n-            TOPIC_NAME,\n-            new Field(PARTITIONS_KEY_NAME, new ArrayOf(DELETE_RECORDS_REQUEST_PARTITION_V0)));\n-\n-    private static final Schema DELETE_RECORDS_REQUEST_V0 = new Schema(\n-            new Field(TOPICS_KEY_NAME, new ArrayOf(DELETE_RECORDS_REQUEST_TOPIC_V0)),\n-            new Field(TIMEOUT_KEY_NAME, INT32, \"The maximum time to await a response in ms.\"));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema DELETE_RECORDS_REQUEST_V1 = DELETE_RECORDS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{DELETE_RECORDS_REQUEST_V0, DELETE_RECORDS_REQUEST_V1};\n-    }\n-\n-    private final int timeout;\n-    private final Map<TopicPartition, Long> partitionOffsets;\n+    private final DeleteRecordsRequestData data;\n+    private final short version;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTgzNzQ5Mg==", "bodyText": "Right side needs to be Errors.NONE.code()", "url": "https://github.com/apache/kafka/pull/7957#discussion_r375837492", "createdAt": "2020-02-06T13:39:01Z", "author": {"login": "mimaison"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -1821,46 +1822,64 @@ class KafkaApis(val requestChannel: RequestChannel,\n   def handleDeleteRecordsRequest(request: RequestChannel.Request): Unit = {\n     val deleteRecordsRequest = request.body[DeleteRecordsRequest]\n \n-    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n-    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n+    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n+    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n     val authorizedForDeleteTopicOffsets = mutable.Map[TopicPartition, Long]()\n \n     val authorizedTopics = filterAuthorized(request, DELETE, TOPIC,\n-      deleteRecordsRequest.partitionOffsets.asScala.toSeq.map(_._1.topic))\n-    for ((topicPartition, offset) <- deleteRecordsRequest.partitionOffsets.asScala) {\n+      deleteRecordsRequest.data.topics.asScala.map(_.name))\n+    for ((topicPartition, offset) <- deleteRecordsRequest.data.topics.asScala.flatMap(deleteTopic => {\n+      deleteTopic.partitions.asScala.map(deletePartition => {\n+        new TopicPartition(deleteTopic.name, deletePartition.partitionIndex) -> deletePartition.offset()\n+      })\n+    })) {\n       if (!authorizedTopics.contains(topicPartition.topic))\n-        unauthorizedTopicResponses += topicPartition -> new DeleteRecordsResponse.PartitionResponse(\n-          DeleteRecordsResponse.INVALID_LOW_WATERMARK, Errors.TOPIC_AUTHORIZATION_FAILED)\n+        unauthorizedTopicResponses += topicPartition -> new DeleteRecordsPartitionResult()\n+          .setLowWatermark(DeleteRecordsResponse.INVALID_LOW_WATERMARK)\n+          .setErrorCode(Errors.TOPIC_AUTHORIZATION_FAILED.code)\n       else if (!metadataCache.contains(topicPartition))\n-        nonExistingTopicResponses += topicPartition -> new DeleteRecordsResponse.PartitionResponse(\n-          DeleteRecordsResponse.INVALID_LOW_WATERMARK, Errors.UNKNOWN_TOPIC_OR_PARTITION)\n+        nonExistingTopicResponses += topicPartition -> new DeleteRecordsPartitionResult()\n+          .setLowWatermark(DeleteRecordsResponse.INVALID_LOW_WATERMARK)\n+          .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code)\n       else\n         authorizedForDeleteTopicOffsets += (topicPartition -> offset)\n     }\n \n     // the callback for sending a DeleteRecordsResponse\n-    def sendResponseCallback(authorizedTopicResponses: Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]): Unit = {\n+    def sendResponseCallback(authorizedTopicResponses: Map[TopicPartition, DeleteRecordsPartitionResult]): Unit = {\n       val mergedResponseStatus = authorizedTopicResponses ++ unauthorizedTopicResponses ++ nonExistingTopicResponses\n       mergedResponseStatus.foreach { case (topicPartition, status) =>\n-        if (status.error != Errors.NONE) {\n+        if (status.errorCode != Errors.NONE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTgzNzg3MQ==", "bodyText": "We don't need for brackets after offset", "url": "https://github.com/apache/kafka/pull/7957#discussion_r375837871", "createdAt": "2020-02-06T13:39:49Z", "author": {"login": "mimaison"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -1821,46 +1822,64 @@ class KafkaApis(val requestChannel: RequestChannel,\n   def handleDeleteRecordsRequest(request: RequestChannel.Request): Unit = {\n     val deleteRecordsRequest = request.body[DeleteRecordsRequest]\n \n-    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n-    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n+    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n+    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n     val authorizedForDeleteTopicOffsets = mutable.Map[TopicPartition, Long]()\n \n     val authorizedTopics = filterAuthorized(request, DELETE, TOPIC,\n-      deleteRecordsRequest.partitionOffsets.asScala.toSeq.map(_._1.topic))\n-    for ((topicPartition, offset) <- deleteRecordsRequest.partitionOffsets.asScala) {\n+      deleteRecordsRequest.data.topics.asScala.map(_.name))\n+    for ((topicPartition, offset) <- deleteRecordsRequest.data.topics.asScala.flatMap(deleteTopic => {\n+      deleteTopic.partitions.asScala.map(deletePartition => {\n+        new TopicPartition(deleteTopic.name, deletePartition.partitionIndex) -> deletePartition.offset()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTgzNzk4Ng==", "bodyText": "We don't need for brackets after topic", "url": "https://github.com/apache/kafka/pull/7957#discussion_r375837986", "createdAt": "2020-02-06T13:40:01Z", "author": {"login": "mimaison"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -1821,46 +1822,64 @@ class KafkaApis(val requestChannel: RequestChannel,\n   def handleDeleteRecordsRequest(request: RequestChannel.Request): Unit = {\n     val deleteRecordsRequest = request.body[DeleteRecordsRequest]\n \n-    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n-    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n+    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n+    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n     val authorizedForDeleteTopicOffsets = mutable.Map[TopicPartition, Long]()\n \n     val authorizedTopics = filterAuthorized(request, DELETE, TOPIC,\n-      deleteRecordsRequest.partitionOffsets.asScala.toSeq.map(_._1.topic))\n-    for ((topicPartition, offset) <- deleteRecordsRequest.partitionOffsets.asScala) {\n+      deleteRecordsRequest.data.topics.asScala.map(_.name))\n+    for ((topicPartition, offset) <- deleteRecordsRequest.data.topics.asScala.flatMap(deleteTopic => {\n+      deleteTopic.partitions.asScala.map(deletePartition => {\n+        new TopicPartition(deleteTopic.name, deletePartition.partitionIndex) -> deletePartition.offset()\n+      })\n+    })) {\n       if (!authorizedTopics.contains(topicPartition.topic))\n-        unauthorizedTopicResponses += topicPartition -> new DeleteRecordsResponse.PartitionResponse(\n-          DeleteRecordsResponse.INVALID_LOW_WATERMARK, Errors.TOPIC_AUTHORIZATION_FAILED)\n+        unauthorizedTopicResponses += topicPartition -> new DeleteRecordsPartitionResult()\n+          .setLowWatermark(DeleteRecordsResponse.INVALID_LOW_WATERMARK)\n+          .setErrorCode(Errors.TOPIC_AUTHORIZATION_FAILED.code)\n       else if (!metadataCache.contains(topicPartition))\n-        nonExistingTopicResponses += topicPartition -> new DeleteRecordsResponse.PartitionResponse(\n-          DeleteRecordsResponse.INVALID_LOW_WATERMARK, Errors.UNKNOWN_TOPIC_OR_PARTITION)\n+        nonExistingTopicResponses += topicPartition -> new DeleteRecordsPartitionResult()\n+          .setLowWatermark(DeleteRecordsResponse.INVALID_LOW_WATERMARK)\n+          .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code)\n       else\n         authorizedForDeleteTopicOffsets += (topicPartition -> offset)\n     }\n \n     // the callback for sending a DeleteRecordsResponse\n-    def sendResponseCallback(authorizedTopicResponses: Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]): Unit = {\n+    def sendResponseCallback(authorizedTopicResponses: Map[TopicPartition, DeleteRecordsPartitionResult]): Unit = {\n       val mergedResponseStatus = authorizedTopicResponses ++ unauthorizedTopicResponses ++ nonExistingTopicResponses\n       mergedResponseStatus.foreach { case (topicPartition, status) =>\n-        if (status.error != Errors.NONE) {\n+        if (status.errorCode != Errors.NONE) {\n           debug(\"DeleteRecordsRequest with correlation id %d from client %s on partition %s failed due to %s\".format(\n             request.header.correlationId,\n             request.header.clientId,\n             topicPartition,\n-            status.error.exceptionName))\n+            Errors.forCode(status.errorCode).exceptionName))\n         }\n       }\n \n       sendResponseMaybeThrottle(request, requestThrottleMs =>\n-        new DeleteRecordsResponse(requestThrottleMs, mergedResponseStatus.asJava))\n+        new DeleteRecordsResponse(new DeleteRecordsResponseData()\n+          .setThrottleTimeMs(requestThrottleMs)\n+          .setTopics(new DeleteRecordsResponseData.DeleteRecordsTopicResultCollection(mergedResponseStatus.groupBy(_._1.topic()).map { case (topic, partitionMap) => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71b6b461894f520a610dfc59b763908e16ddc361", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/71b6b461894f520a610dfc59b763908e16ddc361", "committedDate": "2020-02-25T10:49:03Z", "message": "KAFKA-8768: Replace DeleteRecords request/response with automated protocol\n\nAlso add version 2 to make use of flexible versions, per KIP-482."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/88617744e55ad81566df79789012f31b1dd12f7b", "committedDate": "2020-02-25T11:34:40Z", "message": "Fix comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/88617744e55ad81566df79789012f31b1dd12f7b", "committedDate": "2020-02-25T11:34:40Z", "message": "Fix comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0MjcwNDg1", "url": "https://github.com/apache/kafka/pull/7957#pullrequestreview-374270485", "createdAt": "2020-03-13T12:46:08Z", "commit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxMjo0NjowOFrOF2CUPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxMzowNzoxNFrOF2C7HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwNTM3NA==", "bodyText": "I think we can computeIfAbsent() here. It will replace containsKey, put and get.", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392205374", "createdAt": "2020-03-13T12:46:08Z", "author": {"login": "mimaison"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2469,19 +2474,30 @@ void handleResponse(AbstractResponse abstractResponse) {\n                 Cluster cluster = response.cluster();\n \n                 // Group topic partitions by leader\n-                Map<Node, Map<TopicPartition, Long>> leaders = new HashMap<>();\n+                Map<Node, Map<String, DeleteRecordsTopic>> leaders = new HashMap<>();\n                 for (Map.Entry<TopicPartition, RecordsToDelete> entry: recordsToDelete.entrySet()) {\n-                    KafkaFutureImpl<DeletedRecords> future = futures.get(entry.getKey());\n+                    TopicPartition topicPartition = entry.getKey();\n+                    KafkaFutureImpl<DeletedRecords> future = futures.get(topicPartition);\n \n                     // Fail partitions with topic errors\n-                    Errors topicError = errors.get(entry.getKey().topic());\n-                    if (errors.containsKey(entry.getKey().topic())) {\n+                    Errors topicError = errors.get(topicPartition.topic());\n+                    if (errors.containsKey(topicPartition.topic())) {\n                         future.completeExceptionally(topicError.exception());\n                     } else {\n-                        Node node = cluster.leaderFor(entry.getKey());\n+                        Node node = cluster.leaderFor(topicPartition);\n                         if (node != null) {\n-                            leaders.computeIfAbsent(node, key -> new HashMap<>()).put(entry.getKey(),\n-                                entry.getValue().beforeOffset());\n+                            if (!leaders.containsKey(node))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwNTg0NQ==", "bodyText": "Same here, I think computeIfAbsent would simplify the logic", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392205845", "createdAt": "2020-03-13T12:47:03Z", "author": {"login": "mimaison"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2469,19 +2474,30 @@ void handleResponse(AbstractResponse abstractResponse) {\n                 Cluster cluster = response.cluster();\n \n                 // Group topic partitions by leader\n-                Map<Node, Map<TopicPartition, Long>> leaders = new HashMap<>();\n+                Map<Node, Map<String, DeleteRecordsTopic>> leaders = new HashMap<>();\n                 for (Map.Entry<TopicPartition, RecordsToDelete> entry: recordsToDelete.entrySet()) {\n-                    KafkaFutureImpl<DeletedRecords> future = futures.get(entry.getKey());\n+                    TopicPartition topicPartition = entry.getKey();\n+                    KafkaFutureImpl<DeletedRecords> future = futures.get(topicPartition);\n \n                     // Fail partitions with topic errors\n-                    Errors topicError = errors.get(entry.getKey().topic());\n-                    if (errors.containsKey(entry.getKey().topic())) {\n+                    Errors topicError = errors.get(topicPartition.topic());\n+                    if (errors.containsKey(topicPartition.topic())) {\n                         future.completeExceptionally(topicError.exception());\n                     } else {\n-                        Node node = cluster.leaderFor(entry.getKey());\n+                        Node node = cluster.leaderFor(topicPartition);\n                         if (node != null) {\n-                            leaders.computeIfAbsent(node, key -> new HashMap<>()).put(entry.getKey(),\n-                                entry.getValue().beforeOffset());\n+                            if (!leaders.containsKey(node))\n+                                leaders.put(node, new HashMap<>());\n+                            Map<String, DeleteRecordsTopic> deletionsForLeader = leaders.get(node);\n+                            DeleteRecordsTopic deleteRecords = deletionsForLeader.get(topicPartition.topic());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwNzI4Nw==", "bodyText": "Can we return that directly without creating a variable?", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392207287", "createdAt": "2020-03-13T12:50:22Z", "author": {"login": "mimaison"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2490,36 +2506,45 @@ void handleResponse(AbstractResponse abstractResponse) {\n \n                 final long deleteRecordsCallTimeMs = time.milliseconds();\n \n-                for (final Map.Entry<Node, Map<TopicPartition, Long>> entry : leaders.entrySet()) {\n-                    final Map<TopicPartition, Long> partitionDeleteOffsets = entry.getValue();\n+                for (final Map.Entry<Node, Map<String, DeleteRecordsTopic>> entry : leaders.entrySet()) {\n+                    final Map<String, DeleteRecordsTopic> partitionDeleteOffsets = entry.getValue();\n                     final int brokerId = entry.getKey().id();\n \n                     runnable.call(new Call(\"deleteRecords\", deadline,\n                             new ConstantNodeIdProvider(brokerId)) {\n \n                         @Override\n                         DeleteRecordsRequest.Builder createRequest(int timeoutMs) {\n-                            return new DeleteRecordsRequest.Builder(timeoutMs, partitionDeleteOffsets);\n+                            return new DeleteRecordsRequest.Builder(new DeleteRecordsRequestData()\n+                                    .setTimeoutMs(timeoutMs)\n+                                    .setTopics(new ArrayList<>(partitionDeleteOffsets.values())));\n                         }\n \n                         @Override\n                         void handleResponse(AbstractResponse abstractResponse) {\n                             DeleteRecordsResponse response = (DeleteRecordsResponse) abstractResponse;\n-                            for (Map.Entry<TopicPartition, DeleteRecordsResponse.PartitionResponse> result: response.responses().entrySet()) {\n-\n-                                KafkaFutureImpl<DeletedRecords> future = futures.get(result.getKey());\n-                                if (result.getValue().error == Errors.NONE) {\n-                                    future.complete(new DeletedRecords(result.getValue().lowWatermark));\n-                                } else {\n-                                    future.completeExceptionally(result.getValue().error.exception());\n+                            for (DeleteRecordsTopicResult topicResult: response.data().topics()) {\n+                                for (DeleteRecordsResponseData.DeleteRecordsPartitionResult partitionResult : topicResult.partitions()) {\n+                                    KafkaFutureImpl<DeletedRecords> future = futures.get(new TopicPartition(topicResult.name(), partitionResult.partitionIndex()));\n+                                    if (partitionResult.errorCode() == Errors.NONE.code()) {\n+                                        future.complete(new DeletedRecords(partitionResult.lowWatermark()));\n+                                    } else {\n+                                        future.completeExceptionally(Errors.forCode(partitionResult.errorCode()).exception());\n+                                    }\n                                 }\n                             }\n                         }\n \n                         @Override\n                         void handleFailure(Throwable throwable) {\n                             Stream<KafkaFutureImpl<DeletedRecords>> callFutures =\n-                                    partitionDeleteOffsets.keySet().stream().map(futures::get);\n+                                    partitionDeleteOffsets.values().stream().flatMap(\n+                                        recordsToDelete1 -> {\n+                                            Stream<TopicPartition> topicPartitionStream = recordsToDelete1.partitions().stream().map(partitionsToDelete ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwODQyOA==", "bodyText": "Let's keep the L", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392208428", "createdAt": "2020-03-13T12:52:52Z", "author": {"login": "mimaison"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java", "diffHunk": "@@ -17,64 +17,19 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.DeleteRecordsResponseData;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n \n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT64;\n-\n public class DeleteRecordsResponse extends AbstractResponse {\n \n-    public static final long INVALID_LOW_WATERMARK = -1L;\n-\n-    // request level key names\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    // partition level key names\n-    private static final String LOW_WATERMARK_KEY_NAME = \"low_watermark\";\n-\n-    private static final Schema DELETE_RECORDS_RESPONSE_PARTITION_V0 = new Schema(\n-            PARTITION_ID,\n-            new Field(LOW_WATERMARK_KEY_NAME, INT64, \"Smallest available offset of all live replicas\"),\n-            ERROR_CODE);\n-\n-    private static final Schema DELETE_RECORDS_RESPONSE_TOPIC_V0 = new Schema(\n-            TOPIC_NAME,\n-            new Field(PARTITIONS_KEY_NAME, new ArrayOf(DELETE_RECORDS_RESPONSE_PARTITION_V0)));\n-\n-    private static final Schema DELETE_RECORDS_RESPONSE_V0 = new Schema(\n-            THROTTLE_TIME_MS,\n-            new Field(TOPICS_KEY_NAME, new ArrayOf(DELETE_RECORDS_RESPONSE_TOPIC_V0)));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema DELETE_RECORDS_RESPONSE_V1 = DELETE_RECORDS_RESPONSE_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{DELETE_RECORDS_RESPONSE_V0, DELETE_RECORDS_RESPONSE_V1};\n-    }\n-\n-    private final int throttleTimeMs;\n-    private final Map<TopicPartition, PartitionResponse> responses;\n+    public static final long INVALID_LOW_WATERMARK = -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIxMzgzNA==", "bodyText": "I wonder if it's more readable having 2 nested for loops. What do you think?", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392213834", "createdAt": "2020-03-13T13:04:06Z", "author": {"login": "mimaison"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -1823,46 +1824,64 @@ class KafkaApis(val requestChannel: RequestChannel,\n   def handleDeleteRecordsRequest(request: RequestChannel.Request): Unit = {\n     val deleteRecordsRequest = request.body[DeleteRecordsRequest]\n \n-    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n-    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsResponse.PartitionResponse]()\n+    val unauthorizedTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n+    val nonExistingTopicResponses = mutable.Map[TopicPartition, DeleteRecordsPartitionResult]()\n     val authorizedForDeleteTopicOffsets = mutable.Map[TopicPartition, Long]()\n \n     val authorizedTopics = filterAuthorized(request, DELETE, TOPIC,\n-      deleteRecordsRequest.partitionOffsets.asScala.toSeq.map(_._1.topic))\n-    for ((topicPartition, offset) <- deleteRecordsRequest.partitionOffsets.asScala) {\n+      deleteRecordsRequest.data.topics.asScala.map(_.name))\n+    for ((topicPartition, offset) <- deleteRecordsRequest.data.topics.asScala.flatMap(deleteTopic => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIxNTMyNA==", "bodyText": "No need for brackets after partitions", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392215324", "createdAt": "2020-03-13T13:07:14Z", "author": {"login": "mimaison"}, "path": "core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala", "diffHunk": "@@ -174,7 +174,8 @@ class AuthorizerIntegrationTest extends BaseRequestTest {\n     ApiKeys.CONTROLLED_SHUTDOWN -> ((resp: requests.ControlledShutdownResponse) => resp.error),\n     ApiKeys.CREATE_TOPICS -> ((resp: CreateTopicsResponse) => Errors.forCode(resp.data.topics.find(topic).errorCode())),\n     ApiKeys.DELETE_TOPICS -> ((resp: requests.DeleteTopicsResponse) => Errors.forCode(resp.data.responses.find(topic).errorCode())),\n-    ApiKeys.DELETE_RECORDS -> ((resp: requests.DeleteRecordsResponse) => resp.responses.get(tp).error),\n+    ApiKeys.DELETE_RECORDS -> ((resp: requests.DeleteRecordsResponse) => Errors.forCode(resp.data.topics\n+      .find(tp.topic).partitions().find(tp.partition).errorCode)),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88617744e55ad81566df79789012f31b1dd12f7b"}, "originalPosition": 15}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b037108c621fc4cdc76ad2c64d376fc1bde6d97", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/6b037108c621fc4cdc76ad2c64d376fc1bde6d97", "committedDate": "2020-03-13T13:59:49Z", "message": "Review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0MzY4MjQ0", "url": "https://github.com/apache/kafka/pull/7957#pullrequestreview-374368244", "createdAt": "2020-03-13T14:57:51Z", "commit": {"oid": "6b037108c621fc4cdc76ad2c64d376fc1bde6d97"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1998, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}