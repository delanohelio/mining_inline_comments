{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2ODg5NTUx", "number": 8787, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzoxMDoyNVrOECp6XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzo0MzoxMVrOEDH9zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjE5MjkyOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzoxMDoyNVrOGfPsjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzo0MzozOVrOGgACuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQxNjIwNA==", "bodyText": "What's the idea of dropping this?", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435416204", "createdAt": "2020-06-04T17:10:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5MDQzNg==", "bodyText": "The diff is a bit misleading, this was also factored out into the new ClientUtils#fetchCommittedOffsets", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435590436", "createdAt": "2020-06-04T22:38:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQxNjIwNA=="}, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwODMxMg==", "bodyText": "Ah, now I see it.", "url": "https://github.com/apache/kafka/pull/8787#discussion_r436208312", "createdAt": "2020-06-05T23:43:39Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQxNjIwNA=="}, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjIyMDA1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzoxODoyOVrOGfP-kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjo0Njo1MVrOGfahBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMDgxOQ==", "bodyText": "This seems to be a step backwards, actually. Why wrap it as a StreamsException only just to immediately unwrap it again?", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435420819", "createdAt": "2020-06-04T17:18:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-\n         final Map<TopicPartition, Long> committedOffsets;\n         try {\n-            // those do not have a committed offset would default to 0\n-            committedOffsets =  mainConsumer.committed(partitions).entrySet().stream()\n-                .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue() == null ? 0L : e.getValue().offset()));\n-        } catch (final TimeoutException e) {\n-            // if it timed out we just retry next time.\n-            return Collections.emptyMap();\n-        } catch (final KafkaException e) {\n-            throw new StreamsException(String.format(\"Failed to retrieve end offsets for %s\", partitions), e);\n+            committedOffsets = fetchCommittedOffsets(partitions, mainConsumer);\n+        } catch (final StreamsException e) {\n+            if (e.getCause() instanceof TimeoutException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5MzQ3Nw==", "bodyText": "I thought this might raise some eyebrows. I wanted to keep the ClientUtils methods consistent, and thought wrapping everything as a StreamsException would be cleaner. But maybe it makes more sense to throw the TimeoutException separately...", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435593477", "createdAt": "2020-06-04T22:46:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-\n         final Map<TopicPartition, Long> committedOffsets;\n         try {\n-            // those do not have a committed offset would default to 0\n-            committedOffsets =  mainConsumer.committed(partitions).entrySet().stream()\n-                .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue() == null ? 0L : e.getValue().offset()));\n-        } catch (final TimeoutException e) {\n-            // if it timed out we just retry next time.\n-            return Collections.emptyMap();\n-        } catch (final KafkaException e) {\n-            throw new StreamsException(String.format(\"Failed to retrieve end offsets for %s\", partitions), e);\n+            committedOffsets = fetchCommittedOffsets(partitions, mainConsumer);\n+        } catch (final StreamsException e) {\n+            if (e.getCause() instanceof TimeoutException) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMDgxOQ=="}, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzYyNjIxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMTo1MToxMVrOGfd0Vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMTozNToyN1rOGiLnUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0NzU3NQ==", "bodyText": "@vvcephei I've been wondering if maybe we should only  catch the TimeoutException, and interpret a StreamsException as fatal (like IllegalStateException for example). This is how we were using  Consumer#committed in the StoreChangelogReader, and AFAICT that only throws KafkaException on \"unrecoverable errors\" (quoted from javadocs)\nBut I can't tell whether the Admin's listOffsets might throw on transient errors, so I'm leaning towards catching both just to be safe. WDYT?", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435647575", "createdAt": "2020-06-05T01:51:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -763,18 +778,36 @@ private boolean populateClientStatesMap(final Map<UUID, ClientState> clientState\n                     .flatMap(Collection::stream)\n                     .collect(Collectors.toList());\n \n-            final Collection<TopicPartition> allPreexistingChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allPreexistingChangelogPartitions.removeIf(partition -> newlyCreatedChangelogs.contains(partition.topic()));\n+            final Set<TopicPartition> preexistingChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> preexistingSourceChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> newlyCreatedChangelogPartitions = new HashSet<>();\n+            for (final TopicPartition changelog : allChangelogPartitions) {\n+                if (newlyCreatedChangelogs.contains(changelog.topic())) {\n+                    newlyCreatedChangelogPartitions.add(changelog);\n+                } else if (optimizedSourceChangelogs.contains(changelog.topic())) {\n+                    preexistingSourceChangelogPartitions.add(changelog);\n+                } else {\n+                    preexistingChangelogPartitions.add(changelog);\n+                }\n+            }\n+\n+            // Make the listOffsets request first so it can  fetch the offsets for non-source changelogs\n+            // asynchronously while we use the blocking Consumer#committed call to fetch source-changelog offsets\n+            final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> endOffsetsFuture =\n+                fetchEndOffsetsFuture(preexistingChangelogPartitions, adminClient);\n \n-            final Collection<TopicPartition> allNewlyCreatedChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allNewlyCreatedChangelogPartitions.removeAll(allPreexistingChangelogPartitions);\n+            final Map<TopicPartition, Long> sourceChangelogEndOffsets =\n+                fetchCommittedOffsets(preexistingSourceChangelogPartitions, taskManager.mainConsumer());\n \n-            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets =\n-                fetchEndOffsets(allPreexistingChangelogPartitions, adminClient);\n+            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets = ClientUtils.getEndOffsets(endOffsetsFuture);\n \n-            allTaskEndOffsetSums = computeEndOffsetSumsByTask(endOffsets, changelogsByStatefulTask, allNewlyCreatedChangelogPartitions);\n+            allTaskEndOffsetSums = computeEndOffsetSumsByTask(\n+                changelogsByStatefulTask,\n+                endOffsets,\n+                sourceChangelogEndOffsets,\n+                newlyCreatedChangelogPartitions);\n             fetchEndOffsetsSuccessful = true;\n-        } catch (final StreamsException e) {\n+        } catch (final StreamsException | TimeoutException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ3NzcxOQ==", "bodyText": "That sounds reasonable, but I think if you throw an exception in the assignor, it just calls the assignor again in a tight loop, which seems worse than backing off and trying again later.\nIf you want to propose this change, maybe you can verify what exactly happens if we throw.", "url": "https://github.com/apache/kafka/pull/8787#discussion_r438477719", "createdAt": "2020-06-11T00:26:26Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -763,18 +778,36 @@ private boolean populateClientStatesMap(final Map<UUID, ClientState> clientState\n                     .flatMap(Collection::stream)\n                     .collect(Collectors.toList());\n \n-            final Collection<TopicPartition> allPreexistingChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allPreexistingChangelogPartitions.removeIf(partition -> newlyCreatedChangelogs.contains(partition.topic()));\n+            final Set<TopicPartition> preexistingChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> preexistingSourceChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> newlyCreatedChangelogPartitions = new HashSet<>();\n+            for (final TopicPartition changelog : allChangelogPartitions) {\n+                if (newlyCreatedChangelogs.contains(changelog.topic())) {\n+                    newlyCreatedChangelogPartitions.add(changelog);\n+                } else if (optimizedSourceChangelogs.contains(changelog.topic())) {\n+                    preexistingSourceChangelogPartitions.add(changelog);\n+                } else {\n+                    preexistingChangelogPartitions.add(changelog);\n+                }\n+            }\n+\n+            // Make the listOffsets request first so it can  fetch the offsets for non-source changelogs\n+            // asynchronously while we use the blocking Consumer#committed call to fetch source-changelog offsets\n+            final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> endOffsetsFuture =\n+                fetchEndOffsetsFuture(preexistingChangelogPartitions, adminClient);\n \n-            final Collection<TopicPartition> allNewlyCreatedChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allNewlyCreatedChangelogPartitions.removeAll(allPreexistingChangelogPartitions);\n+            final Map<TopicPartition, Long> sourceChangelogEndOffsets =\n+                fetchCommittedOffsets(preexistingSourceChangelogPartitions, taskManager.mainConsumer());\n \n-            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets =\n-                fetchEndOffsets(allPreexistingChangelogPartitions, adminClient);\n+            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets = ClientUtils.getEndOffsets(endOffsetsFuture);\n \n-            allTaskEndOffsetSums = computeEndOffsetSumsByTask(endOffsets, changelogsByStatefulTask, allNewlyCreatedChangelogPartitions);\n+            allTaskEndOffsetSums = computeEndOffsetSumsByTask(\n+                changelogsByStatefulTask,\n+                endOffsets,\n+                sourceChangelogEndOffsets,\n+                newlyCreatedChangelogPartitions);\n             fetchEndOffsetsSuccessful = true;\n-        } catch (final StreamsException e) {\n+        } catch (final StreamsException | TimeoutException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0NzU3NQ=="}, "originalCommit": null, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ5NTA1Nw==", "bodyText": "if you throw an exception in the assignor, it just calls the assignor again in a tight loop\n\nWouldn't the leader thread just die? Not saying that that's ideal, either. But it's at least in line with how exceptions thrown by other admin client requests in the assignment are currently handled.", "url": "https://github.com/apache/kafka/pull/8787#discussion_r438495057", "createdAt": "2020-06-11T01:35:27Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -763,18 +778,36 @@ private boolean populateClientStatesMap(final Map<UUID, ClientState> clientState\n                     .flatMap(Collection::stream)\n                     .collect(Collectors.toList());\n \n-            final Collection<TopicPartition> allPreexistingChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allPreexistingChangelogPartitions.removeIf(partition -> newlyCreatedChangelogs.contains(partition.topic()));\n+            final Set<TopicPartition> preexistingChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> preexistingSourceChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> newlyCreatedChangelogPartitions = new HashSet<>();\n+            for (final TopicPartition changelog : allChangelogPartitions) {\n+                if (newlyCreatedChangelogs.contains(changelog.topic())) {\n+                    newlyCreatedChangelogPartitions.add(changelog);\n+                } else if (optimizedSourceChangelogs.contains(changelog.topic())) {\n+                    preexistingSourceChangelogPartitions.add(changelog);\n+                } else {\n+                    preexistingChangelogPartitions.add(changelog);\n+                }\n+            }\n+\n+            // Make the listOffsets request first so it can  fetch the offsets for non-source changelogs\n+            // asynchronously while we use the blocking Consumer#committed call to fetch source-changelog offsets\n+            final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> endOffsetsFuture =\n+                fetchEndOffsetsFuture(preexistingChangelogPartitions, adminClient);\n \n-            final Collection<TopicPartition> allNewlyCreatedChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allNewlyCreatedChangelogPartitions.removeAll(allPreexistingChangelogPartitions);\n+            final Map<TopicPartition, Long> sourceChangelogEndOffsets =\n+                fetchCommittedOffsets(preexistingSourceChangelogPartitions, taskManager.mainConsumer());\n \n-            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets =\n-                fetchEndOffsets(allPreexistingChangelogPartitions, adminClient);\n+            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets = ClientUtils.getEndOffsets(endOffsetsFuture);\n \n-            allTaskEndOffsetSums = computeEndOffsetSumsByTask(endOffsets, changelogsByStatefulTask, allNewlyCreatedChangelogPartitions);\n+            allTaskEndOffsetSums = computeEndOffsetSumsByTask(\n+                changelogsByStatefulTask,\n+                endOffsets,\n+                sourceChangelogEndOffsets,\n+                newlyCreatedChangelogPartitions);\n             fetchEndOffsetsSuccessful = true;\n-        } catch (final StreamsException e) {\n+        } catch (final StreamsException | TimeoutException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0NzU3NQ=="}, "originalCommit": null, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNzExNjkzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ClientUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzo0MzoxMVrOGgACcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwMDowNjozOVrOGhgfqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwODI0MA==", "bodyText": "Upon retrospect, I'm not sure if this is possible. The javadoc for Future#get indicates that any exception would be wrapped in an ExecutionException.", "url": "https://github.com/apache/kafka/pull/8787#discussion_r436208240", "createdAt": "2020-06-05T23:43:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ClientUtils.java", "diffHunk": "@@ -95,19 +99,65 @@ public static String getTaskProducerClientId(final String threadClientId, final\n         return result;\n     }\n \n-    public static Map<TopicPartition, ListOffsetsResultInfo> fetchEndOffsets(final Collection<TopicPartition> partitions,\n-                                                                             final Admin adminClient) {\n-        final Map<TopicPartition, ListOffsetsResultInfo> endOffsets;\n+    /**\n+     * @throws StreamsException if the consumer throws an exception\n+     * @throws org.apache.kafka.common.errors.TimeoutException if the request times out\n+     */\n+    public static Map<TopicPartition, Long> fetchCommittedOffsets(final Set<TopicPartition> partitions,\n+                                                                  final Consumer<byte[], byte[]> consumer) {\n+        if (partitions.isEmpty()) {\n+            return Collections.emptyMap();\n+        }\n+\n+        final Map<TopicPartition, Long> committedOffsets;\n         try {\n-            final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> future =  adminClient.listOffsets(\n-                partitions.stream().collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest())))\n-                                                                                        .all();\n-            endOffsets = future.get();\n+            // those which do not have a committed offset would default to 0\n+            committedOffsets = consumer.committed(partitions).entrySet().stream()\n+                .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue() == null ? 0L : e.getValue().offset()));\n+        } catch (final TimeoutException e) {\n+            LOG.warn(\"The committed offsets request timed out, try increasing the consumer client's default.api.timeout.ms\", e);\n+            throw e;\n+        } catch (final KafkaException e) {\n+            LOG.warn(\"The committed offsets request failed.\", e);\n+            throw new StreamsException(String.format(\"Failed to retrieve end offsets for %s\", partitions), e);\n+        }\n+\n+        return committedOffsets;\n+    }\n \n+    public static KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> fetchEndOffsetsFuture(final Collection<TopicPartition> partitions,\n+                                                                                                final Admin adminClient) {\n+        return adminClient.listOffsets(\n+            partitions.stream().collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest())))\n+            .all();\n+    }\n+\n+    /**\n+     * A helper method that wraps the {@code Future#get} call and rethrows any thrown exception as a StreamsException\n+     * @throws StreamsException if the admin client request throws an exception\n+     * @throws org.apache.kafka.common.errors.TimeoutException if the request times out\n+     */\n+    public static Map<TopicPartition, ListOffsetsResultInfo> getEndOffsets(final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> endOffsetsFuture) {\n+        try {\n+            return endOffsetsFuture.get();\n+        } catch (final TimeoutException e) {\n+            LOG.warn(\"The listOffsets request timed out, try increasing the admin client's default.api.timeout.ms\", e);\n+            throw e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc4ODU4NA==", "bodyText": "Good catch. Do you think it should still be thrown/treated separately, though?  See also my comment in StreamsPartitionAssignor below", "url": "https://github.com/apache/kafka/pull/8787#discussion_r437788584", "createdAt": "2020-06-10T00:06:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ClientUtils.java", "diffHunk": "@@ -95,19 +99,65 @@ public static String getTaskProducerClientId(final String threadClientId, final\n         return result;\n     }\n \n-    public static Map<TopicPartition, ListOffsetsResultInfo> fetchEndOffsets(final Collection<TopicPartition> partitions,\n-                                                                             final Admin adminClient) {\n-        final Map<TopicPartition, ListOffsetsResultInfo> endOffsets;\n+    /**\n+     * @throws StreamsException if the consumer throws an exception\n+     * @throws org.apache.kafka.common.errors.TimeoutException if the request times out\n+     */\n+    public static Map<TopicPartition, Long> fetchCommittedOffsets(final Set<TopicPartition> partitions,\n+                                                                  final Consumer<byte[], byte[]> consumer) {\n+        if (partitions.isEmpty()) {\n+            return Collections.emptyMap();\n+        }\n+\n+        final Map<TopicPartition, Long> committedOffsets;\n         try {\n-            final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> future =  adminClient.listOffsets(\n-                partitions.stream().collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest())))\n-                                                                                        .all();\n-            endOffsets = future.get();\n+            // those which do not have a committed offset would default to 0\n+            committedOffsets = consumer.committed(partitions).entrySet().stream()\n+                .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue() == null ? 0L : e.getValue().offset()));\n+        } catch (final TimeoutException e) {\n+            LOG.warn(\"The committed offsets request timed out, try increasing the consumer client's default.api.timeout.ms\", e);\n+            throw e;\n+        } catch (final KafkaException e) {\n+            LOG.warn(\"The committed offsets request failed.\", e);\n+            throw new StreamsException(String.format(\"Failed to retrieve end offsets for %s\", partitions), e);\n+        }\n+\n+        return committedOffsets;\n+    }\n \n+    public static KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> fetchEndOffsetsFuture(final Collection<TopicPartition> partitions,\n+                                                                                                final Admin adminClient) {\n+        return adminClient.listOffsets(\n+            partitions.stream().collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest())))\n+            .all();\n+    }\n+\n+    /**\n+     * A helper method that wraps the {@code Future#get} call and rethrows any thrown exception as a StreamsException\n+     * @throws StreamsException if the admin client request throws an exception\n+     * @throws org.apache.kafka.common.errors.TimeoutException if the request times out\n+     */\n+    public static Map<TopicPartition, ListOffsetsResultInfo> getEndOffsets(final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> endOffsetsFuture) {\n+        try {\n+            return endOffsetsFuture.get();\n+        } catch (final TimeoutException e) {\n+            LOG.warn(\"The listOffsets request timed out, try increasing the admin client's default.api.timeout.ms\", e);\n+            throw e;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwODI0MA=="}, "originalCommit": null, "originalPosition": 74}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2505, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}