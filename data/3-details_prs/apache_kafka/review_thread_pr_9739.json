{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM4ODc2ODM0", "number": 9739, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xM1QwNzoxNzoyMFrOFEmkKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwMTowMTowMVrOFQp85Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQwMzcwNDc0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/raft/KafkaMetadataLog.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xM1QwNzoxNzoyMFrOIEwziw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwNTo0NjoxOFrOIVLqTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg2NDg0Mw==", "bodyText": "I intended to add a new AppendOrigin called Leader for this case, but then think it implies the same as the Replication, so just reuses it.", "url": "https://github.com/apache/kafka/pull/9739#discussion_r541864843", "createdAt": "2020-12-13T07:17:20Z", "author": {"login": "feyman2016"}, "path": "core/src/main/scala/kafka/raft/KafkaMetadataLog.scala", "diffHunk": "@@ -68,7 +68,7 @@ class KafkaMetadataLog(\n \n     val appendInfo = log.appendAsLeader(records.asInstanceOf[MemoryRecords],\n       leaderEpoch = epoch,\n-      origin = AppendOrigin.Coordinator)\n+      origin = AppendOrigin.Replication)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY2NDI5MA==", "bodyText": "I was going to ask about adding a new AppendOrigin. I agree that the behavior should be the same as for Replication, but it seems like it could lead to confusion. Maybe we could add an AppendOrigin.RaftLeader. Then we can add a simple comment which emphasizes that the Raft leader is responsible for assigning offsets. That would also allow us to revert the changes in LogCleanerTest.", "url": "https://github.com/apache/kafka/pull/9739#discussion_r553664290", "createdAt": "2021-01-08T00:05:22Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/raft/KafkaMetadataLog.scala", "diffHunk": "@@ -68,7 +68,7 @@ class KafkaMetadataLog(\n \n     val appendInfo = log.appendAsLeader(records.asInstanceOf[MemoryRecords],\n       leaderEpoch = epoch,\n-      origin = AppendOrigin.Coordinator)\n+      origin = AppendOrigin.Replication)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg2NDg0Mw=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA4MjA2Mw==", "bodyText": "Added a AppendOrigin.RaftLeader and also reverted the changes in LogCleanerTest", "url": "https://github.com/apache/kafka/pull/9739#discussion_r559082063", "createdAt": "2021-01-17T05:46:18Z", "author": {"login": "feyman2016"}, "path": "core/src/main/scala/kafka/raft/KafkaMetadataLog.scala", "diffHunk": "@@ -68,7 +68,7 @@ class KafkaMetadataLog(\n \n     val appendInfo = log.appendAsLeader(records.asInstanceOf[MemoryRecords],\n       leaderEpoch = epoch,\n-      origin = AppendOrigin.Coordinator)\n+      origin = AppendOrigin.Replication)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg2NDg0Mw=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQwMzcxNTI2OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xM1QwNzoyNDo1MVrOIEw3-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwNTo1Mjo1MFrOIVLsZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg2NTk3Nw==", "bodyText": "This is necessary for the sanity check in Log.scala append()\n         if (batch.magic >= RecordBatch.MAGIC_VALUE_V2) {\n              maybeAssignEpochStartOffset(batch.partitionLeaderEpoch, batch.baseOffset)", "url": "https://github.com/apache/kafka/pull/9739#discussion_r541865977", "createdAt": "2020-12-13T07:24:51Z", "author": {"login": "feyman2016"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1477,6 +1477,7 @@ private void appendBatch(\n     ) {\n         try {\n             int epoch = state.epoch();\n+            batch.data.batches().forEach(recordBatch -> recordBatch.setPartitionLeaderEpoch(epoch));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY2NTQ4MQ==", "bodyText": "Hmm.. Do we need this? I thought we already set leader epoch here: https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java#L256.", "url": "https://github.com/apache/kafka/pull/9739#discussion_r553665481", "createdAt": "2021-01-08T00:09:05Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1477,6 +1477,7 @@ private void appendBatch(\n     ) {\n         try {\n             int epoch = state.epoch();\n+            batch.data.batches().forEach(recordBatch -> recordBatch.setPartitionLeaderEpoch(epoch));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg2NTk3Nw=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA4MjU5OQ==", "bodyText": "@hachikuji I thought the same as you, re-check the code, the PARTITION_LEADER_EPOCH_OFFSET has been set in DefaultRecordBatch#writeHeader, but the problem is that the BatchBuilder is always constructed with the epoch=RecordBatch.NO_PARTITION_LEADER_EPOCH, so I updated it and reverted the L1480.", "url": "https://github.com/apache/kafka/pull/9739#discussion_r559082599", "createdAt": "2021-01-17T05:52:50Z", "author": {"login": "feyman2016"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1477,6 +1477,7 @@ private void appendBatch(\n     ) {\n         try {\n             int epoch = state.epoch();\n+            batch.data.batches().forEach(recordBatch -> recordBatch.setPartitionLeaderEpoch(epoch));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg2NTk3Nw=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQwMzg4NTQ3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xM1QwOToyNDo1MFrOIEyC3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwNTo1MzowN1rOIVLseA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg4NTE1MQ==", "bodyText": "This can be replaced by val assignOffsets = if (origin != AppendOrigin.Replication)", "url": "https://github.com/apache/kafka/pull/9739#discussion_r541885151", "createdAt": "2020-12-13T09:24:50Z", "author": {"login": "dengziming"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val assignOffsets = if (origin == AppendOrigin.Replication) false else true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTk0MTg5NA==", "bodyText": "@dengziming Thanks, but it needs else for a complete if expression", "url": "https://github.com/apache/kafka/pull/9739#discussion_r541941894", "createdAt": "2020-12-13T14:59:42Z", "author": {"login": "feyman2016"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val assignOffsets = if (origin == AppendOrigin.Replication) false else true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg4NTE1MQ=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjA4NjU5Mg==", "bodyText": "o, I mean val assignOffsets = origin != AppendOrigin.Replication", "url": "https://github.com/apache/kafka/pull/9739#discussion_r542086592", "createdAt": "2020-12-14T03:26:16Z", "author": {"login": "dengziming"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val assignOffsets = if (origin == AppendOrigin.Replication) false else true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg4NTE1MQ=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM2MjM3Mg==", "bodyText": "Got it, will let @hachikuji review and fix it in one shot, thanks", "url": "https://github.com/apache/kafka/pull/9739#discussion_r542362372", "createdAt": "2020-12-14T12:57:14Z", "author": {"login": "feyman2016"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val assignOffsets = if (origin == AppendOrigin.Replication) false else true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg4NTE1MQ=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY2NjczOA==", "bodyText": "By the way, it seems we might want to rename assignOffsets since we also rely on this flag for record validation. It's a bit on the verbose side, but maybe validateAndAssignOffsets to go along with the similarly named method in LogValidator?", "url": "https://github.com/apache/kafka/pull/9739#discussion_r553666738", "createdAt": "2021-01-08T00:13:36Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val assignOffsets = if (origin == AppendOrigin.Replication) false else true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg4NTE1MQ=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA4MjYxNg==", "bodyText": "Make sense, updated", "url": "https://github.com/apache/kafka/pull/9739#discussion_r559082616", "createdAt": "2021-01-17T05:53:07Z", "author": {"login": "feyman2016"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val assignOffsets = if (origin == AppendOrigin.Replication) false else true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg4NTE1MQ=="}, "originalCommit": {"oid": "ed2fc564ea5431a208cbb7e534996ee1843bb2a6"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQwNDQwMzE0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xM1QxNToxMTo1OVrOIE1pKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwNTo1Mzo0MlrOIVLsng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTk0NDEwNA==", "bodyText": "Since now for AppendOrigin.Replication, the LogValidator is bypassed when Log.appendAsLeader is called, which has an assumption that the records to be appended should have leader epoch ready in its underlying batches. And moreover, the transaction/group coordinator are supposed to be calling the Log.appendAsLeader with origin = AppendOrigin.Coordinator.", "url": "https://github.com/apache/kafka/pull/9739#discussion_r541944104", "createdAt": "2020-12-13T15:11:59Z", "author": {"login": "feyman2016"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerTest.scala", "diffHunk": "@@ -599,12 +599,12 @@ class LogCleanerTest {\n     val log = makeLog(config = LogConfig.fromProps(logConfig.originals, logProps))\n \n     val appendFirstTransaction = appendTransactionalAsLeader(log, producerId, producerEpoch,\n-      origin = AppendOrigin.Replication)\n+      origin = AppendOrigin.Coordinator)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "621bbb2fdb63b3a72e80e0bdefb33e2b0bb3d033"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA4MjY1NA==", "bodyText": "Reverted since we have a newly added AppendOrigin.RaftLeader", "url": "https://github.com/apache/kafka/pull/9739#discussion_r559082654", "createdAt": "2021-01-17T05:53:42Z", "author": {"login": "feyman2016"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerTest.scala", "diffHunk": "@@ -599,12 +599,12 @@ class LogCleanerTest {\n     val log = makeLog(config = LogConfig.fromProps(logConfig.originals, logProps))\n \n     val appendFirstTransaction = appendTransactionalAsLeader(log, producerId, producerEpoch,\n-      origin = AppendOrigin.Replication)\n+      origin = AppendOrigin.Coordinator)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTk0NDEwNA=="}, "originalCommit": {"oid": "621bbb2fdb63b3a72e80e0bdefb33e2b0bb3d033"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ4NDg1MDA4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMDoxNDoyMVrOIQBJeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQxNTo0OToyMFrOIVCudQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY2NjkzOA==", "bodyText": "It would be helpful to have a test case which verifies appendAsLeader with the new append origin.", "url": "https://github.com/apache/kafka/pull/9739#discussion_r553666938", "createdAt": "2021-01-08T00:14:21Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "621bbb2fdb63b3a72e80e0bdefb33e2b0bb3d033"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODkzNTY2OQ==", "bodyText": "fixed", "url": "https://github.com/apache/kafka/pull/9739#discussion_r558935669", "createdAt": "2021-01-16T15:49:20Z", "author": {"login": "feyman2016"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1050,7 +1050,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY2NjkzOA=="}, "originalCommit": {"oid": "621bbb2fdb63b3a72e80e0bdefb33e2b0bb3d033"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyMDA2OTYwOnYy", "diffSide": "LEFT", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwODoyODowMlrOIVMkCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwODoyODowMlrOIVMkCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA5Njg0Mw==", "bodyText": "Fx checkstyle error", "url": "https://github.com/apache/kafka/pull/9739#discussion_r559096843", "createdAt": "2021-01-17T08:28:02Z", "author": {"login": "feyman2016"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -19,7 +19,6 @@\n import org.apache.kafka.common.memory.MemoryPool;\n import org.apache.kafka.common.record.CompressionType;\n import org.apache.kafka.common.record.MemoryRecords;\n-import org.apache.kafka.common.record.RecordBatch;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30952cb2e7bd8eb913d011e6c9f6dde87d14a2e7"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDA3NzY5OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwMDo1NzoxMlrOIWoY9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQwMzo1OToxOFrOIXklXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDYwMTMzNQ==", "bodyText": "Good catch. Do we have any test cases in BatchAccumulatorTest which can be modified to catch this case?", "url": "https://github.com/apache/kafka/pull/9739#discussion_r560601335", "createdAt": "2021-01-20T00:57:12Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -180,7 +179,7 @@ private void startNewBatch() {\n                 nextOffset,\n                 time.milliseconds(),\n                 false,\n-                RecordBatch.NO_PARTITION_LEADER_EPOCH,\n+                epoch,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa1049a848c06b99c2fbf73fbf04d8e52ff50f6a"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTU4NzU1MA==", "bodyText": "Let me check", "url": "https://github.com/apache/kafka/pull/9739#discussion_r561587550", "createdAt": "2021-01-21T03:59:18Z", "author": {"login": "feyman2016"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -180,7 +179,7 @@ private void startNewBatch() {\n                 nextOffset,\n                 time.milliseconds(),\n                 false,\n-                RecordBatch.NO_PARTITION_LEADER_EPOCH,\n+                epoch,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDYwMTMzNQ=="}, "originalCommit": {"oid": "aa1049a848c06b99c2fbf73fbf04d8e52ff50f6a"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDA4ODY5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwMTowMTowMVrOIWoe0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yNVQxODoxOTowOFrOIZ0R2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDYwMjgzMw==", "bodyText": "It is a little odd here is that we have to pass through leaderEpoch even though we expect the Raft leader to have set it already. Perhaps we should be validating it in analyzeAndValidateRecords. We can verify for each batch that the leader epoch matches when the append origin is RaftLeader. Does that make sense?", "url": "https://github.com/apache/kafka/pull/9739#discussion_r560602833", "createdAt": "2021-01-20T01:01:01Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1057,7 +1057,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val validateAndAssignOffsets = origin != AppendOrigin.RaftLeader\n+    append(records, origin, interBrokerProtocolVersion, validateAndAssignOffsets, leaderEpoch, ignoreRecordSize = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa1049a848c06b99c2fbf73fbf04d8e52ff50f6a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTU4OTg2Nw==", "bodyText": "Make sense to me, it is also a little bit odd to me, but I put it here because I think assignOffsets==true for appendAsLeader and ==false for appendAsFollower, which means normally assignOffsets is determined by the caller, the RaftLeader is just a special case for appendAsLeader, if we move the logic in analyzeAndValidateRecords, that means it need to determine whether to assignOffsets without caller info, does that doable?", "url": "https://github.com/apache/kafka/pull/9739#discussion_r561589867", "createdAt": "2021-01-21T04:08:31Z", "author": {"login": "feyman2016"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1057,7 +1057,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val validateAndAssignOffsets = origin != AppendOrigin.RaftLeader\n+    append(records, origin, interBrokerProtocolVersion, validateAndAssignOffsets, leaderEpoch, ignoreRecordSize = false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDYwMjgzMw=="}, "originalCommit": {"oid": "aa1049a848c06b99c2fbf73fbf04d8e52ff50f6a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Mzk0MTg0OA==", "bodyText": "The call to analyzeAndValidateRecords is done both for leader as well as follower appends. Basically we do a shallow iteration over the batches in order to collect some information and validate the CRC. My thought was to pass leaderEpoch into analyzeAndValidateRecords and add a basic check like this:\nrecords.batches.forEach { batch =>\n  ...\n  if (origin === RaftLeader && batch.partitionLeaderEpoch != leaderEpoch) {\n    throw new InvalidRecordException(\"Append from Raft leader did not set the batch epoch correctly\")\n  }\n}", "url": "https://github.com/apache/kafka/pull/9739#discussion_r563941848", "createdAt": "2021-01-25T18:19:08Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1057,7 +1057,8 @@ class Log(@volatile private var _dir: File,\n                      leaderEpoch: Int,\n                      origin: AppendOrigin = AppendOrigin.Client,\n                      interBrokerProtocolVersion: ApiVersion = ApiVersion.latestVersion): LogAppendInfo = {\n-    append(records, origin, interBrokerProtocolVersion, assignOffsets = true, leaderEpoch, ignoreRecordSize = false)\n+    val validateAndAssignOffsets = origin != AppendOrigin.RaftLeader\n+    append(records, origin, interBrokerProtocolVersion, validateAndAssignOffsets, leaderEpoch, ignoreRecordSize = false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDYwMjgzMw=="}, "originalCommit": {"oid": "aa1049a848c06b99c2fbf73fbf04d8e52ff50f6a"}, "originalPosition": 6}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3487, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}