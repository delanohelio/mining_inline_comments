{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAyOTAwNjkx", "number": 8476, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxODo1NDoyMlrODyM6OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxOTowMjowOVrODyNGBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzOTY2OTA0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxODo1NDoyMlrOGGHN3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMToxOTozM1rOGGL8KA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2Mjg3Nw==", "bodyText": "Why in this case we use maxOffsetMetadata and in the else if we use startOffsetMetadata as emptyFetchDataInfo#fetchOffsetMetadata? Is there a specific rationale for that?", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409062877", "createdAt": "2020-04-15T18:54:22Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1483,12 +1480,14 @@ class Log(@volatile private var _dir: File,\n           s\"but we only have log segments in the range $logStartOffset to $endOffset.\")\n \n       val maxOffsetMetadata = isolation match {\n-        case FetchLogEnd => nextOffsetMetadata\n+        case FetchLogEnd => endOffsetMetadata\n         case FetchHighWatermark => fetchHighWatermarkMetadata\n         case FetchTxnCommitted => fetchLastStableOffsetMetadata\n       }\n \n-      if (startOffset > maxOffsetMetadata.messageOffset) {\n+      if (startOffset == maxOffsetMetadata.messageOffset) {\n+        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3NTU1Mw==", "bodyText": "If the start offset matches max offset, then we do not need to lookup the metadata as we already have it. Really this is the bug fix. Without this, then we will proceed to read from the log segment, which results in an error like the following:\njava.util.concurrent.ExecutionException: org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@cc86429`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 85.\n\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat kafka.log.LogConcurrencyTest.testUncommittedDataNotConsumed(LogConcurrencyTest.scala:75)\n\tat kafka.log.LogConcurrencyTest.testUncommittedDataNotConsumedFrequentSegmentRolls(LogConcurrencyTest.scala:61)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\n\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\n\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\n\tat com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)\nCaused by: org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@cc86429`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 85.\n\tat org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:40)\n\tat org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:24)\n\tat org.apache.kafka.common.utils.AbstractIterator.maybeComputeNext(AbstractIterator.java:79)\n\tat org.apache.kafka.common.utils.AbstractIterator.hasNext(AbstractIterator.java:45)\n\tat org.apache.kafka.common.record.FileRecords.searchForOffsetWithSize(FileRecords.java:302)\n\tat kafka.log.LogSegment.translateOffset(LogSegment.scala:275)\n\tat kafka.log.LogSegment.read(LogSegment.scala:298)\n\tat kafka.log.Log.$anonfun$read$2(Log.scala:1515)\n\tat kafka.log.Log.read(Log.scala:2333)\n\tat kafka.log.LogConcurrencyTest$ConsumerTask.call(LogConcurrencyTest.scala:95)\n\tat kafka.log.LogConcurrencyTest$ConsumerTask.call(LogConcurrencyTest.scala:85)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@cc86429`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 85.\n\tat org.apache.kafka.common.utils.Utils.readFullyOrFail(Utils.java:966)\n\tat org.apache.kafka.common.record.FileLogInputStream.nextBatch(FileLogInputStream.java:68)\n\tat org.apache.kafka.common.record.FileLogInputStream.nextBatch(FileLogInputStream.java:41)\n\tat org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:35)\n\t... 14 more", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409075553", "createdAt": "2020-04-15T19:16:14Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1483,12 +1480,14 @@ class Log(@volatile private var _dir: File,\n           s\"but we only have log segments in the range $logStartOffset to $endOffset.\")\n \n       val maxOffsetMetadata = isolation match {\n-        case FetchLogEnd => nextOffsetMetadata\n+        case FetchLogEnd => endOffsetMetadata\n         case FetchHighWatermark => fetchHighWatermarkMetadata\n         case FetchTxnCommitted => fetchLastStableOffsetMetadata\n       }\n \n-      if (startOffset > maxOffsetMetadata.messageOffset) {\n+      if (startOffset == maxOffsetMetadata.messageOffset) {\n+        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2Mjg3Nw=="}, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMjA4MA==", "bodyText": "Yeah I understand this is the bug fix, I'm only curious to see why we used\nemptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n\nnot\nemptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n\nI realized it is just the same, so curious if you intentionally use maxOffsetMetadata for any other reasons, but I think it is just to avoid unnecessary convertToOffsetMetadataOrThrow :)", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409112080", "createdAt": "2020-04-15T20:24:51Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1483,12 +1480,14 @@ class Log(@volatile private var _dir: File,\n           s\"but we only have log segments in the range $logStartOffset to $endOffset.\")\n \n       val maxOffsetMetadata = isolation match {\n-        case FetchLogEnd => nextOffsetMetadata\n+        case FetchLogEnd => endOffsetMetadata\n         case FetchHighWatermark => fetchHighWatermarkMetadata\n         case FetchTxnCommitted => fetchLastStableOffsetMetadata\n       }\n \n-      if (startOffset > maxOffsetMetadata.messageOffset) {\n+      if (startOffset == maxOffsetMetadata.messageOffset) {\n+        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2Mjg3Nw=="}, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE0MDI2NA==", "bodyText": "It lets us skip the call to convertToOffsetMetadataOrThrow in order to find startOffsetMetadata. I tried changing the check below to >= and hit a similar error when trying to translate the offset in LogSegment. We might also be able to fix this by adding some additional checks in LogSegment to avoid the offset translation when the max position matches the start position of the requested offset.", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409140264", "createdAt": "2020-04-15T21:19:33Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1483,12 +1480,14 @@ class Log(@volatile private var _dir: File,\n           s\"but we only have log segments in the range $logStartOffset to $endOffset.\")\n \n       val maxOffsetMetadata = isolation match {\n-        case FetchLogEnd => nextOffsetMetadata\n+        case FetchLogEnd => endOffsetMetadata\n         case FetchHighWatermark => fetchHighWatermarkMetadata\n         case FetchTxnCommitted => fetchLastStableOffsetMetadata\n       }\n \n-      if (startOffset > maxOffsetMetadata.messageOffset) {\n+      if (startOffset == maxOffsetMetadata.messageOffset) {\n+        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2Mjg3Nw=="}, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzOTY5MzI0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxOTowMDoyMlrOGGHcqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDoyNTowOVrOGGKOww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NjY2Nw==", "bodyText": "Could we check last / next offset of the batch as well?", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409066667", "createdAt": "2020-04-15T19:00:22Z", "author": {"login": "guozhangwang"}, "path": "core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.log\n+\n+import java.util.Properties\n+import java.util.concurrent.{Callable, Executors}\n+\n+import kafka.server.{BrokerTopicStats, FetchHighWatermark, LogDirFailureChannel}\n+import kafka.utils.{KafkaScheduler, TestUtils}\n+import org.apache.kafka.common.record.SimpleRecord\n+import org.apache.kafka.common.utils.{Time, Utils}\n+import org.junit.Assert._\n+import org.junit.{After, Before, Test}\n+\n+import scala.collection.mutable.ListBuffer\n+import scala.util.Random\n+\n+class LogConcurrencyTest {\n+  private val brokerTopicStats = new BrokerTopicStats\n+  private val random = new Random()\n+  private val scheduler = new KafkaScheduler(1)\n+  private val tmpDir = TestUtils.tempDir()\n+  private val logDir = TestUtils.randomPartitionLogDir(tmpDir)\n+\n+  @Before\n+  def setup(): Unit = {\n+    scheduler.startup()\n+  }\n+\n+  @After\n+  def shutdown(): Unit = {\n+    scheduler.shutdown()\n+    Utils.delete(tmpDir)\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumed(): Unit = {\n+    testUncommittedDataNotConsumed(createLog())\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumedFrequentSegmentRolls(): Unit = {\n+    val logProps = new Properties()\n+    logProps.put(LogConfig.SegmentBytesProp, 237: Integer)\n+    val logConfig = LogConfig(logProps)\n+    testUncommittedDataNotConsumed(createLog(logConfig))\n+  }\n+\n+  def testUncommittedDataNotConsumed(log: Log): Unit = {\n+    val executor = Executors.newFixedThreadPool(2)\n+    try {\n+      val maxOffset = 5000\n+      val consumer = new ConsumerTask(log, maxOffset)\n+      val appendTask = new LogAppendTask(log, maxOffset)\n+\n+      val consumerFuture = executor.submit(consumer)\n+      val fetcherTaskFuture = executor.submit(appendTask)\n+\n+      fetcherTaskFuture.get()\n+      consumerFuture.get()\n+\n+      validateConsumedData(log, consumer.consumedBatches)\n+    } finally executor.shutdownNow()\n+  }\n+\n+  /**\n+   * Simple consumption task which reads the log in ascending order and collects\n+   * consumed batches for validation\n+   */\n+  private class ConsumerTask(log: Log, lastOffset: Int) extends Callable[Unit] {\n+    val consumedBatches = ListBuffer.empty[FetchedBatch]\n+\n+    override def call(): Unit = {\n+      var fetchOffset = 0L\n+      while (log.highWatermark < lastOffset) {\n+        val readInfo = log.read(\n+          startOffset = fetchOffset,\n+          maxLength = 1,\n+          isolation = FetchHighWatermark,\n+          minOneMessage = true\n+        )\n+        readInfo.records.batches().forEach { batch =>\n+          consumedBatches += FetchedBatch(batch.baseOffset, batch.partitionLeaderEpoch)\n+          fetchOffset = batch.lastOffset + 1\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * This class simulates basic leader/follower behavior.\n+   */\n+  private class LogAppendTask(log: Log, lastOffset: Long) extends Callable[Unit] {\n+    override def call(): Unit = {\n+      var leaderEpoch = 1\n+      var isLeader = true\n+\n+      while (log.highWatermark < lastOffset) {\n+        random.nextInt(2) match {\n+          case 0 =>\n+            val logEndOffsetMetadata = log.logEndOffsetMetadata\n+            val logEndOffset = logEndOffsetMetadata.messageOffset\n+            val batchSize = random.nextInt(9) + 1\n+            val records = (0 to batchSize).map(i => new SimpleRecord(s\"$i\".getBytes))\n+\n+            if (isLeader) {\n+              log.appendAsLeader(TestUtils.records(records), leaderEpoch)\n+              log.maybeIncrementHighWatermark(logEndOffsetMetadata)\n+            } else {\n+              log.appendAsFollower(TestUtils.records(records,\n+                baseOffset = logEndOffset,\n+                partitionLeaderEpoch = leaderEpoch))\n+              log.updateHighWatermark(logEndOffset)\n+            }\n+\n+          case 1 =>\n+            isLeader = !isLeader\n+            leaderEpoch += 1\n+\n+            if (!isLeader) {\n+              log.truncateTo(log.highWatermark)\n+            }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def createLog(config: LogConfig = LogConfig(new Properties())): Log = {\n+    Log(dir = logDir,\n+      config = config,\n+      logStartOffset = 0L,\n+      recoveryPoint = 0L,\n+      scheduler = scheduler,\n+      brokerTopicStats = brokerTopicStats,\n+      time = Time.SYSTEM,\n+      maxProducerIdExpirationMs = 60 * 60 * 1000,\n+      producerIdExpirationCheckIntervalMs = LogManager.ProducerIdExpirationCheckIntervalMs,\n+      logDirFailureChannel = new LogDirFailureChannel(10))\n+  }\n+\n+  private def validateConsumedData(log: Log, consumedBatches: Iterable[FetchedBatch]): Unit = {\n+    val iter = consumedBatches.iterator\n+    log.logSegments.foreach { segment =>\n+      segment.log.batches.forEach { batch =>\n+        if (iter.hasNext) {\n+          val consumedBatch = iter.next()\n+          try {\n+            assertEquals(\"Consumed batch with unexpected leader epoch\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3MDk3Mg==", "bodyText": "We could, but I thought it would be overkill since the generation logic generates batches which are unique by base offset and leader epoch.", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409070972", "createdAt": "2020-04-15T19:08:02Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.log\n+\n+import java.util.Properties\n+import java.util.concurrent.{Callable, Executors}\n+\n+import kafka.server.{BrokerTopicStats, FetchHighWatermark, LogDirFailureChannel}\n+import kafka.utils.{KafkaScheduler, TestUtils}\n+import org.apache.kafka.common.record.SimpleRecord\n+import org.apache.kafka.common.utils.{Time, Utils}\n+import org.junit.Assert._\n+import org.junit.{After, Before, Test}\n+\n+import scala.collection.mutable.ListBuffer\n+import scala.util.Random\n+\n+class LogConcurrencyTest {\n+  private val brokerTopicStats = new BrokerTopicStats\n+  private val random = new Random()\n+  private val scheduler = new KafkaScheduler(1)\n+  private val tmpDir = TestUtils.tempDir()\n+  private val logDir = TestUtils.randomPartitionLogDir(tmpDir)\n+\n+  @Before\n+  def setup(): Unit = {\n+    scheduler.startup()\n+  }\n+\n+  @After\n+  def shutdown(): Unit = {\n+    scheduler.shutdown()\n+    Utils.delete(tmpDir)\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumed(): Unit = {\n+    testUncommittedDataNotConsumed(createLog())\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumedFrequentSegmentRolls(): Unit = {\n+    val logProps = new Properties()\n+    logProps.put(LogConfig.SegmentBytesProp, 237: Integer)\n+    val logConfig = LogConfig(logProps)\n+    testUncommittedDataNotConsumed(createLog(logConfig))\n+  }\n+\n+  def testUncommittedDataNotConsumed(log: Log): Unit = {\n+    val executor = Executors.newFixedThreadPool(2)\n+    try {\n+      val maxOffset = 5000\n+      val consumer = new ConsumerTask(log, maxOffset)\n+      val appendTask = new LogAppendTask(log, maxOffset)\n+\n+      val consumerFuture = executor.submit(consumer)\n+      val fetcherTaskFuture = executor.submit(appendTask)\n+\n+      fetcherTaskFuture.get()\n+      consumerFuture.get()\n+\n+      validateConsumedData(log, consumer.consumedBatches)\n+    } finally executor.shutdownNow()\n+  }\n+\n+  /**\n+   * Simple consumption task which reads the log in ascending order and collects\n+   * consumed batches for validation\n+   */\n+  private class ConsumerTask(log: Log, lastOffset: Int) extends Callable[Unit] {\n+    val consumedBatches = ListBuffer.empty[FetchedBatch]\n+\n+    override def call(): Unit = {\n+      var fetchOffset = 0L\n+      while (log.highWatermark < lastOffset) {\n+        val readInfo = log.read(\n+          startOffset = fetchOffset,\n+          maxLength = 1,\n+          isolation = FetchHighWatermark,\n+          minOneMessage = true\n+        )\n+        readInfo.records.batches().forEach { batch =>\n+          consumedBatches += FetchedBatch(batch.baseOffset, batch.partitionLeaderEpoch)\n+          fetchOffset = batch.lastOffset + 1\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * This class simulates basic leader/follower behavior.\n+   */\n+  private class LogAppendTask(log: Log, lastOffset: Long) extends Callable[Unit] {\n+    override def call(): Unit = {\n+      var leaderEpoch = 1\n+      var isLeader = true\n+\n+      while (log.highWatermark < lastOffset) {\n+        random.nextInt(2) match {\n+          case 0 =>\n+            val logEndOffsetMetadata = log.logEndOffsetMetadata\n+            val logEndOffset = logEndOffsetMetadata.messageOffset\n+            val batchSize = random.nextInt(9) + 1\n+            val records = (0 to batchSize).map(i => new SimpleRecord(s\"$i\".getBytes))\n+\n+            if (isLeader) {\n+              log.appendAsLeader(TestUtils.records(records), leaderEpoch)\n+              log.maybeIncrementHighWatermark(logEndOffsetMetadata)\n+            } else {\n+              log.appendAsFollower(TestUtils.records(records,\n+                baseOffset = logEndOffset,\n+                partitionLeaderEpoch = leaderEpoch))\n+              log.updateHighWatermark(logEndOffset)\n+            }\n+\n+          case 1 =>\n+            isLeader = !isLeader\n+            leaderEpoch += 1\n+\n+            if (!isLeader) {\n+              log.truncateTo(log.highWatermark)\n+            }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def createLog(config: LogConfig = LogConfig(new Properties())): Log = {\n+    Log(dir = logDir,\n+      config = config,\n+      logStartOffset = 0L,\n+      recoveryPoint = 0L,\n+      scheduler = scheduler,\n+      brokerTopicStats = brokerTopicStats,\n+      time = Time.SYSTEM,\n+      maxProducerIdExpirationMs = 60 * 60 * 1000,\n+      producerIdExpirationCheckIntervalMs = LogManager.ProducerIdExpirationCheckIntervalMs,\n+      logDirFailureChannel = new LogDirFailureChannel(10))\n+  }\n+\n+  private def validateConsumedData(log: Log, consumedBatches: Iterable[FetchedBatch]): Unit = {\n+    val iter = consumedBatches.iterator\n+    log.logSegments.foreach { segment =>\n+      segment.log.batches.forEach { batch =>\n+        if (iter.hasNext) {\n+          val consumedBatch = iter.next()\n+          try {\n+            assertEquals(\"Consumed batch with unexpected leader epoch\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NjY2Nw=="}, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMjI1OQ==", "bodyText": "Cool", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409112259", "createdAt": "2020-04-15T20:25:09Z", "author": {"login": "guozhangwang"}, "path": "core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.log\n+\n+import java.util.Properties\n+import java.util.concurrent.{Callable, Executors}\n+\n+import kafka.server.{BrokerTopicStats, FetchHighWatermark, LogDirFailureChannel}\n+import kafka.utils.{KafkaScheduler, TestUtils}\n+import org.apache.kafka.common.record.SimpleRecord\n+import org.apache.kafka.common.utils.{Time, Utils}\n+import org.junit.Assert._\n+import org.junit.{After, Before, Test}\n+\n+import scala.collection.mutable.ListBuffer\n+import scala.util.Random\n+\n+class LogConcurrencyTest {\n+  private val brokerTopicStats = new BrokerTopicStats\n+  private val random = new Random()\n+  private val scheduler = new KafkaScheduler(1)\n+  private val tmpDir = TestUtils.tempDir()\n+  private val logDir = TestUtils.randomPartitionLogDir(tmpDir)\n+\n+  @Before\n+  def setup(): Unit = {\n+    scheduler.startup()\n+  }\n+\n+  @After\n+  def shutdown(): Unit = {\n+    scheduler.shutdown()\n+    Utils.delete(tmpDir)\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumed(): Unit = {\n+    testUncommittedDataNotConsumed(createLog())\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumedFrequentSegmentRolls(): Unit = {\n+    val logProps = new Properties()\n+    logProps.put(LogConfig.SegmentBytesProp, 237: Integer)\n+    val logConfig = LogConfig(logProps)\n+    testUncommittedDataNotConsumed(createLog(logConfig))\n+  }\n+\n+  def testUncommittedDataNotConsumed(log: Log): Unit = {\n+    val executor = Executors.newFixedThreadPool(2)\n+    try {\n+      val maxOffset = 5000\n+      val consumer = new ConsumerTask(log, maxOffset)\n+      val appendTask = new LogAppendTask(log, maxOffset)\n+\n+      val consumerFuture = executor.submit(consumer)\n+      val fetcherTaskFuture = executor.submit(appendTask)\n+\n+      fetcherTaskFuture.get()\n+      consumerFuture.get()\n+\n+      validateConsumedData(log, consumer.consumedBatches)\n+    } finally executor.shutdownNow()\n+  }\n+\n+  /**\n+   * Simple consumption task which reads the log in ascending order and collects\n+   * consumed batches for validation\n+   */\n+  private class ConsumerTask(log: Log, lastOffset: Int) extends Callable[Unit] {\n+    val consumedBatches = ListBuffer.empty[FetchedBatch]\n+\n+    override def call(): Unit = {\n+      var fetchOffset = 0L\n+      while (log.highWatermark < lastOffset) {\n+        val readInfo = log.read(\n+          startOffset = fetchOffset,\n+          maxLength = 1,\n+          isolation = FetchHighWatermark,\n+          minOneMessage = true\n+        )\n+        readInfo.records.batches().forEach { batch =>\n+          consumedBatches += FetchedBatch(batch.baseOffset, batch.partitionLeaderEpoch)\n+          fetchOffset = batch.lastOffset + 1\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * This class simulates basic leader/follower behavior.\n+   */\n+  private class LogAppendTask(log: Log, lastOffset: Long) extends Callable[Unit] {\n+    override def call(): Unit = {\n+      var leaderEpoch = 1\n+      var isLeader = true\n+\n+      while (log.highWatermark < lastOffset) {\n+        random.nextInt(2) match {\n+          case 0 =>\n+            val logEndOffsetMetadata = log.logEndOffsetMetadata\n+            val logEndOffset = logEndOffsetMetadata.messageOffset\n+            val batchSize = random.nextInt(9) + 1\n+            val records = (0 to batchSize).map(i => new SimpleRecord(s\"$i\".getBytes))\n+\n+            if (isLeader) {\n+              log.appendAsLeader(TestUtils.records(records), leaderEpoch)\n+              log.maybeIncrementHighWatermark(logEndOffsetMetadata)\n+            } else {\n+              log.appendAsFollower(TestUtils.records(records,\n+                baseOffset = logEndOffset,\n+                partitionLeaderEpoch = leaderEpoch))\n+              log.updateHighWatermark(logEndOffset)\n+            }\n+\n+          case 1 =>\n+            isLeader = !isLeader\n+            leaderEpoch += 1\n+\n+            if (!isLeader) {\n+              log.truncateTo(log.highWatermark)\n+            }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def createLog(config: LogConfig = LogConfig(new Properties())): Log = {\n+    Log(dir = logDir,\n+      config = config,\n+      logStartOffset = 0L,\n+      recoveryPoint = 0L,\n+      scheduler = scheduler,\n+      brokerTopicStats = brokerTopicStats,\n+      time = Time.SYSTEM,\n+      maxProducerIdExpirationMs = 60 * 60 * 1000,\n+      producerIdExpirationCheckIntervalMs = LogManager.ProducerIdExpirationCheckIntervalMs,\n+      logDirFailureChannel = new LogDirFailureChannel(10))\n+  }\n+\n+  private def validateConsumedData(log: Log, consumedBatches: Iterable[FetchedBatch]): Unit = {\n+    val iter = consumedBatches.iterator\n+    log.logSegments.foreach { segment =>\n+      segment.log.batches.forEach { batch =>\n+        if (iter.hasNext) {\n+          val consumedBatch = iter.next()\n+          try {\n+            assertEquals(\"Consumed batch with unexpected leader epoch\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NjY2Nw=="}, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 163}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzOTY5OTI0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxOTowMjowOVrOGGHgjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxOTowNDozMlrOGGHmHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NzY2Mg==", "bodyText": "I'm just curious how long would a single run take with 5000 records and a max batchsize of 10? Being a bit paranoid of it taking too long.", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409067662", "createdAt": "2020-04-15T19:02:09Z", "author": {"login": "guozhangwang"}, "path": "core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.log\n+\n+import java.util.Properties\n+import java.util.concurrent.{Callable, Executors}\n+\n+import kafka.server.{BrokerTopicStats, FetchHighWatermark, LogDirFailureChannel}\n+import kafka.utils.{KafkaScheduler, TestUtils}\n+import org.apache.kafka.common.record.SimpleRecord\n+import org.apache.kafka.common.utils.{Time, Utils}\n+import org.junit.Assert._\n+import org.junit.{After, Before, Test}\n+\n+import scala.collection.mutable.ListBuffer\n+import scala.util.Random\n+\n+class LogConcurrencyTest {\n+  private val brokerTopicStats = new BrokerTopicStats\n+  private val random = new Random()\n+  private val scheduler = new KafkaScheduler(1)\n+  private val tmpDir = TestUtils.tempDir()\n+  private val logDir = TestUtils.randomPartitionLogDir(tmpDir)\n+\n+  @Before\n+  def setup(): Unit = {\n+    scheduler.startup()\n+  }\n+\n+  @After\n+  def shutdown(): Unit = {\n+    scheduler.shutdown()\n+    Utils.delete(tmpDir)\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumed(): Unit = {\n+    testUncommittedDataNotConsumed(createLog())\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumedFrequentSegmentRolls(): Unit = {\n+    val logProps = new Properties()\n+    logProps.put(LogConfig.SegmentBytesProp, 237: Integer)\n+    val logConfig = LogConfig(logProps)\n+    testUncommittedDataNotConsumed(createLog(logConfig))\n+  }\n+\n+  def testUncommittedDataNotConsumed(log: Log): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2OTA4Nw==", "bodyText": "It takes about 10 second locally, which seems reasonable for a concurrency test.", "url": "https://github.com/apache/kafka/pull/8476#discussion_r409069087", "createdAt": "2020-04-15T19:04:32Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.log\n+\n+import java.util.Properties\n+import java.util.concurrent.{Callable, Executors}\n+\n+import kafka.server.{BrokerTopicStats, FetchHighWatermark, LogDirFailureChannel}\n+import kafka.utils.{KafkaScheduler, TestUtils}\n+import org.apache.kafka.common.record.SimpleRecord\n+import org.apache.kafka.common.utils.{Time, Utils}\n+import org.junit.Assert._\n+import org.junit.{After, Before, Test}\n+\n+import scala.collection.mutable.ListBuffer\n+import scala.util.Random\n+\n+class LogConcurrencyTest {\n+  private val brokerTopicStats = new BrokerTopicStats\n+  private val random = new Random()\n+  private val scheduler = new KafkaScheduler(1)\n+  private val tmpDir = TestUtils.tempDir()\n+  private val logDir = TestUtils.randomPartitionLogDir(tmpDir)\n+\n+  @Before\n+  def setup(): Unit = {\n+    scheduler.startup()\n+  }\n+\n+  @After\n+  def shutdown(): Unit = {\n+    scheduler.shutdown()\n+    Utils.delete(tmpDir)\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumed(): Unit = {\n+    testUncommittedDataNotConsumed(createLog())\n+  }\n+\n+  @Test\n+  def testUncommittedDataNotConsumedFrequentSegmentRolls(): Unit = {\n+    val logProps = new Properties()\n+    logProps.put(LogConfig.SegmentBytesProp, 237: Integer)\n+    val logConfig = LogConfig(logProps)\n+    testUncommittedDataNotConsumed(createLog(logConfig))\n+  }\n+\n+  def testUncommittedDataNotConsumed(log: Log): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NzY2Mg=="}, "originalCommit": {"oid": "d0290c405efe3bdfe6357c350f82e189dbdbc822"}, "originalPosition": 64}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2980, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}