{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3NjE5NTU1", "number": 9177, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowNTowNFrOEZHAeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjoxNzo0N1rOEcplBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NzY0NjY2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowNTowNFrOHBvHdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMzoxNjowMVrOHCTyxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MjU4MA==", "bodyText": "Should we check if (metrics.metric(metricName) == null) again after synchronizing?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471582580", "createdAt": "2020-08-17T16:05:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE4MzQ5Mw==", "bodyText": "I will do that to ensure that the removeAllStoreLevelMetrics() completes before we do the check.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472183493", "createdAt": "2020-08-18T13:16:01Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MjU4MA=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NzY1NTY0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowNzoyNVrOHBvNGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOTowMjo1MFrOHCh_ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ==", "bodyText": "Should we make this all one method, and also synchronize both storeLevel collections on a single monitor?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471584025", "createdAt": "2020-08-17T16:07:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwMDM2MA==", "bodyText": "Do you have performance concerns due to the two monitors? Or what is the main reason for using a single monitor here? By using a single monitor here and in addStoreLevelMutableMetric() and storeLevelSensor(), we do not ensure that no metrics are added to the metrics map during removal of all metrics because each time  Sensor#add() is called a metric is added without synchronizing on the monitor of storeLevelSensors. Single operations on the metrics map are synchronized (through ConcurrentMap), but not multiple operations.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472200360", "createdAt": "2020-08-18T13:33:57Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI1MDYxMg==", "bodyText": "It just seems oddly granular to synchronize them individually, since we always remove all of both collections together. If it doesn't matter, then do we need to synchronize at all?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472250612", "createdAt": "2020-08-18T14:42:26Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI3NjYxNQ==", "bodyText": "Yes, we need to synchronize. At least, we have to ensure that lines 411 and 438 are thread-safe. Then, if we do not want to have duplicates in storeLevelSensors we should ensure to have a lock between line 409 to 411. Between line 434 and 439, we need to ensure that the removal of all store level metrics completed otherwise it could happen that we find a store level metric that would prevent the addition of a metric but then the earlier found metric would be removed during the remainder of the removal process. Similar is true for the store level sensors.\nIt is true that we always remove all of both collections together, but we do not add metric names and sensor names to both collections together.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472276615", "createdAt": "2020-08-18T15:17:18Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxNjEzOA==", "bodyText": "This may be a bit paranoid, but when adding them, the order seem to be initSensors first and initGauges, while removing we call removeAllStoreLevelMetrics first and then the other. I know that today there should be not concurrent threads trying to init / removeAll concurrently, but just to be safe maybe we can make the call ordering to be sensors first and then gauges?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472416138", "createdAt": "2020-08-18T19:02:50Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NzY4MDUyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjoxNDoxMVrOHBvc0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMToxNjowNFrOHCOwVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4ODA0OQ==", "bodyText": "It seems a little risky to use this in a multithreaded context. Why not just create a new short-lived buffer each time for the conversion?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471588049", "createdAt": "2020-08-17T16:14:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -56,6 +62,9 @@ public void maybeCloseStatistics() {\n         }\n     }\n \n+    private static final String ROCKSDB_PROPERTIES_PREFIX = \"rocksdb.\";\n+    private static final ByteBuffer CONVERSION_BUFFER = ByteBuffer.allocate(Long.BYTES);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwMDk1MQ==", "bodyText": "Good point! I missed that the gauge can be called by multiple metrics reporters concurrently.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472100951", "createdAt": "2020-08-18T11:16:04Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -56,6 +62,9 @@ public void maybeCloseStatistics() {\n         }\n     }\n \n+    private static final String ROCKSDB_PROPERTIES_PREFIX = \"rocksdb.\";\n+    private static final ByteBuffer CONVERSION_BUFFER = ByteBuffer.allocate(Long.BYTES);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4ODA0OQ=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NzcyNTc4OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjoyNjoyMlrOHBv40A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMToyNTozMVrOHCPCMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU5NTIxNg==", "bodyText": "would it not be exactly 1?", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471595216", "createdAt": "2020-08-17T16:26:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -609,6 +611,37 @@ public void shouldVerifyThatMetricsGetMeasurementsFromRocksDB() {\n         assertThat((double) bytesWrittenTotal.metricValue(), greaterThan(0d));\n     }\n \n+    @Test\n+    public void shouldVerifyThatMetricsRecordedFromPropertiesGetMeasurementsFromRocksDB() {\n+        final TaskId taskId = new TaskId(0, 0);\n+\n+        final Metrics metrics = new Metrics(new MetricConfig().recordLevel(RecordingLevel.INFO));\n+        final StreamsMetricsImpl streamsMetrics =\n+            new StreamsMetricsImpl(metrics, \"test-application\", StreamsConfig.METRICS_LATEST, time);\n+\n+        context = EasyMock.niceMock(InternalMockProcessorContext.class);\n+        EasyMock.expect(context.metrics()).andStubReturn(streamsMetrics);\n+        EasyMock.expect(context.taskId()).andStubReturn(taskId);\n+        EasyMock.expect(context.appConfigs())\n+                .andStubReturn(new StreamsConfig(StreamsTestUtils.getStreamsConfig()).originals());\n+        EasyMock.expect(context.stateDir()).andStubReturn(dir);\n+        EasyMock.replay(context);\n+\n+        rocksDBStore.init(context, rocksDBStore);\n+        final byte[] key = \"hello\".getBytes();\n+        final byte[] value = \"world\".getBytes();\n+        rocksDBStore.put(Bytes.wrap(key), value);\n+\n+        final Metric numberOfEntriesActiveMemTable = metrics.metric(new MetricName(\n+            \"num-entries-active-mem-table\",\n+            StreamsMetricsImpl.STATE_STORE_LEVEL_GROUP,\n+            \"description is not verified\",\n+            streamsMetrics.storeLevelTagMap(Thread.currentThread().getName(), taskId.toString(), METRICS_SCOPE, DB_NAME)\n+        ));\n+        assertThat(numberOfEntriesActiveMemTable, notNullValue());\n+        assertThat((BigInteger) numberOfEntriesActiveMemTable.metricValue(), greaterThan(BigInteger.valueOf(0)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNTUyMA==", "bodyText": "Yes, but in this test I merely test whether the metric is updated. The correctness of the computation is verified in RocksDBMetricsRecorderGaugesTest. I will improve this test to verify that the metric is zero before the put and greater than zero after the put.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472105520", "createdAt": "2020-08-18T11:25:31Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -609,6 +611,37 @@ public void shouldVerifyThatMetricsGetMeasurementsFromRocksDB() {\n         assertThat((double) bytesWrittenTotal.metricValue(), greaterThan(0d));\n     }\n \n+    @Test\n+    public void shouldVerifyThatMetricsRecordedFromPropertiesGetMeasurementsFromRocksDB() {\n+        final TaskId taskId = new TaskId(0, 0);\n+\n+        final Metrics metrics = new Metrics(new MetricConfig().recordLevel(RecordingLevel.INFO));\n+        final StreamsMetricsImpl streamsMetrics =\n+            new StreamsMetricsImpl(metrics, \"test-application\", StreamsConfig.METRICS_LATEST, time);\n+\n+        context = EasyMock.niceMock(InternalMockProcessorContext.class);\n+        EasyMock.expect(context.metrics()).andStubReturn(streamsMetrics);\n+        EasyMock.expect(context.taskId()).andStubReturn(taskId);\n+        EasyMock.expect(context.appConfigs())\n+                .andStubReturn(new StreamsConfig(StreamsTestUtils.getStreamsConfig()).originals());\n+        EasyMock.expect(context.stateDir()).andStubReturn(dir);\n+        EasyMock.replay(context);\n+\n+        rocksDBStore.init(context, rocksDBStore);\n+        final byte[] key = \"hello\".getBytes();\n+        final byte[] value = \"world\".getBytes();\n+        rocksDBStore.put(Bytes.wrap(key), value);\n+\n+        final Metric numberOfEntriesActiveMemTable = metrics.metric(new MetricName(\n+            \"num-entries-active-mem-table\",\n+            StreamsMetricsImpl.STATE_STORE_LEVEL_GROUP,\n+            \"description is not verified\",\n+            streamsMetrics.storeLevelTagMap(Thread.currentThread().getName(), taskId.toString(), METRICS_SCOPE, DB_NAME)\n+        ));\n+        assertThat(numberOfEntriesActiveMemTable, notNullValue());\n+        assertThat((BigInteger) numberOfEntriesActiveMemTable.metricValue(), greaterThan(BigInteger.valueOf(0)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU5NTIxNg=="}, "originalCommit": {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MzAxODI4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetrics.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOToyNjo0MFrOHCiyVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOToyNjo0MFrOHCiyVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQyOTE0MQ==", "bodyText": "nit: extra space.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472429141", "createdAt": "2020-08-18T19:26:40Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetrics.java", "diffHunk": "@@ -33,7 +35,7 @@\n import static org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.addSumMetricToSensor;\n import static org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.addValueMetricToSensor;\n \n-public class RocksDBMetrics {\n+public class  RocksDBMetrics {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4NDc2ODA0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjoxNzo0N1rOHHTrwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMTozNjozNlrOHIiHFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNDU3OA==", "bodyText": "I'm still mildly concerned about walking back the synchronization here, but I can't think of a realistic scenario in which we'd get a concurrency bug. Then again, the whole point of defaulting to less granular concurrency controls is that it's hard to imagine all the possible scenarios.\nIn this case, it really doesn't seem like there's a good reason to go for super granular concurrency control. Did we spend a lot of time blocked registering sensors before?\nActually, one condition comes to mind: LinkedList is not threadsafe, and accessing the ConcurrentHashMap value is only either a CAS or volatile read, so it doesn't create a memory barrier as synchronized does. Therefore, different threads will only be looking at their own locally cached list for each value in the map, although they'll all agree on the set of keys in the map.\nIf you want to push the current implementation style, then you should use a ConcurrentLinkedDeque instead of LinkedList, but I'd really prefer to see the synchronized blocks come back unless/until there's a compelling performance reason to drop them.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r477424578", "createdAt": "2020-08-26T16:17:47Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -396,34 +398,65 @@ private String cacheSensorPrefix(final String threadId, final String taskId, fin\n             + SENSOR_PREFIX_DELIMITER + SENSOR_CACHE_LABEL + SENSOR_PREFIX_DELIMITER + cacheName;\n     }\n \n-    public final Sensor storeLevelSensor(final String threadId,\n-                                         final String taskId,\n+    public final Sensor storeLevelSensor(final String taskId,\n                                          final String storeName,\n                                          final String sensorName,\n-                                         final Sensor.RecordingLevel recordingLevel,\n+                                         final RecordingLevel recordingLevel,\n                                          final Sensor... parents) {\n-        final String key = storeSensorPrefix(threadId, taskId, storeName);\n-        synchronized (storeLevelSensors) {\n-            final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n-            final Sensor sensor = metrics.getSensor(fullSensorName);\n-            if (sensor == null) {\n+        final String key = storeSensorPrefix(Thread.currentThread().getName(), taskId, storeName);\n+        final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n+        return Optional.ofNullable(metrics.getSensor(fullSensorName))\n+            .orElseGet(() -> {\n                 storeLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName);\n                 return metrics.sensor(fullSensorName, recordingLevel, parents);\n-            } else {\n-                return sensor;\n-            }\n+            });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b1909dc3f9167ef3a16408dabd207059f794904"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODUwNDUzMg==", "bodyText": "After our offline discussion, I added some clarifications in the code.", "url": "https://github.com/apache/kafka/pull/9177#discussion_r478504532", "createdAt": "2020-08-27T15:26:11Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -396,34 +398,65 @@ private String cacheSensorPrefix(final String threadId, final String taskId, fin\n             + SENSOR_PREFIX_DELIMITER + SENSOR_CACHE_LABEL + SENSOR_PREFIX_DELIMITER + cacheName;\n     }\n \n-    public final Sensor storeLevelSensor(final String threadId,\n-                                         final String taskId,\n+    public final Sensor storeLevelSensor(final String taskId,\n                                          final String storeName,\n                                          final String sensorName,\n-                                         final Sensor.RecordingLevel recordingLevel,\n+                                         final RecordingLevel recordingLevel,\n                                          final Sensor... parents) {\n-        final String key = storeSensorPrefix(threadId, taskId, storeName);\n-        synchronized (storeLevelSensors) {\n-            final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n-            final Sensor sensor = metrics.getSensor(fullSensorName);\n-            if (sensor == null) {\n+        final String key = storeSensorPrefix(Thread.currentThread().getName(), taskId, storeName);\n+        final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n+        return Optional.ofNullable(metrics.getSensor(fullSensorName))\n+            .orElseGet(() -> {\n                 storeLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName);\n                 return metrics.sensor(fullSensorName, recordingLevel, parents);\n-            } else {\n-                return sensor;\n-            }\n+            });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNDU3OA=="}, "originalCommit": {"oid": "4b1909dc3f9167ef3a16408dabd207059f794904"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwOTUyNQ==", "bodyText": "Thanks!", "url": "https://github.com/apache/kafka/pull/9177#discussion_r478709525", "createdAt": "2020-08-27T21:36:36Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -396,34 +398,65 @@ private String cacheSensorPrefix(final String threadId, final String taskId, fin\n             + SENSOR_PREFIX_DELIMITER + SENSOR_CACHE_LABEL + SENSOR_PREFIX_DELIMITER + cacheName;\n     }\n \n-    public final Sensor storeLevelSensor(final String threadId,\n-                                         final String taskId,\n+    public final Sensor storeLevelSensor(final String taskId,\n                                          final String storeName,\n                                          final String sensorName,\n-                                         final Sensor.RecordingLevel recordingLevel,\n+                                         final RecordingLevel recordingLevel,\n                                          final Sensor... parents) {\n-        final String key = storeSensorPrefix(threadId, taskId, storeName);\n-        synchronized (storeLevelSensors) {\n-            final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n-            final Sensor sensor = metrics.getSensor(fullSensorName);\n-            if (sensor == null) {\n+        final String key = storeSensorPrefix(Thread.currentThread().getName(), taskId, storeName);\n+        final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n+        return Optional.ofNullable(metrics.getSensor(fullSensorName))\n+            .orElseGet(() -> {\n                 storeLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName);\n                 return metrics.sensor(fullSensorName, recordingLevel, parents);\n-            } else {\n-                return sensor;\n-            }\n+            });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNDU3OA=="}, "originalCommit": {"oid": "4b1909dc3f9167ef3a16408dabd207059f794904"}, "originalPosition": 59}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1916, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}