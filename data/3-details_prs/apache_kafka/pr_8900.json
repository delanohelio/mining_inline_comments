{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2ODEzODAx", "number": 8900, "title": "KAFKA-10169: swallow non-fatal KafkaException and don't abort transaction during clean close", "bodyText": "If there's any pending data and we haven't flushed the producer when we abort a transaction, a KafkaException is returned for the previous send. This is a bit misleading, since the situation is not an unrecoverable error and so the Kafka Exception is really non-fatal. For now, we should just catch and swallow this in the RecordCollector (see also: KAFKA-10169)\nThe reason we ended up aborting an un-flushed transaction was due to the combination of\na. always aborting the ongoing transaction when any task is closed/revoked\nb. only committing (and flushing) if at least one of the revoked tasks needs to be committed (regardless of whether any non-revoked tasks have data/transaction in flight)\nGiven the above, we can end up with an ongoing transaction that isn't committed since none of the revoked tasks have any data in the transaction. We then abort the transaction anyway, when those tasks are closed. So in addition to the above (swallowing this exception), we should avoid unnecessarily aborting data for tasks that haven't been revoked.\nWe can handle this by splitting the RecordCollector's close into a dirty and clean flavor: if dirty, we need to abort the transaction since it may be dirty due to the commit attempt failing. But if clean, we can skip aborting the transaction since we know that either we just committed and thus there is no ongoing transaction to abort, or else the transaction in flight contains no data from the tasks being closed\nNote that this means we still abort the transaction any time a task is closed dirty, so we must close/reinitialize any active task with pending data (that was aborted).\nIn sum:\n\nonly abort the transaction during a dirty close\nrefactor shutdown to make sure we don't closeClean a task whose data was actually aborted", "createdAt": "2020-06-18T23:41:12Z", "url": "https://github.com/apache/kafka/pull/8900", "merged": true, "mergeCommit": {"oid": "448e7d7f0f46db1eae14d4fe7a1d25b7af894b09"}, "closed": true, "closedAt": "2020-06-24T01:08:27Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcsq_8CAFqTQzMzc4ODU1Mw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcuNBMUAFqTQzNjE5ODI5MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNzg4NTUz", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-433788553", "createdAt": "2020-06-19T04:04:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNDowNDozNlrOGmHWQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNDowNDozNlrOGmHWQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTQ1OQ==", "bodyText": "This was a sort-of bug: because we don't close things during handleRevocation, we want to make sure the TM will close this as dirty during handleAssignment. So we throw this just to force it to call closeDirty -- but it wasn't necessarily a fatal exception that caused commit to fail, so we should just throw TaskMigrated here.\nThat said, it doesn't really matter since the ConsumerCoordinator will save and rethrow only the first exception, which is the handleRevocation exception. Anything we throw in handleAssignment is \"lost\" -- but we should do the right thing anyway", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442619459", "createdAt": "2020-06-19T04:04:36Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -522,7 +522,7 @@ private void close(final boolean clean) {\n         if (clean && commitNeeded) {\n             log.debug(\"Tried to close clean but there was pending uncommitted data, this means we failed to\"\n                           + \" commit and should close as dirty instead\");\n-            throw new StreamsException(\"Tried to close dirty task as clean\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNzg4Njc5", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-433788679", "createdAt": "2020-06-19T04:05:04Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNDowNTowNVrOGmHWpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNDowNTowNVrOGmHWpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw==", "bodyText": "My IDEA pointed out that this was never used", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442619557", "createdAt": "2020-06-19T04:05:05Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -69,7 +68,6 @@\n     private final ChangelogReader changelogReader;\n     private final UUID processId;\n     private final String logPrefix;\n-    private final StreamsMetricsImpl streamsMetrics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzODUyMzE5", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-433852319", "createdAt": "2020-06-19T07:13:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNzoxMzoxOVrOGmKgpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNzoxMzoxOVrOGmKgpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ==", "bodyText": "Should we avoid passing this exception to ProductionExceptionHandler as it never breaks sent now.", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442671271", "createdAt": "2020-06-19T07:13:19Z", "author": {"login": "chia7712"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -267,7 +283,17 @@ public void close() {\n \n     private void checkForException() {\n         if (sendException != null) {\n-            throw sendException;\n+            if (sendException.getCause() instanceof KafkaException\n+                && sendException.getCause().getMessage().equals(\"Failing batch since transaction was aborted\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MjgyODAy", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-434282802", "createdAt": "2020-06-19T18:53:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODo1Mzo0NVrOGmecAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODo1ODo0NFrOGmejyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5Nzc2MA==", "bodyText": "I think we should not add this handling for now based on the conclusion that after one task caused abortTxn is called, no other tasks should ever call recordCollector#flush/send/close anymore right?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442997760", "createdAt": "2020-06-19T18:53:45Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -267,7 +283,17 @@ public void close() {\n \n     private void checkForException() {\n         if (sendException != null) {\n-            throw sendException;\n+            if (sendException.getCause() instanceof KafkaException\n+                && sendException.getCause().getMessage().equals(\"Failing batch since transaction was aborted\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5ODQwMg==", "bodyText": "This comment is worthy to be a comment on the code itself :)", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442998402", "createdAt": "2020-06-19T18:55:25Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -522,7 +522,7 @@ private void close(final boolean clean) {\n         if (clean && commitNeeded) {\n             log.debug(\"Tried to close clean but there was pending uncommitted data, this means we failed to\"\n                           + \" commit and should close as dirty instead\");\n-            throw new StreamsException(\"Tried to close dirty task as clean\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTQ1OQ=="}, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTEwOA==", "bodyText": "Hmm, it is used in streamsMetrics.removeAllTaskLevelSensors(threadId, task.id().toString()); in cleanupTask?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442999108", "createdAt": "2020-06-19T18:57:09Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -69,7 +68,6 @@\n     private final ChangelogReader changelogReader;\n     private final UUID processId;\n     private final String logPrefix;\n-    private final StreamsMetricsImpl streamsMetrics;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTc1Mg==", "bodyText": "Why only add active tasks here, not standby tasks?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442999752", "createdAt": "2020-06-19T18:58:44Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be\n+            if (!filterActive(tasksToCloseDirty).isEmpty()) {\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.removeAll(filterActive(tasksToCommit));\n+                tasksToCloseDirty.addAll(activeTaskIterable());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MzY4OTQ1", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-434368945", "createdAt": "2020-06-19T22:36:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMjozNjoxOVrOGmirSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzowMDozOFrOGmi_Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2NzIxMQ==", "bodyText": "In the current code, we might still need to close tasks, right? If a TX is aborted, we need to \"reset\" all active tasks accordingly and this would imply, closing and reviving them? And while closing we would call checkForException and crash without this guard?\nWhat makes we wonder, if we should actually checkForException in closeDirty() above? If a TX is aborted, and we closeDirty and don't call checkForException it seems we don't need this guard? (In general, this guard seems a little bit hacky and it would be great if we could avoid it IMHO.)", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443067211", "createdAt": "2020-06-19T22:36:19Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -267,7 +283,17 @@ public void close() {\n \n     private void checkForException() {\n         if (sendException != null) {\n-            throw sendException;\n+            if (sendException.getCause() instanceof KafkaException\n+                && sendException.getCause().getMessage().equals(\"Failing batch since transaction was aborted\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2ODEwMA==", "bodyText": "It seems this is the only place we call closeDirty, thus, I am wondering if it might be better to use a boolean flag ie, RecordCollector#close(boolean) and just call () -> recordCollector(clean) here?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443068100", "createdAt": "2020-06-19T22:40:21Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -542,7 +542,12 @@ private void close(final boolean clean) {\n                     \"state manager close\",\n                     log);\n \n-                executeAndMaybeSwallow(clean, recordCollector::close, \"record collector close\", log);\n+                executeAndMaybeSwallow(\n+                    clean,\n+                    clean ? recordCollector::closeClean : recordCollector::closeDirty,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2ODk0OA==", "bodyText": "I looked into the code, and the answer is \"no\".  It's called within StreamTask / StandbyTask methods, closeClean, closeDirty, and closeAndRecycleState. So It seems fine?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443068948", "createdAt": "2020-06-19T22:44:03Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -69,7 +68,6 @@\n     private final ChangelogReader changelogReader;\n     private final UUID processId;\n     private final String logPrefix;\n-    private final StreamsMetricsImpl streamsMetrics;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTIxNw==", "bodyText": "typo: closed", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443069217", "createdAt": "2020-06-19T22:45:17Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTQxMA==", "bodyText": "Standbys done affect the TX-producer and thus they can be closed dirty without side effect.", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443069410", "createdAt": "2020-06-19T22:46:11Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be\n+            if (!filterActive(tasksToCloseDirty).isEmpty()) {\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.removeAll(filterActive(tasksToCommit));\n+                tasksToCloseDirty.addAll(activeTaskIterable());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTc1Mg=="}, "originalCommit": null, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTkwNg==", "bodyText": "Should we do an else here and only call commitOffsetsOrTransaction (and do the postCommit loop) in the else branch (I understand that the current code is correct, as commitOffsetOrTx() would be a no-op for this case -- just wondering, if the code would be easier to read?)", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443069906", "createdAt": "2020-06-19T22:48:50Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be\n+            if (!filterActive(tasksToCloseDirty).isEmpty()) {\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.removeAll(filterActive(tasksToCommit));\n+                tasksToCloseDirty.addAll(activeTaskIterable());\n+                consumedOffsetsAndMetadataPerTask.clear();\n             }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDk1OA==", "bodyText": "Similar question as above: should we move this loop into the same try-catch as commitOffsetsOrTransaction ?This would make it clear that postCommit() should only be executed if committing was successful (even if the current code is correct as taskToCommit would empty if an error occurred.", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443070958", "createdAt": "2020-06-19T22:53:30Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be\n+            if (!filterActive(tasksToCloseDirty).isEmpty()) {\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.removeAll(filterActive(tasksToCommit));\n+                tasksToCloseDirty.addAll(activeTaskIterable());\n+                consumedOffsetsAndMetadataPerTask.clear();\n             }\n-        } catch (final RuntimeException e) {\n-            log.error(\"Exception caught while committing tasks during shutdown\", e);\n-            firstException.compareAndSet(null, e);\n-        }\n \n-        for (final Task task : tasksToClose) {\n             try {\n-                completeTaskCloseClean(task);\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n             } catch (final RuntimeException e) {\n+                log.error(\"Exception caught while committing tasks during shutdown\", e);\n                 firstException.compareAndSet(null, e);\n-                closeTaskDirty(task);\n+\n+                // If the commit fails, everyone who participated in it must be closed dirty\n+                tasksToCloseDirty.addAll(filterActive(tasksToCommit));\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.clear();\n+            }\n+\n+            for (final Task task : tasksToCommit) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MTEyMA==", "bodyText": "Good thinking!\nWe should mention RecordCollectorImpl as it's the class the relies on the error message.", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443071120", "createdAt": "2020-06-19T22:54:37Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MTgzNQ==", "bodyText": "Why do we need to set clientId?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443071835", "createdAt": "2020-06-19T22:58:01Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MTk3Mw==", "bodyText": "Should we use try-with-resource? Or at least try-final and call producer.close in the finally block?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443071973", "createdAt": "2020-06-19T22:58:54Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");\n+        configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"txnId\");\n+\n+        final KafkaProducer<String, String> producer =\n+            new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MjExNQ==", "bodyText": "Do we need this?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443072115", "createdAt": "2020-06-19T22:59:35Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");\n+        configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"txnId\");\n+\n+        final KafkaProducer<String, String> producer =\n+            new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer());\n+\n+        final ProducerRecord<String, String> record = new ProducerRecord<>(SINGLE_PARTITION_INPUT_TOPIC, \"value\");\n+\n+        producer.initTransactions();\n+        producer.beginTransaction();\n+\n+        producer.send(record);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MjMxOA==", "bodyText": "Do we need to make them producer config changes to ensure that the producer does not flush the record by chance? (eg, increase linger.ms to MAX_VALUE or similar?)", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443072318", "createdAt": "2020-06-19T23:00:38Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");\n+        configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"txnId\");\n+\n+        final KafkaProducer<String, String> producer =\n+            new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer());\n+\n+        final ProducerRecord<String, String> record = new ProducerRecord<>(SINGLE_PARTITION_INPUT_TOPIC, \"value\");\n+\n+        producer.initTransactions();\n+        producer.beginTransaction();\n+\n+        producer.send(record);\n+\n+        final AtomicReference<Exception> receivedException = new AtomicReference<>(null);\n+        producer.send(record, (recordMetadata, exception) -> receivedException.compareAndSet(null, exception));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1MzEwNjM5", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-435310639", "createdAt": "2020-06-22T22:15:12Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQyMjoxNToxMlrOGnSy8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQyMjoxNTozNFrOGnSzcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1NTYwMw==", "bodyText": "nit: clean close", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443855603", "createdAt": "2020-06-22T22:15:12Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -250,10 +250,26 @@ public void flush() {\n      * @throws TaskMigratedException recoverable error that would cause the task to be removed\n      */\n     @Override\n-    public void close() {\n-        log.info(\"Closing record collector\");\n+    public void closeClean() {\n+        log.info(\"Closing record collector clean\");\n+\n+        // No need to abort transaction during a clean  close: either we have successfully committed the ongoing", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1NTczMQ==", "bodyText": "nit: period at the end", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443855731", "createdAt": "2020-06-22T22:15:34Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -250,10 +250,26 @@ public void flush() {\n      * @throws TaskMigratedException recoverable error that would cause the task to be removed\n      */\n     @Override\n-    public void close() {\n-        log.info(\"Closing record collector\");\n+    public void closeClean() {\n+        log.info(\"Closing record collector clean\");\n+\n+        // No need to abort transaction during a clean  close: either we have successfully committed the ongoing\n+        // transaction during handleRevocation and thus there is no transaction in flight, or else none of the revoked\n+        // tasks had any data in the current transaction and therefore there is no need to commit or abort it", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1NDI2NDUy", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-435426452", "createdAt": "2020-06-23T04:12:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNDoxMjoyN1rOGnYmNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNDoxMjoyN1rOGnYmNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDY0Ng==", "bodyText": "Saw this fail locally so just did a minor flaky test fix on the side", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443950646", "createdAt": "2020-06-23T04:12:27Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -359,7 +359,9 @@ public void shouldRecycleStateFromStandbyTaskPromotedToActiveTaskAndNotRestore()\n         waitForStandbyCompletion(client1, 1, 30 * 1000L);\n         waitForStandbyCompletion(client2, 1, 30 * 1000L);\n \n-        assertThat(CloseCountingInMemoryStore.numStoresClosed(), CoreMatchers.equalTo(0));\n+        // Sometimes the store happens to have already been closed sometime during startup, so just keep track\n+        // of where it started and make sure it doesn't happen more times from there\n+        final int initialStoreCloseCount = CloseCountingInMemoryStore.numStoresClosed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1NDI2Nzcx", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-435426771", "createdAt": "2020-06-23T04:13:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNDoxMzozN1rOGnYnOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNDoxMzozN1rOGnYnOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDkwNg==", "bodyText": "These tests used to rely on the fact that the sendException was never forgotten by just setting it once and then asserting that multiple subsequent calls also threw it. So now we need to call send before each to re-insert the exception", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443950906", "createdAt": "2020-06-23T04:13:37Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -474,6 +492,7 @@ public void shouldThrowTaskMigratedExceptionOnSubsequentCallWhenProducerFencedIn\n                 \" indicating the task may be migrated out; it means all tasks belonging to this thread should be migrated.\")\n         );\n \n+        collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/4185c54b001ac381a8737683e8ae1a4aceb4a875", "committedDate": "2020-06-23T17:08:01Z", "message": "swallow this exception, dont abort during a clean close, and tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/4185c54b001ac381a8737683e8ae1a4aceb4a875", "committedDate": "2020-06-23T17:08:01Z", "message": "swallow this exception, dont abort during a clean close, and tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTY4NzE2", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-436168716", "createdAt": "2020-06-23T21:23:44Z", "commit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyMzo0NFrOGn7J-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyMzo0NFrOGn7J-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNjg1OA==", "bodyText": "Should we fix this in older branches (2.5/2.4), too? (ie follow up PR)", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444516858", "createdAt": "2020-06-23T21:23:44Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java", "diffHunk": "@@ -152,9 +152,9 @@ public StreamsMetadata getLocalMetadata() {\n         }\n \n         if (globalStores.contains(storeName)) {\n-            // global stores are on every node. if we dont' have the host info\n+            // global stores are on every node. if we don't have the host info\n             // for this host then just pick the first metadata\n-            if (thisHost == UNKNOWN_HOST) {\n+            if (thisHost.equals(UNKNOWN_HOST)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTY5Nzk1", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-436169795", "createdAt": "2020-06-23T21:25:43Z", "commit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyNTo0M1rOGn7Ncg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyNTo0M1rOGn7Ncg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNzc0Ng==", "bodyText": "If tasksToCloseDirty is not empty, should we close dirty, too, ie pass in clean && tasksToCloseDirty.isEmpty() ?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444517746", "createdAt": "2020-06-23T21:25:43Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,92 +675,166 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n+        tasksToCloseDirty.addAll(tryCloseCleanAllActiveTasks(clean, firstException));\n+        tasksToCloseDirty.addAll(tryCloseCleanAllStandbyTasks(clean, firstException));\n+\n+        for (final Task task : tasksToCloseDirty) {\n+            closeTaskDirty(task);\n+        }\n+\n+        for (final Task task : activeTaskIterable()) {\n+            executeAndMaybeSwallow(\n+                clean,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTcwMDEw", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-436170010", "createdAt": "2020-06-23T21:26:04Z", "commit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyNjowNFrOGn7OEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyNjowNFrOGn7OEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNzkwNw==", "bodyText": "As above?", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444517907", "createdAt": "2020-06-23T21:26:04Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,92 +675,166 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n+        tasksToCloseDirty.addAll(tryCloseCleanAllActiveTasks(clean, firstException));\n+        tasksToCloseDirty.addAll(tryCloseCleanAllStandbyTasks(clean, firstException));\n+\n+        for (final Task task : tasksToCloseDirty) {\n+            closeTaskDirty(task);\n+        }\n+\n+        for (final Task task : activeTaskIterable()) {\n+            executeAndMaybeSwallow(\n+                clean,\n+                () -> activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id()),\n+                e -> firstException.compareAndSet(null, e),\n+                e -> log.warn(\"Ignoring an exception while closing task \" + task.id() + \" producer.\", e)\n+            );\n+        }\n+\n+        executeAndMaybeSwallow(\n+            clean,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTcwNTQz", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-436170543", "createdAt": "2020-06-23T21:26:57Z", "commit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyNjo1N1rOGn7PuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyNjo1N1rOGn7PuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxODMyOQ==", "bodyText": "As above.", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444518329", "createdAt": "2020-06-23T21:26:57Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,92 +675,166 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n+        tasksToCloseDirty.addAll(tryCloseCleanAllActiveTasks(clean, firstException));\n+        tasksToCloseDirty.addAll(tryCloseCleanAllStandbyTasks(clean, firstException));\n+\n+        for (final Task task : tasksToCloseDirty) {\n+            closeTaskDirty(task);\n+        }\n+\n+        for (final Task task : activeTaskIterable()) {\n+            executeAndMaybeSwallow(\n+                clean,\n+                () -> activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id()),\n+                e -> firstException.compareAndSet(null, e),\n+                e -> log.warn(\"Ignoring an exception while closing task \" + task.id() + \" producer.\", e)\n+            );\n+        }\n+\n+        executeAndMaybeSwallow(\n+            clean,\n+            activeTaskCreator::closeThreadProducerIfNeeded,\n+            e -> firstException.compareAndSet(null, e),\n+            e -> log.warn(\"Ignoring an exception while closing thread producer.\", e)\n+        );\n+\n+        tasks.clear();\n+\n+\n+        // this should be called after closing all tasks, to make sure we unlock the task dir for tasks that may\n+        // have still been in CREATED at the time of shutdown, since Task#close will not do so\n+        executeAndMaybeSwallow(\n+            clean,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875"}, "originalPosition": 67}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20321fd1325d402405a4b5899c22c83a51732b1c", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/20321fd1325d402405a4b5899c22c83a51732b1c", "committedDate": "2020-06-23T22:08:59Z", "message": "split tests up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTk4Mjkx", "url": "https://github.com/apache/kafka/pull/8900#pullrequestreview-436198291", "createdAt": "2020-06-23T22:16:41Z", "commit": {"oid": "20321fd1325d402405a4b5899c22c83a51732b1c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 675, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}