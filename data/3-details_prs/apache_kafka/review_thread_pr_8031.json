{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY5OTE5MDUw", "number": 8031, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzozNlrODcbWgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxODoxMzozM1rODdyB8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMTM0ODQ4OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsResult.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzozNlrOFkeYjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzozNlrOFkeYjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc5MDg2Mw==", "bodyText": "side cleanup", "url": "https://github.com/apache/kafka/pull/8031#discussion_r373790863", "createdAt": "2020-02-01T17:17:36Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsResult.java", "diffHunk": "@@ -51,21 +51,18 @@\n      */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "392b7190313411c3949c4a707820088f3754175f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMTM0ODU1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsResult.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzo0MVrOFkeYlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzo0MVrOFkeYlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc5MDg3MA==", "bodyText": "side cleanup", "url": "https://github.com/apache/kafka/pull/8031#discussion_r373790870", "createdAt": "2020-02-01T17:17:41Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsResult.java", "diffHunk": "@@ -48,23 +48,13 @@\n      * Return a future which yields a collection of TopicListing objects.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "392b7190313411c3949c4a707820088f3754175f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc5MDk2OnYy", "diffSide": "RIGHT", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozOTo0MlrOFlkZOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMzoyNzowMlrOFlnFnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNzkxNQ==", "bodyText": "Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374937915", "createdAt": "2020-02-04T21:39:42Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4MjA0Nw==", "bodyText": "No, we are intended to keep both", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374982047", "createdAt": "2020-02-04T23:27:02Z", "author": {"login": "abbccdda"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNzkxNQ=="}, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc5NTY2OnYy", "diffSide": "RIGHT", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo0MToyNFrOFlkcKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMzoyNDo1M1rOFlnC6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzODY2NQ==", "bodyText": "Why we need an atomic long here? Seems there's no concurrency.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374938665", "createdAt": "2020-02-04T21:41:24Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4MTM1NA==", "bodyText": "The tricky thing here is that if we define a primitive long outside of the rebalance callback, it won't compile.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374981354", "createdAt": "2020-02-04T23:24:53Z", "author": {"login": "abbccdda"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzODY2NQ=="}, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc5ODY5OnYy", "diffSide": "RIGHT", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo0MjoyMVrOFlkeEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMzoyNTozM1rOFlnDwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTE1NA==", "bodyText": "Why divide the key by two?", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374939154", "createdAt": "2020-02-04T21:42:21Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));\n+            printWithTxnId(\"Message remaining: \" + messageRemaining);\n+        }\n+\n+        printWithTxnId(\"Finished processing \" + messageProcessed + \" records\");\n+        latch.countDown();\n+    }\n+\n+    private void printWithTxnId(final String message) {\n+        System.out.println(transactionalId + \": \" + message);\n+    }\n+\n+    private ProducerRecord<Integer, String> transform(final ConsumerRecord<Integer, String> record) {\n+        printWithTxnId(\"Transformed record (\" + record.key() + \",\" + record.value() + \")\");\n+        return new ProducerRecord<>(outputTopic, record.key() / 2, \"Transformed_\" + record.value());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4MTU3MA==", "bodyText": "It's just a way of showing the key gets processed by message copier, all the produced message key are even keys.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374981570", "createdAt": "2020-02-04T23:25:33Z", "author": {"login": "abbccdda"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));\n+            printWithTxnId(\"Message remaining: \" + messageRemaining);\n+        }\n+\n+        printWithTxnId(\"Finished processing \" + messageProcessed + \" records\");\n+        latch.countDown();\n+    }\n+\n+    private void printWithTxnId(final String message) {\n+        System.out.println(transactionalId + \": \" + message);\n+    }\n+\n+    private ProducerRecord<Integer, String> transform(final ConsumerRecord<Integer, String> record) {\n+        printWithTxnId(\"Transformed record (\" + record.key() + \",\" + record.value() + \")\");\n+        return new ProducerRecord<>(outputTopic, record.key() / 2, \"Transformed_\" + record.value());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTE1NA=="}, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODgwMDc4OnYy", "diffSide": "RIGHT", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo0MzowNlrOFlkfYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwNDoxNzoxMFrOFlrWwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA==", "bodyText": "prop: abortTransaction can also throw ProducerFenced.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374939488", "createdAt": "2020-02-04T21:43:06Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4NTY0NQ==", "bodyText": "Yea, which is ok as we will throw on ProducerFenced anyway", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374985645", "createdAt": "2020-02-04T23:38:25Z", "author": {"login": "abbccdda"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA=="}, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAyNTYzMw==", "bodyText": "Not sure I understand: we try to capture ProducerFenced below and wrap it as a KafkaException, but here if we throw ProducerFenced it would not be captured and wrapped, is that intentional?", "url": "https://github.com/apache/kafka/pull/8031#discussion_r375025633", "createdAt": "2020-02-05T02:07:25Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA=="}, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTA1MTk3MA==", "bodyText": "Oh I see your pt.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r375051970", "createdAt": "2020-02-05T04:17:10Z", "author": {"login": "abbccdda"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA=="}, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODgwNTg3OnYy", "diffSide": "RIGHT", "path": "examples/src/main/java/kafka/examples/KafkaExactlyOnceDemo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo0NDo1M1rOFlking==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo0NDo1M1rOFlking==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0MDMxOA==", "bodyText": "nit: we can just pass in the boolean values directly.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374940318", "createdAt": "2020-02-04T21:44:53Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/KafkaExactlyOnceDemo.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.errors.TopicExistsException;\n+import org.apache.kafka.common.errors.UnknownTopicOrPartitionException;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * This exactly once demo driver takes 4 arguments:\n+ *   - mode: whether to run as standalone app, or a group\n+ *   - partition: number of partitions for input/output topic\n+ *   - instances: number of instances\n+ *   - records: number of records\n+ * An example argument list would be `groupMode 6 3 50000`\n+ *\n+ * The driver could be decomposed as following stages:\n+ *\n+ * 1. Cleanup any topic whose name conflicts with input and output topic, so that we have a clean-start.\n+ *\n+ * 2. Set up a producer in a separate thread to pre-populate a set of records with even number keys into\n+ *    the input topic. The driver will block for the record generation to finish, so the producer\n+ *    must be in synchronous sending mode.\n+ *\n+ * 3. Set up transactional instances in separate threads which does a consume-process-produce loop,\n+ *    tailing data from input topic (See {@link ExactlyOnceMessageProcessor}). Each EOS instance will\n+ *    drain all the records from either given partitions or auto assigned partitions by actively\n+ *    comparing log end offset with committed offset. Each record will be processed exactly once\n+ *    as dividing the key by 2, and extend the value message. The driver will block for all the record\n+ *    processing to finish. The transformed record shall be written to the output topic, with\n+ *    transactional guarantee.\n+ *\n+ * 4. Set up a read committed consumer in a separate thread to verify we have all records within\n+ *    the output topic, while the message ordering on partition level is maintained.\n+ *    The driver will block for the consumption of all committed records.\n+ *\n+ * From this demo, you could see that all the records from pre-population are processed exactly once,\n+ * in either standalone mode or group mode, with strong partition level ordering guarantee.\n+ *\n+ * Note: please start the kafka broker and zookeeper in local first. The broker version must be >= 2.5\n+ * in order to run group mode, otherwise the app could throw\n+ * {@link org.apache.kafka.common.errors.UnsupportedVersionException}.\n+ */\n+public class KafkaExactlyOnceDemo {\n+\n+    private static final String INPUT_TOPIC = \"input-topic\";\n+    private static final String OUTPUT_TOPIC = \"output-topic\";\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+        if (args.length != 4) {\n+            throw new IllegalArgumentException(\"Should accept 4 parameters: [mode], \" +\n+                \"[number of partitions], [number of instances], [number of records]\");\n+        }\n+\n+        String mode = args[0];\n+        int numPartitions = Integer.valueOf(args[1]);\n+        int numInstances = Integer.valueOf(args[2]);\n+        int numRecords = Integer.valueOf(args[3]);\n+\n+        /* Stage 1: topic cleanup and recreation */\n+        recreateTopics(numPartitions);\n+\n+        CountDownLatch prePopulateLatch = new CountDownLatch(1);\n+\n+        /* Stage 2: pre-populate records */\n+        final boolean isAsync = false;\n+        final boolean enableIdempotency = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNTU0OTkyOnYy", "diffSide": "RIGHT", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxODoxMzozM1rOFmlK7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxODoxMzozM1rOFmlK7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk5OTIxNQ==", "bodyText": "It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this:\ntry {\n  producer.beginTransaction()\n  producer.send(...)\n  producer.sendOffsetsToTransaction(...)\n  producer.commitTransaction()\n} catch (Exception ) {\n  producer.abortTransaction()\n}", "url": "https://github.com/apache/kafka/pull/8031#discussion_r375999215", "createdAt": "2020-02-06T18:13:33Z", "author": {"login": "hachikuji"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        boolean abortPreviousTransaction = false;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Abort previous transaction if instructed.\n+                    if (abortPreviousTransaction) {\n+                        producer.abortTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a748240c100014e86e831f614bf4219974fbd32"}, "originalPosition": 131}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4179, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}