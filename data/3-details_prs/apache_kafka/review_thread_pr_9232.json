{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc2MjMwMTU0", "number": 9232, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo0OToxMlrOEehhuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo1MjozMlrOEehkOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDQyMDQzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo0OToxMlrOHKOBxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNzoyNDo1MlrOHKotXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3NzYzOQ==", "bodyText": "Hmm, why we need the second condition to determine singleCache = false here?", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480477639", "createdAt": "2020-08-31T23:49:12Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDkxNDc4Mg==", "bodyText": "When RocksDB uses the memory bounded configuration, all RocksDB instances use the same cache, i.e., the reference to the same cache.\nAt this point in the code the value providers for the given segment with the cache cache has not been added yet to the value providers used in this recorder. To understand if different caches are used for different segments (i.e., singleCache = false), it is not enough to just check if only one single cache has been already added, we also need to check if the already added cache (i.e., valueProviders.cache) is different from the cache to add (cache).", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480914782", "createdAt": "2020-09-01T07:24:52Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3NzYzOQ=="}, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDQyMjg2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo1MDoyNlrOHKODRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNzozNjoxMFrOHKpSiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODAyMw==", "bodyText": "nit: verifyConsistentSegmentValueProviders?", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480478023", "createdAt": "2020-08-31T23:50:26Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {\n+                singleCache = false;\n+            } else if (singleCache && cache != valueProviders.cache || !singleCache && cache == valueProviders.cache) {\n+                throw new IllegalStateException(\"Caches for store \" + storeName + \" of task \" + taskId +\n+                    \" are either not all distinct or do not all refer to the same cache. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+        }\n+    }\n+\n+    private void verifyIfSomeAreNull(final String segmentName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDkyNDI5Nw==", "bodyText": "I will rename it to verifyConsistencyOfValueProvidersAcrossSegments(). WDYT?", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480924297", "createdAt": "2020-09-01T07:36:10Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {\n+                singleCache = false;\n+            } else if (singleCache && cache != valueProviders.cache || !singleCache && cache == valueProviders.cache) {\n+                throw new IllegalStateException(\"Caches for store \" + storeName + \" of task \" + taskId +\n+                    \" are either not all distinct or do not all refer to the same cache. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+        }\n+    }\n+\n+    private void verifyIfSomeAreNull(final String segmentName,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODAyMw=="}, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDQyNjgwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo1MjozMlrOHKOFnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNzozOTozNFrOHKpdJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODYyMQ==", "bodyText": "Is this a piggy-backed fix to wrap RocksDBException here?", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480478621", "createdAt": "2020-08-31T23:52:32Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -174,22 +241,163 @@ private void initSensors(final StreamsMetricsImpl streamsMetrics, final RocksDBM\n         numberOfFileErrorsSensor = RocksDBMetrics.numberOfFileErrorsSensor(streamsMetrics, metricContext);\n     }\n \n-    private void initGauges(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext) {\n-        RocksDBMetrics.addNumEntriesActiveMemTableMetric(streamsMetrics, metricContext, (metricsConfig, now) -> {\n+    private void initGauges(final StreamsMetricsImpl streamsMetrics,\n+                            final RocksDBMetricContext metricContext) {\n+        RocksDBMetrics.addNumImmutableMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addCurSizeActiveMemTable(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addCurSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumEntriesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumDeletesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumEntriesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumDeletesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addMemTableFlushPending(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(MEMTABLE_FLUSH_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningFlushesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_FLUSHES)\n+        );\n+        RocksDBMetrics.addCompactionPendingMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(COMPACTION_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningCompactionsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_COMPACTIONS)\n+        );\n+        RocksDBMetrics.addEstimatePendingCompactionBytesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_BYTES_OF_PENDING_COMPACTION)\n+        );\n+        RocksDBMetrics.addTotalSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(TOTAL_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addLiveSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(LIVE_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addNumLiveVersionMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_LIVE_VERSIONS)\n+        );\n+        RocksDBMetrics.addEstimateNumKeysMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_NUMBER_OF_KEYS)\n+        );\n+        RocksDBMetrics.addEstimateTableReadersMemMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_MEMORY_OF_TABLE_READERS)\n+        );\n+        RocksDBMetrics.addBackgroundErrorsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_BACKGROUND_ERRORS)\n+        );\n+        RocksDBMetrics.addBlockCacheCapacityMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(CAPACITY_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCacheUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(USAGE_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCachePinnedUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(PINNED_USAGE_OF_BLOCK_CACHE)\n+        );\n+    }\n+\n+    private Gauge<BigInteger> gaugeToComputeSumOfProperties(final String propertyName) {\n+        return (metricsConfig, now) -> {\n             BigInteger result = BigInteger.valueOf(0);\n             for (final DbAndCacheAndStatistics valueProvider : storeToValueProviders.values()) {\n                 try {\n                     // values of RocksDB properties are of type unsigned long in C++, i.e., in Java we need to use\n                     // BigInteger and construct the object from the byte representation of the value\n                     result = result.add(new BigInteger(1, longToBytes(\n-                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE))));\n+                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + propertyName)\n+                    )));\n+                } catch (final RocksDBException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 261}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDkyNzAxMw==", "bodyText": "Actually, I just followed the pattern found in RocksDBStore.", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480927013", "createdAt": "2020-09-01T07:39:34Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -174,22 +241,163 @@ private void initSensors(final StreamsMetricsImpl streamsMetrics, final RocksDBM\n         numberOfFileErrorsSensor = RocksDBMetrics.numberOfFileErrorsSensor(streamsMetrics, metricContext);\n     }\n \n-    private void initGauges(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext) {\n-        RocksDBMetrics.addNumEntriesActiveMemTableMetric(streamsMetrics, metricContext, (metricsConfig, now) -> {\n+    private void initGauges(final StreamsMetricsImpl streamsMetrics,\n+                            final RocksDBMetricContext metricContext) {\n+        RocksDBMetrics.addNumImmutableMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addCurSizeActiveMemTable(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addCurSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumEntriesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumDeletesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumEntriesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumDeletesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addMemTableFlushPending(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(MEMTABLE_FLUSH_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningFlushesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_FLUSHES)\n+        );\n+        RocksDBMetrics.addCompactionPendingMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(COMPACTION_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningCompactionsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_COMPACTIONS)\n+        );\n+        RocksDBMetrics.addEstimatePendingCompactionBytesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_BYTES_OF_PENDING_COMPACTION)\n+        );\n+        RocksDBMetrics.addTotalSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(TOTAL_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addLiveSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(LIVE_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addNumLiveVersionMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_LIVE_VERSIONS)\n+        );\n+        RocksDBMetrics.addEstimateNumKeysMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_NUMBER_OF_KEYS)\n+        );\n+        RocksDBMetrics.addEstimateTableReadersMemMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_MEMORY_OF_TABLE_READERS)\n+        );\n+        RocksDBMetrics.addBackgroundErrorsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_BACKGROUND_ERRORS)\n+        );\n+        RocksDBMetrics.addBlockCacheCapacityMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(CAPACITY_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCacheUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(USAGE_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCachePinnedUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(PINNED_USAGE_OF_BLOCK_CACHE)\n+        );\n+    }\n+\n+    private Gauge<BigInteger> gaugeToComputeSumOfProperties(final String propertyName) {\n+        return (metricsConfig, now) -> {\n             BigInteger result = BigInteger.valueOf(0);\n             for (final DbAndCacheAndStatistics valueProvider : storeToValueProviders.values()) {\n                 try {\n                     // values of RocksDB properties are of type unsigned long in C++, i.e., in Java we need to use\n                     // BigInteger and construct the object from the byte representation of the value\n                     result = result.add(new BigInteger(1, longToBytes(\n-                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE))));\n+                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + propertyName)\n+                    )));\n+                } catch (final RocksDBException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODYyMQ=="}, "originalCommit": {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9"}, "originalPosition": 261}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1971, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}