{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUxMzQ4OTky", "number": 9039, "title": "KAFKA-5636: SlidingWindows (KIP-450)", "bodyText": "Updating basic classes for SlidingWindows implementation, WIP and POC, things will shift as KIP discussion continues.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-07-17T18:34:40Z", "url": "https://github.com/apache/kafka/pull/9039", "merged": true, "mergeCommit": {"oid": "85b6545b8159885c57ab67e08b7185be8a607988"}, "closed": true, "closedAt": "2020-08-31T22:22:00Z", "author": {"login": "lct45"}, "timelineItems": {"totalCount": 42, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc11xOMAH2gAyNDUxMzQ4OTkyOjFiMmI3NTgzYzQ4MWE3ZmJkM2RlZGMzNWM4MzhlM2Q5ODYwM2UxNmQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdEaa43AH2gAyNDUxMzQ4OTkyOmRlOTdkYjZjZjM5ZWIzNGVhYjAyMDdmM2NjNDVhNTA4NTMxN2IyYTQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "1b2b7583c481a7fbd3dedc35c838e3d98603e16d", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/1b2b7583c481a7fbd3dedc35c838e3d98603e16d", "committedDate": "2020-07-17T15:42:48Z", "message": "Initial classes for SlidingWindows, changes to processors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ee045a49a0c5e6478112af400e57c277b38f312", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/3ee045a49a0c5e6478112af400e57c277b38f312", "committedDate": "2020-07-17T18:31:10Z", "message": "updates to processor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff6ca4f62482d32f2f22a45647448727cfb67980", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/ff6ca4f62482d32f2f22a45647448727cfb67980", "committedDate": "2020-07-17T20:19:34Z", "message": "updated for grace period checks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56a86f1718ba74e6ce4b808cb205ecf71a3d0b16", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/56a86f1718ba74e6ce4b808cb205ecf71a3d0b16", "committedDate": "2020-07-20T14:17:03Z", "message": "fixes for checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a3d6d490f0894ad128d4f981f9d7f87cf165f7d", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/0a3d6d490f0894ad128d4f981f9d7f87cf165f7d", "committedDate": "2020-07-20T14:20:42Z", "message": "fixes for checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6d358dfb62d09f4e8f406487b1f887c3db49267", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/c6d358dfb62d09f4e8f406487b1f887c3db49267", "committedDate": "2020-07-27T22:19:10Z", "message": "update algorithm, KIP changes, test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c1541ab543d2cfa0cb5e1e5288d39ec86187317", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/0c1541ab543d2cfa0cb5e1e5288d39ec86187317", "committedDate": "2020-07-27T22:27:14Z", "message": "Merge remote-tracking branch 'upstream/trunk' into slidingwindows"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f1b886f9da1f0ee5d84806875f93088c7b3c4d8", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/7f1b886f9da1f0ee5d84806875f93088c7b3c4d8", "committedDate": "2020-07-28T16:36:31Z", "message": "updates for windowedBy and associated processors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/de89fe07b9ac515a0c430274200c39cadddbf64b", "committedDate": "2020-07-29T17:10:05Z", "message": "test fixes, updating algorithms"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MDIzMjkx", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-457023291", "createdAt": "2020-07-28T21:23:12Z", "commit": {"oid": "7f1b886f9da1f0ee5d84806875f93088c7b3c4d8"}, "state": "COMMENTED", "comments": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQyMToyMzoxMlrOG4fZgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwMTozNjowMFrOG5P8eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTg4Nzg3Mw==", "bodyText": "Not sure if you did this or your IDE did it automatically, but nice \ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/9039#discussion_r461887873", "createdAt": "2020-07-28T21:23:12Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImpl.java", "diffHunk": "@@ -19,7 +19,16 @@\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.utils.Bytes;\n-import org.apache.kafka.streams.kstream.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f1b886f9da1f0ee5d84806875f93088c7b3c4d8"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyMjYwOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Create a new {@link SessionWindowedCogroupedKStream} instance that can be used to perform session\n          \n          \n            \n                 * Create a new {@link TimeWindowedCogroupedKStream} instance that can be used to perform sliding", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462622608", "createdAt": "2020-07-29T22:18:47Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java", "diffHunk": "@@ -275,6 +275,15 @@\n      */\n     <W extends Window> TimeWindowedCogroupedKStream<K, VOut> windowedBy(final Windows<W> windows);\n \n+    /**\n+     * Create a new {@link SessionWindowedCogroupedKStream} instance that can be used to perform session", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNTA3Mg==", "bodyText": "nit: call this timeDifferenceMs to be in sync with graceMs. Also it can be private", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462625072", "createdAt": "2020-07-29T22:24:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNjc3Mw==", "bodyText": "nit: also rename the method timeDifferenceMs to be consistent with gracePeriodMs", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462626773", "createdAt": "2020-07-29T22:29:17Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifference, final long graceMs) {\n+        this.timeDifference = timeDifference;\n+        this.graceMs = graceMs;\n+\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");\n+        }\n+\n+        return new SlidingWindows(timeDifferenceMs, graceMs);\n+    }\n+\n+\n+    public long timeDifference() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNzE4Ng==", "bodyText": "I think we can remove this suppression (and all the ones below)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462627186", "createdAt": "2020-07-29T22:30:26Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifference, final long graceMs) {\n+        this.timeDifference = timeDifference;\n+        this.graceMs = graceMs;\n+\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");\n+        }\n+\n+        return new SlidingWindows(timeDifferenceMs, graceMs);\n+    }\n+\n+\n+    public long timeDifference() {\n+        return timeDifference;\n+    }\n+\n+\n+\n+\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNzI3Ng==", "bodyText": "nit: extra space after return", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462627276", "createdAt": "2020-07-29T22:30:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifference, final long graceMs) {\n+        this.timeDifference = timeDifference;\n+        this.graceMs = graceMs;\n+\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");\n+        }\n+\n+        return new SlidingWindows(timeDifferenceMs, graceMs);\n+    }\n+\n+\n+    public long timeDifference() {\n+        return timeDifference;\n+    }\n+\n+\n+\n+\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    public long gracePeriodMs() {\n+        return graceMs;\n+    }\n+\n+\n+    @SuppressWarnings(\"deprecation\") // removing segments from Windows will fix this\n+    @Override\n+    public boolean equals(final Object o) {\n+        if (this == o) {\n+            return true;\n+        }\n+        if (o == null || getClass() != o.getClass()) {\n+            return false;\n+        }\n+        final SlidingWindows that = (SlidingWindows) o;\n+        return  timeDifference == that.timeDifference &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA==", "bodyText": "Seems like this should have also had a check for sessionWindows != null, right? Can we add that as well?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462640050", "createdAt": "2020-07-29T23:06:31Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedStreamAggregateBuilder.java", "diffHunk": "@@ -132,16 +135,19 @@\n                                                                                     final boolean stateCreated,\n                                                                                     final StoreBuilder<?> storeBuilder,\n                                                                                     final Windows<W> windows,\n+                                                                                    final SlidingWindows slidingWindows,\n                                                                                     final SessionWindows sessionWindows,\n                                                                                     final Merger<? super K, VOut> sessionMerger) {\n \n         final ProcessorSupplier<K, ?> kStreamAggregate;\n \n-        if (windows == null && sessionWindows == null) {\n+        if (windows == null && slidingWindows == null && sessionWindows == null) {\n             kStreamAggregate = new KStreamAggregate<>(storeBuilder.name(), initializer, aggregator);\n-        } else if (windows != null && sessionWindows == null) {\n+        } else if (windows != null && slidingWindows == null && sessionWindows == null) {\n             kStreamAggregate = new KStreamWindowAggregate<>(windows, storeBuilder.name(), initializer, aggregator);\n-        } else if (windows == null && sessionMerger != null) {\n+        } else if (windows == null && slidingWindows != null && sessionWindows == null) {\n+            kStreamAggregate = new KStreamSlidingWindowAggregate<>(slidingWindows, storeBuilder.name(), initializer, aggregator);\n+        } else if (windows == null && slidingWindows == null && sessionMerger != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NjMxOQ==", "bodyText": "nit: alignment is off by one on the parameters", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462646319", "createdAt": "2020-07-29T23:26:36Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NzAyOQ==", "bodyText": "I think it's ok to skip this; since it's a new operator, there's no old topology to be compatible with", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462647029", "createdAt": "2020-07-29T23:29:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NzY1Ng==", "bodyText": "I wonder why we have to do this for count but not for aggregate and reduce? Is this intentional or an oversight? cc @mjsax @vvcephei @guozhangwang", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462647656", "createdAt": "2020-07-29T23:30:52Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NzAyOQ=="}, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MzA4OA==", "bodyText": "Can we add an else here with builder.withCachingDisabled()? It doesn't make a difference logically, it just seems easier to understand (again, also in SlidingWindowedCogroupedKStreamImpl)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653088", "createdAt": "2020-07-29T23:48:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            // new style retention: use Materialized retention and default segmentInterval\n+            final long retentionPeriod = materialized.retention().toMillis();\n+\n+            if ((windows.timeDifference() + windows.gracePeriodMs()) > retentionPeriod) {\n+                throw new IllegalArgumentException(\"The retention period of the window store \"\n+                        + name + \" must be no smaller than its window time difference plus the grace period.\"\n+                        + \" Got time difference=[\" + windows.timeDifference() + \"],\"\n+                        + \" grace=[\" + windows.gracePeriodMs() + \"],\"\n+                        + \" retention=[\" + retentionPeriod + \"]\");\n+            }\n+\n+            supplier = Stores.persistentTimestampedWindowStore(\n+                    materialized.storeName(),\n+                    Duration.ofMillis(retentionPeriod),\n+                    Duration.ofMillis(windows.timeDifference()),\n+                    false\n+            );\n+        }\n+        final StoreBuilder<TimestampedWindowStore<K, VR>> builder = Stores.timestampedWindowStoreBuilder(\n+                supplier,\n+                materialized.keySerde(),\n+                materialized.valueSerde()\n+        );\n+\n+        if (materialized.loggingEnabled()) {\n+            builder.withLoggingEnabled(materialized.logConfig());\n+        } else {\n+            builder.withLoggingDisabled();\n+        }\n+\n+        if (materialized.cachingEnabled()) {\n+            builder.withCachingEnabled();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MzIzNw==", "bodyText": "You should be able to remove this suppression and comment (here and in SlidingWindowedCogroupedKStreamImpl)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653237", "createdAt": "2020-07-29T23:49:12Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MzUwNQ==", "bodyText": "let's just remove this comment since it's the only style retention here (also in SlidingWindowedCogroupedKStreamImpl)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653505", "createdAt": "2020-07-29T23:50:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            // new style retention: use Materialized retention and default segmentInterval", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1Mzc1Mw==", "bodyText": "We can remove this", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653753", "createdAt": "2020-07-29T23:50:47Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                     final InternalStreamsBuilder builder,\n+                                     final Set<String> subTopologySourceNodes,\n+                                     final String name,\n+                                     final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                     final StreamsGraphNode streamsGraphNode,\n+                                     final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    // continuing to support Windows#maintainMs/segmentInterval in fallback mode", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1NTg1OA==", "bodyText": "I was wondering what this method is actually used for so I checked out the callers of KStreamWindowAggregate#windows. There's a method called extractGracePeriod in GraphGraceSearchUtil where we might actually need to make a small addition to include the new sliding window processor.\nI think it's for Suppression, which needs to figure out the grace period of the upstream operator since grace period doesn't get passed in directly to suppress", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462655858", "createdAt": "2020-07-29T23:58:03Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1ODkwMw==", "bodyText": "Do you think we actually need to enforce that the retention period be a little longer for sliding windows? I was just thinking that since the range scan starts at timestamp - 2 * windows.timeDifference(), maybe we should actually enforce that the retention period be >= 2 * timeDifference + gracePeriod in case we need to get the aggregate value from some older window that has technically expired.\nHaven't checked the math so I'm not sure that's the correct value exactly, but it seems like it might need to be a little bigger. Any thoughts?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462658903", "createdAt": "2020-07-30T00:08:03Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            // new style retention: use Materialized retention and default segmentInterval\n+            final long retentionPeriod = materialized.retention().toMillis();\n+\n+            if ((windows.timeDifference() + windows.gracePeriodMs()) > retentionPeriod) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MDMyMA==", "bodyText": "In general it's better to use a more descriptive variable name than a shorted one with a comment. It's not always possible to describe a variable exactly in a reasonable length, but I think in this case we can say curLeftWindowAlreadyExists or curLeftWindowAlreadyCreated or something\nMight be better to use AlreadyCreated when we're specifically talking about whether or not a window already exists in the window store, and can use Exists when we're talking about whether a window is possible regardless of whether it currently has been created or not", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462660320", "createdAt": "2020-07-30T00:13:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MTkyMg==", "bodyText": "Does that make sense? In particular I feel like we're using Exists to mean one thing for left/RightWindowExists, and then we mean another thing entirely in prevRightWindowExists. ie prevRightWinAlreadyCreated is more similar to what we mean by the left/RightWindowExists variables", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462661922", "createdAt": "2020-07-30T00:19:14Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MDMyMA=="}, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MjEzMg==", "bodyText": "this comment doesn't seem quite correct", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462662132", "createdAt": "2020-07-30T00:19:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MjQ5OA==", "bodyText": "Can we also name this variable a bit more clearly instead of the comment? Like foundClosestStartTimeWindow or something. Same with foundFirstEndTime", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462662498", "createdAt": "2020-07-30T00:21:20Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzI5Nw==", "bodyText": "maybe add a comment saying that this condition will only be hit on the very first record. Or it might be reasonable to pull this one condition out of the loop and just handle it before entering the loop", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462663297", "createdAt": "2020-07-30T00:24:13Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzkxMQ==", "bodyText": "Actually maybe foundRight/LeftWindowAggregate would be good, since that's what \"the window with the closest start/end time to the record\" actually means to us", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462663911", "createdAt": "2020-07-30T00:26:22Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MjQ5OA=="}, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2NzM4Nw==", "bodyText": "Should this be inside the if (!foundFirst) condition above? We only want to save the aggregate of the first window we find with a start time less than the timestamp right?\nAlso, I think we might need to check that the max timestamp of this window is greater than the current record's timestamp. If not, then the right window will be empty.\nFor example, we have a record A at 10 and a record B at 11 and then process a record at 15. Obviously, the new right window will be empty. But the first window we'll find with a start time less than 15 will be [11, 21] with agg B.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462667387", "createdAt": "2020-07-30T00:38:53Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2Nzk4OA==", "bodyText": "nit: put each parameter on its own line", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462667988", "createdAt": "2020-07-30T00:40:56Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    //potentially need to change long to instant\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next = iterator.next();\n+\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinExists = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        foundLeftFirst = isLeftWindow(next) ? true : false;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWindowExists = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (windowStartTimes.contains(rightWinStart)) {\n+                    prevRightWinAlreadyCreated = true;\n+                } else {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                if (latestLeftTypeWindow != null) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+\n+        }\n+\n+        private boolean isLeftWindow(final KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> window) {\n+            return window.key.window().end() == window.value.timestamp();\n+        }\n+\n+        private void putAndForward(final Window window, final ValueAndTimestamp<Agg> valueAndTime, final K key,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 344}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2OTI1MA==", "bodyText": "Seems like we're aggregating with the new value twice; we call aggregator.apply once in this if/else branch but then also call it again in putAndForward, right?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462669250", "createdAt": "2020-07-30T00:45:20Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3NzQyNw==", "bodyText": "Can we add a comment to clarify that we're checking whether it's a left window because that tells us there was a record at this window's end time", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462677427", "createdAt": "2020-07-30T01:16:27Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3Nzc3NA==", "bodyText": "I feel like I'm just way overthinking this, but I keep getting these variables confused. Maybe we could call this guy prevRightWindowCanExist? Does that seem to get at its underlying purpose?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462677774", "createdAt": "2020-07-30T01:17:55Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3ODc3Nw==", "bodyText": "Can we remove this and just break out of the loop immediately at the end of the isLeftWindow condition block?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462678777", "createdAt": "2020-07-30T01:21:38Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3OTk5MQ==", "bodyText": "I think we need to pass in the new maximum window timestamp here, not the window start time", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462679991", "createdAt": "2020-07-30T01:26:22Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    //potentially need to change long to instant\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next = iterator.next();\n+\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinExists = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        foundLeftFirst = isLeftWindow(next) ? true : false;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWindowExists = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (windowStartTimes.contains(rightWinStart)) {\n+                    prevRightWinAlreadyCreated = true;\n+                } else {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                if (latestLeftTypeWindow != null) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+\n+        }\n+\n+        private boolean isLeftWindow(final KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> window) {\n+            return window.key.window().end() == window.value.timestamp();\n+        }\n+\n+        private void putAndForward(final Window window, final ValueAndTimestamp<Agg> valueAndTime, final K key,\n+                                   final V value, final long closeTime, final long timestamp) {\n+            final long windowStart = window.start();\n+            final long windowEnd = window.end();\n+            if (windowEnd > closeTime) {\n+                //get aggregate from existing window\n+                final Agg oldAgg = getValueOrNull(valueAndTime);\n+                //add record's value to existing aggregate\n+                final Agg newAgg = aggregator.apply(key, value, oldAgg);\n+\n+                windowStore.put(key,\n+                        ValueAndTimestamp.make(newAgg, Math.max(timestamp, valueAndTime.timestamp())),\n+                        windowStart);\n+                tupleForwarder.maybeForward(\n+                        new Windowed<K>(key, window),\n+                        newAgg,\n+                        sendOldValues ? oldAgg : null,\n+                        windowStart);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MjI2Ng==", "bodyText": "Ok I may have lost the trail of logic here...are we just checking prevRightWinExists as an indicator of whether we actually found any records to the left of our record within range? Could/should we check foundFirstEndTime instead?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462682266", "createdAt": "2020-07-30T01:32:45Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MjcxOA==", "bodyText": "Since it's a left window, the max timestamp should always be timestamp, right?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462682718", "createdAt": "2020-07-30T01:34:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MzI1OA==", "bodyText": "Can we put this condition into a method and give it a clear name to describe what this means? eg\nprivate boolean rightWindowHasNotBeenCreatedAndIsNonEmpty(..) {\n    return !rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)\n}", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462683258", "createdAt": "2020-07-30T01:36:00Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 225}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/35e637d6c0d932129d69b50072d2d1b412af7d10", "committedDate": "2020-07-31T19:59:41Z", "message": ":review updates"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNDE3NTY3", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-460417567", "createdAt": "2020-08-04T00:21:03Z", "commit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMDoyMTowM1rOG7MzRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMDoyMTowM1rOG7MzRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ==", "bodyText": "This PR is rather larger. Would it maybe make sense to split it into 2 and add co-group in it's own PR?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464728901", "createdAt": "2020-08-04T00:21:03Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java", "diffHunk": "@@ -275,6 +275,15 @@\n      */\n     <W extends Window> TimeWindowedCogroupedKStream<K, VOut> windowedBy(final Windows<W> windows);\n \n+    /**\n+     * Create a new {@link TimeWindowedCogroupedKStream} instance that can be used to perform sliding\n+     * windowed aggregations.\n+     *\n+     * @param windows the specification of the aggregation {@link SlidingWindows}\n+     * @return an instance of {@link TimeWindowedCogroupedKStream}\n+     */\n+    TimeWindowedCogroupedKStream<K, VOut> windowedBy(final SlidingWindows windows);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNDE3Nzky", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-460417792", "createdAt": "2020-08-04T00:21:44Z", "commit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "state": "COMMENTED", "comments": {"totalCount": 39, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMDoyMTo0NFrOG7M0Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMTozMDowNVrOG7N55Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTEyNg==", "bodyText": "nit: double /**", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729126", "createdAt": "2020-08-04T00:21:44Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTMxMQ==", "bodyText": "nit: if you want to have a new paragraph, you need to insert <p> tag -- otherwise, the empty line is just ignored all it's going to be one paragraph. -- If you don't want a paragraph, please remove the empty line.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729311", "createdAt": "2020-08-04T00:22:28Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTM0OA==", "bodyText": "as above.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729348", "createdAt": "2020-08-04T00:22:35Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTUzNA==", "bodyText": "type [w]indows", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729534", "createdAt": "2020-08-04T00:23:14Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTc0Mw==", "bodyText": "and [a] given ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729743", "createdAt": "2020-08-04T00:23:56Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMDY4Ng==", "bodyText": "We must use HTML list markup to get bullet points rendered, ie, <ul> and <li> (cf https://www.w3schools.com/html/html_lists.asp)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464730686", "createdAt": "2020-08-04T00:26:58Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMTA5Mg==", "bodyText": "are processed -> sounds like processing time semantics; maybe better occur in the stream (i.e., event timestamps)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464731092", "createdAt": "2020-08-04T00:28:25Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMTU1NA==", "bodyText": "Reference to CogroupedKStream is missing", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464731554", "createdAt": "2020-08-04T00:29:58Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMTc2Mg==", "bodyText": "as above (won't comment on this again) -- please address throughput the whole PR.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464731762", "createdAt": "2020-08-04T00:30:46Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMjIwNA==", "bodyText": "timeDifference (timeDifference) (redundant) should be time difference (timeDifference)\nmust be larger than zero. -> must not be negative.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464732204", "createdAt": "2020-08-04T00:32:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMjQ2Mg==", "bodyText": "I guess a timeDifference of zero should be allowed to define a sliding window of size 1ms", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464732462", "createdAt": "2020-08-04T00:33:30Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMjU3Mg==", "bodyText": "as above -> should be timeDifferenceMs < 0", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464732572", "createdAt": "2020-08-04T00:33:53Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMzAxNA==", "bodyText": "For consistency: Grace period (grace) must ? Frankly, I am not sure if we need to have \"natural language\" and the parameter name in those error messages -- also above. But we should do it in a consistent manner IMHO.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464733014", "createdAt": "2020-08-04T00:35:30Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNDA0MQ==", "bodyText": "I agree with Sophie that his check seems a little weird. We should check that either both (sessionWindows and sessionMerger) are null or not null.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464734041", "createdAt": "2020-08-04T00:39:23Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedStreamAggregateBuilder.java", "diffHunk": "@@ -132,16 +135,19 @@\n                                                                                     final boolean stateCreated,\n                                                                                     final StoreBuilder<?> storeBuilder,\n                                                                                     final Windows<W> windows,\n+                                                                                    final SlidingWindows slidingWindows,\n                                                                                     final SessionWindows sessionWindows,\n                                                                                     final Merger<? super K, VOut> sessionMerger) {\n \n         final ProcessorSupplier<K, ?> kStreamAggregate;\n \n-        if (windows == null && sessionWindows == null) {\n+        if (windows == null && slidingWindows == null && sessionWindows == null) {\n             kStreamAggregate = new KStreamAggregate<>(storeBuilder.name(), initializer, aggregator);\n-        } else if (windows != null && sessionWindows == null) {\n+        } else if (windows != null && slidingWindows == null && sessionWindows == null) {\n             kStreamAggregate = new KStreamWindowAggregate<>(windows, storeBuilder.name(), initializer, aggregator);\n-        } else if (windows == null && sessionMerger != null) {\n+        } else if (windows == null && slidingWindows != null && sessionWindows == null) {\n+            kStreamAggregate = new KStreamSlidingWindowAggregate<>(slidingWindows, storeBuilder.name(), initializer, aggregator);\n+        } else if (windows == null && slidingWindows == null && sessionMerger != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNTE3Mw==", "bodyText": "I think we should do this check in init and use a Runnable that we just call blindly (ie, depending on the check, we instantiate the one or other Runnable and each Runnable implements a different algorithm.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464735173", "createdAt": "2020-08-04T00:43:50Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNTI1Mg==", "bodyText": "Why do we suppress instead of fix the issue? (or add an exception to the suppress.xml file if we really need it)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464735252", "createdAt": "2020-08-04T00:44:11Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjA1Mg==", "bodyText": "I think we should also drop if value == null ? (It seem this null check is missing in the existing time/session-window aggregate processors, too)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464736052", "createdAt": "2020-08-04T00:47:16Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjU1MA==", "bodyText": "Maybe we have the same issue in other processors, too (we might even have a ticket for it?) but won't we need to preserve observedStreamTime across restarts? It's transient atm... (Just want to confirm -- maybe it's ok as other processor do it the same way and we need to fix if for all of them at once?)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464736550", "createdAt": "2020-08-04T00:49:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjk2OQ==", "bodyText": "The comment seems redundant -- it just says exactly what the next line of code says.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464736969", "createdAt": "2020-08-04T00:51:12Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNzYwMg==", "bodyText": "No need to pass in an Instant -- we should just pass in the long directly.\nIt might not be clear from the type hierarchy, but the overloads that accept long are only deprecated for the ReadOnlyXxx store, but are still available on the \"read/write\" stores to avoid unnecessary runtime overhead.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464737602", "createdAt": "2020-08-04T00:53:54Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczODIxMg==", "bodyText": "Why + 1 ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464738212", "createdAt": "2020-08-04T00:56:11Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),\n+                            Instant.ofEpochMilli(timestamp + 1))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczODkzMg==", "bodyText": "as above. (also, this code seems to be duplicated; we should move it into process() before we call the the actual processReverse or processInOrder methods.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464738932", "createdAt": "2020-08-04T00:59:18Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTA4NA==", "bodyText": "as above", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464739084", "createdAt": "2020-08-04T00:59:53Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTE1Mw==", "bodyText": "nit: move key to the next line", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464739153", "createdAt": "2020-08-04T01:00:07Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTk3Nw==", "bodyText": "why * 2 ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464739977", "createdAt": "2020-08-04T01:03:19Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                               final InternalStreamsBuilder builder,\n+                               final Set<String> subTopologySourceNodes,\n+                               final String name,\n+                               final Serde<K> keySerde,\n+                               final Serde<V> valueSerde,\n+                               final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                               final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention() != null ? materialized.retention().toMillis() : windows.gracePeriodMs() + 2 * windows.timeDifferenceMs();\n+\n+            if ((windows.timeDifferenceMs() * 2 + windows.gracePeriodMs()) > retentionPeriod) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDEwOA==", "bodyText": "why * 2 ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740108", "createdAt": "2020-08-04T01:03:51Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                               final InternalStreamsBuilder builder,\n+                               final Set<String> subTopologySourceNodes,\n+                               final String name,\n+                               final Serde<K> keySerde,\n+                               final Serde<V> valueSerde,\n+                               final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                               final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention() != null ? materialized.retention().toMillis() : windows.gracePeriodMs() + 2 * windows.timeDifferenceMs();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDMyNQ==", "bodyText": "Why this?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740325", "createdAt": "2020-08-04T01:04:45Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStreamThreadTest.java", "diffHunk": "@@ -221,7 +221,6 @@ public void shouldTransitionToRunningOnStart() throws Exception {\n         globalStreamThread.shutdown();\n     }\n \n-    @Test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDcxMg==", "bodyText": "we should not use this annotation (even if we still have code that used it... we are working on migrating test away lazily). We should instead use assertThrows and also verify the exception error message.\nSame below.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740712", "createdAt": "2020-08-04T01:06:18Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDk2MA==", "bodyText": "Beside the fact, that zero should be valid IMHO, what do we gain by testing 0 and -1 ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740960", "createdAt": "2020-08-04T01:07:06Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTEwMw==", "bodyText": "This line seems to be unnecessary for this test?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741103", "createdAt": "2020-08-04T01:07:51Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTIwMg==", "bodyText": "This is also a pattern we try to move off. Use assertThrows instead.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741202", "createdAt": "2020-08-04T01:08:16Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTY4OA==", "bodyText": "Why do we need three tests? If you want to \"randomize\" it, maybe just use Random to generate difference and grace input instead of hard coding them?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741688", "createdAt": "2020-08-04T01:10:08Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTgwMQ==", "bodyText": "As above.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741801", "createdAt": "2020-08-04T01:10:33Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForNegativeCases() {\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(2)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(9)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTk4Mw==", "bodyText": "What is the difference between verifyInEquality and assertNotEquals ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741983", "createdAt": "2020-08-04T01:11:13Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForNegativeCases() {\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(2)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(9)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+\n+        verifyInEquality(\n+                SlidingWindows.withTimeDifferenceAndGrace(ofMillis(4), ofMillis(2)),\n+                SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(2))\n+        );\n+\n+        assertNotEquals(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MjMzMA==", "bodyText": "This should be two test:\n\nshouldNotBeEqualForDifferentTimeDifference\nshouldNotBeEqualForDifferentGracePeriod", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464742330", "createdAt": "2020-08-04T01:12:30Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForNegativeCases() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA==", "bodyText": "Why is endTime = Long.MAX_VALUE? Should it not be firstBatchTimestamp ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464744504", "createdAt": "2020-08-04T01:20:48Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);\n+\n+        final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);\n+        groupedStream\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(timeDifference), ofMillis(2000L)))\n+                .reduce(reducer)\n+                .toStream()\n+                .to(outputTopic, Produced.with(windowedSerde, Serdes.String()));\n+\n+        startStreams();\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> windowedOutput = receiveMessages(\n+                new TimeWindowedDeserializer<>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25);\n+\n+        // read from ConsoleConsumer\n+        final String resultFromConsoleConsumer = readWindowedKeyedMessagesViaConsoleConsumer(\n+                new TimeWindowedDeserializer<String>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25,\n+                true);\n+\n+        final Comparator<KeyValueTimestamp<Windowed<String>, String>> comparator =\n+                Comparator.comparing((KeyValueTimestamp<Windowed<String>, String> o) -> o.key().key())\n+                        .thenComparing(KeyValueTimestamp::value);\n+\n+        windowedOutput.sort(comparator);\n+        final long firstBatchLeftWindow = firstBatchTimestamp - timeDifference;\n+        final long firstBatchRightWindow = firstBatchTimestamp + 1;\n+        final long secondBatchLeftWindow = secondBatchTimestamp - timeDifference;\n+        final long secondBatchRightWindow = secondBatchTimestamp + 1;\n+        final long thirdBatchLeftWindow = thirdBatchTimestamp - timeDifference;\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> expectResult = Arrays.asList(\n+                new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchLeftWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDk2Mw==", "bodyText": "firstBatchLeftWindow -> firstBatchLeftWindowStart\nMaybe also introduce firstBatchLeftWindowEnd = firstBatchTimestamp", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464744963", "createdAt": "2020-08-04T01:22:32Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);\n+\n+        final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);\n+        groupedStream\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(timeDifference), ofMillis(2000L)))\n+                .reduce(reducer)\n+                .toStream()\n+                .to(outputTopic, Produced.with(windowedSerde, Serdes.String()));\n+\n+        startStreams();\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> windowedOutput = receiveMessages(\n+                new TimeWindowedDeserializer<>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25);\n+\n+        // read from ConsoleConsumer\n+        final String resultFromConsoleConsumer = readWindowedKeyedMessagesViaConsoleConsumer(\n+                new TimeWindowedDeserializer<String>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25,\n+                true);\n+\n+        final Comparator<KeyValueTimestamp<Windowed<String>, String>> comparator =\n+                Comparator.comparing((KeyValueTimestamp<Windowed<String>, String> o) -> o.key().key())\n+                        .thenComparing(KeyValueTimestamp::value);\n+\n+        windowedOutput.sort(comparator);\n+        final long firstBatchLeftWindow = firstBatchTimestamp - timeDifference;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NjAxMw==", "bodyText": "Maybe add comment to clarify which input should trigger which output:\n// process A @ 2000ms\nnew KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchLeftWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),\n// process A @ 2500ms\nnew KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchRightWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),\nnew KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(secondBatchLeftWindow, Long.MAX_VALUE)), \"A:A\", secondBatchTimestamp),\n// process A @ 2900ms\n...", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464746013", "createdAt": "2020-08-04T01:26:26Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);\n+\n+        final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);\n+        groupedStream\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(timeDifference), ofMillis(2000L)))\n+                .reduce(reducer)\n+                .toStream()\n+                .to(outputTopic, Produced.with(windowedSerde, Serdes.String()));\n+\n+        startStreams();\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> windowedOutput = receiveMessages(\n+                new TimeWindowedDeserializer<>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25);\n+\n+        // read from ConsoleConsumer\n+        final String resultFromConsoleConsumer = readWindowedKeyedMessagesViaConsoleConsumer(\n+                new TimeWindowedDeserializer<String>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25,\n+                true);\n+\n+        final Comparator<KeyValueTimestamp<Windowed<String>, String>> comparator =\n+                Comparator.comparing((KeyValueTimestamp<Windowed<String>, String> o) -> o.key().key())\n+                        .thenComparing(KeyValueTimestamp::value);\n+\n+        windowedOutput.sort(comparator);\n+        final long firstBatchLeftWindow = firstBatchTimestamp - timeDifference;\n+        final long firstBatchRightWindow = firstBatchTimestamp + 1;\n+        final long secondBatchLeftWindow = secondBatchTimestamp - timeDifference;\n+        final long secondBatchRightWindow = secondBatchTimestamp + 1;\n+        final long thirdBatchLeftWindow = thirdBatchTimestamp - timeDifference;\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> expectResult = Arrays.asList(\n+                new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchLeftWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0Njk4MQ==", "bodyText": "We should add a fourth batch with ts like 10K to get the windows when the second batch drops outs, too.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464746981", "createdAt": "2020-08-04T01:30:05Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjM5NTc5", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-461239579", "createdAt": "2020-08-04T23:19:29Z", "commit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMzoxOToyOVrOG70tBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMzoxOToyOVrOG70tBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM4MjY2Mw==", "bodyText": "It took me a second to understand the structure of this sentence, can we insert an and after the record's timestamp?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465382663", "createdAt": "2020-08-04T23:19:29Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 29}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/dc2f65f711a54e995094a1885df1dc728c479b1b", "committedDate": "2020-08-05T00:09:33Z", "message": "review updates, test additions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjU3Nzky", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-461257792", "createdAt": "2020-08-05T00:13:32Z", "commit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "state": "COMMENTED", "comments": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDoxMzozM1rOG71s9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMTo1MDozMlrOG73Swg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5OTAzMQ==", "bodyText": "extra /* here", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465399031", "createdAt": "2020-08-05T00:13:33Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5OTg1Mw==", "bodyText": "Technically this is an XOR not an OR \ud83d\ude1b", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465399853", "createdAt": "2020-08-05T00:16:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedStreamAggregateBuilder.java", "diffHunk": "@@ -132,19 +135,25 @@\n                                                                                     final boolean stateCreated,\n                                                                                     final StoreBuilder<?> storeBuilder,\n                                                                                     final Windows<W> windows,\n+                                                                                    final SlidingWindows slidingWindows,\n                                                                                     final SessionWindows sessionWindows,\n                                                                                     final Merger<? super K, VOut> sessionMerger) {\n-\n         final ProcessorSupplier<K, ?> kStreamAggregate;\n \n-        if (windows == null && sessionWindows == null) {\n+        if (windows == null && slidingWindows == null && sessionWindows == null && sessionMerger == null) {\n             kStreamAggregate = new KStreamAggregate<>(storeBuilder.name(), initializer, aggregator);\n-        } else if (windows != null && sessionWindows == null) {\n+        } else if (windows != null && slidingWindows == null && sessionWindows == null && sessionMerger == null) {\n             kStreamAggregate = new KStreamWindowAggregate<>(windows, storeBuilder.name(), initializer, aggregator);\n-        } else if (windows == null && sessionMerger != null) {\n-            kStreamAggregate = new KStreamSessionWindowAggregate<>(sessionWindows, storeBuilder.name(), initializer, aggregator, sessionMerger);\n+        } else if (windows == null && slidingWindows != null && sessionWindows == null && sessionMerger == null) {\n+            kStreamAggregate = new KStreamSlidingWindowAggregate<>(slidingWindows, storeBuilder.name(), initializer, aggregator);\n+        } else if (windows == null && slidingWindows == null && sessionWindows != null) {\n+            if (sessionMerger == null) {\n+                throw new IllegalArgumentException(\"sessionMerger cannot be null for sessionWindows\");\n+            } else {\n+                kStreamAggregate = new KStreamSessionWindowAggregate<>(sessionWindows, storeBuilder.name(), initializer, aggregator, sessionMerger);\n+            }\n         } else {\n-            throw new IllegalArgumentException(\"must include windows OR sessionWindows + sessionMerger OR all must be null\");\n+            throw new IllegalArgumentException(\"must include windows OR slidingWindows OR sessionWindows + sessionMerger OR all must be null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwMDQwMw==", "bodyText": "can you revert the line changes here and below? Nothing wrong with them, but the fewer lines/classes changed in the PR, the better", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465400403", "createdAt": "2020-08-05T00:18:42Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregate.java", "diffHunk": "@@ -120,7 +120,6 @@ public void process(final K key, final V value) {\n             final long timestamp = context().timestamp();\n             observedStreamTime = Math.max(observedStreamTime, timestamp);\n             final long closeTime = observedStreamTime - windows.gracePeriodMs();\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwMTE4OQ==", "bodyText": "Think you missed changing this in the cogrouped class, this should be 2*timeDifference right?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465401189", "createdAt": "2020-08-05T00:21:40Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention().toMillis();\n+\n+            if ((windows.timeDifferenceMs() + windows.gracePeriodMs()) > retentionPeriod) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwMjQ3Mw==", "bodyText": "I think we need a null check here like we have down in SlidingWindowedKStreamImpl#materialize", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465402473", "createdAt": "2020-08-05T00:26:16Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention().toMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNzQ0OA==", "bodyText": "+1 to add a comment on the extra retention (here and in SlidingWindowedCoGroupedKStreamImpl)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465407448", "createdAt": "2020-08-05T00:45:26Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                               final InternalStreamsBuilder builder,\n+                               final Set<String> subTopologySourceNodes,\n+                               final String name,\n+                               final Serde<K> keySerde,\n+                               final Serde<V> valueSerde,\n+                               final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                               final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention() != null ? materialized.retention().toMillis() : windows.gracePeriodMs() + 2 * windows.timeDifferenceMs();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDEwOA=="}, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNzYzOA==", "bodyText": "Should be no smaller than twice its window time difference...", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465407638", "createdAt": "2020-08-05T00:46:16Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                               final InternalStreamsBuilder builder,\n+                               final Set<String> subTopologySourceNodes,\n+                               final String name,\n+                               final Serde<K> keySerde,\n+                               final Serde<V> valueSerde,\n+                               final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                               final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention() != null ? materialized.retention().toMillis() : windows.gracePeriodMs() + 2 * windows.timeDifferenceMs();\n+\n+            if ((windows.timeDifferenceMs() * 2 + windows.gracePeriodMs()) > retentionPeriod) {\n+                throw new IllegalArgumentException(\"The retention period of the window store \"\n+                        + name + \" must be no smaller than its window time difference plus the grace period.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwODg3Ng==", "bodyText": "+1 to using a random number instead of multiple lines. If it does happen to fail on a specific random number, we should be sure to print that number for reproducing it later. See TaskAssignorConvergenceTest#runRandomizedScenario for example", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465408876", "createdAt": "2020-08-05T00:50:59Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTY4OA=="}, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwOTMzMA==", "bodyText": "Took me a second to understand this test, \"NegativeCases\" made me think the timeDifference/grace were supposed to be negative. +1 to Matthias's suggestion for naming (and splitting into two tests)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465409330", "createdAt": "2020-08-05T00:52:44Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForNegativeCases() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MjMzMA=="}, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMjQ4Mg==", "bodyText": "Can you leave a TODO here to make sure we remember to change this to reverseFetch? Seems unlikely we'd forget, but you never know", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465412482", "createdAt": "2020-08-05T01:03:59Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNDMxNg==", "bodyText": "unless we also move a check for if the first is a window whose aggregate we need to update\n\nWhich check? I was just thinking that, since the window starting at record.timestamp + 1 is basically a special case, we can just pull it out of the loop completely. We don't have to update anything since the record doesn't fall into this window, right? Basically just before entering the loop we check if next.key.window().start() == timestamp + 1 and if so set rightWinAlreadyCreated and then skip to the next record", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465414316", "createdAt": "2020-08-05T01:11:09Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzI5Nw=="}, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNDg2Mw==", "bodyText": "We don't technically need continue at the end of each condition, right?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465414863", "createdAt": "2020-08-05T01:13:02Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNjU4NQ==", "bodyText": "Awesome! I might still recommend pulling the rightWinAgg != null && rightWinAgg.timestamp() > timestamp check out into a method called rightWindowIsNonEmpty or something, but it's definitely a lot easier to understand now even without that \ud83d\ude04", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465416585", "createdAt": "2020-08-05T01:19:27Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MzI1OA=="}, "originalCommit": {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODk0Mg==", "bodyText": "We need to check leftWinAgg for null, right? Also, is it ever possible for leftWinAgg to be non-null but not satisfy this condition? Maybe we can just check leftWinAgg != null and if so, then assert that leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs() is always true (eg throw an IllegalStateException if it's not)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465418942", "createdAt": "2020-08-05T01:28:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxOTUzMQ==", "bodyText": "Can we remove the Math.max thing for now and just drop records that are too early for us to process for now?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465419531", "createdAt": "2020-08-05T01:30:09Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA==", "bodyText": "Just a note to other reviewers: we're planning to revisit the issue of \"early\" records later and are just dropping them for now to make the general algorithm easier to review and understand. It needs some special handling for the edge case of records that arrive earlier than the full sliding window due to the inability to store windows with negative start times", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421190", "createdAt": "2020-08-05T01:36:28Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTI2NQ==", "bodyText": "nit: put this on one line", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421265", "createdAt": "2020-08-05T01:36:42Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTUxNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        //to keep find the left type window closest to the record\n          \n          \n            \n                        // keep the left type window closest to the record", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421514", "createdAt": "2020-08-05T01:37:36Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTY1Nw==", "bodyText": "comment doesn't seem to match the query bounds (missing a +1?)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421657", "createdAt": "2020-08-05T01:38:18Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTg4MA==", "bodyText": "Is there any reason this wouldn't just be window.start + timeDifferenceMs?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421880", "createdAt": "2020-08-05T01:39:08Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            // to catch the current record's right window, if it exists, without more calls to the store\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long endTime = next.key.window().end();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyNDM0Ng==", "bodyText": "Same here, let's not pin the start time to 0 for now and just drop the early records", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465424346", "createdAt": "2020-08-05T01:48:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            // to catch the current record's right window, if it exists, without more calls to the store\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyNTA5MA==", "bodyText": "Yeah especially since we use the same condition for both the forward and reverse case, let's just pull the rightWinAgg != null && rightWinAgg.timestamp() > timestamp out into a separate method", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465425090", "createdAt": "2020-08-05T01:50:32Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            // to catch the current record's right window, if it exists, without more calls to the store\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 299}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cca939e7b54f60cbbf0fc8abdd0f94fbbe19f45", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/9cca939e7b54f60cbbf0fc8abdd0f94fbbe19f45", "committedDate": "2020-08-05T14:20:20Z", "message": "sophie's reviews"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5a0d4b4a60f55d2ee7f412486ed8b567d493e25", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/e5a0d4b4a60f55d2ee7f412486ed8b567d493e25", "committedDate": "2020-08-05T15:37:05Z", "message": "cleaning up pr"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0afd88e97047c97e91e6d6273eb909e76eb782f4", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/0afd88e97047c97e91e6d6273eb909e76eb782f4", "committedDate": "2020-08-05T18:54:34Z", "message": "Merge branch 'trunk' of github.com:apache/kafka into slidingwindows"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24d91d8c33d3d3660f548623d6bcc7ef727d0862", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/24d91d8c33d3d3660f548623d6bcc7ef727d0862", "committedDate": "2020-08-05T19:43:05Z", "message": "updated tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65231139fb27f8e380d8bf6552a31c09f5fd28ff", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/65231139fb27f8e380d8bf6552a31c09f5fd28ff", "committedDate": "2020-08-07T15:17:24Z", "message": "grouped k stream and suppression tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/34b3f5a2f6dee16a514698f39187eddcfab134bc", "committedDate": "2020-08-07T20:41:17Z", "message": "removing reverse iterator, to be implemented later"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/34b3f5a2f6dee16a514698f39187eddcfab134bc", "committedDate": "2020-08-07T20:41:17Z", "message": "removing reverse iterator, to be implemented later"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY0NjI0MDYw", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-464624060", "createdAt": "2020-08-10T22:13:38Z", "commit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMjoxNjo0OFrOG-hrLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwMjo0Mzo0MVrOG-maJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNjYyMA==", "bodyText": "Comment on the reverse case left behind", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468216620", "createdAt": "2020-08-10T22:16:48Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNzM1OA==", "bodyText": "#9157", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468217358", "createdAt": "2020-08-10T22:18:45Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA=="}, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNzQzNw==", "bodyText": "nit: extra line breaks", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468217437", "createdAt": "2020-08-10T22:19:00Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNzk4Nw==", "bodyText": "You should check to make sure all of these are still needed. In particular I bet we can get rid of the CogroupedStreamAggregateBuilder suppression once your cleanup PR is merged and this one is rebased", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468217987", "createdAt": "2020-08-10T22:20:29Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -163,6 +163,12 @@\n     <suppress checks=\"(FinalLocalVariable|UnnecessaryParentheses|BooleanExpressionComplexity|CyclomaticComplexity|WhitespaceAfter|LocalVariableName)\"\n               files=\"Murmur3.java\"/>\n \n+    <suppress checks=\"(NPathComplexity|MethodLength|CyclomaticComplexity)\"\n+              files=\"KStreamSlidingWindowAggregate.java\"/>\n+\n+    <suppress checks=\"(CyclomaticComplexity)\"\n+              files=\"CogroupedStreamAggregateBuilder.java\"/>\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxODU0MA==", "bodyText": "I think we usually leave the arguments on the same line as the method declaration (even if that line ends up way too long)", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468218540", "createdAt": "2020-08-10T22:22:02Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxOTExMw==", "bodyText": "Kind of hard to tell, but is the alignment in this method a bit off?  Might be good to just highlight and auto-indent everything, intellij will take care of any issues if it's configured properly", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468219113", "createdAt": "2020-08-10T22:23:34Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MDA2OQ==", "bodyText": "nit: extra spaces after the ->", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468290069", "createdAt": "2020-08-11T02:28:07Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+                groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .count()\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .reduce(MockReducer.STRING_ADDER)\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.with(Serdes.String(), Serdes.String()))\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+                Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () ->    windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 324}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MTU2Mw==", "bodyText": "The input is the same for each test so the output is too, right? Maybe we can we pull all the output verification into a single method", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468291563", "createdAt": "2020-08-11T02:34:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+                groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .count()\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .reduce(MockReducer.STRING_ADDER)\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.with(Serdes.String(), Serdes.String()))\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+                Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 311}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MjcxMQ==", "bodyText": "Can we add some tests to verify the other Materialized properties, specifically the retention? You can just pick a single operator (eg reduce) and write a test to make sure data is available (only) within the retention period.\nAlso, do you think we can write a test to verify that the default retention is as expected when we don't specify it?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468292711", "createdAt": "2020-08-11T02:38:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+                groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .count()\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .reduce(MockReducer.STRING_ADDER)\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.with(Serdes.String(), Serdes.String()))\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+                Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MzI0NA==", "bodyText": "This comment needs to be updated, looks like we do allow a grace period of zero in the code/tests", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468293244", "createdAt": "2020-08-11T02:40:05Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding windows are defined by a record's timestamp, with window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and a given window grace period. While the window is sliding over the input data stream, a new window is\n+ * created each time a record enters the sliding window or a record drops out of the sliding window.\n+ * <p>\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * <ul>\n+ *     <li>window {@code [3000;8000]} contains [1] (created when first record enters the window)</li>\n+ *     <li>window {@code [4200;9200]} contains [1,2] (created when second record enters the window)</li>\n+ *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n+ *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n+ *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>\n+ * </ul>\n+ *<p>\n+ * Note that while SlidingWindows are of a fixed size, as are {@link TimeWindows}, the start and end points of the window\n+ * depend on when events occur in the stream (i.e., event timestamps), similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(SlidingWindows)\n+ * @see CogroupedKStream#windowedBy(SlidingWindows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period. Reject out-of-order events that arrive after {@code grace}.\n+     * A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size is < 0 or grace <= 0, or either can't be represented as {@code long milliseconds}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MzkzMA==", "bodyText": "assertThrows \ud83d\ude42", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468293930", "createdAt": "2020-08-11T02:42:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImplTest.java", "diffHunk": "@@ -145,6 +146,11 @@ public void shouldNotHaveNullWindowOnWindowedBySession() {\n         cogroupedStream.windowedBy((SessionWindows) null);\n     }\n \n+    @Test(expected = NullPointerException.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5NDE4MQ==", "bodyText": "Awesome, thanks for cleaning up some of these older tests \ud83d\ude04", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468294181", "createdAt": "2020-08-11T02:43:41Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImplTest.java", "diffHunk": "@@ -77,82 +80,212 @@ public void before() {\n         groupedStream = stream.groupByKey(Grouped.with(Serdes.String(), Serdes.String()));\n     }\n \n-    @Test(expected = NullPointerException.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1071098d77ebda24f4a5791ca449efbb0cdabd48", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/1071098d77ebda24f4a5791ca449efbb0cdabd48", "committedDate": "2020-08-12T15:54:25Z", "message": "sophie's comments and testing updates"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "1071098d77ebda24f4a5791ca449efbb0cdabd48", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/1071098d77ebda24f4a5791ca449efbb0cdabd48", "committedDate": "2020-08-12T15:54:25Z", "message": "sophie's comments and testing updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "824f8702114bff71944a2004cf8eadbd7144c785", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/824f8702114bff71944a2004cf8eadbd7144c785", "committedDate": "2020-08-17T14:38:32Z", "message": "trunk updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/1163c2faa84d3bb05c178ce67ecb047a92b9a054", "committedDate": "2020-08-17T14:44:58Z", "message": "cogrouped builder updates"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4ODU5Njc0", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-468859674", "createdAt": "2020-08-17T21:57:26Z", "commit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMTo1NzoyNlrOHB8Org==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMzoyMzozNVrOHB-Dqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5NzQyMg==", "bodyText": "Do we still need this one after the cleanup you did?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471797422", "createdAt": "2020-08-17T21:57:26Z", "author": {"login": "ableegoldman"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -167,6 +167,9 @@\n     <suppress checks=\"(FinalLocalVariable|UnnecessaryParentheses|BooleanExpressionComplexity|CyclomaticComplexity|WhitespaceAfter|LocalVariableName)\"\n               files=\"Murmur3.java\"/>\n \n+    <suppress checks=\"(CyclomaticComplexity)\"\n+              files=\"CogroupedStreamAggregateBuilder.java\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5ODI1Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        final HashSet<Long> windowStartTimes = new HashSet<Long>();\n          \n          \n            \n                        final Set<Long> windowStartTimes = new HashSet<>();\n          \n      \n    \n    \n  \n\nAlso I think this set is pretty clearly named, so we probably don't need a comment for it", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471798256", "createdAt": "2020-08-17T21:59:31Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5ODgwMQ==", "bodyText": "nit: can we use the full word Window in method names at least", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471798801", "createdAt": "2020-08-17T22:00:50Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long startTime = next.key.window().start();\n+                    final long endTime = startTime + windows.timeDifferenceMs();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWinAlreadyCreated && rightWinIsNotEmpty(rightWinAgg, timestamp)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        private boolean rightWinIsNotEmpty(final ValueAndTimestamp<Agg> rightWinAgg, final long timestamp) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5OTY5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void ShouldDropWindowsOutsideOfRetention() {\n          \n          \n            \n                public void shouldDropWindowsOutsideOfRetention() {", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471799698", "createdAt": "2020-08-17T22:03:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 390}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwMTA4Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);\n          \n          \n            \n                    final WindowBytesStoreSupplier storeSupplier = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471801086", "createdAt": "2020-08-17T22:06:28Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {\n+        final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 391}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwMTc0Mw==", "bodyText": "nit: you could use the version of fetch that just takes a single key instead of a key range, since there's only one key here", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471801743", "createdAt": "2020-08-17T22:08:09Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {\n+        final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String>as(storeSupplies)\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String())\n+                .withCachingDisabled());\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(TOPIC, new StringSerializer(), new StringSerializer());\n+\n+            inputTopic.pipeInput(\"1\", \"2\", 100L);\n+            inputTopic.pipeInput(\"1\", \"3\", 500L);\n+            inputTopic.pipeInput(\"1\", \"4\", 1000L);\n+            inputTopic.pipeInput(\"1\", \"5\", 2000L);\n+\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"1\", ofEpochMilli(0), ofEpochMilli(10000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(900, 1000)), \"0+4\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(1900, 2000)), \"0+5\"))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 415}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyMDExNQ==", "bodyText": "Can we insert one that's like right on the border of the retention period? So if the streamtime at the end is 2,000 then the window cut off is 800 (or start time of 700), and verify that anything starting before 699 is gone and everything after that is there.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471820115", "createdAt": "2020-08-17T23:01:12Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {\n+        final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String>as(storeSupplies)\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String())\n+                .withCachingDisabled());\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(TOPIC, new StringSerializer(), new StringSerializer());\n+\n+            inputTopic.pipeInput(\"1\", \"2\", 100L);\n+            inputTopic.pipeInput(\"1\", \"3\", 500L);\n+            inputTopic.pipeInput(\"1\", \"4\", 1000L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 406}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyMzA2MQ==", "bodyText": "For readability, could we mark the final results for each window? We want to make sure all the intermediate results are as expected, but what we really care about is what we got in the end. It would just help to have the critical output easier to find and get oriented in the tests", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471823061", "createdAt": "2020-08-17T23:09:53Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"1\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"3\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 21);\n+            inputTopic1.pipeInput(\"C\", \"1\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // A@10 left window created when A@10 processed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNDIwOQ==", "bodyText": "It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471824209", "createdAt": "2020-08-17T23:13:31Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"2\", 12L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNTQ1Nw==", "bodyText": "I still don't exactly understand why we have a join test in the KStreamXXWindowAggregateTest, but thanks for adding it for sliding windows. I'm sure there was a good reason for it, probably long ago", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471825457", "createdAt": "2020-08-17T23:17:24Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"1\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"3\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 21);\n+            inputTopic1.pipeInput(\"C\", \"1\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+1\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+1\", 20),\n+                        // A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+1\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+1+1\", 22),\n+                        // A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+1+3\", 20),\n+                        // A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+1+3\", 20),\n+                        // A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+1+1+3\", 22),\n+                        // A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+3\", 15),\n+                        // A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+1+1\", 22),\n+\n+                        // B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+2\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+2+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+2+2+2\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2+1\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2+1\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+1\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+2+2+2+1\", 19),\n+                        // B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+1+2\", 25),\n+                        // B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+2\", 25),\n+                        // B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+2+1+2\", 25),\n+                        // B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+2+2+2+3\", 18),\n+                        // B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+2+2+2+1+3\", 19),\n+                        // B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2+1+3\", 19),\n+                        // B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2+1+3\", 19),\n+                        // B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+2+2+3\", 14),\n+\n+                        // C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+3\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4\", 15),\n+                        // C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+3+4\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4+1\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1\", 16),\n+                        // C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+3+4+1\", 16),\n+                        // C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4+1+1\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1+1\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+1\", 21),\n+                        // C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+3+4+1+1\", 21),\n+                        // C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1+1+1\", 23),\n+                        // C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+1+1\", 23),\n+                        // C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+1\", 23),\n+                        // C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+4+1+1+1\", 23),\n+\n+                        // D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 222}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNzM3MQ==", "bodyText": "Sorry that I only just got to looking through this class \ud83d\ude1e . The tests here look good but can we add some more test coverage of possible edge cases? I know we can't test early records until the next PR, but we should probably have more than just the one test of the core functionality.\nI know it's really annoying to have to think through all the intermediate output, so maybe you can write a helper method that just grabs the final result of each window in the output? Then we could have a number of tests that go through a larger number of input records without you having to spend all day manually processing them yourself \ud83d\ude04", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471827371", "createdAt": "2020-08-17T23:23:35Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 70}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "296a6c3035f7eda71729d5cec7e16778383b7fad", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/296a6c3035f7eda71729d5cec7e16778383b7fad", "committedDate": "2020-08-19T18:12:59Z", "message": "test updates and clean up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyODE2MTEz", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-472816113", "createdAt": "2020-08-21T20:59:10Z", "commit": {"oid": "296a6c3035f7eda71729d5cec7e16778383b7fad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMDo1OToxMFrOHE9pwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMDo1OToxMFrOHE9pwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk2NjQ2NQ==", "bodyText": "Can we actually wrap the whole testProcessorRandomInput test in the try-catch? Or at least, everything after the initial setup? Would be nice to have the seed in case something weird happens during the processing itself", "url": "https://github.com/apache/kafka/pull/9039#discussion_r474966465", "createdAt": "2020-08-21T20:59:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,579 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+\n+        final List<ValueAndTimestamp<String>> input = Arrays.asList(\n+            ValueAndTimestamp.make(\"A\", 10L),\n+            ValueAndTimestamp.make(\"A\", 15L),\n+            ValueAndTimestamp.make(\"A\", 16L),\n+            ValueAndTimestamp.make(\"A\", 18L),\n+            ValueAndTimestamp.make(\"A\", 30L),\n+            ValueAndTimestamp.make(\"A\", 40L),\n+            ValueAndTimestamp.make(\"A\", 55L),\n+            ValueAndTimestamp.make(\"A\", 56L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 62L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 76L),\n+            ValueAndTimestamp.make(\"A\", 77L),\n+            ValueAndTimestamp.make(\"A\", 80L)\n+        );\n+\n+        final long seed = new Random().nextLong();\n+        final Random shuffle = new Random(seed);\n+        Collections.shuffle(input, shuffle);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            for (int i = 0; i < input.size(); i++) {\n+                inputTopic1.pipeInput(\"A\", input.get(i).value(), input.get(i).timestamp());\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> results = new HashMap<>();\n+\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (results.putIfAbsent(start, valueAndTimestamp) != null) {\n+                results.replace(start, valueAndTimestamp);\n+            }\n+        }\n+        randomEqualityCheck(results, seed);\n+    }\n+\n+    private void randomEqualityCheck(final Map<Long, ValueAndTimestamp<String>> actual, final Long seed) {\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+A\", 10L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+A+A+A\", 18L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+A+A\", 15L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+A+A\", 18L));\n+        expected.put(6L, ValueAndTimestamp.make(\"0+A+A+A\", 16L));\n+        expected.put(17L, ValueAndTimestamp.make(\"0+A\", 18L));\n+        expected.put(8L, ValueAndTimestamp.make(\"0+A+A+A+A\", 18L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+A\", 30L));\n+        expected.put(31L, ValueAndTimestamp.make(\"0+A\", 40L));\n+        expected.put(30L, ValueAndTimestamp.make(\"0+A+A\", 40L));\n+        expected.put(45L, ValueAndTimestamp.make(\"0+A\", 55L));\n+        expected.put(56L, ValueAndTimestamp.make(\"0+A+A+A+A+A+A+A\", 63L));\n+        expected.put(46L, ValueAndTimestamp.make(\"0+A+A\", 56L));\n+        expected.put(57L, ValueAndTimestamp.make(\"0+A+A+A+A+A+A\", 63L));\n+        expected.put(48L, ValueAndTimestamp.make(\"0+A+A+A+A\", 58L));\n+        expected.put(59L, ValueAndTimestamp.make(\"0+A+A+A+A\", 63L));\n+        expected.put(52L, ValueAndTimestamp.make(\"0+A+A+A+A+A\", 62L));\n+        expected.put(63L, ValueAndTimestamp.make(\"0+A+A+A\", 63L));\n+        expected.put(53L, ValueAndTimestamp.make(\"0+A+A+A+A+A+A+A+A\", 63L));\n+        expected.put(66L, ValueAndTimestamp.make(\"0+A\", 76L));\n+        expected.put(77L, ValueAndTimestamp.make(\"0+A+A\", 80L));\n+        expected.put(67L, ValueAndTimestamp.make(\"0+A+A\", 77L));\n+        expected.put(78L, ValueAndTimestamp.make(\"0+A\", 80L));\n+        expected.put(70L, ValueAndTimestamp.make(\"0+A+A+A\", 80L));\n+\n+        try {\n+            assertEquals(expected, actual);\n+        } catch (final AssertionError t) {\n+            throw new AssertionError(\n+                \"Assertion failed in randomized test. Reproduce with seed: \" + seed + \".\",\n+                t\n+            );\n+        } catch (final Throwable t) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296a6c3035f7eda71729d5cec7e16778383b7fad"}, "originalPosition": 517}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyODI4OTEz", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-472828913", "createdAt": "2020-08-21T21:28:41Z", "commit": {"oid": "296a6c3035f7eda71729d5cec7e16778383b7fad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMToyODo0MlrOHE-WLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMToyODo0MlrOHE-WLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk3NzgzNg==", "bodyText": "Just a minor note, can we order the expected results by window timestamp?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r474977836", "createdAt": "2020-08-21T21:28:42Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,579 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+\n+        final List<ValueAndTimestamp<String>> input = Arrays.asList(\n+            ValueAndTimestamp.make(\"A\", 10L),\n+            ValueAndTimestamp.make(\"A\", 15L),\n+            ValueAndTimestamp.make(\"A\", 16L),\n+            ValueAndTimestamp.make(\"A\", 18L),\n+            ValueAndTimestamp.make(\"A\", 30L),\n+            ValueAndTimestamp.make(\"A\", 40L),\n+            ValueAndTimestamp.make(\"A\", 55L),\n+            ValueAndTimestamp.make(\"A\", 56L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 62L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 76L),\n+            ValueAndTimestamp.make(\"A\", 77L),\n+            ValueAndTimestamp.make(\"A\", 80L)\n+        );\n+\n+        final long seed = new Random().nextLong();\n+        final Random shuffle = new Random(seed);\n+        Collections.shuffle(input, shuffle);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            for (int i = 0; i < input.size(); i++) {\n+                inputTopic1.pipeInput(\"A\", input.get(i).value(), input.get(i).timestamp());\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> results = new HashMap<>();\n+\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (results.putIfAbsent(start, valueAndTimestamp) != null) {\n+                results.replace(start, valueAndTimestamp);\n+            }\n+        }\n+        randomEqualityCheck(results, seed);\n+    }\n+\n+    private void randomEqualityCheck(final Map<Long, ValueAndTimestamp<String>> actual, final Long seed) {\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+A\", 10L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296a6c3035f7eda71729d5cec7e16778383b7fad"}, "originalPosition": 485}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb609a89ec7543943d03a1939e61f8208847ce8e", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/bb609a89ec7543943d03a1939e61f8208847ce8e", "committedDate": "2020-08-24T17:07:55Z", "message": "randomized and small test improvements"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczODg3Njgw", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-473887680", "createdAt": "2020-08-24T22:02:05Z", "commit": {"oid": "bb609a89ec7543943d03a1939e61f8208847ce8e"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQyMjowMjowNVrOHF30NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQyMjoxMzoxOVrOHF4FTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkxOTQxMw==", "bodyText": "nit: can you call this something a bit more direct, eg verifyRandomTestResults ?", "url": "https://github.com/apache/kafka/pull/9039#discussion_r475919413", "createdAt": "2020-08-24T22:02:05Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,690 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                () -> \"\",\n+                (key, value, aggregate) -> {\n+                    aggregate += value;\n+                    final char[] ch = aggregate.toCharArray();\n+                    Arrays.sort(ch);\n+                    aggregate = String.valueOf(ch);\n+                    return aggregate;\n+                },\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        final long seed = new Random().nextLong();\n+        final Random shuffle = new Random(seed);\n+\n+        try {\n+\n+            final List<ValueAndTimestamp<String>> input = Arrays.asList(\n+                ValueAndTimestamp.make(\"A\", 10L),\n+                ValueAndTimestamp.make(\"B\", 15L),\n+                ValueAndTimestamp.make(\"C\", 16L),\n+                ValueAndTimestamp.make(\"D\", 18L),\n+                ValueAndTimestamp.make(\"E\", 30L),\n+                ValueAndTimestamp.make(\"F\", 40L),\n+                ValueAndTimestamp.make(\"G\", 55L),\n+                ValueAndTimestamp.make(\"H\", 56L),\n+                ValueAndTimestamp.make(\"I\", 58L),\n+                ValueAndTimestamp.make(\"J\", 58L),\n+                ValueAndTimestamp.make(\"K\", 62L),\n+                ValueAndTimestamp.make(\"L\", 63L),\n+                ValueAndTimestamp.make(\"M\", 63L),\n+                ValueAndTimestamp.make(\"N\", 63L),\n+                ValueAndTimestamp.make(\"O\", 76L),\n+                ValueAndTimestamp.make(\"P\", 77L),\n+                ValueAndTimestamp.make(\"Q\", 80L)\n+            );\n+\n+            Collections.shuffle(input, shuffle);\n+            try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+                final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+                for (int i = 0; i < input.size(); i++) {\n+                    inputTopic1.pipeInput(\"A\", input.get(i).value(), input.get(i).timestamp());\n+                }\n+            }\n+\n+            final Map<Long, ValueAndTimestamp<String>> results = new HashMap<>();\n+\n+            for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+                final Windowed<String> window = (Windowed<String>) entry.key();\n+                final Long start = window.window().start();\n+                final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+                if (results.putIfAbsent(start, valueAndTimestamp) != null) {\n+                    results.replace(start, valueAndTimestamp);\n+                }\n+            }\n+            randomEqualityCheck(results, seed);\n+        } catch (final AssertionError t) {\n+            throw new AssertionError(\n+                \"Assertion failed in randomized test. Reproduce with seed: \" + seed + \".\",\n+                t\n+            );\n+        } catch (final Throwable t) {\n+            final StringBuilder sb =\n+                new StringBuilder()\n+                    .append(\"Exception in randomized scenario. Reproduce with seed: \")\n+                    .append(seed)\n+                    .append(\".\");\n+            throw new AssertionError(sb.toString(), t);\n+        }\n+    }\n+\n+    private void randomEqualityCheck(final Map<Long, ValueAndTimestamp<String>> actual, final Long seed) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb609a89ec7543943d03a1939e61f8208847ce8e"}, "originalPosition": 607}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkyMjc2MQ==", "bodyText": "nit: testAggregateRandomInput to match up with other test names", "url": "https://github.com/apache/kafka/pull/9039#discussion_r475922761", "createdAt": "2020-08-24T22:10:32Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,690 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb609a89ec7543943d03a1939e61f8208847ce8e"}, "originalPosition": 525}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkyMzc4OA==", "bodyText": "Can you leave a brief comment here explaining why we're doing something slightly more complicated in the aggregator for this test", "url": "https://github.com/apache/kafka/pull/9039#discussion_r475923788", "createdAt": "2020-08-24T22:13:19Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,690 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                () -> \"\",\n+                (key, value, aggregate) -> {\n+                    aggregate += value;\n+                    final char[] ch = aggregate.toCharArray();\n+                    Arrays.sort(ch);\n+                    aggregate = String.valueOf(ch);\n+                    return aggregate;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb609a89ec7543943d03a1939e61f8208847ce8e"}, "originalPosition": 541}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/64d4cbbdec80580a91c13a57e4d091efebf749d7", "committedDate": "2020-08-26T16:30:45Z", "message": "testing clean up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2ODUzNzE0", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-476853714", "createdAt": "2020-08-27T15:45:49Z", "commit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxNTo0NTo0OVrOHIWd6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMTozMjo1MlrOHIiBHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODUxODc2MA==", "bodyText": "I'm reviewing this whole PR as-is, so there's no need to do anything now, but @mjsax 's specific suggestion is beside the point. The general feedback is that this PR is too large, which it is. We shoot for under 1K, and it's the PR author's responsibility to figure out the best way to break it up.\nThis policy isn't just \"reviewers complaining,\" it's an important component of ensuring AK's quality. Long PRs overwhelm any reviewer's cognitive capacity to pay attention to every detail, so oversights are more likely to slip through into the codebase, and once they're there, you're really at the mercy of the testing layers to catch them. When the oversights are very subtle, they wind up getting released and then surface as user-reported bugs. Reviewers can't guarantee to notice every problem, but our capacity to notice problems is inversely proportional to the length of the PR.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478518760", "createdAt": "2020-08-27T15:45:49Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java", "diffHunk": "@@ -275,6 +275,15 @@\n      */\n     <W extends Window> TimeWindowedCogroupedKStream<K, VOut> windowedBy(final Windows<W> windows);\n \n+    /**\n+     * Create a new {@link TimeWindowedCogroupedKStream} instance that can be used to perform sliding\n+     * windowed aggregations.\n+     *\n+     * @param windows the specification of the aggregation {@link SlidingWindows}\n+     * @return an instance of {@link TimeWindowedCogroupedKStream}\n+     */\n+    TimeWindowedCogroupedKStream<K, VOut> windowedBy(final SlidingWindows windows);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ=="}, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU4ODYwMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n          \n          \n            \n             *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n          \n          \n            \n             *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>\n          \n          \n            \n             *     <li>window {@code [7400;12400]} contains [1,2,3] (created when third record enters the window)</li>\n          \n          \n            \n             *     <li>window {@code [8001;13001]} contains [2,3] (created when the first record drops out of the window)</li>\n          \n          \n            \n             *     <li>window {@code [9201;14201]} contains [3] (created when the second record drops out of the window)</li>", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478588602", "createdAt": "2020-08-27T17:41:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, the window size based on the given maximum time difference (inclusive) between\n+ * records in the same window, and the given window grace period. While the window is sliding over the input data stream, a new window is\n+ * created each time a record enters the sliding window or a record drops out of the sliding window.\n+ * <p>\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * <ul>\n+ *     <li>window {@code [3000;8000]} contains [1] (created when first record enters the window)</li>\n+ *     <li>window {@code [4200;9200]} contains [1,2] (created when second record enters the window)</li>\n+ *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n+ *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n+ *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYxNjU3MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n          \n          \n            \n                    final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"grace\");", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478616571", "createdAt": "2020-08-27T18:32:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, the window size based on the given maximum time difference (inclusive) between\n+ * records in the same window, and the given window grace period. While the window is sliding over the input data stream, a new window is\n+ * created each time a record enters the sliding window or a record drops out of the sliding window.\n+ * <p>\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * <ul>\n+ *     <li>window {@code [3000;8000]} contains [1] (created when first record enters the window)</li>\n+ *     <li>window {@code [4200;9200]} contains [1,2] (created when second record enters the window)</li>\n+ *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n+ *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n+ *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>\n+ * </ul>\n+ *<p>\n+ * Note that while SlidingWindows are of a fixed size, as are {@link TimeWindows}, the start and end points of the window\n+ * depend on when events occur in the stream (i.e., event timestamps), similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(SlidingWindows)\n+ * @see CogroupedKStream#windowedBy(SlidingWindows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period. Reject out-of-order events that arrive after {@code grace}.\n+     * A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size is < 0 or grace < 0, or either can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs < 0) {\n+            throw new IllegalArgumentException(\"Window time difference must not be negative.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYxNzE0Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        (Aggregator<? super K, ? super Object, VOut>) aggregator);\n          \n          \n            \n                                      (Aggregator<? super K, ? super Object, VOut>) aggregator);", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478617147", "createdAt": "2020-08-27T18:33:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java", "diffHunk": "@@ -61,7 +62,7 @@\n         Objects.requireNonNull(groupedStream, \"groupedStream can't be null\");\n         Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n         groupPatterns.put((KGroupedStreamImpl<K, ?>) groupedStream,\n-                          (Aggregator<? super K, ? super Object, VOut>) aggregator);\n+            (Aggregator<? super K, ? super Object, VOut>) aggregator);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyNzgwMA==", "bodyText": "Is this condition supposed to be checking whether records are \"early\" with respect to now? It looks like it should be:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (timestamp < windows.timeDifferenceMs()) {\n          \n          \n            \n                        if (timestamp < (observedStreamTime - windows.timeDifferenceMs())) {", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478627800", "createdAt": "2020-08-27T18:53:31Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA=="}, "originalCommit": {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2NDE0OQ==", "bodyText": "minor: this could be declared final at the assignment on line 161", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478664149", "createdAt": "2020-08-27T20:03:13Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final Set<Long> windowStartTimes = new HashSet<>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2OTk4Ng==", "bodyText": "Might not be a bad idea to have an assertion here that the timestamp is actually in the window boundaries.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478669986", "createdAt": "2020-08-27T20:14:32Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final Set<Long> windowStartTimes = new HashSet<>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long startTime = next.key.window().start();\n+                    final long endTime = startTime + windows.timeDifferenceMs();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWinAlreadyCreated && rightWindowIsNotEmpty(rightWinAgg, timestamp)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        private boolean rightWindowIsNotEmpty(final ValueAndTimestamp<Agg> rightWinAgg, final long timestamp) {\n+            return rightWinAgg != null && rightWinAgg.timestamp() > timestamp;\n+        }\n+\n+        private boolean isLeftWindow(final KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> window) {\n+            return window.key.window().end() == window.value.timestamp();\n+        }\n+\n+        private void putAndForward(final Window window,\n+                                   final ValueAndTimestamp<Agg> valueAndTime,\n+                                   final K key,\n+                                   final V value,\n+                                   final long closeTime,\n+                                   final long timestamp) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY3MTE5MA==", "bodyText": "Is it already guaranteed that this window actually contains the current record? It doesn't look like we're checking that endTime >= timestamp anywhere, and it seems like the start of the range (timestamp - 2 * windows.timeDifferenceMs()) could give back a window that starts and ends before the current record's timestamp.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478671190", "createdAt": "2020-08-27T20:16:55Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final Set<Long> windowStartTimes = new HashSet<>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long startTime = next.key.window().start();\n+                    final long endTime = startTime + windows.timeDifferenceMs();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwMjUxNw==", "bodyText": "The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478702517", "createdAt": "2020-08-27T21:20:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"1\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"3\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 21);\n+            inputTopic1.pipeInput(\"C\", \"1\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+1\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+1\", 20),\n+                        // A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+1\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+1+1\", 22),\n+                        // A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+1+3\", 20),\n+                        // A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+1+3\", 20),\n+                        // A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+1+1+3\", 22),\n+                        // A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+3\", 15),\n+                        // A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+1+1\", 22),\n+\n+                        // B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+2\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+2+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+2+2+2\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2+1\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2+1\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+1\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+2+2+2+1\", 19),\n+                        // B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+1+2\", 25),\n+                        // B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+2\", 25),\n+                        // B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+2+1+2\", 25),\n+                        // B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+2+2+2+3\", 18),\n+                        // B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+2+2+2+1+3\", 19),\n+                        // B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2+1+3\", 19),\n+                        // B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2+1+3\", 19),\n+                        // B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+2+2+3\", 14),\n+\n+                        // C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+3\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4\", 15),\n+                        // C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+3+4\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4+1\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1\", 16),\n+                        // C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+3+4+1\", 16),\n+                        // C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4+1+1\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1+1\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+1\", 21),\n+                        // C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+3+4+1+1\", 21),\n+                        // C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1+1+1\", 23),\n+                        // C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+1+1\", 23),\n+                        // C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+1\", 23),\n+                        // C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+4+1+1+1\", 23),\n+\n+                        // D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNTQ1Nw=="}, "originalCommit": {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054"}, "originalPosition": 222}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwMzQ1Mg==", "bodyText": "I'd normally say we should have a test also to verify we log properly on early records, but you already opened the PR to add early record handling, so we're good.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478703452", "createdAt": "2020-08-27T21:22:51Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,692 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 443}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwNDM1OQ==", "bodyText": "Awesome test. Thanks!", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478704359", "createdAt": "2020-08-27T21:24:45Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,692 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateRandomInput() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 525}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwNTk0Mg==", "bodyText": "Aside from join, forgetting to test new operators in front of Suppress has also been an issue. It's great to see this test here!", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478705942", "createdAt": "2020-08-27T21:28:18Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressScenarioTest.java", "diffHunk": "@@ -459,6 +460,89 @@ public void shouldSupportFinalResultsForTimeWindowsWithLargeJump() {\n         }\n     }\n \n+    @Test\n+    public void shouldSupportFinalResultsForSlidingWindows() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwNzk5Nw==", "bodyText": "Coming back to this after completing the review, I'd say the biggest advice I'd share is to avoid whitespace changes and cleanups on the side when the PR is so long already. In fact, for my own super-complex PRs, I tend to go back over the whole diff and back out anything that's not critically important, just to lighten the load on the reviewers.\nCleanups are nice to have, but it's better to keep them in their own PRs or in more trivial ones.", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478707997", "createdAt": "2020-08-27T21:32:52Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java", "diffHunk": "@@ -275,6 +275,15 @@\n      */\n     <W extends Window> TimeWindowedCogroupedKStream<K, VOut> windowedBy(final Windows<W> windows);\n \n+    /**\n+     * Create a new {@link TimeWindowedCogroupedKStream} instance that can be used to perform sliding\n+     * windowed aggregations.\n+     *\n+     * @param windows the specification of the aggregation {@link SlidingWindows}\n+     * @return an instance of {@link TimeWindowedCogroupedKStream}\n+     */\n+    TimeWindowedCogroupedKStream<K, VOut> windowedBy(final SlidingWindows windows);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ=="}, "originalCommit": {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10"}, "originalPosition": 11}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48328f651602f2078add6f780bb42199cd2c0316", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/48328f651602f2078add6f780bb42199cd2c0316", "committedDate": "2020-08-28T15:04:32Z", "message": "udpates wtih john's comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fdfb1cde038e3750bb6d8c3900558bacc18feabc", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/fdfb1cde038e3750bb6d8c3900558bacc18feabc", "committedDate": "2020-08-28T15:42:10Z", "message": "udpates wtih john's comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ce06f9321e8c74eeb0c80daaf1acb0ca70f8bd8", "author": {"user": {"login": "lct45", "name": "leah"}}, "url": "https://github.com/apache/kafka/commit/1ce06f9321e8c74eeb0c80daaf1acb0ca70f8bd8", "committedDate": "2020-08-28T15:43:54Z", "message": "Merge branch 'slidingwindows' of github.com:lct45/kafka into slidingwindows"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c367ff37e0805bf0e9a318dc96e918c09377e5e", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/1c367ff37e0805bf0e9a318dc96e918c09377e5e", "committedDate": "2020-08-31T21:06:15Z", "message": "Merge remote-tracking branch 'apache/trunk' into pull/9039"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MDAwODYw", "url": "https://github.com/apache/kafka/pull/9039#pullrequestreview-479000860", "createdAt": "2020-08-31T22:19:32Z", "commit": {"oid": "de97db6cf39eb34eab0207f3cc45a5085317b2a4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de97db6cf39eb34eab0207f3cc45a5085317b2a4", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/de97db6cf39eb34eab0207f3cc45a5085317b2a4", "committedDate": "2020-08-31T22:19:50Z", "message": "fix conflict with trunk"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1332, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}