{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ5MDU3NzEz", "number": 9022, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOTowMTo0MFrOEOZCeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMjoyNzo0MlrOEO5HXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNTI1NzU0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOTowMTo0MFrOGxhN6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQxNTo0Mjo0NFrOG3FmZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3NzY0MQ==", "bodyText": "Note that although we're bumping this to 1000, it should be a single produce batch with really small data, so on the order of a couple KBs.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454577641", "createdAt": "2020-07-14T19:01:40Z", "author": {"login": "bdbyrne"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4OTA2Mw==", "bodyText": "With the throttle approach we don't want it to be produced in a single batch. If all messages are produced in a single batch then you will still fetch the entire batch in a single request even if replica.fetch.max.bytes is 1, as we allow this limit to be broken to fetch at least one message.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454589063", "createdAt": "2020-07-14T19:22:33Z", "author": {"login": "lbradstreet"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3NzY0MQ=="}, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU5ODkxMQ==", "bodyText": "So each message creates its own ProducerRecord, I was under the assumption that the limit to fetch expands to a single record, and not the batch itself. Or am I misunderstanding?", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454598911", "createdAt": "2020-07-14T19:41:08Z", "author": {"login": "bdbyrne"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3NzY0MQ=="}, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzODIxNQ==", "bodyText": "Each message creates its own ProducerRecord but these get collected into batches by the producer. Fetch requests will not break up a batch unless down-converting to the old record batch format, so you could end up with 1000 messages in a single batch being fetched in a single fetch request.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454638215", "createdAt": "2020-07-14T20:53:23Z", "author": {"login": "lbradstreet"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3NzY0MQ=="}, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQxNjYxNA==", "bodyText": "The broker only works in terms of record batches, not individual records.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r460416614", "createdAt": "2020-07-25T15:42:44Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3NzY0MQ=="}, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNTMxNjQ4OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOToxODozNlrOGxhyIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOTo0MToxMlrOGxihKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NjkxMg==", "bodyText": "It might be worth adding a comment somewhere in testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress to describe what was done in the replicaFetchMaxBytes def and why.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454586912", "createdAt": "2020-07-14T19:18:36Z", "author": {"login": "lbradstreet"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU5ODk1Mg==", "bodyText": "Done.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454598952", "createdAt": "2020-07-14T19:41:12Z", "author": {"login": "bdbyrne"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NjkxMg=="}, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNTMxOTkzOnYy", "diffSide": "LEFT", "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOToxOTozNlrOGxh0Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQxNTo0Mjo0MlrOG3FmYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NzQ2Mg==", "bodyText": "Why did we remove this? We still need the throttle to be set so that the replica doesn't join the ISR. It could easily do this even if it only fetches a message at a time. The idea with the new replica fetch max bytes setting is to give the broker a chance to throttle itself before joining the ISR.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454587462", "createdAt": "2020-07-14T19:19:36Z", "author": {"login": "lbradstreet"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)\n \n     val brokerIds = servers.map(_.config.brokerId)\n-    TestUtils.setReplicationThrottleForPartitions(adminClient, brokerIds, Set(tp), throttleBytes = 1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMDU3NA==", "bodyText": "Added back, but to clarify - are you suggesting the high watermark could still be at 0 (or relatively low) even after producing the messages? Or just that both combined are a more effective throttle, essentially putting it at 1 record/sec?", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454600574", "createdAt": "2020-07-14T19:44:18Z", "author": {"login": "bdbyrne"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)\n \n     val brokerIds = servers.map(_.config.brokerId)\n-    TestUtils.setReplicationThrottleForPartitions(adminClient, brokerIds, Set(tp), throttleBytes = 1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NzQ2Mg=="}, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQxNjYxMQ==", "bodyText": "I simply meant that we still need the replication throttle so the reassignment doesn't complete before we perform our checks.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r460416611", "createdAt": "2020-07-25T15:42:42Z", "author": {"login": "lbradstreet"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -672,11 +677,9 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     adminClient.createTopics(\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n-    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 1000, acks = -1)\n \n     val brokerIds = servers.map(_.config.brokerId)\n-    TestUtils.setReplicationThrottleForPartitions(adminClient, brokerIds, Set(tp), throttleBytes = 1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NzQ2Mg=="}, "originalCommit": {"oid": "8f38528b988a287c4142f87d42bfd62f5b428449"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNTY2Mjk3OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQyMDo1OTowMFrOGxlGhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQxNTo0ODo1MlrOG3Fodw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MTI4NQ==", "bodyText": "Do we want to leave a comment here to say that we are trying to ensure that messages are produced in multiple batches? I do wonder if there's a better way to achieve this via a producer setting, e.g. batch.size?", "url": "https://github.com/apache/kafka/pull/9022#discussion_r454641285", "createdAt": "2020-07-14T20:59:00Z", "author": {"login": "lbradstreet"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -673,10 +678,14 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n     TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    Thread.sleep(10)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92499138238e0e6b219a9f5af22b736ab704ca56"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQxNjc5Nw==", "bodyText": "We can call flush on the producer to force it to send the messages.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r460416797", "createdAt": "2020-07-25T15:45:02Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -673,10 +678,14 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n     TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    Thread.sleep(10)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MTI4NQ=="}, "originalCommit": {"oid": "92499138238e0e6b219a9f5af22b736ab704ca56"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQxNzE0Mw==", "bodyText": "Looking a bit more, we actually close the producer after we generate the messages, so I don't think this sleep is needed at all.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r460417143", "createdAt": "2020-07-25T15:48:52Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -673,10 +678,14 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n       Collections.singletonList(new NewTopic(testTopicName, partitions, replicationFactor).configs(configMap))).all().get()\n     waitForTopicCreated(testTopicName)\n     TestUtils.generateAndProduceMessages(servers, testTopicName, numMessages = 10, acks = -1)\n+    Thread.sleep(10)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MTI4NQ=="}, "originalCommit": {"oid": "92499138238e0e6b219a9f5af22b736ab704ca56"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0MDUxMjkyOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMjoyNzo0MlrOGyTeZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxODo0NjoyOFrOGy37pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQwMTA2MA==", "bodyText": "It's always an annoyance with Junit that there is no way for the test case to override initialization in a @Before. What we often end up doing is removing the annotation and calling the setup method explicitly in each test case. I slightly prefer that option since it is easier to understand, but not sure it is possible since we're extending KafkaServerTestHarness, which has its own initialization logic. For the sake of argument, would it be possible to set max fetch bytes to 1 for all tests? Either that or maybe we should just produce more data in the test case.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r455401060", "createdAt": "2020-07-15T22:27:42Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -56,12 +56,17 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     zkConnect = zkConnect,\n     rackInfo = Map(0 -> \"rack1\", 1 -> \"rack2\", 2 -> \"rack2\", 3 -> \"rack1\", 4 -> \"rack3\", 5 -> \"rack3\"),\n     numPartitions = numPartitions,\n-    defaultReplicationFactor = defaultReplicationFactor\n+    defaultReplicationFactor = defaultReplicationFactor,\n+    replicaFetchMaxBytes = replicaFetchMaxBytes(),\n   ).map(KafkaConfig.fromProps)\n \n   private val numPartitions = 1\n   private val defaultReplicationFactor = 1.toShort\n \n+  private def replicaFetchMaxBytes() =\n+    if (testName.getMethodName == \"testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress\") Some(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f28cc408a9544a72ca872b9cc934472f87de4ef"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk5ODM3NA==", "bodyText": "Agreed, the KafkaServerTestHarness makes it more difficult. I've updated the test to set max fetch bytes to 1 for all tests, which is fine given none of the other tests produce data.", "url": "https://github.com/apache/kafka/pull/9022#discussion_r455998374", "createdAt": "2020-07-16T18:46:28Z", "author": {"login": "bdbyrne"}, "path": "core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala", "diffHunk": "@@ -56,12 +56,17 @@ class TopicCommandWithAdminClientTest extends KafkaServerTestHarness with Loggin\n     zkConnect = zkConnect,\n     rackInfo = Map(0 -> \"rack1\", 1 -> \"rack2\", 2 -> \"rack2\", 3 -> \"rack1\", 4 -> \"rack3\", 5 -> \"rack3\"),\n     numPartitions = numPartitions,\n-    defaultReplicationFactor = defaultReplicationFactor\n+    defaultReplicationFactor = defaultReplicationFactor,\n+    replicaFetchMaxBytes = replicaFetchMaxBytes(),\n   ).map(KafkaConfig.fromProps)\n \n   private val numPartitions = 1\n   private val defaultReplicationFactor = 1.toShort\n \n+  private def replicaFetchMaxBytes() =\n+    if (testName.getMethodName == \"testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress\") Some(1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQwMTA2MA=="}, "originalCommit": {"oid": "5f28cc408a9544a72ca872b9cc934472f87de4ef"}, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2256, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}