{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzODgxNjE1", "number": 8223, "title": "KAFKA-9654 ReplicaAlterLogDirsThread can't be created again if the pr\u2026", "bodyText": "ReplicaManager does create ReplicaAlterLogDirsThread only if an new future log is created. If the previous ReplicaAlterLogDirsThread encounters error when moving data, the target partition is moved to \"failedPartitions\" and ReplicaAlterLogDirsThread get idle due to empty partitions. The future log is still existent so we CAN'T either create another ReplicaAlterLogDirsThread to handle the parition or update the paritions of the idler ReplicaAlterLogDirsThread.\nReplicaManager should call ReplicaAlterLogDirsManager#addFetcherForPartitions even if there is already a future log since we can create an new ReplicaAlterLogDirsThread to handle the new partitions or update the partitions of existent ReplicaAlterLogDirsThread to make it busy again.\nhttps://issues.apache.org/jira/browse/KAFKA-9654\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-03-04T21:11:55Z", "url": "https://github.com/apache/kafka/pull/8223", "merged": true, "mergeCommit": {"oid": "c27f629e953025d726f722c15bd18c78f5f6cb66"}, "closed": true, "closedAt": "2020-03-19T23:49:36Z", "author": {"login": "chia7712"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcKdogegFqTM2OTEyODc2OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcQ5tcpAFqTM4MDcwNjk2Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MTI4NzY5", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-369128769", "createdAt": "2020-03-04T21:16:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMToxNjo0OVrOFx97hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMToxNjo0OVrOFx97hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzkzOTIwNw==", "bodyText": "the tests in ReassignPartitionsCommandTest cover this PR. However, ReassignPartitionsCommandTest is still not stable even if this PR fixes the ReplicaAlterLogDirsThread since ReassignPartitionsCommand does not have correct \"wait\" for replica folder change.\nInstead of checking return value of Admin#alterReplicaLogDirs, this PR get the replica folder via Admin#describeLogDirs and make sure the target folder is existent and isFuture = false", "url": "https://github.com/apache/kafka/pull/8223#discussion_r387939207", "createdAt": "2020-03-04T21:16:49Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -634,18 +634,43 @@ class ReassignPartitionsCommand(zkClient: KafkaZkClient,\n         // Create reassignment znode so that controller will send LeaderAndIsrRequest to create replica in the broker\n         zkClient.createPartitionReassignment(validPartitions.map({case (key, value) => (new TopicPartition(key.topic, key.partition), value)}).toMap)\n \n-        // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n-        // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n-        var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n-        val replicasAssignedToFutureDir = mutable.Set.empty[TopicPartitionReplica]\n-        while (remainingTimeMs > 0 && replicasAssignedToFutureDir.size < proposedReplicaAssignment.size) {\n-          replicasAssignedToFutureDir ++= alterReplicaLogDirsIgnoreReplicaNotAvailable(\n-            proposedReplicaAssignment.filter { case (replica, _) => !replicasAssignedToFutureDir.contains(replica) },\n-            adminClientOpt.get, remainingTimeMs)\n-          Thread.sleep(100)\n-          remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+        if (proposedReplicaAssignment.nonEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5OTYzMDU0", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-369963054", "createdAt": "2020-03-05T22:33:43Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQyMjozMzo0M1rOFymo_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQyMjozMzo0M1rOFymo_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODYwNjIwNg==", "bodyText": "Hmm, not sure about this. A given topic partition is always hashed into the same ReplicaAlterLogDirsThread. So, if that thread is dead, adding the same partition to the same thread won't help.\nDo you know why the ReplicaAlterLogDirsThread died? If it's a bug there, we probably should fix the logic in ReplicaAlterLogDirsThread.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r388606206", "createdAt": "2020-03-05T22:33:43Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -278,7 +278,7 @@ class Partition(val topicPartition: TopicPartition,\n             if (futureLogDir != logDir)\n               throw new IllegalStateException(s\"The future log dir $futureLogDir of $topicPartition is \" +\n                 s\"different from the requested log dir $logDir\")\n-            false\n+            true", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcwOTQ1Nzk3", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-370945797", "createdAt": "2020-03-09T06:55:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwNjo1NTo0OFrOFzcbJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwNjo1NTo0OFrOFzcbJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ4NzM5Ng==", "bodyText": "Moving the replica folder to another location has chance to produce this issue so I separate the origin test case to two cases. The first case always move the folder to same location. Another case does move the folder to different location.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r389487396", "createdAt": "2020-03-09T06:55:48Z", "author": {"login": "chia7712"}, "path": "core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala", "diffHunk": "@@ -161,19 +161,31 @@ class ReassignPartitionsClusterTest extends ZooKeeperTestHarness with Logging {\n   }\n \n   @Test\n-  def shouldMoveSinglePartitionWithinBroker(): Unit = {\n+  def shouldMoveSinglePartitionToSameFolderWithinBroker(): Unit = shouldMoveSinglePartitionWithinBroker(true)\n+\n+  @Test\n+  def shouldMoveSinglePartitionToDifferentFolderWithinBroker(): Unit = shouldMoveSinglePartitionWithinBroker(false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczODgwOTU5", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-373880959", "createdAt": "2020-03-12T20:32:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMDozMjo1M1rOF1uYBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMDozMjo1M1rOF1uYBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg3ODY2MA==", "bodyText": "For the normal fetcher, we force the replica into a truncating phase after each epoch change. I'm trying to convince myself whether we need to do this for the log dir fetcher or not. Although we have similar log reconciliation logic implemented for the log dir fetcher, it seems we only exercise it when the log dir fetcher is first initialized for a partition. Outside of that, we have an asynchronous truncation path which gets executed after the current log is truncated upon becoming follower. If we think this is sufficient to guarantee consistency, then I'm wondering whether the epoch validation when the log dir fetcher is fetching against the active log is actually buying us anything. An alternative would be that we leave the epoch uninitialized which would cause us to skip epoch validation.\n@junrao Any thoughts?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r391878660", "createdAt": "2020-03-12T20:32:53Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1297,7 +1299,10 @@ class ReplicaManager(val config: KafkaConfig,\n             }\n           }\n         }\n+        // handle the new partition\n         replicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset)\n+        // handle the partitions having new epoch\n+        replicaAlterLogDirsManager.updateEpoch(updatedPartitions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0Mzk3MDYy", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-374397062", "createdAt": "2020-03-13T15:31:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNTozMTo1OFrOF2ILvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNTo1MjozNlrOF2I7bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwMTUwMQ==", "bodyText": "I'm not sure about this. If we know the partition has been fenced, why retry until the epoch has been updated? Alternatively, we could let it be marked failed and remove it when the epoch gets updated. The downside of the approach here is that a partition which is in a permanently failed state due to a stale epoch will not get reflected in the metric.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392301501", "createdAt": "2020-03-13T15:31:58Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -255,7 +255,7 @@ abstract class AbstractFetcherThread(name: String,\n             fetchOffsets.put(tp, offsetTruncationState)\n \n         case Errors.FENCED_LEADER_EPOCH =>\n-          onPartitionFenced(tp)\n+          partitionsWithError += onPartitionFenced(tp)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwODg1MQ==", "bodyText": "Would it be reasonable to pull this change into a separate PR? I think the fix in the log dir fetcher is more important and a more isolated change will be easier to backport to older versions.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392308851", "createdAt": "2020-03-13T15:44:22Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -634,18 +634,43 @@ class ReassignPartitionsCommand(zkClient: KafkaZkClient,\n         // Create reassignment znode so that controller will send LeaderAndIsrRequest to create replica in the broker\n         zkClient.createPartitionReassignment(validPartitions.map({case (key, value) => (new TopicPartition(key.topic, key.partition), value)}).toMap)\n \n-        // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n-        // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n-        var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n-        val replicasAssignedToFutureDir = mutable.Set.empty[TopicPartitionReplica]\n-        while (remainingTimeMs > 0 && replicasAssignedToFutureDir.size < proposedReplicaAssignment.size) {\n-          replicasAssignedToFutureDir ++= alterReplicaLogDirsIgnoreReplicaNotAvailable(\n-            proposedReplicaAssignment.filter { case (replica, _) => !replicasAssignedToFutureDir.contains(replica) },\n-            adminClientOpt.get, remainingTimeMs)\n-          Thread.sleep(100)\n-          remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+        if (proposedReplicaAssignment.nonEmpty) {\n+          // Send AlterReplicaLogDirsRequest again to make sure broker will start to move replica to the specified log directory.\n+          // It may take some time for controller to create replica in the broker. Retry if the replica has not been created.\n+          var remainingTimeMs = startTimeMs + timeoutMs - System.currentTimeMillis()\n+          def nonFutureReplicas() = adminClientOpt.get", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMxMzcxMA==", "bodyText": "I gave this a bit more thought. I think it would be safer to have the log dir fetcher mimic the regular fetcher when it comes to the truncation protocol. That said, I don't think we need to address this here. The fix in this patch addresses the main problem at the moment.", "url": "https://github.com/apache/kafka/pull/8223#discussion_r392313710", "createdAt": "2020-03-13T15:52:36Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1297,7 +1299,10 @@ class ReplicaManager(val config: KafkaConfig,\n             }\n           }\n         }\n+        // handle the new partition\n         replicaAlterLogDirsManager.addFetcherForPartitions(futureReplicasAndInitialOffset)\n+        // handle the partitions having new epoch\n+        replicaAlterLogDirsManager.updateEpoch(updatedPartitions)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg3ODY2MA=="}, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MTQ4ODA1", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-377148805", "createdAt": "2020-03-18T18:54:33Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxODo1NDozM1rOF4Ss6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxOTowODozN1rOF4TK6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3MDk4Ng==", "bodyText": "Could this be requestEpoch.contains(currentLeaderEpoch)? If we got this error and we didn't provide an epoch, then it suggests an error on the leader. Maybe retrying is best in that case?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394570986", "createdAt": "2020-03-18T18:54:33Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -266,12 +271,22 @@ abstract class AbstractFetcherThread(name: String,\n     ResultWithPartitions(fetchOffsets, partitionsWithError)\n   }\n \n-  private def onPartitionFenced(tp: TopicPartition): Unit = inLock(partitionMapLock) {\n-    Option(partitionStates.stateValue(tp)).foreach { currentFetchState =>\n+  /**\n+   * remove the partition if the partition state is NOT updated. Otherwise, keep the partition active.\n+   * @return true if the epoch is updated\n+   */\n+  private def onPartitionFenced(tp: TopicPartition, requestEpoch: Option[Int]): Boolean = inLock(partitionMapLock) {\n+    Option(partitionStates.stateValue(tp)).exists { currentFetchState =>\n       val currentLeaderEpoch = currentFetchState.currentLeaderEpoch\n-      info(s\"Partition $tp has an older epoch ($currentLeaderEpoch) than the current leader. Will await \" +\n-        s\"the new LeaderAndIsr state before resuming fetching.\")\n-      markPartitionFailed(tp)\n+      if (requestEpoch.forall(_ == currentLeaderEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3NTgxNA==", "bodyText": "Shouldn't we return the result of onPartitionFenced?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394575814", "createdAt": "2020-03-18T19:03:13Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -531,7 +548,7 @@ abstract class AbstractFetcherThread(name: String,\n       true\n     } catch {\n       case _: FencedLeaderEpochException =>\n-        onPartitionFenced(topicPartition)\n+        onPartitionFenced(topicPartition, requestEpoch)\n         true", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3ODY2NQ==", "bodyText": "Should this be an assertion? Is the test not deterministic for some reason?", "url": "https://github.com/apache/kafka/pull/8223#discussion_r394578665", "createdAt": "2020-03-18T19:08:37Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,56 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")\n+      // the partition is added to failedPartitions if fenced error happens\n+      if (replicaManager.replicaAlterLogDirsManager.failedPartitions.size != 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/5134b2ddacb42642b8bbdf91b6df8417be731e4a", "committedDate": "2020-03-19T05:41:46Z", "message": "KAFKA-9654 ReplicaAlterLogDirsThread can't be created again if the previous ReplicaAlterLogDirsThreadmeet encounters leader epoch error"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/5134b2ddacb42642b8bbdf91b6df8417be731e4a", "committedDate": "2020-03-19T05:41:46Z", "message": "KAFKA-9654 ReplicaAlterLogDirsThread can't be created again if the previous ReplicaAlterLogDirsThreadmeet encounters leader epoch error"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4MTYxMzM3", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-378161337", "createdAt": "2020-03-19T23:31:18Z", "commit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwNzA2OTYy", "url": "https://github.com/apache/kafka/pull/8223#pullrequestreview-380706962", "createdAt": "2020-03-24T21:23:05Z", "commit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQyMToyMzowNVrOF7DnYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQyMToyMzowNVrOF7DnYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ2OTUzNw==", "bodyText": "@chia7712 In our IC we are consistently getting a failure in this check. Do you have any suggestion on what is happening and how to fix it?\nError Message\norg.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending state\n\nStacktrace\norg.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending state\n\tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)\n\tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)\n\tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)\n\tat org.scalatest.Assertions.fail(Assertions.scala:1091)\n\tat org.scalatest.Assertions.fail$(Assertions.scala:1087)\n\tat org.scalatest.Assertions$.fail(Assertions.scala:1389)\n\tat kafka.server.ReplicaManagerTest.testFencedErrorCausedByBecomeLeader(ReplicaManagerTest.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\n\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\n\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)\n\tat org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)\n\tat sun.reflect.GeneratedMethodAccessor112.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat com.sun.proxy.$Proxy2.processTestClass(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)\n\tat sun.reflect.GeneratedMethodAccessor111.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)\n\tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)\n\tat org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)\n\tat java.lang.Thread.run(Thread.java:748)\n\nStandard Output\n[2020-03-24 17:39:19,377] ERROR [ReplicaAlterLogDirsThread-0]: Error due to (kafka.server.ReplicaAlterLogDirsThread:76)\norg.apache.kafka.common.errors.ReplicaNotAvailableException: Future log for partition test-topic-0 is not available on broker 0\n[2020-03-24 17:39:38,537] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-topic-0 (kafka.server.ReplicaManager:76)\norg.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producerId 234 at offset 3 in partition test-topic-0: 13 (incoming seq. number), 2 (current end sequence number)", "url": "https://github.com/apache/kafka/pull/8223#discussion_r397469537", "createdAt": "2020-03-24T21:23:05Z", "author": {"login": "jsancio"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -209,6 +209,58 @@ class ReplicaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testFencedErrorCausedByBecomeLeader(): Unit = {\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(new MockTimer)\n+    try {\n+      val brokerList = Seq[Integer](0, 1).asJava\n+      val topicPartition = new TopicPartition(topic, 0)\n+      replicaManager.createPartition(topicPartition)\n+        .createLogIfNotExists(0, isNew = false, isFutureReplica = false,\n+        new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints))\n+\n+      def leaderAndIsrRequest(epoch: Int): LeaderAndIsrRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+        Seq(new LeaderAndIsrPartitionState()\n+          .setTopicName(topic)\n+          .setPartitionIndex(0)\n+          .setControllerEpoch(0)\n+          .setLeader(0)\n+          .setLeaderEpoch(epoch)\n+          .setIsr(brokerList)\n+          .setZkVersion(0)\n+          .setReplicas(brokerList)\n+          .setIsNew(true)).asJava,\n+        Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(0), (_, _) => ())\n+      val partition = replicaManager.getPartitionOrException(new TopicPartition(topic, 0), expectLeader = true)\n+        .localLogOrException\n+      assertEquals(1, replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).size)\n+\n+      // find the live and different folder\n+      val newReplicaFolder = replicaManager.logManager.liveLogDirs.filterNot(_ == partition.dir.getParentFile).head\n+      assertEquals(0, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      replicaManager.alterReplicaLogDirs(Map(topicPartition -> newReplicaFolder.getAbsolutePath))\n+      replicaManager.futureLocalLogOrException(topicPartition)\n+      assertEquals(1, replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.size)\n+      // change the epoch from 0 to 1 in order to make fenced error\n+      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())\n+      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),\n+        s\"the partition=$topicPartition should be removed from pending state\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5134b2ddacb42642b8bbdf91b6df8417be731e4a"}, "originalPosition": 41}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 389, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}