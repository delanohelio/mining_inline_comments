{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyMTA3NzE2", "number": 8053, "title": "KAFKA-9499; Improve deletion process by batching more aggressively", "bodyText": "This PR speeds up the deletion process by doing the following:\n\nBatch whenever possible to minimize the number of requests sent out to other brokers;\nRefactor onPartitionDeletion to remove the usage of allLiveReplicas.\n\nThe tests covers the code which has been updated thus I haven't extended the tests in this area.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-02-06T20:55:31Z", "url": "https://github.com/apache/kafka/pull/8053", "merged": true, "mergeCommit": {"oid": "2cbd3d7519fd9cc6a7e74c20cd058cc833952c90"}, "closed": true, "closedAt": "2020-02-12T19:46:55Z", "author": {"login": "dajac"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcBwylVAH2gAyMzcyMTA3NzE2OjVmZTE5MDNmZmMxNmNiYmM3NmFmY2QxZTU1NTc1NzZlZDQ0MTA1NDQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcDpuueAFqTM1NzY0MzI5Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "5fe1903ffc16cbbc76afcd1e5557576ed4410544", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/5fe1903ffc16cbbc76afcd1e5557576ed4410544", "committedDate": "2020-02-06T20:30:42Z", "message": "KAFKA-9499; Improve deletion process by batching more aggressively"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0NzkwMDU3", "url": "https://github.com/apache/kafka/pull/8053#pullrequestreview-354790057", "createdAt": "2020-02-06T21:37:57Z", "commit": {"oid": "5fe1903ffc16cbbc76afcd1e5557576ed4410544"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozNzo1N1rOFmrFhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTo1Mjo1N1rOFmrfvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5NjEzNA==", "bodyText": "nit: not from this patch, but can we document or remove this @param", "url": "https://github.com/apache/kafka/pull/8053#discussion_r376096134", "createdAt": "2020-02-06T21:37:57Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -292,22 +291,35 @@ class TopicDeletionManager(config: KafkaConfig,\n    * @param replicasForTopicsToBeDeleted", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5fe1903ffc16cbbc76afcd1e5557576ed4410544"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjEwMDgyNA==", "bodyText": "In this function, we start with a list of topics and convert it to a flattened list of partitions. Ultimately in startReplicaDeletion, we end up regrouping by topic. I wonder if there is an opportunity to save a little work by avoiding the intermediate conversions.\nBy the way, because we are doing more batching now, this may be a good time to reduce or remove the info log line in onPartitionDeletion which can already be a bit noisy.", "url": "https://github.com/apache/kafka/pull/8053#discussion_r376100824", "createdAt": "2020-02-06T21:48:24Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -270,9 +271,7 @@ class TopicDeletionManager(config: KafkaConfig,\n     }\n \n     client.sendMetadataUpdate(partitions)\n-    topics.foreach { topic =>\n-      onPartitionDeletion(controllerContext.partitionsForTopic(topic))\n-    }\n+    onPartitionDeletion(partitions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5fe1903ffc16cbbc76afcd1e5557576ed4410544"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjEwMjg0Nw==", "bodyText": "Not sure it matters too much, but because of the way these are collected, I don't think they need to be sets. We could use ListBuffer instead, which is already a Seq.", "url": "https://github.com/apache/kafka/pull/8053#discussion_r376102847", "createdAt": "2020-02-06T21:52:57Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -292,22 +291,35 @@ class TopicDeletionManager(config: KafkaConfig,\n    * @param replicasForTopicsToBeDeleted\n    */\n   private def startReplicaDeletion(replicasForTopicsToBeDeleted: Set[PartitionAndReplica]): Unit = {\n-    replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach { topic =>\n-      val aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p => p.topic == topic)\n-      val deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic\n+    val allDeadReplicasForTopic = mutable.Set.empty[PartitionAndReplica]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5fe1903ffc16cbbc76afcd1e5557576ed4410544"}, "originalPosition": 26}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d8f86d33a2d1593d63cd30380185c32899dff22", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/9d8f86d33a2d1593d63cd30380185c32899dff22", "committedDate": "2020-02-07T09:13:48Z", "message": "Address Jason's comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79e218ff74e32bd86961238a3f2d9dfc84847bfc", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/79e218ff74e32bd86961238a3f2d9dfc84847bfc", "committedDate": "2020-02-07T21:42:09Z", "message": "more refactoring"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93202700b6cad9f9fa0fb7ed2d5bb2f444b2685c", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/93202700b6cad9f9fa0fb7ed2d5bb2f444b2685c", "committedDate": "2020-02-07T23:01:21Z", "message": "more bits"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1NDk0OTg5", "url": "https://github.com/apache/kafka/pull/8053#pullrequestreview-355494989", "createdAt": "2020-02-07T23:28:14Z", "commit": {"oid": "93202700b6cad9f9fa0fb7ed2d5bb2f444b2685c"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QyMzoyODoxNVrOFnNKew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wOFQwMDowMjoxOFrOFnNnhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY1NDQ1OQ==", "bodyText": "nit: leftover from debugging i assume", "url": "https://github.com/apache/kafka/pull/8053#discussion_r376654459", "createdAt": "2020-02-07T23:28:15Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -257,78 +260,81 @@ class TopicDeletionManager(config: KafkaConfig,\n    */\n   private def onTopicDeletion(topics: Set[String]): Unit = {\n     info(s\"Topic deletion callback for ${topics.mkString(\",\")}\")\n-    // send update metadata so that brokers stop serving data for topics to be deleted\n-    val partitions = topics.flatMap(controllerContext.partitionsForTopic)\n+    val partitions = mutable.Set.empty[TopicPartition]\n+    val mapBuilder = Map.newBuilder[String, Set[TopicPartition]]\n+    topics.foreach { topic =>\n+      val topicPartitions = controllerContext.partitionsForTopic(topic)\n+      mapBuilder += topic -> topicPartitions\n+      partitions ++= topicPartitions\n+    }\n+    val partitionsByTopic = mapBuilder.result\n+\n     val unseenTopicsForDeletion = topics -- controllerContext.topicsWithDeletionStarted\n     if (unseenTopicsForDeletion.nonEmpty) {\n-      val unseenPartitionsForDeletion = unseenTopicsForDeletion.flatMap(controllerContext.partitionsForTopic)\n+      val unseenPartitionsForDeletion = unseenTopicsForDeletion.flatMap(partitionsByTopic)\n       partitionStateMachine.handleStateChanges(unseenPartitionsForDeletion.toSeq, OfflinePartition)\n       partitionStateMachine.handleStateChanges(unseenPartitionsForDeletion.toSeq, NonExistentPartition)\n       // adding of unseenTopicsForDeletion to topics with deletion started must be done after the partition\n       // state changes to make sure the offlinePartitionCount metric is properly updated\n       controllerContext.beginTopicDeletion(unseenTopicsForDeletion)\n     }\n \n+    // send update metadata so that brokers stop serving data for topics to be deleted\n     client.sendMetadataUpdate(partitions)\n-    topics.foreach { topic =>\n-      onPartitionDeletion(controllerContext.partitionsForTopic(topic))\n-    }\n+\n+    onPartitionDeletion(partitionsByTopic)\n   }\n \n   /**\n-   * Invoked by onPartitionDeletion. It is the 2nd step of topic deletion, the first being sending\n-   * UpdateMetadata requests to all brokers to start rejecting requests for deleted topics. As part of starting deletion,\n-   * the topics are added to the in progress list. As long as a topic is in the in progress list, deletion for that topic\n-   * is never retried. A topic is removed from the in progress list when\n-   * 1. Either the topic is successfully deleted OR\n-   * 2. No replica for the topic is in ReplicaDeletionStarted state and at least one replica is in ReplicaDeletionIneligible state\n-   * If the topic is queued for deletion but deletion is not currently under progress, then deletion is retried for that topic\n-   * As part of starting deletion, all replicas are moved to the ReplicaDeletionStarted state where the controller sends\n-   * the replicas a StopReplicaRequest (delete=true)\n-   * This method does the following things -\n+   * Invoked by onTopicDeletion with the list of partitions for topics to be deleted\n+   * It does the following -\n    * 1. Move all dead replicas directly to ReplicaDeletionIneligible state. Also mark the respective topics ineligible\n    *    for deletion if some replicas are dead since it won't complete successfully anyway\n-   * 2. Move all alive replicas to ReplicaDeletionStarted state so they can be deleted successfully\n-   * @param replicasForTopicsToBeDeleted\n+   * 2. Move all replicas for the partitions to OfflineReplica state. This will send StopReplicaRequest to the replicas\n+   *    and LeaderAndIsrRequest to the leader with the shrunk ISR. When the leader replica itself is moved to OfflineReplica state,\n+   *    it will skip sending the LeaderAndIsrRequest since the leader will be updated to -1\n+   * 3. Move all replicas to ReplicaDeletionStarted state. This will send StopReplicaRequest with deletePartition=true. And\n+   *    will delete all persistent data from all replicas of the respective partitions\n    */\n-  private def startReplicaDeletion(replicasForTopicsToBeDeleted: Set[PartitionAndReplica]): Unit = {\n-    replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach { topic =>\n-      val aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p => p.topic == topic)\n-      val deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic\n+  private def onPartitionDeletion(partitionsToBeDeleted: Map[String, Set[TopicPartition]]): Unit = {\n+    val allDeadReplicasForTopic = mutable.ListBuffer.empty[PartitionAndReplica]\n+    val allReplicasForDeletionRetry = mutable.ListBuffer.empty[PartitionAndReplica]\n+    val allTopicsIneligibleForDeletion = mutable.Set.empty[String]\n+\n+    partitionsToBeDeleted.foreach { case (topic, partitions) =>\n+      val replicasForTopicToBeDeleted = controllerContext.replicasForPartition(partitions)\n+      val aliveReplicasForTopic = controllerContext.liveReplicasForTopic(topic)\n+      val deadReplicasForTopic = replicasForTopicToBeDeleted -- aliveReplicasForTopic\n       val successfullyDeletedReplicas = controllerContext.replicasInState(topic, ReplicaDeletionSuccessful)\n       val replicasForDeletionRetry = aliveReplicasForTopic -- successfullyDeletedReplicas\n-      // move dead replicas directly to failed state\n-      replicaStateMachine.handleStateChanges(deadReplicasForTopic.toSeq, ReplicaDeletionIneligible)\n-      // send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader\n-      replicaStateMachine.handleStateChanges(replicasForDeletionRetry.toSeq, OfflineReplica)\n-      debug(s\"Deletion started for replicas ${replicasForDeletionRetry.mkString(\",\")}\")\n-      replicaStateMachine.handleStateChanges(replicasForDeletionRetry.toSeq, ReplicaDeletionStarted)\n+\n+      allDeadReplicasForTopic ++= deadReplicasForTopic\n+      allReplicasForDeletionRetry ++= replicasForDeletionRetry\n+\n       if (deadReplicasForTopic.nonEmpty) {\n         debug(s\"Dead Replicas (${deadReplicasForTopic.mkString(\",\")}) found for topic $topic\")\n-        markTopicIneligibleForDeletion(Set(topic), reason = \"offline replicas\")\n+        allTopicsIneligibleForDeletion += topic\n       }\n     }\n-  }\n \n-  /**\n-   * Invoked by onTopicDeletion with the list of partitions for topics to be deleted\n-   * It does the following -\n-   * 1. Send UpdateMetadataRequest to all live brokers (that are not shutting down) for partitions that are being\n-   *    deleted. The brokers start rejecting all client requests with UnknownTopicOrPartitionException\n-   * 2. Move all replicas for the partitions to OfflineReplica state. This will send StopReplicaRequest to the replicas\n-   *    and LeaderAndIsrRequest to the leader with the shrunk ISR. When the leader replica itself is moved to OfflineReplica state,\n-   *    it will skip sending the LeaderAndIsrRequest since the leader will be updated to -1\n-   * 3. Move all replicas to ReplicaDeletionStarted state. This will send StopReplicaRequest with deletePartition=true. And\n-   *    will delete all persistent data from all replicas of the respective partitions\n-   */\n-  private def onPartitionDeletion(partitionsToBeDeleted: Set[TopicPartition]): Unit = {\n-    info(s\"Partition deletion callback for ${partitionsToBeDeleted.mkString(\",\")}\")\n-    val replicasPerPartition = controllerContext.replicasForPartition(partitionsToBeDeleted)\n-    startReplicaDeletion(replicasPerPartition)\n+    // move dead replicas directly to failed state\n+    replicaStateMachine.handleStateChanges(allDeadReplicasForTopic, ReplicaDeletionIneligible)\n+    // send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader\n+    replicaStateMachine.handleStateChanges(allReplicasForDeletionRetry, OfflineReplica)\n+    debug(s\"Deletion started for replicas ${allReplicasForDeletionRetry.mkString(\",\")}\")\n+    replicaStateMachine.handleStateChanges(allReplicasForDeletionRetry, ReplicaDeletionStarted)\n+\n+    if (allTopicsIneligibleForDeletion.nonEmpty) {\n+      markTopicIneligibleForDeletion(allTopicsIneligibleForDeletion, reason = \"offline replicas\")\n+    }\n   }\n \n   private def resumeDeletions(): Unit = {\n+    println(\"resumeDeletions\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93202700b6cad9f9fa0fb7ed2d5bb2f444b2685c"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY2MDU2Nw==", "bodyText": "I think onTopicDeletion doesn't do any filtering on partitions that are passed through to this method. Would it be simpler to skip the construction of the map and pass through the set of topics? Then we could provide a convenient accessor in ControllerContext to access the PartitionAndReplica objects by topic.", "url": "https://github.com/apache/kafka/pull/8053#discussion_r376660567", "createdAt": "2020-02-07T23:55:50Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -257,78 +260,81 @@ class TopicDeletionManager(config: KafkaConfig,\n    */\n   private def onTopicDeletion(topics: Set[String]): Unit = {\n     info(s\"Topic deletion callback for ${topics.mkString(\",\")}\")\n-    // send update metadata so that brokers stop serving data for topics to be deleted\n-    val partitions = topics.flatMap(controllerContext.partitionsForTopic)\n+    val partitions = mutable.Set.empty[TopicPartition]\n+    val mapBuilder = Map.newBuilder[String, Set[TopicPartition]]\n+    topics.foreach { topic =>\n+      val topicPartitions = controllerContext.partitionsForTopic(topic)\n+      mapBuilder += topic -> topicPartitions\n+      partitions ++= topicPartitions\n+    }\n+    val partitionsByTopic = mapBuilder.result\n+\n     val unseenTopicsForDeletion = topics -- controllerContext.topicsWithDeletionStarted\n     if (unseenTopicsForDeletion.nonEmpty) {\n-      val unseenPartitionsForDeletion = unseenTopicsForDeletion.flatMap(controllerContext.partitionsForTopic)\n+      val unseenPartitionsForDeletion = unseenTopicsForDeletion.flatMap(partitionsByTopic)\n       partitionStateMachine.handleStateChanges(unseenPartitionsForDeletion.toSeq, OfflinePartition)\n       partitionStateMachine.handleStateChanges(unseenPartitionsForDeletion.toSeq, NonExistentPartition)\n       // adding of unseenTopicsForDeletion to topics with deletion started must be done after the partition\n       // state changes to make sure the offlinePartitionCount metric is properly updated\n       controllerContext.beginTopicDeletion(unseenTopicsForDeletion)\n     }\n \n+    // send update metadata so that brokers stop serving data for topics to be deleted\n     client.sendMetadataUpdate(partitions)\n-    topics.foreach { topic =>\n-      onPartitionDeletion(controllerContext.partitionsForTopic(topic))\n-    }\n+\n+    onPartitionDeletion(partitionsByTopic)\n   }\n \n   /**\n-   * Invoked by onPartitionDeletion. It is the 2nd step of topic deletion, the first being sending\n-   * UpdateMetadata requests to all brokers to start rejecting requests for deleted topics. As part of starting deletion,\n-   * the topics are added to the in progress list. As long as a topic is in the in progress list, deletion for that topic\n-   * is never retried. A topic is removed from the in progress list when\n-   * 1. Either the topic is successfully deleted OR\n-   * 2. No replica for the topic is in ReplicaDeletionStarted state and at least one replica is in ReplicaDeletionIneligible state\n-   * If the topic is queued for deletion but deletion is not currently under progress, then deletion is retried for that topic\n-   * As part of starting deletion, all replicas are moved to the ReplicaDeletionStarted state where the controller sends\n-   * the replicas a StopReplicaRequest (delete=true)\n-   * This method does the following things -\n+   * Invoked by onTopicDeletion with the list of partitions for topics to be deleted\n+   * It does the following -\n    * 1. Move all dead replicas directly to ReplicaDeletionIneligible state. Also mark the respective topics ineligible\n    *    for deletion if some replicas are dead since it won't complete successfully anyway\n-   * 2. Move all alive replicas to ReplicaDeletionStarted state so they can be deleted successfully\n-   * @param replicasForTopicsToBeDeleted\n+   * 2. Move all replicas for the partitions to OfflineReplica state. This will send StopReplicaRequest to the replicas\n+   *    and LeaderAndIsrRequest to the leader with the shrunk ISR. When the leader replica itself is moved to OfflineReplica state,\n+   *    it will skip sending the LeaderAndIsrRequest since the leader will be updated to -1\n+   * 3. Move all replicas to ReplicaDeletionStarted state. This will send StopReplicaRequest with deletePartition=true. And\n+   *    will delete all persistent data from all replicas of the respective partitions\n    */\n-  private def startReplicaDeletion(replicasForTopicsToBeDeleted: Set[PartitionAndReplica]): Unit = {\n-    replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach { topic =>\n-      val aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p => p.topic == topic)\n-      val deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic\n+  private def onPartitionDeletion(partitionsToBeDeleted: Map[String, Set[TopicPartition]]): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93202700b6cad9f9fa0fb7ed2d5bb2f444b2685c"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY2MTg5NQ==", "bodyText": "nit: remove this", "url": "https://github.com/apache/kafka/pull/8053#discussion_r376661895", "createdAt": "2020-02-08T00:02:18Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/controller/MockReplicaStateMachine.scala", "diffHunk": "@@ -17,10 +17,23 @@\n package kafka.controller\n \n import scala.collection.Seq\n+import scala.collection.mutable\n \n class MockReplicaStateMachine(controllerContext: ControllerContext) extends ReplicaStateMachine(controllerContext) {\n+  val stateChangesByTargetState = mutable.Map.empty[ReplicaState, Int].withDefaultValue(0)\n+\n+  def stateChangesCalls(targetState: ReplicaState): Int = {\n+    stateChangesByTargetState(targetState)\n+  }\n+\n+  def clear(): Unit = {\n+    stateChangesByTargetState.clear()\n+  }\n \n   override def handleStateChanges(replicas: Seq[PartitionAndReplica], targetState: ReplicaState): Unit = {\n+    println(replicas, targetState)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93202700b6cad9f9fa0fb7ed2d5bb2f444b2685c"}, "originalPosition": 18}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5bdc951a4b00e30a617ca6684d7476018f025427", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/5bdc951a4b00e30a617ca6684d7476018f025427", "committedDate": "2020-02-10T09:56:22Z", "message": "simplify"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2074b76f72a441e91f6f52ba339e1a5644abfdd", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/e2074b76f72a441e91f6f52ba339e1a5644abfdd", "committedDate": "2020-02-10T10:26:46Z", "message": "small refactor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47431e693f64e7d199248633b4deec47681dcb6d", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/47431e693f64e7d199248633b4deec47681dcb6d", "committedDate": "2020-02-10T10:36:59Z", "message": "fix log"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MjgyNTkz", "url": "https://github.com/apache/kafka/pull/8053#pullrequestreview-356282593", "createdAt": "2020-02-10T21:38:13Z", "commit": {"oid": "47431e693f64e7d199248633b4deec47681dcb6d"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMTozODoxM1rOFn2fVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMjo1NTo0NVrOFn8WVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzMTU0MA==", "bodyText": "nit: space before @param", "url": "https://github.com/apache/kafka/pull/8053#discussion_r377331540", "createdAt": "2020-02-10T21:38:13Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -226,12 +227,12 @@ class TopicDeletionManager(config: KafkaConfig,\n   /**\n    * If the topic is queued for deletion but deletion is not currently under progress, then deletion is retried for that topic\n    * To ensure a successful retry, reset states for respective replicas from ReplicaDeletionIneligible to OfflineReplica state\n-   *@param topic Topic for which deletion should be retried\n+   *@param topics Topics for which deletion should be retried", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47431e693f64e7d199248633b4deec47681dcb6d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQyNjQ5MQ==", "bodyText": "Since we're in here, I wonder if info is appropriate. Retry seems more like a debug?", "url": "https://github.com/apache/kafka/pull/8053#discussion_r377426491", "createdAt": "2020-02-11T02:49:57Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -226,12 +227,12 @@ class TopicDeletionManager(config: KafkaConfig,\n   /**\n    * If the topic is queued for deletion but deletion is not currently under progress, then deletion is retried for that topic\n    * To ensure a successful retry, reset states for respective replicas from ReplicaDeletionIneligible to OfflineReplica state\n-   *@param topic Topic for which deletion should be retried\n+   *@param topics Topics for which deletion should be retried\n    */\n-  private def retryDeletionForIneligibleReplicas(topic: String): Unit = {\n+  private def retryDeletionForIneligibleReplicas(topics: Set[String]): Unit = {\n     // reset replica states from ReplicaDeletionIneligible to OfflineReplica\n-    val failedReplicas = controllerContext.replicasInState(topic, ReplicaDeletionIneligible)\n-    info(s\"Retrying deletion of topic $topic since replicas ${failedReplicas.mkString(\",\")} were not successfully deleted\")\n+    val failedReplicas = topics.flatMap(controllerContext.replicasInState(_, ReplicaDeletionIneligible))\n+    info(s\"Retrying deletion of topics ${topics.mkString(\",\")} since replicas ${failedReplicas.mkString(\",\")} were not successfully deleted\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47431e693f64e7d199248633b4deec47681dcb6d"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQyNzU0MQ==", "bodyText": "Another message that could be quite large with a large batch of deletions. Do you think this message is useful at all? There are already log messages for the state transitions.", "url": "https://github.com/apache/kafka/pull/8053#discussion_r377427541", "createdAt": "2020-02-11T02:55:45Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -269,66 +268,62 @@ class TopicDeletionManager(config: KafkaConfig,\n       controllerContext.beginTopicDeletion(unseenTopicsForDeletion)\n     }\n \n-    client.sendMetadataUpdate(partitions)\n-    topics.foreach { topic =>\n-      onPartitionDeletion(controllerContext.partitionsForTopic(topic))\n-    }\n-  }\n+    // send update metadata so that brokers stop serving data for topics to be deleted\n+    client.sendMetadataUpdate(topics.flatMap(controllerContext.partitionsForTopic))\n \n-  /**\n-   * Invoked by onPartitionDeletion. It is the 2nd step of topic deletion, the first being sending\n-   * UpdateMetadata requests to all brokers to start rejecting requests for deleted topics. As part of starting deletion,\n-   * the topics are added to the in progress list. As long as a topic is in the in progress list, deletion for that topic\n-   * is never retried. A topic is removed from the in progress list when\n-   * 1. Either the topic is successfully deleted OR\n-   * 2. No replica for the topic is in ReplicaDeletionStarted state and at least one replica is in ReplicaDeletionIneligible state\n-   * If the topic is queued for deletion but deletion is not currently under progress, then deletion is retried for that topic\n-   * As part of starting deletion, all replicas are moved to the ReplicaDeletionStarted state where the controller sends\n-   * the replicas a StopReplicaRequest (delete=true)\n-   * This method does the following things -\n-   * 1. Move all dead replicas directly to ReplicaDeletionIneligible state. Also mark the respective topics ineligible\n-   *    for deletion if some replicas are dead since it won't complete successfully anyway\n-   * 2. Move all alive replicas to ReplicaDeletionStarted state so they can be deleted successfully\n-   * @param replicasForTopicsToBeDeleted\n-   */\n-  private def startReplicaDeletion(replicasForTopicsToBeDeleted: Set[PartitionAndReplica]): Unit = {\n-    replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach { topic =>\n-      val aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p => p.topic == topic)\n-      val deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic\n-      val successfullyDeletedReplicas = controllerContext.replicasInState(topic, ReplicaDeletionSuccessful)\n-      val replicasForDeletionRetry = aliveReplicasForTopic -- successfullyDeletedReplicas\n-      // move dead replicas directly to failed state\n-      replicaStateMachine.handleStateChanges(deadReplicasForTopic.toSeq, ReplicaDeletionIneligible)\n-      // send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader\n-      replicaStateMachine.handleStateChanges(replicasForDeletionRetry.toSeq, OfflineReplica)\n-      debug(s\"Deletion started for replicas ${replicasForDeletionRetry.mkString(\",\")}\")\n-      replicaStateMachine.handleStateChanges(replicasForDeletionRetry.toSeq, ReplicaDeletionStarted)\n-      if (deadReplicasForTopic.nonEmpty) {\n-        debug(s\"Dead Replicas (${deadReplicasForTopic.mkString(\",\")}) found for topic $topic\")\n-        markTopicIneligibleForDeletion(Set(topic), reason = \"offline replicas\")\n-      }\n-    }\n+    onPartitionDeletion(topics)\n   }\n \n   /**\n    * Invoked by onTopicDeletion with the list of partitions for topics to be deleted\n    * It does the following -\n-   * 1. Send UpdateMetadataRequest to all live brokers (that are not shutting down) for partitions that are being\n-   *    deleted. The brokers start rejecting all client requests with UnknownTopicOrPartitionException\n+   * 1. Move all dead replicas directly to ReplicaDeletionIneligible state. Also mark the respective topics ineligible\n+   *    for deletion if some replicas are dead since it won't complete successfully anyway\n    * 2. Move all replicas for the partitions to OfflineReplica state. This will send StopReplicaRequest to the replicas\n    *    and LeaderAndIsrRequest to the leader with the shrunk ISR. When the leader replica itself is moved to OfflineReplica state,\n    *    it will skip sending the LeaderAndIsrRequest since the leader will be updated to -1\n    * 3. Move all replicas to ReplicaDeletionStarted state. This will send StopReplicaRequest with deletePartition=true. And\n    *    will delete all persistent data from all replicas of the respective partitions\n    */\n-  private def onPartitionDeletion(partitionsToBeDeleted: Set[TopicPartition]): Unit = {\n-    info(s\"Partition deletion callback for ${partitionsToBeDeleted.mkString(\",\")}\")\n-    val replicasPerPartition = controllerContext.replicasForPartition(partitionsToBeDeleted)\n-    startReplicaDeletion(replicasPerPartition)\n+  private def onPartitionDeletion(topicsToBeDeleted: Set[String]): Unit = {\n+    val allDeadReplicas = mutable.ListBuffer.empty[PartitionAndReplica]\n+    val allReplicasForDeletionRetry = mutable.ListBuffer.empty[PartitionAndReplica]\n+    val allTopicsIneligibleForDeletion = mutable.Set.empty[String]\n+\n+    topicsToBeDeleted.foreach { topic =>\n+      val (aliveReplicas, deadReplicas) = controllerContext.replicasForTopic(topic).partition { r =>\n+        controllerContext.isReplicaOnline(r.replica, r.topicPartition)\n+      }\n+\n+      val successfullyDeletedReplicas = controllerContext.replicasInState(topic, ReplicaDeletionSuccessful)\n+      val replicasForDeletionRetry = aliveReplicas -- successfullyDeletedReplicas\n+\n+      allDeadReplicas ++= deadReplicas\n+      allReplicasForDeletionRetry ++= replicasForDeletionRetry\n+\n+      if (deadReplicas.nonEmpty) {\n+        debug(s\"Dead Replicas (${deadReplicas.mkString(\",\")}) found for topic $topic\")\n+        allTopicsIneligibleForDeletion += topic\n+      }\n+    }\n+\n+    // move dead replicas directly to failed state\n+    replicaStateMachine.handleStateChanges(allDeadReplicas, ReplicaDeletionIneligible)\n+    // send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader\n+    replicaStateMachine.handleStateChanges(allReplicasForDeletionRetry, OfflineReplica)\n+    debug(s\"Deletion started for replicas ${allReplicasForDeletionRetry.mkString(\",\")}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47431e693f64e7d199248633b4deec47681dcb6d"}, "originalPosition": 125}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4375e6c07c8bf446fd441dce79de047553a8946e", "author": {"user": {"login": "dajac", "name": "David Jacot"}}, "url": "https://github.com/apache/kafka/commit/4375e6c07c8bf446fd441dce79de047553a8946e", "committedDate": "2020-02-11T08:33:41Z", "message": "remove logs or reduce level"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2NDc5MjQx", "url": "https://github.com/apache/kafka/pull/8053#pullrequestreview-356479241", "createdAt": "2020-02-11T08:36:33Z", "commit": {"oid": "4375e6c07c8bf446fd441dce79de047553a8946e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODozNjozM1rOFoAj_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODozNjozM1rOFoAj_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5NjU3NA==", "bodyText": "FYI - I have removed this one as well because the information is redundant with the log in resumeDeletions.", "url": "https://github.com/apache/kafka/pull/8053#discussion_r377496574", "createdAt": "2020-02-11T08:36:33Z", "author": {"login": "dajac"}, "path": "core/src/main/scala/kafka/controller/TopicDeletionManager.scala", "diffHunk": "@@ -256,9 +257,6 @@ class TopicDeletionManager(config: KafkaConfig,\n    * removed from their caches.\n    */\n   private def onTopicDeletion(topics: Set[String]): Unit = {\n-    info(s\"Topic deletion callback for ${topics.mkString(\",\")}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4375e6c07c8bf446fd441dce79de047553a8946e"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3NjQzMjk3", "url": "https://github.com/apache/kafka/pull/8053#pullrequestreview-357643297", "createdAt": "2020-02-12T17:25:00Z", "commit": {"oid": "4375e6c07c8bf446fd441dce79de047553a8946e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1770, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}