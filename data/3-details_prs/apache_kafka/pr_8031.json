{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY5OTE5MDUw", "number": 8031, "title": "KAFKA-9447: Add new customized EOS model example", "bodyText": "With the improvement of 447, we are now offering developers a better experience on writing their customized EOS apps with group subscription, instead of manual assignments. With the demo, user should be able to get started more quickly on writing their own EOS app, and understand the processing logic much better.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-02-01T17:17:11Z", "url": "https://github.com/apache/kafka/pull/8031", "merged": true, "mergeCommit": {"oid": "9d17bf98b6eae2d5a91a9a719e5c447b55ded3a5"}, "closed": true, "closedAt": "2020-02-06T00:51:08Z", "author": {"login": "abbccdda"}, "timelineItems": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb9s-CxAH2gAyMzY5OTE5MDUwOmI0YTZhM2E3Mjg4N2MxOGJlZDFlM2NhZmJlZjViNjhmZDhkNDdjYTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcBu1CLAFqTM1NDY2NDkzMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b4a6a3a72887c18bed1e3cafbef5b68fd8d47ca5", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/b4a6a3a72887c18bed1e3cafbef5b68fd8d47ca5", "committedDate": "2020-01-25T05:47:54Z", "message": "add eos example"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0cf5868943fe5f4cde514d14b049d4b7a13a32b5", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/0cf5868943fe5f4cde514d14b049d4b7a13a32b5", "committedDate": "2020-01-31T21:39:52Z", "message": "extend producer demo with even number keys"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b038d7fa18760f50c87cd07f87336f9a90084037", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/b038d7fa18760f50c87cd07f87336f9a90084037", "committedDate": "2020-02-01T00:24:25Z", "message": "build entire java class with workflow"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfc1b93c24dc17687456ebfce5adee5282751aeb", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/bfc1b93c24dc17687456ebfce5adee5282751aeb", "committedDate": "2020-02-01T03:22:44Z", "message": "create topic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "239e3bb794c978d54ead2578f02c4fc5be7cdd91", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/239e3bb794c978d54ead2578f02c4fc5be7cdd91", "committedDate": "2020-02-01T05:28:00Z", "message": "Only topic clean-up remaining"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db292ccd153797c5f9442848eee345a95f1a10e9", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/db292ccd153797c5f9442848eee345a95f1a10e9", "committedDate": "2020-02-01T05:40:45Z", "message": "fix topic deletion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "392b7190313411c3949c4a707820088f3754175f", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/392b7190313411c3949c4a707820088f3754175f", "committedDate": "2020-02-01T17:14:51Z", "message": "fix group mode"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxODk0MjUw", "url": "https://github.com/apache/kafka/pull/8031#pullrequestreview-351894250", "createdAt": "2020-02-01T17:17:36Z", "commit": {"oid": "392b7190313411c3949c4a707820088f3754175f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzozNlrOFkeYjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzozNlrOFkeYjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc5MDg2Mw==", "bodyText": "side cleanup", "url": "https://github.com/apache/kafka/pull/8031#discussion_r373790863", "createdAt": "2020-02-01T17:17:36Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsResult.java", "diffHunk": "@@ -51,21 +51,18 @@\n      */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "392b7190313411c3949c4a707820088f3754175f"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxODk0MjUy", "url": "https://github.com/apache/kafka/pull/8031#pullrequestreview-351894252", "createdAt": "2020-02-01T17:17:40Z", "commit": {"oid": "392b7190313411c3949c4a707820088f3754175f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzo0MVrOFkeYlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQxNzoxNzo0MVrOFkeYlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc5MDg3MA==", "bodyText": "side cleanup", "url": "https://github.com/apache/kafka/pull/8031#discussion_r373790870", "createdAt": "2020-02-01T17:17:41Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsResult.java", "diffHunk": "@@ -48,23 +48,13 @@\n      * Return a future which yields a collection of TopicListing objects.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "392b7190313411c3949c4a707820088f3754175f"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6752749cb3a61b01638c5d2d31e46f2f19cea3c", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/a6752749cb3a61b01638c5d2d31e46f2f19cea3c", "committedDate": "2020-02-01T17:21:43Z", "message": "style fix"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/217e3db74237e5f10a7c639e52048c00401ed22d", "committedDate": "2020-02-01T22:23:55Z", "message": "More comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/217e3db74237e5f10a7c639e52048c00401ed22d", "committedDate": "2020-02-01T22:23:55Z", "message": "More comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzMzIwNTg5", "url": "https://github.com/apache/kafka/pull/8031#pullrequestreview-353320589", "createdAt": "2020-02-04T21:39:41Z", "commit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozOTo0MlrOFlkZOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo0NDo1M1rOFlking==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNzkxNQ==", "bodyText": "Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374937915", "createdAt": "2020-02-04T21:39:42Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzODY2NQ==", "bodyText": "Why we need an atomic long here? Seems there's no concurrency.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374938665", "createdAt": "2020-02-04T21:41:24Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTE1NA==", "bodyText": "Why divide the key by two?", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374939154", "createdAt": "2020-02-04T21:42:21Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));\n+            printWithTxnId(\"Message remaining: \" + messageRemaining);\n+        }\n+\n+        printWithTxnId(\"Finished processing \" + messageProcessed + \" records\");\n+        latch.countDown();\n+    }\n+\n+    private void printWithTxnId(final String message) {\n+        System.out.println(transactionalId + \": \" + message);\n+    }\n+\n+    private ProducerRecord<Integer, String> transform(final ConsumerRecord<Integer, String> record) {\n+        printWithTxnId(\"Transformed record (\" + record.key() + \",\" + record.value() + \")\");\n+        return new ProducerRecord<>(outputTopic, record.key() / 2, \"Transformed_\" + record.value());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA==", "bodyText": "prop: abortTransaction can also throw ProducerFenced.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374939488", "createdAt": "2020-02-04T21:43:06Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0MDMxOA==", "bodyText": "nit: we can just pass in the boolean values directly.", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374940318", "createdAt": "2020-02-04T21:44:53Z", "author": {"login": "guozhangwang"}, "path": "examples/src/main/java/kafka/examples/KafkaExactlyOnceDemo.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.errors.TopicExistsException;\n+import org.apache.kafka.common.errors.UnknownTopicOrPartitionException;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * This exactly once demo driver takes 4 arguments:\n+ *   - mode: whether to run as standalone app, or a group\n+ *   - partition: number of partitions for input/output topic\n+ *   - instances: number of instances\n+ *   - records: number of records\n+ * An example argument list would be `groupMode 6 3 50000`\n+ *\n+ * The driver could be decomposed as following stages:\n+ *\n+ * 1. Cleanup any topic whose name conflicts with input and output topic, so that we have a clean-start.\n+ *\n+ * 2. Set up a producer in a separate thread to pre-populate a set of records with even number keys into\n+ *    the input topic. The driver will block for the record generation to finish, so the producer\n+ *    must be in synchronous sending mode.\n+ *\n+ * 3. Set up transactional instances in separate threads which does a consume-process-produce loop,\n+ *    tailing data from input topic (See {@link ExactlyOnceMessageProcessor}). Each EOS instance will\n+ *    drain all the records from either given partitions or auto assigned partitions by actively\n+ *    comparing log end offset with committed offset. Each record will be processed exactly once\n+ *    as dividing the key by 2, and extend the value message. The driver will block for all the record\n+ *    processing to finish. The transformed record shall be written to the output topic, with\n+ *    transactional guarantee.\n+ *\n+ * 4. Set up a read committed consumer in a separate thread to verify we have all records within\n+ *    the output topic, while the message ordering on partition level is maintained.\n+ *    The driver will block for the consumption of all committed records.\n+ *\n+ * From this demo, you could see that all the records from pre-population are processed exactly once,\n+ * in either standalone mode or group mode, with strong partition level ordering guarantee.\n+ *\n+ * Note: please start the kafka broker and zookeeper in local first. The broker version must be >= 2.5\n+ * in order to run group mode, otherwise the app could throw\n+ * {@link org.apache.kafka.common.errors.UnsupportedVersionException}.\n+ */\n+public class KafkaExactlyOnceDemo {\n+\n+    private static final String INPUT_TOPIC = \"input-topic\";\n+    private static final String OUTPUT_TOPIC = \"output-topic\";\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+        if (args.length != 4) {\n+            throw new IllegalArgumentException(\"Should accept 4 parameters: [mode], \" +\n+                \"[number of partitions], [number of instances], [number of records]\");\n+        }\n+\n+        String mode = args[0];\n+        int numPartitions = Integer.valueOf(args[1]);\n+        int numInstances = Integer.valueOf(args[2]);\n+        int numRecords = Integer.valueOf(args[3]);\n+\n+        /* Stage 1: topic cleanup and recreation */\n+        recreateTopics(numPartitions);\n+\n+        CountDownLatch prePopulateLatch = new CountDownLatch(1);\n+\n+        /* Stage 2: pre-populate records */\n+        final boolean isAsync = false;\n+        final boolean enableIdempotency = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d"}, "originalPosition": 91}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5827af7522abb6206e61733180e39200811338c3", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/5827af7522abb6206e61733180e39200811338c3", "committedDate": "2020-02-04T23:37:33Z", "message": "guozhang's comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a748240c100014e86e831f614bf4219974fbd32", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/7a748240c100014e86e831f614bf4219974fbd32", "committedDate": "2020-02-05T20:48:16Z", "message": "move abort txn out and add commit reset"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "7a748240c100014e86e831f614bf4219974fbd32", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/7a748240c100014e86e831f614bf4219974fbd32", "committedDate": "2020-02-05T20:48:16Z", "message": "move abort txn out and add commit reset"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0NjY0OTMy", "url": "https://github.com/apache/kafka/pull/8031#pullrequestreview-354664932", "createdAt": "2020-02-06T18:13:33Z", "commit": {"oid": "7a748240c100014e86e831f614bf4219974fbd32"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxODoxMzozM1rOFmlK7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxODoxMzozM1rOFmlK7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk5OTIxNQ==", "bodyText": "It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this:\ntry {\n  producer.beginTransaction()\n  producer.send(...)\n  producer.sendOffsetsToTransaction(...)\n  producer.commitTransaction()\n} catch (Exception ) {\n  producer.abortTransaction()\n}", "url": "https://github.com/apache/kafka/pull/8031#discussion_r375999215", "createdAt": "2020-02-06T18:13:33Z", "author": {"login": "hachikuji"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        boolean abortPreviousTransaction = false;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Abort previous transaction if instructed.\n+                    if (abortPreviousTransaction) {\n+                        producer.abortTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a748240c100014e86e831f614bf4219974fbd32"}, "originalPosition": 131}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1703, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}