{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0NTQyMzcx", "number": 8235, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODowMzoxMVrODmZ_XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMzoyOToyMVrODmfyRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNTk4MzAxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODowMzoxMVrOFzzkvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMTo1OTozM1rOFz61BA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n          \n          \n            \n                        // we can try to update the limit offset as either committed offset for source changelog partitions;\n          \n          \n            \n                        // when the interval has elapsed we should try to update the limit offset for standbys reading from\n          \n          \n            \n                        // a source changelog with the new committed offset, unless there are no buffered records since \n          \n          \n            \n                        // we only need the limit when processing new records", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389866684", "createdAt": "2020-03-09T18:03:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg3MTYxMg==", "bodyText": "Q: if we skip updating the limit offsets because the buffer is empty, and then fetch new records immediately after, we'll still have to wait another commit interval to update the limit offsets. This means the standbys are potentially  lagging behind by up to the commit interval. Do we think that's worth the optimization of skipping the limit offset update?. Note I've seen a number of users report setting the commit interval quite large for various reasons (eg avoid flushing the memtables prematurely).\nMaybe we can find some middle ground by updating the limits before the commit interval if we do get some buffered records and skipped updating previously?", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389871612", "createdAt": "2020-03-09T18:12:16Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA=="}, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk3NDg5Nw==", "bodyText": "We only update lastUpdateOffsetTime = time.milliseconds(); when we've indeed refreshed our committed offsets, so if we did not refresh the committed offsets the timer would not be reset. I think that should be sufficient for covering your concern?", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389974897", "createdAt": "2020-03-09T21:34:52Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA=="}, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk4NTU0MA==", "bodyText": "Cool, yeah that addresses my concern \ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389985540", "createdAt": "2020-03-09T21:59:33Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA=="}, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNjI4NTAxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxOTozMzo1NFrOFz2gog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMTozNzozMVrOFz6P5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTkxNDc4Ng==", "bodyText": "Could we just add one more boolean condition into the filter and check whether changelogsWithLimitOffsets is empty or not.", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389914786", "createdAt": "2020-03-09T19:33:54Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;\n+            // for other changelog partitions we do not need to update limit offset at all since we never need to\n+            // check when it completes based on limit offset anyways: the end offset would keep increasing and the\n+            // standby never need to stop\n+            final Set<TopicPartition> changelogsWithLimitOffsets = changelogs.entrySet().stream()\n+                .filter(entry -> entry.getValue().stateManager.taskType() == Task.TaskType.STANDBY &&\n+                    entry.getValue().stateManager.changelogAsSource(entry.getKey()))\n+                .map(Map.Entry::getKey).collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk3NjAzOA==", "bodyText": "If changelogsWithLimitOffsets is empty then the for loop would be a no-op and the updateLimitOffsetsForStandbyChangelogs would not be called.", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389976038", "createdAt": "2020-03-09T21:37:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;\n+            // for other changelog partitions we do not need to update limit offset at all since we never need to\n+            // check when it completes based on limit offset anyways: the end offset would keep increasing and the\n+            // standby never need to stop\n+            final Set<TopicPartition> changelogsWithLimitOffsets = changelogs.entrySet().stream()\n+                .filter(entry -> entry.getValue().stateManager.taskType() == Task.TaskType.STANDBY &&\n+                    entry.getValue().stateManager.changelogAsSource(entry.getKey()))\n+                .map(Map.Entry::getKey).collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTkxNDc4Ng=="}, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNjkzMjUzOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMzoyOToyMVrOFz8tCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMzozMDoyNlrOFz8uJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxNjI2NQ==", "bodyText": "Is this really sufficient? We did call restore() once above with no available records and thus we don't expect that offset limits are updated. Now we call restore() again but would potentially update the offset limits at the very end -- hence would we not need one more call to restore() that the offset limits did no change?", "url": "https://github.com/apache/kafka/pull/8235#discussion_r390016265", "createdAt": "2020-03-09T23:29:21Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "diffHunk": "@@ -627,6 +627,58 @@ public void shouldOnlyRestoreStandbyChangelogInUpdateStandbyState() {\n         assertTrue(changelogReader.changelogMetadata(tp).bufferedRecords().isEmpty());\n     }\n \n+    @Test\n+    public void shouldNotUpdateLimitForNonSourceStandbyChangelog() {\n+        EasyMock.expect(standbyStateManager.changelogAsSource(tp)).andReturn(false).anyTimes();\n+        EasyMock.replay(standbyStateManager, storeMetadata, store);\n+\n+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {\n+            @Override\n+            public Map<TopicPartition, OffsetAndMetadata> committed(final Set<TopicPartition> partitions) {\n+                throw new AssertionError(\"Should not try to fetch committed offsets\");\n+            }\n+        };\n+\n+        final Properties properties = new Properties();\n+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100L);\n+        final StreamsConfig config = new StreamsConfig(StreamsTestUtils.getStreamsConfig(\"test-reader\", properties));\n+        final StoreChangelogReader changelogReader = new StoreChangelogReader(time, config, logContext, consumer, callback);\n+        changelogReader.setMainConsumer(consumer);\n+        changelogReader.transitToUpdateStandby();\n+\n+        consumer.updateBeginningOffsets(Collections.singletonMap(tp, 5L));\n+        changelogReader.register(tp, standbyStateManager);\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        // if there's no records fetchable, nothings gets restored\n+        changelogReader.restore();\n+        assertNull(callback.restoreTopicPartition);\n+        assertNull(callback.storeNameCalledStates.get(RESTORE_START));\n+        assertEquals(StoreChangelogReader.ChangelogState.RESTORING, changelogReader.changelogMetadata(tp).state());\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 5L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 6L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 7L, \"key\".getBytes(), \"value\".getBytes()));\n+        // null key should be ignored\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 8L, null, \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 9L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 10L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 11L, \"key\".getBytes(), \"value\".getBytes()));\n+\n+        // we should be able to restore to the log end offsets since there's no limit\n+        changelogReader.restore();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxNjU1MA==", "bodyText": "Or is this covered by the mockconsumer that would throw?", "url": "https://github.com/apache/kafka/pull/8235#discussion_r390016550", "createdAt": "2020-03-09T23:30:26Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "diffHunk": "@@ -627,6 +627,58 @@ public void shouldOnlyRestoreStandbyChangelogInUpdateStandbyState() {\n         assertTrue(changelogReader.changelogMetadata(tp).bufferedRecords().isEmpty());\n     }\n \n+    @Test\n+    public void shouldNotUpdateLimitForNonSourceStandbyChangelog() {\n+        EasyMock.expect(standbyStateManager.changelogAsSource(tp)).andReturn(false).anyTimes();\n+        EasyMock.replay(standbyStateManager, storeMetadata, store);\n+\n+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {\n+            @Override\n+            public Map<TopicPartition, OffsetAndMetadata> committed(final Set<TopicPartition> partitions) {\n+                throw new AssertionError(\"Should not try to fetch committed offsets\");\n+            }\n+        };\n+\n+        final Properties properties = new Properties();\n+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100L);\n+        final StreamsConfig config = new StreamsConfig(StreamsTestUtils.getStreamsConfig(\"test-reader\", properties));\n+        final StoreChangelogReader changelogReader = new StoreChangelogReader(time, config, logContext, consumer, callback);\n+        changelogReader.setMainConsumer(consumer);\n+        changelogReader.transitToUpdateStandby();\n+\n+        consumer.updateBeginningOffsets(Collections.singletonMap(tp, 5L));\n+        changelogReader.register(tp, standbyStateManager);\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        // if there's no records fetchable, nothings gets restored\n+        changelogReader.restore();\n+        assertNull(callback.restoreTopicPartition);\n+        assertNull(callback.storeNameCalledStates.get(RESTORE_START));\n+        assertEquals(StoreChangelogReader.ChangelogState.RESTORING, changelogReader.changelogMetadata(tp).state());\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 5L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 6L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 7L, \"key\".getBytes(), \"value\".getBytes()));\n+        // null key should be ignored\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 8L, null, \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 9L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 10L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 11L, \"key\".getBytes(), \"value\".getBytes()));\n+\n+        // we should be able to restore to the log end offsets since there's no limit\n+        changelogReader.restore();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxNjI2NQ=="}, "originalCommit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9"}, "originalPosition": 46}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3222, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}