{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQwOTk4NTcx", "number": 9758, "title": "MINOR: remove FetchResponse.AbortedTransaction and redundant construc\u2026", "bodyText": "Changes\n\nrename INVALID_HIGHWATERMARK  to INVALID_HIGH_WATERMARK\nreplace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction\nremove redundant constructors from FetchResponse.PartitionData\nrename recordSet to records\n\nPerformance Tests\nloop 10 times and get average.\ncase 1:  @parametrize(acks=1, topic=TOPIC_REP_ONE)\ndiff: -0.3994016685 %\n\ntrunk: 64.847 MB/sec\npatch: 64.588 MB/sec\n\ncase 2:  @parametrize(acks=-1, topic=TOPIC_REP_THREE)\ndiff: +0.4917916785 %\n\ntrunk: 28.264 MB/sec\npatch: 28.403 MB/sec\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-12-16T08:52:35Z", "url": "https://github.com/apache/kafka/pull/9758", "merged": true, "mergeCommit": {"oid": "8205051e90e3ea16165f8dc1f5c81af744bb1b9a"}, "closed": true, "closedAt": "2021-03-04T10:06:50Z", "author": {"login": "chia7712"}, "timelineItems": {"totalCount": 63, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdmwfHLgBqjQxMjAyMzk5MTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABd_wnh2gH2gAyNTQwOTk4NTcxOmFlMjU1NTExNzFmZDRlM2I4ODljYTk0ZDQ5NGUzMjA3NTQ1MzIwZTU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b3a08a9f317c490e14d1ac0c1451cdcc2db7a3bc", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/b3a08a9f317c490e14d1ac0c1451cdcc2db7a3bc", "committedDate": "2020-12-16T08:51:23Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate constructors of PartitionData"}, "afterCommit": {"oid": "0fe301e1ad9e4b8514c75547379465a7b051da01", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/0fe301e1ad9e4b8514c75547379465a7b051da01", "committedDate": "2020-12-16T15:12:27Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate constructors of PartitionData"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0fe301e1ad9e4b8514c75547379465a7b051da01", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/0fe301e1ad9e4b8514c75547379465a7b051da01", "committedDate": "2020-12-16T15:12:27Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate constructors of PartitionData"}, "afterCommit": {"oid": "bae931be1b6b558ae70498ec5f25412361d12e5a", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/bae931be1b6b558ae70498ec5f25412361d12e5a", "committedDate": "2020-12-19T06:35:21Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate  constructors of PartitionData"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1OTIzMTU4", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-555923158", "createdAt": "2020-12-19T07:08:26Z", "commit": {"oid": "bae931be1b6b558ae70498ec5f25412361d12e5a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwNzowODoyNlrOII5pCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwNzowODoyNlrOII5pCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjIwMzkxNQ==", "bodyText": "@ijuma This is the only case that we create LazyDownConversionRecords in production. Through this PR, this case can get rid of generic FetchResponse.PartitionData. Hence, we can remove generic from FetchResponse.PartitionData after this PR goes in trunk.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r546203915", "createdAt": "2020-12-19T07:08:26Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -777,12 +775,12 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n \n     def maybeConvertFetchedData(tp: TopicPartition,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bae931be1b6b558ae70498ec5f25412361d12e5a"}, "originalPosition": 30}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bae931be1b6b558ae70498ec5f25412361d12e5a", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/bae931be1b6b558ae70498ec5f25412361d12e5a", "committedDate": "2020-12-19T06:35:21Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate  constructors of PartitionData"}, "afterCommit": {"oid": "2a3ce401125c0b4afbfd357a936d14f7da22c92a", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/2a3ce401125c0b4afbfd357a936d14f7da22c92a", "committedDate": "2020-12-29T07:06:55Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate  constructors of PartitionData"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2a3ce401125c0b4afbfd357a936d14f7da22c92a", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/2a3ce401125c0b4afbfd357a936d14f7da22c92a", "committedDate": "2020-12-29T07:06:55Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate  constructors of PartitionData"}, "afterCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/3b606da01cf5567a9ab3fd76fc8f223c6200b4a3", "committedDate": "2020-12-29T07:25:02Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate constructors of PartitionData"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxMTUxMDM0", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-561151034", "createdAt": "2021-01-04T16:11:11Z", "commit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxMToxMVrOIN3hqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxNDo1N1rOIN3s0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMjEzOQ==", "bodyText": "These changes seem unrelated?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551412139", "createdAt": "2021-01-04T16:11:11Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -733,11 +733,11 @@ void resetOffsetIfNeeded(TopicPartition partition, OffsetResetStrategy requested\n     }\n \n     private void resetOffsetsAsync(Map<TopicPartition, Long> partitionResetTimestamps) {\n-        Map<Node, Map<TopicPartition, ListOffsetsPartition>> timestampsToSearchByNode =\n+        Map<Node, Map<TopicPartition, ListOffsetsRequestData.ListOffsetsPartition>> timestampsToSearchByNode =\n                 groupListOffsetRequests(partitionResetTimestamps, new HashSet<>());\n-        for (Map.Entry<Node, Map<TopicPartition, ListOffsetsPartition>> entry : timestampsToSearchByNode.entrySet()) {\n+        for (Map.Entry<Node, Map<TopicPartition, ListOffsetsRequestData.ListOffsetsPartition>> entry : timestampsToSearchByNode.entrySet()) {\n             Node node = entry.getKey();\n-            final Map<TopicPartition, ListOffsetsPartition> resetTimestamps = entry.getValue();\n+            final Map<TopicPartition, ListOffsetsRequestData.ListOffsetsPartition> resetTimestamps = entry.getValue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzkwMQ==", "bodyText": "Can we remove this altogether?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551413901", "createdAt": "2021-01-04T16:13:22Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxNDk5Mg==", "bodyText": "Do we need this class? What does it add over FetchResponseData.FetchablePartitionRespons?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r551414992", "createdAt": "2021-01-04T16:14:57Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -59,136 +60,140 @@\n  */\n public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n \n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+    public static FetchResponseData.FetchablePartitionResponse partitionResponse(Errors error) {\n+        return new FetchResponseData.FetchablePartitionResponse()\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecordSet(MemoryRecords.EMPTY)\n+                .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID);\n+    }\n+\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> partitionData;\n+    // lazily instantiate this field\n+    private LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap = null;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n+    /**\n+     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n+     * `FetchRequest.fetchData`.\n+     *\n+     * @param error             The top-level error code.\n+     * @param responseData      The fetched data grouped by partition.\n+     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n+     * @param sessionId         The fetch session id.\n+     */\n+    public FetchResponse(Errors error,\n+                         LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n+                         int throttleTimeMs,\n+                         int sessionId) {\n+        this(error, throttleTimeMs, sessionId, responseData.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey,\n+            entry -> entry.getValue().partitionResponse, (o1, o2) -> {\n+                throw new RuntimeException(\"this is impossible\");\n+            }, LinkedHashMap::new)));\n+    }\n \n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n+    public FetchResponse(Errors error,\n+                         int throttleTimeMs,\n+                         int sessionId,\n+                         LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = new FetchResponseData()\n+                .setSessionId(sessionId)\n+                .setErrorCode(error.code())\n+                .setThrottleTimeMs(throttleTimeMs);\n+        responseData.forEach((tp, tpData) -> data.responses().add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(tp.topic())\n+            .setPartitionResponses(Collections.singletonList(tpData.setPartition(tp.partition())))));\n+        this.partitionData = responseData;\n+    }\n \n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n+    public FetchResponse(FetchResponseData fetchResponseData) {\n+        super(ApiKeys.FETCH);\n+        this.data = fetchResponseData;\n+        this.partitionData = new LinkedHashMap<>();\n+        fetchResponseData.responses().forEach(topicResponse ->\n+            topicResponse.partitionResponses().forEach(partitionResponse ->\n+                partitionData.put(new TopicPartition(topicResponse.topic(), partitionResponse.partition()), partitionResponse))\n+        );\n+    }\n \n-            AbortedTransaction that = (AbortedTransaction) o;\n+    public Errors error() {\n+        return Errors.forCode(data.errorCode());\n+    }\n \n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n+    public LinkedHashMap<TopicPartition, PartitionData<T>> responseData() {\n+        if (responseDataMap == null) {\n+            responseDataMap = new LinkedHashMap<>(partitionData.size());\n+            partitionData.forEach((tp, d) -> responseDataMap.put(tp, new PartitionData<>(d)));\n         }\n+        return responseDataMap;\n+    }\n \n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n+    @Override\n+    public int throttleTimeMs() {\n+        return data.throttleTimeMs();\n+    }\n \n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n+    public int sessionId() {\n+        return data.sessionId();\n+    }\n \n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n+    @Override\n+    public Map<Errors, Integer> errorCounts() {\n+        Map<Errors, Integer> errorCounts = new HashMap<>();\n+        updateErrorCounts(errorCounts, error());\n+        partitionData.values().forEach(response ->\n+            updateErrorCounts(errorCounts, Errors.forCode(response.errorCode()))\n+        );\n+        return errorCounts;\n     }\n \n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n+    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n+        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n+    }\n \n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n+    /**\n+     * Convenience method to find the size of a response.\n+     *\n+     * @param version       The version of the response to use.\n+     * @param partIterator  The partition iterator.\n+     * @return              The response size in bytes.\n+     */\n+    public static <T extends Records> int sizeOf(short version,\n+                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+        // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n+        // use arbitrary values here without affecting the result.\n+        LinkedHashMap<TopicPartition, PartitionData<T>> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        return 4 + new FetchResponse<>(Errors.NONE, data, 0, INVALID_SESSION_ID).data.size(cache, version);\n+    }\n \n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n+    @Override\n+    public boolean shouldClientThrottle(short version) {\n+        return version >= 8;\n+    }\n \n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n \n-            this.partitionResponse = partitionResponse;\n-        }\n+    public static final class PartitionData<T extends BaseRecords> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3"}, "originalPosition": 230}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3b606da01cf5567a9ab3fd76fc8f223c6200b4a3", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/3b606da01cf5567a9ab3fd76fc8f223c6200b4a3", "committedDate": "2020-12-29T07:25:02Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate constructors of PartitionData"}, "afterCommit": {"oid": "86da6527bff7fd091ea88af7f7e86f1464daa443", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/86da6527bff7fd091ea88af7f7e86f1464daa443", "committedDate": "2021-01-04T16:46:43Z", "message": "remove unnecessary changes and remove partitionData"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "86da6527bff7fd091ea88af7f7e86f1464daa443", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/86da6527bff7fd091ea88af7f7e86f1464daa443", "committedDate": "2021-01-04T16:46:43Z", "message": "remove unnecessary changes and remove partitionData"}, "afterCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/1a5def5bfb97d673243203f90cc9e723b3e24915", "committedDate": "2021-01-05T20:02:53Z", "message": "remove PartitionData"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyMzY0OTc3", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-562364977", "createdAt": "2021-01-06T07:02:12Z", "commit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNzowMjoxMlrOIO0BHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNzowMjoxMlrOIO0BHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQwMzIzMQ==", "bodyText": "I don't batch the partitions again in this PR as it create a new FetchResponseData.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552403231", "createdAt": "2021-01-06T07:02:12Z", "author": {"login": "chia7712"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -304,58 +115,12 @@ public int sessionId() {\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n         updateErrorCounts(errorCounts, error());\n-        responseDataMap.values().forEach(response ->\n-            updateErrorCounts(errorCounts, response.error())\n-        );\n+        dataByTopicPartition.values().forEach(response -> updateErrorCounts(errorCounts, Errors.forCode(response.errorCode())));\n         return errorCounts;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n-        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static <T extends BaseRecords> LinkedHashMap<TopicPartition, PartitionData<T>> toResponseDataMap(\n-            FetchResponseData message) {\n-        LinkedHashMap<TopicPartition, PartitionData<T>> responseMap = new LinkedHashMap<>();\n-        message.responses().forEach(topicResponse -> {\n-            topicResponse.partitionResponses().forEach(partitionResponse -> {\n-                TopicPartition tp = new TopicPartition(topicResponse.topic(), partitionResponse.partition());\n-                PartitionData<T> partitionData = new PartitionData<>(partitionResponse);\n-                responseMap.put(tp, partitionData);\n-            });\n-        });\n-        return responseMap;\n-    }\n-\n-    private static <T extends BaseRecords> FetchResponseData toMessage(int throttleTimeMs, Errors error,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "originalPosition": 306}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyNzA0Mzg0", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-562704384", "createdAt": "2021-01-06T13:25:21Z", "commit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxMzoyNToyMlrOIPAbrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxMzoyNToyMlrOIPAbrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjYwNjYzOQ==", "bodyText": "Maybe we could have a isPreferredReplica method since there are at least two places where we just call isPresent on the result.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r552606639", "createdAt": "2021-01-06T13:25:22Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/FetchSession.scala", "diffHunk": "@@ -142,19 +141,20 @@ class CachedPartition(val topic: String,\n       if (updateResponseData)\n         localLogStartOffset = respData.logStartOffset\n     }\n-    if (respData.preferredReadReplica.isPresent) {\n+    if (FetchResponse.preferredReadReplica(respData).isPresent) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "originalPosition": 62}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5fa28b5d1529181c7b21340541d8622a5681735d", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/5fa28b5d1529181c7b21340541d8622a5681735d", "committedDate": "2021-01-06T13:45:14Z", "message": "add isDivergingEpoch and isPreferredReplica"}, "afterCommit": {"oid": "665e6c94bc4c0560c2c46a872844d2890425865b", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/665e6c94bc4c0560c2c46a872844d2890425865b", "committedDate": "2021-01-11T14:04:21Z", "message": "add isDivergingEpoch and isPreferredReplica"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3e592f8e22b60e781e3037a266c3d060432a5fb", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/b3e592f8e22b60e781e3037a266c3d060432a5fb", "committedDate": "2021-01-13T13:50:26Z", "message": "MINOR: remove FetchResponse.AbortedTransaction and duplicate constructors of PartitionData"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f217bb53ee96972d639a85348dd862f99e529f42", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/f217bb53ee96972d639a85348dd862f99e529f42", "committedDate": "2021-01-13T13:50:26Z", "message": "remove unnecessary changes and remove partitionData"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f219e5951133f791ab0cc546bb33fcbc199a608", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/3f219e5951133f791ab0cc546bb33fcbc199a608", "committedDate": "2021-01-13T13:51:15Z", "message": "remove PartitionData"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83a26f0b0b86af15516556f84130a46deb0ea6a9", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/83a26f0b0b86af15516556f84130a46deb0ea6a9", "committedDate": "2021-01-13T13:51:15Z", "message": "add isDivergingEpoch and isPreferredReplica"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c386a0a9504489fd8db5871d84abfef2dff270d", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/7c386a0a9504489fd8db5871d84abfef2dff270d", "committedDate": "2021-01-13T13:53:24Z", "message": "remove unused import"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "665e6c94bc4c0560c2c46a872844d2890425865b", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/665e6c94bc4c0560c2c46a872844d2890425865b", "committedDate": "2021-01-11T14:04:21Z", "message": "add isDivergingEpoch and isPreferredReplica"}, "afterCommit": {"oid": "7c386a0a9504489fd8db5871d84abfef2dff270d", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/7c386a0a9504489fd8db5871d84abfef2dff270d", "committedDate": "2021-01-13T13:53:24Z", "message": "remove unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c010e4e154931d6474d89d9bee50be1b594dfff3", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/c010e4e154931d6474d89d9bee50be1b594dfff3", "committedDate": "2021-01-14T06:16:14Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30695f8563a2ebb0eb235d35a652d66aac5c42ad", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/30695f8563a2ebb0eb235d35a652d66aac5c42ad", "committedDate": "2021-01-15T19:18:48Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9122aee548963feca3f50fcebd6b59479a1a07aa", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/9122aee548963feca3f50fcebd6b59479a1a07aa", "committedDate": "2021-01-16T19:10:53Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4106d2fab6241bb5bdbb0706681e16f819114f62", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/4106d2fab6241bb5bdbb0706681e16f819114f62", "committedDate": "2021-01-18T17:31:14Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d97de8ac517e40c4981c2e33a50e547c4d9d0ff6", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/d97de8ac517e40c4981c2e33a50e547c4d9d0ff6", "committedDate": "2021-01-18T17:38:10Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13f7710839530ffa780d84c24099d4cc0f138bbe", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/13f7710839530ffa780d84c24099d4cc0f138bbe", "committedDate": "2021-01-18T19:21:18Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "52ab1569ba84f7ea96253c08acc2817fd7cd6a65", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/52ab1569ba84f7ea96253c08acc2817fd7cd6a65", "committedDate": "2021-01-18T19:25:27Z", "message": "tweak style"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb157fe1696414f0840d1ed8c619bda1f2c240a2", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/cb157fe1696414f0840d1ed8c619bda1f2c240a2", "committedDate": "2021-01-18T19:30:34Z", "message": "rename dataByTopicPartition to responseData"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9627cd99d569b90868ae436b43bcc9f53d564a79", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/9627cd99d569b90868ae436b43bcc9f53d564a79", "committedDate": "2021-01-18T19:41:39Z", "message": "rename recordSet to records"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e15ee0cc6e57ab00a70c0ed099ce86a7ba778d6e", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/e15ee0cc6e57ab00a70c0ed099ce86a7ba778d6e", "committedDate": "2021-01-19T10:19:26Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "868ff0ceb01ecbb8313839554c302c48a6d095e7", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/868ff0ceb01ecbb8313839554c302c48a6d095e7", "committedDate": "2021-01-19T16:40:01Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef45f01869c65f68be019c9f833d3ad2a15a4ba6", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ef45f01869c65f68be019c9f833d3ad2a15a4ba6", "committedDate": "2021-01-21T04:11:38Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc5c494c16a750c8624fd6440cc68149550de4be", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/bc5c494c16a750c8624fd6440cc68149550de4be", "committedDate": "2021-01-27T07:55:25Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce991c36425c1941127c69b701c9dc814ddd047e", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ce991c36425c1941127c69b701c9dc814ddd047e", "committedDate": "2021-02-01T07:42:04Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a83601ad09c96081fded8cf9118e372fff21938", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/4a83601ad09c96081fded8cf9118e372fff21938", "committedDate": "2021-02-02T08:48:00Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b55cd7bbb53f221ac2e30e6eaf8689f7a39b09b", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/3b55cd7bbb53f221ac2e30e6eaf8689f7a39b09b", "committedDate": "2021-02-12T19:51:05Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd1c981d487be38f19ee2b116995650af8c5257f", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/cd1c981d487be38f19ee2b116995650af8c5257f", "committedDate": "2021-02-12T19:54:46Z", "message": "fix build"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fbca0494018d96d583bbef4c146d01dd00ca1db6", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/fbca0494018d96d583bbef4c146d01dd00ca1db6", "committedDate": "2021-02-17T05:21:06Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4077553bb23057cd41a80db90998343fa5b1354b", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/4077553bb23057cd41a80db90998343fa5b1354b", "committedDate": "2021-02-18T03:25:37Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d513d855ad7990371d80792255f7c46143726f34", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/d513d855ad7990371d80792255f7c46143726f34", "committedDate": "2021-02-20T03:17:01Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e17a7eeac81d554a43917a0022ede41e4a8ff5e", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/9e17a7eeac81d554a43917a0022ede41e4a8ff5e", "committedDate": "2021-02-20T03:23:07Z", "message": "fix build error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/568d8f56fc0a9c052031d87f4b506c27383442ae", "committedDate": "2021-02-23T10:02:26Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk2MzQ0NjE4", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-596344618", "createdAt": "2021-02-23T13:43:52Z", "commit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxMzo0Mzo1MlrOIqIJSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNDoxNDoyOFrOIqJpDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA0NDU1Mg==", "bodyText": "FetchablePartitionResponse is a bit long and redundant. Could we find a shorter name for it now that PartitionData is gone?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581044552", "createdAt": "2021-02-23T13:43:52Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -290,7 +291,7 @@ public void onSuccess(ClientResponse resp) {\n                             Set<TopicPartition> partitions = new HashSet<>(response.responseData().keySet());\n                             FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);\n \n-                            for (Map.Entry<TopicPartition, FetchResponse.PartitionData<Records>> entry : response.responseData().entrySet()) {\n+                            for (Map.Entry<TopicPartition, FetchResponseData.FetchablePartitionResponse> entry : response.responseData().entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA1NDAzOA==", "bodyText": "Could we encapsulate this cast in a utility method?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581054038", "createdAt": "2021-02-23T13:56:24Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1257,7 +1259,7 @@ private CompletedFetch initializeCompletedFetch(CompletedFetch nextCompletedFetc\n \n                 log.trace(\"Preparing to read {} bytes of data for partition {} with offset {}\",\n                         partition.records().sizeInBytes(), tp, position);\n-                Iterator<? extends RecordBatch> batches = partition.records().batches().iterator();\n+                Iterator<? extends RecordBatch> batches = ((Records) partition.records()).batches().iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA1ODE2Mg==", "bodyText": "Maybe we can remove the defaults from this and every other place where we build FetchablePartitionResponse", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581058162", "createdAt": "2021-02-23T14:01:46Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java", "diffHunk": "@@ -70,24 +70,26 @@ public void setUp() {\n         handler = new FetchSessionHandler(LOG_CONTEXT, 1);\n         FetchSessionHandler.Builder builder = handler.newBuilder();\n \n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> respMap = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> respMap = new LinkedHashMap<>();\n         for (int i = 0; i < partitionCount; i++) {\n             TopicPartition tp = new TopicPartition(\"foo\", i);\n             FetchRequest.PartitionData partitionData = new FetchRequest.PartitionData(0, 0, 200,\n                     Optional.empty());\n             fetches.put(tp, partitionData);\n             builder.add(tp, partitionData);\n-            respMap.put(tp, new FetchResponse.PartitionData<>(\n-                    Errors.NONE,\n-                    0L,\n-                    0L,\n-                    0,\n-                    null,\n-                    null));\n+            respMap.put(tp, new FetchResponseData.FetchablePartitionResponse()\n+                            .setPartition(tp.partition())\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setHighWatermark(0)\n+                            .setLastStableOffset(0)\n+                            .setLogStartOffset(0)\n+                            .setAbortedTransactions(null)\n+                            .setRecords(null)\n+                            .setPreferredReadReplica(FetchResponse.INVALID_PREFERRED_REPLICA_ID));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA1OTE0OQ==", "bodyText": "Similar to elsewhere, it would be useful to have a utility method to avoid unsafe operations all over the code.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581059149", "createdAt": "2021-02-23T14:02:57Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/LogOffsetTest.scala", "diffHunk": "@@ -127,7 +124,7 @@ class LogOffsetTest extends BaseRequestTest {\n       Map(topicPartition -> new FetchRequest.PartitionData(consumerOffsets.head, FetchRequest.INVALID_LOG_START_OFFSET,\n         300 * 1024, Optional.empty())).asJava).build()\n     val fetchResponse = sendFetchRequest(fetchRequest)\n-    assertFalse(fetchResponse.responseData.get(topicPartition).records.batches.iterator.hasNext)\n+    assertFalse(fetchResponse.responseData.get(topicPartition).records.asInstanceOf[Records].batches.iterator.hasNext)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA2MzI5Ng==", "bodyText": "Could this be FetchData too? Are there other places like it?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581063296", "createdAt": "2021-02-23T14:07:48Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala", "diffHunk": "@@ -110,7 +113,7 @@ class ReplicaAlterLogDirsThread(name: String,\n   // process fetched data\n   override def processPartitionData(topicPartition: TopicPartition,\n                                     fetchOffset: Long,\n-                                    partitionData: PartitionData[Records]): Option[LogAppendInfo] = {\n+                                    partitionData: FetchResponseData.FetchablePartitionResponse): Option[LogAppendInfo] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA2NDY2OQ==", "bodyText": "I think it would be better to make this a static factory method and keep the constructor for the case where we receive FetchResponseData.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581064669", "createdAt": "2021-02-23T14:09:15Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +55,51 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n-\n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            AbortedTransaction that = (AbortedTransaction) o;\n-\n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n-\n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n-    }\n-\n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n-\n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n-\n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n-\n-            this.partitionResponse = partitionResponse;\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, preferredReadReplica,\n-                abortedTransactions, Optional.empty(), records);\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, Optional.empty(), abortedTransactions, records);\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            PartitionData that = (PartitionData) o;\n-\n-            return this.partitionResponse.equals(that.partitionResponse);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return this.partitionResponse.hashCode();\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(error=\" + error() +\n-                    \", highWaterMark=\" + highWatermark() +\n-                    \", lastStableOffset = \" + lastStableOffset() +\n-                    \", logStartOffset = \" + logStartOffset() +\n-                    \", preferredReadReplica = \" + preferredReadReplica().map(Object::toString).orElse(\"absent\") +\n-                    \", abortedTransactions = \" + abortedTransactions() +\n-                    \", divergingEpoch =\" + divergingEpoch() +\n-                    \", recordsSizeInBytes=\" + records().sizeInBytes() + \")\";\n-        }\n-\n-        public Errors error() {\n-            return error;\n-        }\n-\n-        public long highWatermark() {\n-            return partitionResponse.highWatermark();\n-        }\n-\n-        public long lastStableOffset() {\n-            return partitionResponse.lastStableOffset();\n-        }\n-\n-        public long logStartOffset() {\n-            return partitionResponse.logStartOffset();\n-        }\n-\n-        public Optional<Integer> preferredReadReplica() {\n-            return preferredReplica;\n-        }\n-\n-        public List<AbortedTransaction> abortedTransactions() {\n-            return abortedTransactions;\n-        }\n-\n-        public Optional<FetchResponseData.EpochEndOffset> divergingEpoch() {\n-            FetchResponseData.EpochEndOffset epochEndOffset = partitionResponse.divergingEpoch();\n-            if (epochEndOffset.epoch() < 0) {\n-                return Optional.empty();\n-            } else {\n-                return Optional.of(epochEndOffset);\n-            }\n-        }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        public T records() {\n-            return (T) partitionResponse.recordSet();\n-        }\n-    }\n-\n-    /**\n-     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n-     * `FetchRequest.fetchData`.\n-     *\n-     * @param error             The top-level error code.\n-     * @param responseData      The fetched data grouped by partition.\n-     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n-     * @param sessionId         The fetch session id.\n-     */\n     public FetchResponse(Errors error,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTA2OTA3MQ==", "bodyText": "This isn't needed when we return a fetch from the broker, right? If this is true, can we remove it from the fetch response and build it on the client when needed?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581069071", "createdAt": "2021-02-23T14:14:28Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +55,51 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    private final LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData;\n \n     @Override\n     public FetchResponseData data() {\n         return data;\n     }\n \n-    public static final class AbortedTransaction {\n-        public final long producerId;\n-        public final long firstOffset;\n-\n-        public AbortedTransaction(long producerId, long firstOffset) {\n-            this.producerId = producerId;\n-            this.firstOffset = firstOffset;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            AbortedTransaction that = (AbortedTransaction) o;\n-\n-            return producerId == that.producerId && firstOffset == that.firstOffset;\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Long.hashCode(producerId);\n-            result = 31 * result + Long.hashCode(firstOffset);\n-            return result;\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(producerId=\" + producerId + \", firstOffset=\" + firstOffset + \")\";\n-        }\n-\n-        static AbortedTransaction fromMessage(FetchResponseData.AbortedTransaction abortedTransaction) {\n-            return new AbortedTransaction(abortedTransaction.producerId(), abortedTransaction.firstOffset());\n-        }\n-    }\n-\n-    public static final class PartitionData<T extends BaseRecords> {\n-        private final FetchResponseData.FetchablePartitionResponse partitionResponse;\n-\n-        // Derived fields\n-        private final Optional<Integer> preferredReplica;\n-        private final List<AbortedTransaction> abortedTransactions;\n-        private final Errors error;\n-\n-        private PartitionData(FetchResponseData.FetchablePartitionResponse partitionResponse) {\n-            // We partially construct FetchablePartitionResponse since we don't know the partition ID at this point\n-            // When we convert the PartitionData (and other fields) into FetchResponseData down in toMessage, we\n-            // set the partition IDs.\n-            this.partitionResponse = partitionResponse;\n-            this.preferredReplica = Optional.of(partitionResponse.preferredReadReplica())\n-                .filter(replicaId -> replicaId != INVALID_PREFERRED_REPLICA_ID);\n-\n-            if (partitionResponse.abortedTransactions() == null) {\n-                this.abortedTransactions = null;\n-            } else {\n-                this.abortedTransactions = partitionResponse.abortedTransactions().stream()\n-                    .map(AbortedTransaction::fromMessage)\n-                    .collect(Collectors.toList());\n-            }\n-\n-            this.error = Errors.forCode(partitionResponse.errorCode());\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             Optional<FetchResponseData.EpochEndOffset> divergingEpoch,\n-                             T records) {\n-            this.preferredReplica = preferredReadReplica;\n-            this.abortedTransactions = abortedTransactions;\n-            this.error = error;\n-\n-            FetchResponseData.FetchablePartitionResponse partitionResponse =\n-                new FetchResponseData.FetchablePartitionResponse();\n-            partitionResponse.setErrorCode(error.code())\n-                .setHighWatermark(highWatermark)\n-                .setLastStableOffset(lastStableOffset)\n-                .setLogStartOffset(logStartOffset);\n-            if (abortedTransactions != null) {\n-                partitionResponse.setAbortedTransactions(abortedTransactions.stream().map(\n-                    aborted -> new FetchResponseData.AbortedTransaction()\n-                        .setProducerId(aborted.producerId)\n-                        .setFirstOffset(aborted.firstOffset))\n-                    .collect(Collectors.toList()));\n-            } else {\n-                partitionResponse.setAbortedTransactions(null);\n-            }\n-            partitionResponse.setPreferredReadReplica(preferredReadReplica.orElse(INVALID_PREFERRED_REPLICA_ID));\n-            partitionResponse.setRecordSet(records);\n-            divergingEpoch.ifPresent(partitionResponse::setDivergingEpoch);\n-\n-            this.partitionResponse = partitionResponse;\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             Optional<Integer> preferredReadReplica,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, preferredReadReplica,\n-                abortedTransactions, Optional.empty(), records);\n-        }\n-\n-        public PartitionData(Errors error,\n-                             long highWatermark,\n-                             long lastStableOffset,\n-                             long logStartOffset,\n-                             List<AbortedTransaction> abortedTransactions,\n-                             T records) {\n-            this(error, highWatermark, lastStableOffset, logStartOffset, Optional.empty(), abortedTransactions, records);\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o)\n-                return true;\n-            if (o == null || getClass() != o.getClass())\n-                return false;\n-\n-            PartitionData that = (PartitionData) o;\n-\n-            return this.partitionResponse.equals(that.partitionResponse);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return this.partitionResponse.hashCode();\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"(error=\" + error() +\n-                    \", highWaterMark=\" + highWatermark() +\n-                    \", lastStableOffset = \" + lastStableOffset() +\n-                    \", logStartOffset = \" + logStartOffset() +\n-                    \", preferredReadReplica = \" + preferredReadReplica().map(Object::toString).orElse(\"absent\") +\n-                    \", abortedTransactions = \" + abortedTransactions() +\n-                    \", divergingEpoch =\" + divergingEpoch() +\n-                    \", recordsSizeInBytes=\" + records().sizeInBytes() + \")\";\n-        }\n-\n-        public Errors error() {\n-            return error;\n-        }\n-\n-        public long highWatermark() {\n-            return partitionResponse.highWatermark();\n-        }\n-\n-        public long lastStableOffset() {\n-            return partitionResponse.lastStableOffset();\n-        }\n-\n-        public long logStartOffset() {\n-            return partitionResponse.logStartOffset();\n-        }\n-\n-        public Optional<Integer> preferredReadReplica() {\n-            return preferredReplica;\n-        }\n-\n-        public List<AbortedTransaction> abortedTransactions() {\n-            return abortedTransactions;\n-        }\n-\n-        public Optional<FetchResponseData.EpochEndOffset> divergingEpoch() {\n-            FetchResponseData.EpochEndOffset epochEndOffset = partitionResponse.divergingEpoch();\n-            if (epochEndOffset.epoch() < 0) {\n-                return Optional.empty();\n-            } else {\n-                return Optional.of(epochEndOffset);\n-            }\n-        }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        public T records() {\n-            return (T) partitionResponse.recordSet();\n-        }\n-    }\n-\n-    /**\n-     * From version 3 or later, the entries in `responseData` should be in the same order as the entries in\n-     * `FetchRequest.fetchData`.\n-     *\n-     * @param error             The top-level error code.\n-     * @param responseData      The fetched data grouped by partition.\n-     * @param throttleTimeMs    The time in milliseconds that the response was throttled\n-     * @param sessionId         The fetch session id.\n-     */\n     public FetchResponse(Errors error,\n-                         LinkedHashMap<TopicPartition, PartitionData<T>> responseData,\n                          int throttleTimeMs,\n-                         int sessionId) {\n-        super(ApiKeys.FETCH);\n-        this.data = toMessage(throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);\n-        this.responseDataMap = responseData;\n+                         int sessionId,\n+                         LinkedHashMap<TopicPartition, FetchResponseData.FetchablePartitionResponse> responseData) {\n+        this(new FetchResponseData()\n+            .setSessionId(sessionId)\n+            .setErrorCode(error.code())\n+            .setThrottleTimeMs(throttleTimeMs)\n+            .setResponses(responseData.entrySet().stream().map(entry -> new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(entry.getKey().topic())\n+                .setPartitionResponses(Collections.singletonList(entry.getValue().setPartition(entry.getKey().partition()))))\n+                .collect(Collectors.toList())));\n     }\n \n     public FetchResponse(FetchResponseData fetchResponseData) {\n         super(ApiKeys.FETCH);\n         this.data = fetchResponseData;\n-        this.responseDataMap = toResponseDataMap(fetchResponseData);\n+        this.responseData = new LinkedHashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "568d8f56fc0a9c052031d87f4b506c27383442ae"}, "originalPosition": 258}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eddfffd349a34cc26ddc5db284b5526c40f5fcc8", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/eddfffd349a34cc26ddc5db284b5526c40f5fcc8", "committedDate": "2021-02-23T15:15:35Z", "message": "rename FetchablePartitionResponse to PartitionData; add helper method to construct FetchResponse; add helper method to cast records"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk2NDU1OTcx", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-596455971", "createdAt": "2021-02-23T15:21:34Z", "commit": {"oid": "eddfffd349a34cc26ddc5db284b5526c40f5fcc8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNToyMTozNFrOIqNQug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yM1QxNToyMTozNFrOIqNQug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTEyODM3OA==", "bodyText": "We typically call this PartitionIndex in other requests/responses. Is that right?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r581128378", "createdAt": "2021-02-23T15:21:34Z", "author": {"login": "ijuma"}, "path": "clients/src/main/resources/common/message/FetchResponse.json", "diffHunk": "@@ -53,9 +53,9 @@\n       \"about\": \"The response topics.\", \"fields\": [\n       { \"name\": \"Topic\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n         \"about\": \"The topic name.\" },\n-      { \"name\": \"PartitionResponses\", \"type\": \"[]FetchablePartitionResponse\", \"versions\": \"0+\",\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionData\", \"versions\": \"0+\",\n         \"about\": \"The topic partitions.\", \"fields\": [\n-        { \"name\": \"Partition\", \"type\": \"int32\", \"versions\": \"0+\",\n+        { \"name\": \"Index\", \"type\": \"int32\", \"versions\": \"0+\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eddfffd349a34cc26ddc5db284b5526c40f5fcc8"}, "originalPosition": 8}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b7637645b76d04eed4a606e75580aa8d1c1a2d0", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/9b7637645b76d04eed4a606e75580aa8d1c1a2d0", "committedDate": "2021-02-23T15:29:46Z", "message": "rename index to partitionIndex"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d5a5a3800ebf01665fe97c935428fb2e0ec2601", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/7d5a5a3800ebf01665fe97c935428fb2e0ec2601", "committedDate": "2021-02-25T15:36:24Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4edde39d57282bb0acb018ff3eb91a5cf1321e8", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/a4edde39d57282bb0acb018ff3eb91a5cf1321e8", "committedDate": "2021-02-25T15:42:50Z", "message": "fix broken build"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b929215cae4ebcfb5e104d4d387c9c48b507fdf1", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/b929215cae4ebcfb5e104d4d387c9c48b507fdf1", "committedDate": "2021-02-26T04:07:38Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/e6e1478661e398fb813c8ca77f9d25bc5bd37d86", "committedDate": "2021-02-27T15:20:38Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMTYyMzY3", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-600162367", "createdAt": "2021-02-27T15:32:16Z", "commit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxNTozMjoxN1rOItFMPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxNTozMjoxN1rOItFMPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE0MTg4Nw==", "bodyText": "This is not thread-safe and requests are typically thread-safe. What's the thinking here?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584141887", "createdAt": "2021-02-27T15:32:17Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -57,238 +56,39 @@\n  *     the fetch offset after the index lookup\n  * - {@link Errors#UNKNOWN_SERVER_ERROR} For any unexpected errors\n  */\n-public class FetchResponse<T extends BaseRecords> extends AbstractResponse {\n-\n-    public static final long INVALID_HIGHWATERMARK = -1L;\n+public class FetchResponse extends AbstractResponse {\n+    public static final long INVALID_HIGH_WATERMARK = -1L;\n     public static final long INVALID_LAST_STABLE_OFFSET = -1L;\n     public static final long INVALID_LOG_START_OFFSET = -1L;\n     public static final int INVALID_PREFERRED_REPLICA_ID = -1;\n \n     private final FetchResponseData data;\n-    private final LinkedHashMap<TopicPartition, PartitionData<T>> responseDataMap;\n+    // we build responseData when needed.\n+    private LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e6e1478661e398fb813c8ca77f9d25bc5bd37d86"}, "originalPosition": 34}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9", "committedDate": "2021-02-27T17:40:51Z", "message": "make FetchResponse thread-safe"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMTgzMTE4", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-600183118", "createdAt": "2021-02-27T18:30:34Z", "commit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "state": "COMMENTED", "comments": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QxODozMDozNVrOItHCkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yN1QyMDozMjowNFrOItH-LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjE3Nw==", "bodyText": "Can we update this not to use responseData? Then we at least have the right behavior for the broker and we can fix the clients in the subsequent PR.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172177", "createdAt": "2021-02-27T18:30:35Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -304,58 +108,12 @@ public int sessionId() {\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n         updateErrorCounts(errorCounts, error());\n-        responseDataMap.values().forEach(response ->\n-            updateErrorCounts(errorCounts, response.error())\n-        );\n+        responseData().values().forEach(response -> updateErrorCounts(errorCounts, Errors.forCode(response.errorCode())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjI5Mg==", "bodyText": "Can you clarify what you mean here?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172292", "createdAt": "2021-02-27T18:31:43Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -304,58 +115,12 @@ public int sessionId() {\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n         updateErrorCounts(errorCounts, error());\n-        responseDataMap.values().forEach(response ->\n-            updateErrorCounts(errorCounts, response.error())\n-        );\n+        dataByTopicPartition.values().forEach(response -> updateErrorCounts(errorCounts, Errors.forCode(response.errorCode())));\n         return errorCounts;\n     }\n \n-    public static FetchResponse<MemoryRecords> parse(ByteBuffer buffer, short version) {\n-        return new FetchResponse<>(new FetchResponseData(new ByteBufferAccessor(buffer), version));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static <T extends BaseRecords> LinkedHashMap<TopicPartition, PartitionData<T>> toResponseDataMap(\n-            FetchResponseData message) {\n-        LinkedHashMap<TopicPartition, PartitionData<T>> responseMap = new LinkedHashMap<>();\n-        message.responses().forEach(topicResponse -> {\n-            topicResponse.partitionResponses().forEach(partitionResponse -> {\n-                TopicPartition tp = new TopicPartition(topicResponse.topic(), partitionResponse.partition());\n-                PartitionData<T> partitionData = new PartitionData<>(partitionResponse);\n-                responseMap.put(tp, partitionData);\n-            });\n-        });\n-        return responseMap;\n-    }\n-\n-    private static <T extends BaseRecords> FetchResponseData toMessage(int throttleTimeMs, Errors error,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQwMzIzMQ=="}, "originalCommit": {"oid": "1a5def5bfb97d673243203f90cc9e723b3e24915"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjQ0NQ==", "bodyText": "Aren't many of these set automatically by the generated classes?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172445", "createdAt": "2021-02-27T18:33:25Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +123,70 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+                .setPartitionIndex(partition)\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 386}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjUzMA==", "bodyText": "Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also \"This is used to eliminate duplicate code of type casting.\" seems a bit redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172530", "createdAt": "2021-02-27T18:34:16Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +123,70 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+                .setPartitionIndex(partition)\n+                .setErrorCode(error.code())\n+                .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. This is used to eliminate duplicate code of type casting.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 390}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjYwNg==", "bodyText": "Nit: indenting seems excessive.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172606", "createdAt": "2021-02-27T18:35:10Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java", "diffHunk": "@@ -150,22 +150,23 @@ private static void assertListEquals(List<TopicPartition> expected, List<TopicPa\n \n     private static final class RespEntry {\n         final TopicPartition part;\n-        final FetchResponse.PartitionData<MemoryRecords> data;\n+        final FetchResponseData.PartitionData data;\n \n         RespEntry(String topic, int partition, long highWatermark, long lastStableOffset) {\n             this.part = new TopicPartition(topic, partition);\n-            this.data = new FetchResponse.PartitionData<>(\n-                Errors.NONE,\n-                highWatermark,\n-                lastStableOffset,\n-                0,\n-                null,\n-                null);\n+\n+            this.data = new FetchResponseData.PartitionData()\n+                        .setErrorCode(Errors.NONE.code())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MjkzNw==", "bodyText": "Are some of these redundant? (eg setAbortedTransactions(null))", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172937", "createdAt": "2021-02-27T18:37:41Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java", "diffHunk": "@@ -2377,14 +2378,19 @@ private ListOffsetsResponse listOffsetsResponse(Map<TopicPartition, Long> partit\n                     builder.append(0L, (\"key-\" + i).getBytes(), (\"value-\" + i).getBytes());\n                 records = builder.build();\n             }\n-            tpResponses.put(partition, new FetchResponse.PartitionData<>(\n-                    Errors.NONE, highWatermark, FetchResponse.INVALID_LAST_STABLE_OFFSET,\n-                    logStartOffset, null, records));\n+            tpResponses.put(partition,\n+                    new FetchResponseData.PartitionData()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setHighWatermark(highWatermark)\n+                            .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                            .setLogStartOffset(logStartOffset)\n+                            .setAbortedTransactions(null)\n+                            .setRecords(records));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3Mjk4NA==", "bodyText": "Are some of these redundant? (eg setAbortedTransactions(null))", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584172984", "createdAt": "2021-02-27T18:38:01Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java", "diffHunk": "@@ -150,22 +150,23 @@ private static void assertListEquals(List<TopicPartition> expected, List<TopicPa\n \n     private static final class RespEntry {\n         final TopicPartition part;\n-        final FetchResponse.PartitionData<MemoryRecords> data;\n+        final FetchResponseData.PartitionData data;\n \n         RespEntry(String topic, int partition, long highWatermark, long lastStableOffset) {\n             this.part = new TopicPartition(topic, partition);\n-            this.data = new FetchResponse.PartitionData<>(\n-                Errors.NONE,\n-                highWatermark,\n-                lastStableOffset,\n-                0,\n-                null,\n-                null);\n+\n+            this.data = new FetchResponseData.PartitionData()\n+                        .setErrorCode(Errors.NONE.code())\n+                        .setHighWatermark(highWatermark)\n+                        .setLastStableOffset(lastStableOffset)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(null)\n+                        .setRecords(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzE1Ng==", "bodyText": "Are some of these redundant? (eg setAbortedTransactions(null)). Other examples in the same file.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173156", "createdAt": "2021-02-27T18:39:09Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -1270,13 +1271,24 @@ public void testFetchPositionAfterException() {\n \n         assertEquals(1, fetcher.sendFetches());\n \n-        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new LinkedHashMap<>();\n-        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n-            FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, records));\n-        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.OFFSET_OUT_OF_RANGE, 100,\n-            FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));\n-        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n-            0, INVALID_SESSION_ID));\n+\n+        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new LinkedHashMap<>();\n+        partitions.put(tp1, new FetchResponseData.PartitionData()\n+                .setErrorCode(Errors.NONE.code())\n+                .setHighWatermark(100)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(records));\n+        partitions.put(tp0, new FetchResponseData.PartitionData()\n+                .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n+                .setHighWatermark(100)\n+                .setLastStableOffset(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n+                .setLogStartOffset(FetchResponse.INVALID_LOG_START_OFFSET)\n+                .setAbortedTransactions(null)\n+                .setRecords(MemoryRecords.EMPTY));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzMwNw==", "bodyText": "Nit: indenting seems wrong.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173307", "createdAt": "2021-02-27T18:40:10Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -364,7 +361,7 @@ abstract class AbstractFetcherThread(name: String,\n                       }\n                     }\n                     if (isTruncationOnFetchSupported) {\n-                      partitionData.divergingEpoch.ifPresent { divergingEpoch =>\n+                     FetchResponse.divergingEpoch(partitionData).ifPresent { divergingEpoch =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzQ0MQ==", "bodyText": "Not clear why we need this val. Seems like we can introduce a variable in the case _ instead.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173441", "createdAt": "2021-02-27T18:41:41Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -340,7 +336,8 @@ abstract class AbstractFetcherThread(name: String,\n             // the current offset is the same as the offset requested.\n             val fetchPartitionData = sessionPartitions.get(topicPartition)\n             if (fetchPartitionData != null && fetchPartitionData.fetchOffset == currentFetchState.fetchOffset && currentFetchState.isReadyForFetch) {\n-              partitionData.error match {\n+              val partitionError = Errors.forCode(partitionData.errorCode)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE3MzUzMg==", "bodyText": "Do we have to update the matching inside the method to handle other potential records types? Or do we want to avoid changing this method signature instead?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584173532", "createdAt": "2021-02-27T18:42:34Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -734,7 +730,7 @@ abstract class AbstractFetcherThread(name: String,\n     Option(partitionStates.stateValue(topicPartition))\n   }\n \n-  protected def toMemoryRecords(records: Records): MemoryRecords = {\n+  protected def toMemoryRecords(records: BaseRecords): MemoryRecords = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjU2NQ==", "bodyText": "We can remove some redundant set calls?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186565", "createdAt": "2021-02-27T20:24:50Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java", "diffHunk": "@@ -70,24 +70,25 @@ public void setUp() {\n         handler = new FetchSessionHandler(LOG_CONTEXT, 1);\n         FetchSessionHandler.Builder builder = handler.newBuilder();\n \n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> respMap = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> respMap = new LinkedHashMap<>();\n         for (int i = 0; i < partitionCount; i++) {\n             TopicPartition tp = new TopicPartition(\"foo\", i);\n             FetchRequest.PartitionData partitionData = new FetchRequest.PartitionData(0, 0, 200,\n                     Optional.empty());\n             fetches.put(tp, partitionData);\n             builder.add(tp, partitionData);\n-            respMap.put(tp, new FetchResponse.PartitionData<>(\n-                    Errors.NONE,\n-                    0L,\n-                    0L,\n-                    0,\n-                    null,\n-                    null));\n+            respMap.put(tp, new FetchResponseData.PartitionData()\n+                            .setPartitionIndex(tp.partition())\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setHighWatermark(0)\n+                            .setLastStableOffset(0)\n+                            .setLogStartOffset(0)\n+                            .setAbortedTransactions(null)\n+                            .setRecords(null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjYxOQ==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186619", "createdAt": "2021-02-27T20:25:10Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java", "diffHunk": "@@ -174,8 +173,14 @@ public int sizeInBytes() {\n                     return null;\n                 }\n             };\n-            initialFetched.put(tp, new FetchResponse.PartitionData<>(Errors.NONE, 0, 0, 0,\n-                    new LinkedList<>(), fetched));\n+            initialFetched.put(tp, new FetchResponseData.PartitionData()\n+                    .setPartitionIndex(tp.partition())\n+                    .setErrorCode(Errors.NONE.code())\n+                    .setHighWatermark(0)\n+                    .setLastStableOffset(0)\n+                    .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjY4Nw==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186687", "createdAt": "2021-02-27T20:25:28Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java", "diffHunk": "@@ -78,19 +78,25 @@ public void setup() {\n         for (int topicIdx = 0; topicIdx < topicCount; topicIdx++) {\n             String topic = UUID.randomUUID().toString();\n             for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n-                FetchResponse.PartitionData<MemoryRecords> partitionData = new FetchResponse.PartitionData<>(\n-                    Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records);\n+                FetchResponseData.PartitionData partitionData = new FetchResponseData.PartitionData()\n+                                .setPartitionIndex(partitionId)\n+                                .setErrorCode(Errors.NONE.code())\n+                                .setHighWatermark(0)\n+                                .setLastStableOffset(0)\n+                                .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjcyNA==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186724", "createdAt": "2021-02-27T20:25:54Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -963,9 +966,13 @@ class ReplicaFetcherThreadTest {\n \n     val records = MemoryRecords.withRecords(CompressionType.NONE,\n       new SimpleRecord(1000, \"foo\".getBytes(StandardCharsets.UTF_8)))\n-\n-    val partitionData: thread.FetchData = new FetchResponse.PartitionData[Records](\n-      Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records)\n+    val partitionData: thread.FetchData = new FetchResponseData.PartitionData()\n+        .setErrorCode(Errors.NONE.code)\n+        .setHighWatermark(0)\n+        .setLastStableOffset(0)\n+        .setLogStartOffset(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NjgwMA==", "bodyText": "We can remove some redundant setters?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584186800", "createdAt": "2021-02-27T20:26:34Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -155,13 +155,28 @@ class FetchSessionTest {\n     assertEquals(Optional.of(1), epochs1(tp1))\n     assertEquals(Optional.of(2), epochs1(tp2))\n \n-    val response = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    response.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100, 100,\n-      100, null, null))\n-    response.put(tp1, new FetchResponse.PartitionData(\n-      Errors.NONE, 10, 10, 10, null, null))\n-    response.put(tp2, new FetchResponse.PartitionData(\n-      Errors.NONE, 5, 5, 5, null, null))\n+    val response = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    response.put(tp0, new FetchResponseData.PartitionData()\n+      .setErrorCode(Errors.NONE.code)\n+      .setHighWatermark(100)\n+      .setLastStableOffset(100)\n+      .setLogStartOffset(100)\n+      .setAbortedTransactions(null)\n+      .setRecords(null))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzA2MA==", "bodyText": "Nit: remove () twice.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187060", "createdAt": "2021-02-27T20:28:20Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala", "diffHunk": "@@ -403,25 +401,24 @@ private class ReplicaFetcher(name: String, sourceBroker: Node, topicPartitions:\n \n     debug(\"Issuing fetch request \")\n \n-    var fetchResponse: FetchResponse[MemoryRecords] = null\n+    var fetchResponse: FetchResponse = null\n     try {\n       val clientResponse = fetchEndpoint.sendRequest(fetchRequestBuilder)\n-      fetchResponse = clientResponse.responseBody.asInstanceOf[FetchResponse[MemoryRecords]]\n+      fetchResponse = clientResponse.responseBody.asInstanceOf[FetchResponse]\n     } catch {\n       case t: Throwable =>\n         if (!isRunning)\n           throw t\n     }\n \n     if (fetchResponse != null) {\n-      fetchResponse.responseData.forEach { (tp, partitionData) =>\n-        replicaBuffer.addFetchedData(tp, sourceBroker.id, partitionData)\n-      }\n+      fetchResponse.data.responses().forEach(topicResponse =>\n+        topicResponse.partitions().forEach(partitionResponse =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzM2Ng==", "bodyText": "Do we have to copy like this or can we mutate the response?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187366", "createdAt": "2021-02-27T20:31:14Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzM5Nw==", "bodyText": "Similar question, is the copy required?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187397", "createdAt": "2021-02-27T20:31:38Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE4NzQzNg==", "bodyText": "Similar question, is the copy required?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584187436", "createdAt": "2021-02-27T20:32:04Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,76 +754,80 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        partitions.put(tp, new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp.partition)\n+            .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+            .setHighWatermark(data.highWatermark)\n+            .setLastStableOffset(lastStableOffset)\n+            .setLogStartOffset(data.logStartOffset)\n+            .setAbortedTransactions(abortedTransactions)\n+            .setRecords(data.records)\n+            .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+            .setDivergingEpoch(data.divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff57d4e58c1fc1389a5e87bf4734fd5ded385ee9"}, "originalPosition": 210}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e9e84739ea59f6ad2aaba99f5926bf469adf8a9", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/0e9e84739ea59f6ad2aaba99f5926bf469adf8a9", "committedDate": "2021-02-28T04:09:03Z", "message": "remove duplicate assignments; batch data; add more comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/033b9338f148ed87c22206e08ede12f16d2ead35", "committedDate": "2021-03-01T10:41:09Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwODY2NDA1", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-600866405", "createdAt": "2021-03-01T15:36:47Z", "commit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "state": "COMMENTED", "comments": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNTozNjo0N1rOItuICA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxNjowMzo1N1rOItv5jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxMjU1Mg==", "bodyText": "Aborted transactions is empty by default.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584812552", "createdAt": "2021-03-01T15:36:47Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -791,15 +791,18 @@ public void produceRequestGetErrorResponseTest() {\n \n     @Test\n     public void fetchResponseVersionTest() {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n \n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(\n-                Errors.NONE, 1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET,\n-                0L, Optional.empty(), Collections.emptyList(), records));\n-\n-        FetchResponse<MemoryRecords> v0Response = new FetchResponse<>(Errors.NONE, responseData, 0, INVALID_SESSION_ID);\n-        FetchResponse<MemoryRecords> v1Response = new FetchResponse<>(Errors.NONE, responseData, 10, INVALID_SESSION_ID);\n+        responseData.put(new TopicPartition(\"test\", 0),\n+                new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxNTI4OQ==", "bodyText": "Do we need to set the partition id here? There are a few other cases in this file that are similar.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584815289", "createdAt": "2021-03-01T15:39:13Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -808,22 +811,32 @@ public void fetchResponseVersionTest() {\n \n     @Test\n     public void testFetchResponseV4() {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n \n-        List<FetchResponse.AbortedTransaction> abortedTransactions = asList(\n-                new FetchResponse.AbortedTransaction(10, 100),\n-                new FetchResponse.AbortedTransaction(15, 50)\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = asList(\n+                new FetchResponseData.AbortedTransaction().setProducerId(10).setFirstOffset(100),\n+                new FetchResponseData.AbortedTransaction().setProducerId(15).setFirstOffset(50)\n         );\n-        responseData.put(new TopicPartition(\"bar\", 0), new FetchResponse.PartitionData<>(Errors.NONE, 100000,\n-                FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), abortedTransactions, records));\n-        responseData.put(new TopicPartition(\"bar\", 1), new FetchResponse.PartitionData<>(Errors.NONE, 900000,\n-                5, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, records));\n-        responseData.put(new TopicPartition(\"foo\", 0), new FetchResponse.PartitionData<>(Errors.NONE, 70000,\n-                6, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), emptyList(), records));\n-\n-        FetchResponse<MemoryRecords> response = new FetchResponse<>(Errors.NONE, responseData, 10, INVALID_SESSION_ID);\n-        FetchResponse<MemoryRecords> deserialized = FetchResponse.parse(response.serialize((short) 4), (short) 4);\n+        responseData.put(new TopicPartition(\"bar\", 0),\n+                new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setAbortedTransactions(abortedTransactions)\n+                        .setRecords(records));\n+        responseData.put(new TopicPartition(\"bar\", 1),\n+                new FetchResponseData.PartitionData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxNzQ5Mg==", "bodyText": "No need to set aborted transactions.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584817492", "createdAt": "2021-03-01T15:40:40Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1159,47 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgxODk0Mw==", "bodyText": "Aborted transactions is empty by default.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584818943", "createdAt": "2021-03-01T15:41:34Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1159,47 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(abortedTransactions));\n+        return FetchResponse.of(Errors.NONE, 25, sessionId, responseData);\n+    }\n+\n+    private FetchResponse createFetchResponse(boolean includeAborted) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyMDcyMA==", "bodyText": "We should not have : Errors here as it introduces a type test. What we want is for this to be a catch all.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584820720", "createdAt": "2021-03-01T15:42:51Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -416,9 +412,8 @@ abstract class AbstractFetcherThread(name: String,\n                        \"expected to persist.\")\n                   partitionsWithError += topicPartition\n \n-                case _ =>\n-                  error(s\"Error for partition $topicPartition at offset ${currentFetchState.fetchOffset}\",\n-                    partitionData.error.exception)\n+                case partitionError: Errors =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyMzgyMg==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584823822", "createdAt": "2021-03-01T15:44:46Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "diffHunk": "@@ -1144,8 +1143,14 @@ class AbstractFetcherThreadTest {\n           (Errors.NONE, records)\n         }\n \n-        (partition, new FetchData(error, leaderState.highWatermark, leaderState.highWatermark, leaderState.logStartOffset,\n-          Optional.empty[Integer], List.empty.asJava, divergingEpoch.asJava, records))\n+        (partition, new FetchResponseData.PartitionData()\n+          .setErrorCode(error.code)\n+          .setHighWatermark(leaderState.highWatermark)\n+          .setLastStableOffset(leaderState.highWatermark)\n+          .setLogStartOffset(leaderState.logStartOffset)\n+          .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNDg5Mg==", "bodyText": "The previous code did asJava, why did we change it?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584824892", "createdAt": "2021-03-01T15:45:23Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "diffHunk": "@@ -1144,8 +1143,14 @@ class AbstractFetcherThreadTest {\n           (Errors.NONE, records)\n         }\n \n-        (partition, new FetchData(error, leaderState.highWatermark, leaderState.highWatermark, leaderState.logStartOffset,\n-          Optional.empty[Integer], List.empty.asJava, divergingEpoch.asJava, records))\n+        (partition, new FetchResponseData.PartitionData()\n+          .setErrorCode(error.code)\n+          .setHighWatermark(leaderState.highWatermark)\n+          .setLastStableOffset(leaderState.highWatermark)\n+          .setLogStartOffset(leaderState.logStartOffset)\n+          .setAbortedTransactions(Collections.emptyList())\n+          .setRecords(records)\n+          .setDivergingEpoch(divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNTgzNQ==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584825835", "createdAt": "2021-03-01T15:45:54Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -963,9 +963,11 @@ class ReplicaFetcherThreadTest {\n \n     val records = MemoryRecords.withRecords(CompressionType.NONE,\n       new SimpleRecord(1000, \"foo\".getBytes(StandardCharsets.UTF_8)))\n-\n-    val partitionData: thread.FetchData = new FetchResponse.PartitionData[Records](\n-      Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records)\n+    val partitionData: thread.FetchData = new FetchResponseData.PartitionData()\n+        .setLastStableOffset(0)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNjE1Ng==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584826156", "createdAt": "2021-03-01T15:46:05Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -531,10 +529,12 @@ class ReplicaFetcherThreadTest {\n     assertEquals(1, mockNetwork.fetchCount)\n     partitions.foreach { tp => assertEquals(Fetching, thread.fetchState(tp).get.state) }\n \n-    def partitionData(divergingEpoch: FetchResponseData.EpochEndOffset): FetchResponse.PartitionData[Records] = {\n-      new FetchResponse.PartitionData[Records](\n-        Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(),\n-        Optional.of(divergingEpoch), MemoryRecords.EMPTY)\n+    def partitionData(divergingEpoch: FetchResponseData.EpochEndOffset): FetchResponseData.PartitionData = {\n+      new FetchResponseData.PartitionData()\n+          .setLastStableOffset(0)\n+          .setLogStartOffset(0)\n+          .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNjUxOQ==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584826519", "createdAt": "2021-03-01T15:46:18Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java", "diffHunk": "@@ -78,19 +78,23 @@ public void setup() {\n         for (int topicIdx = 0; topicIdx < topicCount; topicIdx++) {\n             String topic = UUID.randomUUID().toString();\n             for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n-                FetchResponse.PartitionData<MemoryRecords> partitionData = new FetchResponse.PartitionData<>(\n-                    Errors.NONE, 0, 0, 0, Optional.empty(), Collections.emptyList(), records);\n+                FetchResponseData.PartitionData partitionData = new FetchResponseData.PartitionData()\n+                                .setPartitionIndex(partitionId)\n+                                .setLastStableOffset(0)\n+                                .setLogStartOffset(0)\n+                                .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyNjg3Mg==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584826872", "createdAt": "2021-03-01T15:46:31Z", "author": {"login": "ijuma"}, "path": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java", "diffHunk": "@@ -174,8 +173,12 @@ public int sizeInBytes() {\n                     return null;\n                 }\n             };\n-            initialFetched.put(tp, new FetchResponse.PartitionData<>(Errors.NONE, 0, 0, 0,\n-                    new LinkedList<>(), fetched));\n+            initialFetched.put(tp, new FetchResponseData.PartitionData()\n+                    .setPartitionIndex(tp.partition())\n+                    .setLastStableOffset(0)\n+                    .setLogStartOffset(0)\n+                    .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODAyNg==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828026", "createdAt": "2021-03-01T15:47:43Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -679,17 +760,24 @@ class FetchSessionTest {\n     assertEquals(Collections.singleton(tp2), resp2.responseData.keySet)\n \n     // All partitions with divergent epoch should be returned.\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)\n+          .setAbortedTransactions(Collections.emptyList())\n+          .setDivergingEpoch(divergingEpoch))\n     val resp3 = context2.updateAndGenerateResponseData(respData)\n     assertEquals(Errors.NONE, resp3.error)\n     assertEquals(resp1.sessionId, resp3.sessionId)\n     assertEquals(Utils.mkSet(tp1, tp2), resp3.responseData.keySet)\n \n     // Partitions that meet other conditions should be returned regardless of whether\n     // divergingEpoch is set or not.\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      110, 110, 0, Optional.empty(), Collections.emptyList(), Optional.empty(), null))\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+        .setHighWatermark(110)\n+        .setLastStableOffset(110)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 498}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODMwOA==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828308", "createdAt": "2021-03-01T15:48:02Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -679,17 +760,24 @@ class FetchSessionTest {\n     assertEquals(Collections.singleton(tp2), resp2.responseData.keySet)\n \n     // All partitions with divergent epoch should be returned.\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)\n+          .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 483}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODQxNA==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828414", "createdAt": "2021-03-01T15:48:10Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -658,12 +732,19 @@ class FetchSessionTest {\n     // Full fetch context returns all partitions in the response\n     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData, EMPTY_PART_LIST, isFollower = false)\n     assertEquals(classOf[FullFetchContext], context1.getClass)\n-    val respData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), Optional.empty(), null))\n-    val divergingEpoch = Optional.of(new FetchResponseData.EpochEndOffset().setEpoch(3).setEndOffset(90))\n-    respData.put(tp2, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    val respData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+        .setHighWatermark(105)\n+        .setLastStableOffset(105)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList()))\n+    val divergingEpoch = new FetchResponseData.EpochEndOffset().setEpoch(3).setEndOffset(90)\n+    respData.put(tp2, new FetchResponseData.PartitionData()\n+        .setHighWatermark(105)\n+        .setLastStableOffset(105)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 468}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyODQ5NQ==", "bodyText": "Redundant.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584828495", "createdAt": "2021-03-01T15:48:15Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -658,12 +732,19 @@ class FetchSessionTest {\n     // Full fetch context returns all partitions in the response\n     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData, EMPTY_PART_LIST, isFollower = false)\n     assertEquals(classOf[FullFetchContext], context1.getClass)\n-    val respData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    respData.put(tp1, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), Optional.empty(), null))\n-    val divergingEpoch = Optional.of(new FetchResponseData.EpochEndOffset().setEpoch(3).setEndOffset(90))\n-    respData.put(tp2, new FetchResponse.PartitionData(Errors.NONE,\n-      105, 105, 0, Optional.empty(), Collections.emptyList(), divergingEpoch, null))\n+    val respData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    respData.put(tp1, new FetchResponseData.PartitionData()\n+        .setHighWatermark(105)\n+        .setLastStableOffset(105)\n+        .setLogStartOffset(0)\n+        .setAbortedTransactions(Collections.emptyList()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 462}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgyOTU3OA==", "bodyText": "Do we need to set the partition id here and other cases?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584829578", "createdAt": "2021-03-01T15:49:32Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala", "diffHunk": "@@ -534,15 +588,21 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session2context = fetchManager.newContext(JFetchMetadata.INITIAL, session1req, EMPTY_PART_LIST, false)\n     assertEquals(classOf[FullFetchContext], session2context.getClass)\n-    val session2RespData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n-    session2RespData.put(new TopicPartition(\"foo\", 0), new FetchResponse.PartitionData(\n-      Errors.NONE, 100, 100, 100, null, null))\n-    session2RespData.put(new TopicPartition(\"foo\", 1), new FetchResponse.PartitionData(\n-      Errors.NONE, 10, 10, 10, null, null))\n+    val session2RespData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+    session2RespData.put(new TopicPartition(\"foo\", 0),\n+      new FetchResponseData.PartitionData()\n+        .setHighWatermark(100)\n+        .setLastStableOffset(100)\n+        .setLogStartOffset(100))\n+    session2RespData.put(new TopicPartition(\"foo\", 1),\n+      new FetchResponseData.PartitionData()\n+        .setHighWatermark(10)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 335}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgzMzY1Mg==", "bodyText": "Seems that we could set the diverging offset only if set and leave the default otherwise.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584833652", "createdAt": "2021-03-01T15:54:40Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,84 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        partitions.put(tp, new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp.partition)\n+            .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+            .setHighWatermark(data.highWatermark)\n+            .setLastStableOffset(lastStableOffset)\n+            .setLogStartOffset(data.logStartOffset)\n+            .setAbortedTransactions(abortedTransactions)\n+            .setRecords(data.records)\n+            .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+            .setDivergingEpoch(data.divergingEpoch.getOrElse(new FetchResponseData.EpochEndOffset)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDgzNjEwOA==", "bodyText": "There is one place in this PR that we check for null when computing the records size, maybe we can use this utility function there.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584836108", "createdAt": "2021-03-01T15:57:20Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 394}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDg0MTIxNA==", "bodyText": "Suggestion:\nReturns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`.\n\nIf this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire).", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584841214", "createdAt": "2021-03-01T16:03:25Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 389}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDg0MTYxNQ==", "bodyText": "Instead of casting blindly, can we include a reasonable error message if the cast fails?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r584841615", "createdAt": "2021-03-01T16:03:57Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,92 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * cast the BaseRecords of PartitionData to Records. KRPC converts the byte array to MemoryRecords so this method\n+     * never fail if the data is from KRPC.\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {\n+        return partition.records() == null ? MemoryRecords.EMPTY : (Records) partition.records();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "033b9338f148ed87c22206e08ede12f16d2ead35"}, "originalPosition": 395}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8263bda330b3736447900a86728e779699eaeaf8", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/8263bda330b3736447900a86728e779699eaeaf8", "committedDate": "2021-03-02T03:14:56Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/529d81df199555611e7721753ba2ea29cbef3880", "committedDate": "2021-03-02T04:17:00Z", "message": "remove redundant code; add more comment for casting; use helper method to get records size"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAxOTM3NzUw", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-601937750", "createdAt": "2021-03-02T15:21:38Z", "commit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyMTozOFrOIuhnLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMlQxNToyNDoxOVrOIuhxKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NjExMQ==", "bodyText": "No else needed since we used return for both other cases. For the exception, I think we can just throw ClassCastException since IllegalStateException doesn't fit very well for this case. I would also make the message a bit more generic to avoid it going stale when we add more Records subtypes. For example:\n\"The record type is \" + partition.records().getClass().getSimpleName() + \", which is not a subtype of \" +\nRecords.class.getSimpleName() + \". This method is only safe to call if the `FetchResponse` was\ndeserialized from bytes.\"", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585656111", "createdAt": "2021-03-02T15:21:38Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,98 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));\n         ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, data).data.size(cache, version);\n     }\n \n     @Override\n     public boolean shouldClientThrottle(short version) {\n         return version >= 8;\n     }\n-}\n+\n+    public static Optional<FetchResponseData.EpochEndOffset> divergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() < 0 ? Optional.empty()\n+                : Optional.of(partitionResponse.divergingEpoch());\n+    }\n+\n+    public static boolean isDivergingEpoch(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.divergingEpoch().epoch() >= 0;\n+    }\n+\n+    public static Optional<Integer> preferredReadReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() == INVALID_PREFERRED_REPLICA_ID ? Optional.empty()\n+                : Optional.of(partitionResponse.preferredReadReplica());\n+    }\n+\n+    public static boolean isPreferredReplica(FetchResponseData.PartitionData partitionResponse) {\n+        return partitionResponse.preferredReadReplica() != INVALID_PREFERRED_REPLICA_ID;\n+    }\n+\n+    public static FetchResponseData.PartitionData partitionResponse(int partition, Errors error) {\n+        return new FetchResponseData.PartitionData()\n+            .setPartitionIndex(partition)\n+            .setErrorCode(error.code())\n+            .setHighWatermark(FetchResponse.INVALID_HIGH_WATERMARK);\n+    }\n+\n+    /**\n+     * Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`.\n+     *\n+     * If this response was deserialized after a fetch, this method should never fail. An example where this would\n+     * fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and\n+     * sent on the wire).\n+     *\n+     * @param partition partition data\n+     * @return Records or empty record if the records in PartitionData is null.\n+     */\n+    public static Records records(FetchResponseData.PartitionData partition) {\n+        if (partition.records() == null) return MemoryRecords.EMPTY;\n+        else if (partition.records() instanceof Records) return (Records) partition.records();\n+        else throw new IllegalStateException(\"the record type is \" + partition.records().getClass().getSimpleName() +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 400}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1Njc5MQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585656791", "createdAt": "2021-03-02T15:22:21Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -791,15 +791,17 @@ public void produceRequestGetErrorResponseTest() {\n \n     @Test\n     public void fetchResponseVersionTest() {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n \n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(\n-                Errors.NONE, 1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET,\n-                0L, Optional.empty(), Collections.emptyList(), records));\n-\n-        FetchResponse<MemoryRecords> v0Response = new FetchResponse<>(Errors.NONE, responseData, 0, INVALID_SESSION_ID);\n-        FetchResponse<MemoryRecords> v1Response = new FetchResponse<>(Errors.NONE, responseData, 10, INVALID_SESSION_ID);\n+        responseData.put(new TopicPartition(\"test\", 0),\n+                new FetchResponseData.PartitionData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NzExOQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657119", "createdAt": "2021-03-02T15:22:43Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NzI3OQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657279", "createdAt": "2021-03-02T15:22:52Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1NzU1NQ==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657555", "createdAt": "2021-03-02T15:23:08Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(abortedTransactions));\n+        return FetchResponse.of(Errors.NONE, 25, sessionId, responseData);\n+    }\n+\n+    private FetchResponse createFetchResponse(boolean includeAborted) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1Nzc2OA==", "bodyText": "Set partition id.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585657768", "createdAt": "2021-03-02T15:23:20Z", "author": {"login": "ijuma"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1146,38 +1160,45 @@ private FetchRequest createFetchRequest(int version) {\n         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(Errors error, int sessionId) {\n-        return new FetchResponse<>(error, new LinkedHashMap<>(), 25, sessionId);\n+    private FetchResponse createFetchResponse(Errors error, int sessionId) {\n+        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n     }\n \n-    private FetchResponse<MemoryRecords> createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+    private FetchResponse createFetchResponse(int sessionId) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.singletonList(\n-            new FetchResponse.AbortedTransaction(234L, 999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-            1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n-        return new FetchResponse<>(Errors.NONE, responseData, 25, sessionId);\n-    }\n-\n-    private FetchResponse<MemoryRecords> createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n+            new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setAbortedTransactions(abortedTransactions));\n+        return FetchResponse.of(Errors.NONE, 25, sessionId, responseData);\n+    }\n+\n+    private FetchResponse createFetchResponse(boolean includeAborted) {\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n+        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n \n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponse.PartitionData<>(Errors.NONE,\n-                1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), Collections.emptyList(), records));\n-\n-        List<FetchResponse.AbortedTransaction> abortedTransactions = Collections.emptyList();\n+        List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.emptyList();\n         if (includeAborted) {\n             abortedTransactions = Collections.singletonList(\n-                    new FetchResponse.AbortedTransaction(234L, 999L));\n+                    new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n         }\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponse.PartitionData<>(Errors.NONE,\n-                1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, Optional.empty(), abortedTransactions, MemoryRecords.EMPTY));\n+        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n+                        .setHighWatermark(1000000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NTY1ODY2NQ==", "bodyText": "We can use the utility method you added to avoid the null check.", "url": "https://github.com/apache/kafka/pull/9758#discussion_r585658665", "createdAt": "2021-03-02T15:24:19Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -761,79 +754,85 @@ class KafkaApis(val requestChannel: RequestChannel,\n             // For fetch requests from clients, check if down-conversion is disabled for the particular partition\n             if (!fetchRequest.isFromFollower && !logConfig.forall(_.messageDownConversionEnable)) {\n               trace(s\"Conversion to message format ${downConvertMagic.get} is disabled for partition $tp. Sending unsupported version response to $clientId.\")\n-              errorResponse(Errors.UNSUPPORTED_VERSION)\n+              FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_VERSION)\n             } else {\n               try {\n                 trace(s\"Down converting records from partition $tp to message format version $magic for fetch request from $clientId\")\n                 // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much\n                 // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked\n                 // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the\n                 // client.\n-                val error = maybeDownConvertStorageError(partitionData.error)\n-                new FetchResponse.PartitionData[BaseRecords](error, partitionData.highWatermark,\n-                  partitionData.lastStableOffset, partitionData.logStartOffset,\n-                  partitionData.preferredReadReplica, partitionData.abortedTransactions,\n-                  new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                new FetchResponseData.PartitionData()\n+                  .setPartitionIndex(tp.partition)\n+                  .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+                  .setHighWatermark(partitionData.highWatermark)\n+                  .setLastStableOffset(partitionData.lastStableOffset)\n+                  .setLogStartOffset(partitionData.logStartOffset)\n+                  .setAbortedTransactions(partitionData.abortedTransactions)\n+                  .setRecords(new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time))\n+                  .setPreferredReadReplica(partitionData.preferredReadReplica())\n               } catch {\n                 case e: UnsupportedCompressionTypeException =>\n                   trace(\"Received unsupported compression type error during down-conversion\", e)\n-                  errorResponse(Errors.UNSUPPORTED_COMPRESSION_TYPE)\n+                  FetchResponse.partitionResponse(tp.partition, Errors.UNSUPPORTED_COMPRESSION_TYPE)\n               }\n             }\n           case None =>\n-            val error = maybeDownConvertStorageError(partitionData.error)\n-            new FetchResponse.PartitionData[BaseRecords](error,\n-              partitionData.highWatermark,\n-              partitionData.lastStableOffset,\n-              partitionData.logStartOffset,\n-              partitionData.preferredReadReplica,\n-              partitionData.abortedTransactions,\n-              partitionData.divergingEpoch,\n-              unconvertedRecords)\n+            new FetchResponseData.PartitionData()\n+              .setPartitionIndex(tp.partition)\n+              .setErrorCode(maybeDownConvertStorageError(Errors.forCode(partitionData.errorCode)).code)\n+              .setHighWatermark(partitionData.highWatermark)\n+              .setLastStableOffset(partitionData.lastStableOffset)\n+              .setLogStartOffset(partitionData.logStartOffset)\n+              .setAbortedTransactions(partitionData.abortedTransactions)\n+              .setRecords(unconvertedRecords)\n+              .setPreferredReadReplica(partitionData.preferredReadReplica)\n+              .setDivergingEpoch(partitionData.divergingEpoch)\n         }\n       }\n     }\n \n     // the callback for process a fetch response, invoked before throttling\n     def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[Records]]\n+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n       val reassigningPartitions = mutable.Set[TopicPartition]()\n       responsePartitionData.foreach { case (tp, data) =>\n         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch)\n-          reassigningPartitions.add(tp)\n-        val error = maybeDownConvertStorageError(data.error)\n-        partitions.put(tp, new FetchResponse.PartitionData(\n-          error,\n-          data.highWatermark,\n-          lastStableOffset,\n-          data.logStartOffset,\n-          data.preferredReadReplica.map(int2Integer).asJava,\n-          abortedTransactions,\n-          data.divergingEpoch.asJava,\n-          data.records))\n+        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n+        val partitionData = new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp.partition)\n+          .setErrorCode(maybeDownConvertStorageError(data.error).code)\n+          .setHighWatermark(data.highWatermark)\n+          .setLastStableOffset(lastStableOffset)\n+          .setLogStartOffset(data.logStartOffset)\n+          .setAbortedTransactions(abortedTransactions)\n+          .setRecords(data.records)\n+          .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+        data.divergingEpoch.foreach(partitionData.setDivergingEpoch)\n+        partitions.put(tp, partitionData)\n       }\n       erroneous.foreach { case (tp, data) => partitions.put(tp, data) }\n \n-      var unconvertedFetchResponse: FetchResponse[Records] = null\n+      var unconvertedFetchResponse: FetchResponse = null\n \n-      def createResponse(throttleTimeMs: Int): FetchResponse[BaseRecords] = {\n+      def createResponse(throttleTimeMs: Int): FetchResponse = {\n         // Down-convert messages for each partition if required\n-        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData[BaseRecords]]\n+        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n         unconvertedFetchResponse.responseData.forEach { (tp, unconvertedPartitionData) =>\n-          if (unconvertedPartitionData.error != Errors.NONE)\n+          val error = Errors.forCode(unconvertedPartitionData.errorCode)\n+          if (error != Errors.NONE)\n             debug(s\"Fetch request with correlation id ${request.header.correlationId} from client $clientId \" +\n-              s\"on partition $tp failed due to ${unconvertedPartitionData.error.exceptionName}\")\n+              s\"on partition $tp failed due to ${error.exceptionName}\")\n           convertedData.put(tp, maybeConvertFetchedData(tp, unconvertedPartitionData))\n         }\n \n         // Prepare fetch response from converted data\n-        val response = new FetchResponse(unconvertedFetchResponse.error, convertedData, throttleTimeMs,\n-          unconvertedFetchResponse.sessionId)\n+        val response = FetchResponse.of(unconvertedFetchResponse.error, throttleTimeMs, unconvertedFetchResponse.sessionId, convertedData)\n         // record the bytes out metrics only when the response is being sent\n         response.responseData.forEach { (tp, data) =>\n-          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower, reassigningPartitions.contains(tp), data.records.sizeInBytes)\n+          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower,\n+            reassigningPartitions.contains(tp), if (data.records  == null) 0 else data.records.sizeInBytes)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "529d81df199555611e7721753ba2ea29cbef3880"}, "originalPosition": 241}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f8e97b9822c12c0e56e4bc37a07a48f27993769", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/9f8e97b9822c12c0e56e4bc37a07a48f27993769", "committedDate": "2021-03-02T15:32:44Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4fa69c58ffe3e9a40dbb3556da92ed8a30f23936", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/4fa69c58ffe3e9a40dbb3556da92ed8a30f23936", "committedDate": "2021-03-02T15:37:19Z", "message": "fix build error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55b35ab5764d8129e23f386229eb350603b7b0aa", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/55b35ab5764d8129e23f386229eb350603b7b0aa", "committedDate": "2021-03-02T16:22:16Z", "message": "set partition index; add recordsSize;"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAzNzQ4MDkx", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-603748091", "createdAt": "2021-03-04T06:33:33Z", "commit": {"oid": "55b35ab5764d8129e23f386229eb350603b7b0aa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwNjozMzozM1rOIv_S4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwNjozMzozM1rOIv_S4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzE5MTAwOA==", "bodyText": "Is this an additional copy compared to previous behavior?", "url": "https://github.com/apache/kafka/pull/9758#discussion_r587191008", "createdAt": "2021-03-04T06:33:33Z", "author": {"login": "ijuma"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java", "diffHunk": "@@ -365,17 +126,105 @@ public int sessionId() {\n      * @param partIterator  The partition iterator.\n      * @return              The response size in bytes.\n      */\n-    public static <T extends BaseRecords> int sizeOf(short version,\n-                                                     Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator) {\n+    public static int sizeOf(short version,\n+                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(0, Errors.NONE, partIterator, INVALID_SESSION_ID);\n+        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> data = new LinkedHashMap<>();\n+        partIterator.forEachRemaining(entry -> data.put(entry.getKey(), entry.getValue()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55b35ab5764d8129e23f386229eb350603b7b0aa"}, "originalPosition": 350}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAzNzQ4Njg4", "url": "https://github.com/apache/kafka/pull/9758#pullrequestreview-603748688", "createdAt": "2021-03-04T06:34:51Z", "commit": {"oid": "55b35ab5764d8129e23f386229eb350603b7b0aa"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "821c307389bf613b15c92bd868cdeeac557b39d4", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/821c307389bf613b15c92bd868cdeeac557b39d4", "committedDate": "2021-03-04T07:29:06Z", "message": "Merge branch 'trunk' into MINOR-9758"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae25551171fd4e3b889ca94d494e3207545320e5", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ae25551171fd4e3b889ca94d494e3207545320e5", "committedDate": "2021-03-04T07:33:05Z", "message": "revert sizeOf"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2326, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}