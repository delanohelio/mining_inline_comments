{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1MTA4MTg4", "number": 8248, "title": "KAFKA-9501: convert between active and standby without closing stores", "bodyText": "This PR has gone through several significant transitions of its own, but here's the latest:\n\nTaskManager just collects the tasks to transition and refers to the active/standby task creator to handle closing & recycling the old task and creating the new one. If we ever hit an exception during the close, we bail and close all the remaining tasks as dirty.\nThe task creators tell the task to \"close but recycle state\". If this is successful, it tells the recycled processor context and state manager that they should transition to the new type.\nDuring \"close and recycle\" the task just does a normal clean close, but instead of closing the state manager it informs it to recycle itself:  maintain all of its store information (most importantly the current store offsets) but unregister the changelogs from the changelog reader\nThe new task will (re-)register its changelogs during initialization, but skip re-registering any stores. It will still read the checkpoint file, but only use the written offsets if the store offsets are not already initialized from pre-transition\nTo ensure we don't end up with manual compaction disabled for standbys, we have to call the state restore listener's onRestoreEnd for any active restoring stores that are switching to standbys", "createdAt": "2020-03-07T05:47:44Z", "url": "https://github.com/apache/kafka/pull/8248", "merged": true, "mergeCommit": {"oid": "9d52deca247d9e16cf530d655891b2bbe474ffae"}, "closed": true, "closedAt": "2020-05-29T17:48:04Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 47, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcMxyVZAFqTM3MzIzMjY5Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcmDfXpgFqTQyMTAxMzEwNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczMjMyNjk3", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-373232697", "createdAt": "2020-03-12T01:53:29Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1MzoyOVrOF1O3lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1MzoyOVrOF1O3lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MjQ1NQ==", "bodyText": "Factored out the following to be reused in the case of transitioning from standby; a standby has only the state manager, everything else must be created just like a new task", "url": "https://github.com/apache/kafka/pull/8248#discussion_r391362455", "createdAt": "2020-03-12T01:53:29Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -132,47 +130,74 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                 partitions\n             );\n \n-            if (threadProducer == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczMjMzMTI5", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-373233129", "createdAt": "2020-03-12T01:55:14Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1NToxNFrOF1O5Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTo1NToxNFrOF1O5Fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MjgzOA==", "bodyText": "Pretty sure we should still count a task transition as a task creation event for the purposes of metrics", "url": "https://github.com/apache/kafka/pull/8248#discussion_r391362838", "createdAt": "2020-03-12T01:55:14Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -132,47 +130,74 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                 partitions\n             );\n \n-            if (threadProducer == null) {\n-                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n-                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n-                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n-                log.info(\"Creating producer client for task {}\", taskId);\n-                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n-            }\n-\n-            final RecordCollector recordCollector = new RecordCollectorImpl(\n-                logContext,\n-                taskId,\n-                consumer,\n-                threadProducer != null ?\n-                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n-                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n-                config.defaultProductionExceptionHandler(),\n-                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n-                streamsMetrics\n-            );\n-\n-            final Task task = new StreamTask(\n+            createdTasks.add(createStreamTask(\n                 taskId,\n                 partitions,\n-                topology,\n                 consumer,\n-                config,\n-                streamsMetrics,\n-                stateDirectory,\n-                cache,\n-                time,\n+                logContext,\n                 stateManager,\n-                recordCollector\n-            );\n-\n-            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n-            createdTasks.add(task);\n-            createTaskSensor.record();\n+                topology));\n         }\n         return createdTasks;\n     }\n \n+    private StreamTask createStreamTask(final TaskId taskId,\n+                                        final Set<TopicPartition> partitions,\n+                                        final Consumer<byte[], byte[]> consumer,\n+                                        final LogContext logContext,\n+                                        final ProcessorStateManager stateManager,\n+                                        final ProcessorTopology topology) {\n+        if (threadProducer == null) {\n+            final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+            producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+            log.info(\"Creating producer client for task {}\", taskId);\n+            taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+        }\n+\n+        final RecordCollector recordCollector = new RecordCollectorImpl(\n+            logContext,\n+            taskId,\n+            consumer,\n+            threadProducer != null ?\n+                new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                                                                                          new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+            config.defaultProductionExceptionHandler(),\n+            EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+            streamsMetrics\n+        );\n+\n+        final StreamTask task = new StreamTask(\n+            taskId,\n+            partitions,\n+            topology,\n+            consumer,\n+            config,\n+            streamsMetrics,\n+            stateDirectory,\n+            cache,\n+            time,\n+            stateManager,\n+            recordCollector\n+        );\n+\n+        log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+        createTaskSensor.record();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5NjAwMTg3", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-399600187", "createdAt": "2020-04-24T02:31:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMjozMToxM1rOGLDpJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMjozMToxM1rOGLDpJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI0NzIwNA==", "bodyText": "Wasn't really sure where to put this test, but this class seemed close enough. Verified that this does indeed fail on trunk without this fix", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414247204", "createdAt": "2020-04-24T02:31:13Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskCreationIntegrationTest.java", "diffHunk": "@@ -148,6 +161,82 @@ public void shouldCreateStandByTasksForMaterializedAndOptimizedSourceTables() th\n         );\n     }\n \n+    @Test\n+    public void shouldRecycleStateFromStandbyTaskPromotedToActiveTask() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwMjQzMDQ1", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-400243045", "createdAt": "2020-04-24T20:16:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMDoxNjozNVrOGLnrPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjozMTo0NVrOGPXYEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgzNzU2NQ==", "bodyText": "Ok, it seems like this one at least is avoidable.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414837565", "createdAt": "2020-04-24T20:16:35Z", "author": {"login": "vvcephei"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -156,7 +156,7 @@\n               files=\"(TopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamThread|StreamTask).java\"/>\n \n     <suppress checks=\"MethodLength\"\n-              files=\"(KTableImpl|StreamsPartitionAssignor.java)\"/>\n+              files=\"(KTableImpl|StreamsPartitionAssignor|TaskManager).java\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg4MjYwMA==", "bodyText": "I agree.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414882600", "createdAt": "2020-04-24T21:48:08Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -132,47 +130,74 @@ private static String getTaskProducerClientId(final String threadClientId, final\n                 partitions\n             );\n \n-            if (threadProducer == null) {\n-                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n-                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n-                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n-                log.info(\"Creating producer client for task {}\", taskId);\n-                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n-            }\n-\n-            final RecordCollector recordCollector = new RecordCollectorImpl(\n-                logContext,\n-                taskId,\n-                consumer,\n-                threadProducer != null ?\n-                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n-                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n-                config.defaultProductionExceptionHandler(),\n-                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n-                streamsMetrics\n-            );\n-\n-            final Task task = new StreamTask(\n+            createdTasks.add(createStreamTask(\n                 taskId,\n                 partitions,\n-                topology,\n                 consumer,\n-                config,\n-                streamsMetrics,\n-                stateDirectory,\n-                cache,\n-                time,\n+                logContext,\n                 stateManager,\n-                recordCollector\n-            );\n-\n-            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n-            createdTasks.add(task);\n-            createTaskSensor.record();\n+                topology));\n         }\n         return createdTasks;\n     }\n \n+    private StreamTask createStreamTask(final TaskId taskId,\n+                                        final Set<TopicPartition> partitions,\n+                                        final Consumer<byte[], byte[]> consumer,\n+                                        final LogContext logContext,\n+                                        final ProcessorStateManager stateManager,\n+                                        final ProcessorTopology topology) {\n+        if (threadProducer == null) {\n+            final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+            producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+            log.info(\"Creating producer client for task {}\", taskId);\n+            taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+        }\n+\n+        final RecordCollector recordCollector = new RecordCollectorImpl(\n+            logContext,\n+            taskId,\n+            consumer,\n+            threadProducer != null ?\n+                new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                                                                                          new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+            config.defaultProductionExceptionHandler(),\n+            EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+            streamsMetrics\n+        );\n+\n+        final StreamTask task = new StreamTask(\n+            taskId,\n+            partitions,\n+            topology,\n+            consumer,\n+            config,\n+            streamsMetrics,\n+            stateDirectory,\n+            cache,\n+            time,\n+            stateManager,\n+            recordCollector\n+        );\n+\n+        log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+        createTaskSensor.record();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MjgzOA=="}, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwNjE3NQ==", "bodyText": "It seems like duelling formatters here and above. Do you want to propose that these it's better this way?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r414906175", "createdAt": "2020-04-24T22:53:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -235,9 +264,16 @@ void closeAndRemoveTaskProducerIfNeeded(final TaskId id) {\n             return Collections.singleton(getThreadProducerClientId(threadId));\n         } else {\n             return taskProducers.keySet()\n-                                .stream()\n-                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n-                                .collect(Collectors.toSet());\n+                .stream()\n+                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                .collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1NTkyOA==", "bodyText": "This comment needs to move into isLoggingEnabled. It doesn't make sense in this context anymore.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418755928", "createdAt": "2020-05-01T21:59:30Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +271,25 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {\n+        // if the store name does not exist in the changelog map, it means the underlying store\n+        // is not log enabled (including global stores), and hence it does not need to be restored", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1Njc5Nw==", "bodyText": "And this comment should move into getStorePartition", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418756797", "createdAt": "2020-05-01T22:02:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +271,25 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {\n+        // if the store name does not exist in the changelog map, it means the underlying store\n+        // is not log enabled (including global stores), and hence it does not need to be restored\n+        if (isLoggingEnabled(storeName)) {\n+            // NOTE we assume the partition of the topic can always be inferred from the task id;\n+            // if user ever use a custom partition grouper (deprecated in KIP-528) this would break and\n+            // it is not a regression (it would always break anyways)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1ODAwNw==", "bodyText": "Seems like this comment has also become displaced from its intended location before L83, stateMgr.initializeStoreOffsetsFromCheckpoint(storeDirsEmpty)", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418758007", "createdAt": "2020-05-01T22:06:28Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -70,18 +69,16 @@ static void registerStateStores(final Logger log,\n                 e\n             );\n         }\n+\n         log.debug(\"Acquired state directory lock\");\n \n         final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n \n         // We should only load checkpoint AFTER the corresponding state directory lock has been acquired and\n         // the state stores have been registered; we should not try to load at the state manager construction time.\n         // See https://issues.apache.org/jira/browse/KAFKA-8574", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1OTY2OA==", "bodyText": "It seems like the intent here is to skip initializing the store again if it's already been initialized, but still register it to the changelog reader. Which is ok, since we unregisterAllStoresWithChangelogReader in the recycleState, right?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418759668", "createdAt": "2020-05-01T22:12:27Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -70,18 +69,16 @@ static void registerStateStores(final Logger log,\n                 e\n             );\n         }\n+\n         log.debug(\"Acquired state directory lock\");\n \n         final boolean storeDirsEmpty = stateDirectory.directoryForTaskIsEmpty(id);\n \n         // We should only load checkpoint AFTER the corresponding state directory lock has been acquired and\n         // the state stores have been registered; we should not try to load at the state manager construction time.\n         // See https://issues.apache.org/jira/browse/KAFKA-8574\n-        for (final StateStore store : topology.stateStores()) {\n-            processorContext.uninitialize();\n-            store.init(processorContext, store);\n-            log.trace(\"Registered state store {}\", store.name());\n-        }\n+        stateMgr.registerStateStores(topology.stateStores(), processorContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2MTUyMg==", "bodyText": "It seems a bit \"bold\" here to assume that we want to flip the active/standby state. The TaskManager objectively knows what type it wants the task to become, so it seems it should just inform us of the desired task type in prepareForRecycle.\nOr, better yet, just null it out and we can set it during createStandbyTaskFromActive and createActiveTaskFromStandby. This kind of thing might also be nice, since we could have a more explicit lifecycle for this object:\n\nI'm ready (created, valid, good to go)\nI'm getting recycled\nI'm closed\n\nThen, we can ensure that these \"createXFromY\" methods cleanly take the state manager from \"closed\" to \"ready\".\nI'm not saying to add another state enum, but having a clearly defined lifecycle will help us later in maintenance.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418761522", "createdAt": "2020-05-01T22:19:20Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -505,4 +550,35 @@ private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n \n         return found.isEmpty() ? null : found.get(0);\n     }\n+\n+    void prepareForRecycle() {\n+        log.debug(\"Preparing to recycle state for {} task {}.\", taskType, taskId);\n+\n+        if (recyclingState) {\n+            throw new IllegalStateException(\"Attempted to re-recycle state without completing first recycle\");\n+        }\n+        recyclingState = true;\n+    }\n+\n+    void recycleState() {\n+        log.debug(\"Completed recycling state for formerly {} task {}.\", taskType, taskId);\n+\n+        if (!recyclingState) {\n+            throw new IllegalStateException(\"Attempted to complete recycle but state is not currently being recycled\");\n+        }\n+        recyclingState = false;\n+\n+        if (taskType == TaskType.ACTIVE) {\n+            taskType = TaskType.STANDBY;\n+        } else {\n+            taskType = TaskType.ACTIVE;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2NDgxOQ==", "bodyText": "I lost track of how this is now handled. Can you enlighten me?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418764819", "createdAt": "2020-05-01T22:31:45Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -155,9 +155,6 @@ void handleCorruption(final Map<TaskId, Collection<TopicPartition>> taskWithChan\n             final TaskId taskId = entry.getKey();\n             final Task task = tasks.get(taskId);\n \n-            // this call is idempotent so even if the task is only CREATED we can still call it\n-            changelogReader.remove(task.changelogPartitions());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NDkyOTg2", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-404492986", "createdAt": "2020-05-02T02:33:23Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwMjozMzoyNFrOGPb-JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQwMjozMzoyNFrOGPb-JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg0MDEwMQ==", "bodyText": "I know this might make the changes a bit harder to follow in the review, but I think this renaming will make things less confusing in the long run to be symmetrical to register", "url": "https://github.com/apache/kafka/pull/8248#discussion_r418840101", "createdAt": "2020-05-02T02:33:24Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -789,7 +789,7 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n     }\n \n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NDI0MDkw", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-405424090", "createdAt": "2020-05-04T23:57:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMzo1NzoxMVrOGQWR9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDozNzozMVrOGQW_aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5NTQ0NA==", "bodyText": "The storeMetadata.changelogPartition maybe null, while ArrayList would still put that null into the array.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419795444", "createdAt": "2020-05-04T23:57:11Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +270,20 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {\n+        if (isLoggingEnabled(storeName)) {\n+            changelogReader.register(getStorePartition(storeName), this);\n+        }\n+    }\n+\n+    private void unregisterAllStoresWithChangelogReader() {\n+        final List<TopicPartition> allChangelogPartitions = new ArrayList<>();\n+        for (final StateStoreMetadata storeMetadata : stores.values()) {\n+            allChangelogPartitions.add(storeMetadata.changelogPartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5NTkyMg==", "bodyText": "Some callers may be passing null into the collection, see my other comments.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419795922", "createdAt": "2020-05-04T23:58:55Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -789,7 +789,7 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n     }\n \n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODg0MDEwMQ=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5NjA0MA==", "bodyText": "nit: maybeRegisterStoreWithChangelogReader.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419796040", "createdAt": "2020-05-04T23:59:16Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -247,6 +270,20 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n         }\n     }\n \n+    private void registerStoreWithChangelogReader(final String storeName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwNzA4Mw==", "bodyText": "This is a meta comment: I'm a bit inclined to suggest that we do this recycle on the task-manager, i.e. do not call task.close() inside task-manager and let the close() internally check a boolean whether it is prepareRecycle.\nInstead, we can do the following:\n\n\nAdd a Task#convertTo(TaskType) interface which would return an active / standby task copying the fields of the original task, originated in RESTORING state.\n\n\nFor active task, the implementation would be:\n\n\n\n\nfirst transit to the RESTORING state (we would allow SUSPENDED to transit to RESTORING too, so if it is not in CREATED, we can first suspend it and then transit to RESTORING).\n\n\nand then return a newly created standby task initialized state as RESTORING.\n\n\n\nFor standby task, the implementation would be:\n\n\n\nfirst transit to RESTORING state (which would usually be a no-op).\n\n\nand then return a newly created active task initialized state as RESTORING.\n\n\nAlso I realized that recordCollector.initialize(); in active task should be moved from initializeIfNeeded to completeRestoration. This is a minor bug that may block this proposal --- I will prepare a PR fixing this.\n\nThen on task manager, for those convertible tasks we would call convertTo instead of close / re-create via the task-creators.\n\nThe key behind this proposal is that:\n\nSuspended and Restoring states are actually the same for active and standby tasks.\nIn the future when we remove Suspended state we would just have Restoring.\nActive and Standby's Restoring state are actually the same in terms of functionality.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r419807083", "createdAt": "2020-05-05T00:37:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -204,7 +203,14 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 task.resume();\n                 standbyTasksToCreate.remove(task.id());\n-            } else /* we previously owned this task, and we don't have it anymore, or it has changed active/standby state */ {\n+            } else {\n+                // check for tasks that were owned previously but have changed active/standby status\n+                final boolean isTransitioningType = activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id());\n+                if (isTransitioningType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4NTk4NzM4", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-408598738", "createdAt": "2020-05-09T01:32:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTozMjoxMVrOGS3j_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTozMjoxMVrOGS3j_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNzg4Ng==", "bodyText": "Alright I tried to shore up the test to verify that we actually do not restore anything in addition to not closing the store itself. This should be closer to testing the specific behavior pointed out in the ticket", "url": "https://github.com/apache/kafka/pull/8248#discussion_r422437886", "createdAt": "2020-05-09T01:32:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -311,9 +325,98 @@ public void shouldProcessDataFromStoresWithLoggingDisabled() throws InterruptedE\n         latch.await(30, TimeUnit.SECONDS);\n \n         assertTrue(processorLatch.await(30, TimeUnit.SECONDS));\n+    }\n+\n+    @Test\n+    public void shouldRecycleStateFromStandbyTaskPromotedToActiveTaskAndNotRestore() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE1ODY1MTQy", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-415865142", "createdAt": "2020-05-21T04:06:46Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDowNjo0NlrOGYlhcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNDowNjo0NlrOGYlhcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzc3OQ==", "bodyText": "I tried to reduce the number of unnecessary local variables that could potentially get out of sync", "url": "https://github.com/apache/kafka/pull/8248#discussion_r428433779", "createdAt": "2020-05-21T04:06:46Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -109,7 +94,7 @@ public void logChange(final String storeName,\n                           final long timestamp) {\n         throwUnsupportedOperationExceptionIfStandby(\"logChange\");\n         // Sending null headers to changelog topics (KIP-244)\n-        collector.send(\n+        streamTask.recordCollector().send(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3NDA5MjAz", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-417409203", "createdAt": "2020-05-25T00:43:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMDo0MzoxMlrOGZyYcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMToyNzo1NlrOGZyuxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzA0MQ==", "bodyText": "newType is not needed since it is overridden via state manager?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429693041", "createdAt": "2020-05-25T00:43:12Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzYwMQ==", "bodyText": "Not a comment for this PR: since in 2.6+ we always try to commit all tasks when user request commits via context, we do not need to maintain the flag per-task, but per-thread. And then when we removed the deprecated schedule function we can remove the stream-task reference inside.\nCan we add a TODO marker to do that in the future?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429693601", "createdAt": "2020-05-25T00:47:58Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;\n+        streamTask = null;\n     }\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamsConfig config,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics) {\n-        this(\n-            id,\n-            null,\n-            config,\n-            null,\n-            stateMgr,\n-            metrics,\n-            new ThreadCache(\n-                new LogContext(String.format(\"stream-thread [%s] \", Thread.currentThread().getName())),\n-                0,\n-                metrics\n-            )\n-        );\n+    @Override\n+    public void registerNewTask(final Task task) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NDM5Mg==", "bodyText": "Thinking about this a bit more, seems we do not need transition but just a registerCache? Could this be consolidated with the function below?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429694392", "createdAt": "2020-05-25T00:54:09Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -42,48 +41,36 @@\n import static org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator.getReadWriteStore;\n \n public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier {\n-    // The below are both null for standby tasks\n-    private final StreamTask streamTask;\n-    private final RecordCollector collector;\n+    private StreamTask streamTask;     // always null for standby tasks\n \n     private final ToInternal toInternal = new ToInternal();\n     private final static To SEND_TO_ALL = To.all();\n \n     final Map<String, String> storeToChangelogTopic = new HashMap<>();\n \n-    ProcessorContextImpl(final TaskId id,\n-                         final StreamTask streamTask,\n-                         final StreamsConfig config,\n-                         final RecordCollector collector,\n-                         final ProcessorStateManager stateMgr,\n-                         final StreamsMetricsImpl metrics,\n-                         final ThreadCache cache) {\n+    public ProcessorContextImpl(final TaskId id,\n+                                final StreamsConfig config,\n+                                final ProcessorStateManager stateMgr,\n+                                final StreamsMetricsImpl metrics,\n+                                final ThreadCache cache) {\n         super(id, config, metrics, stateMgr, cache);\n-        this.streamTask = streamTask;\n-        this.collector = collector;\n+    }\n \n-        if (streamTask == null && taskType() == TaskType.ACTIVE) {\n-            throw new IllegalStateException(\"Tried to create context for active task but the streamtask was null\");\n-        }\n+    @Override\n+    public void transitionTaskType(final TaskType newType,\n+                                   final ThreadCache cache) {\n+        this.cache = cache;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzA0MQ=="}, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NDc3NQ==", "bodyText": "maybe we can refactor the interfaces to transitToActive and transitToStandby such that:\n\ntransitToActive: takes the streamTask (in the future I'd suggest we just pass in the record-collector after removing schedule and refactored request-commit) and the cache as parameters, check the current state manager's taskType is active.\ntransitToStandby: takes no parameter, reset streamTask and cache, check the current state manager's taskType is active.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429694775", "createdAt": "2020-05-25T00:57:31Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java", "diffHunk": "@@ -109,7 +94,7 @@ public void logChange(final String storeName,\n                           final long timestamp) {\n         throwUnsupportedOperationExceptionIfStandby(\"logChange\");\n         // Sending null headers to changelog topics (KIP-244)\n-        collector.send(\n+        streamTask.recordCollector().send(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzc3OQ=="}, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NTc0NQ==", "bodyText": "nit: I think now it's better to move the debug line 464 before unregisterAllStoresWithChangelogReader.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429695745", "createdAt": "2020-05-25T01:05:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -425,12 +449,14 @@ public void flush() {\n \n     /**\n      * {@link StateStore#close() Close} all stores (even in case of failure).\n-     * Log all exception and re-throw the first exception that did occur at the end.\n+     * Log all exceptions and re-throw the first exception that occurred at the end.\n      *\n      * @throws ProcessorStateException if any error happens when closing the state stores\n      */\n     @Override\n     public void close() throws ProcessorStateException {\n+        unregisterAllStoresWithChangelogReader();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NjczMg==", "bodyText": "nit: I'd suggest we have a separate recycle() function rather than piggy-backing on close with additional flag, since the clean is always true, i.e. we would always just 1) write checkpoints, 2) call stateMgr.recycle(), 3) then transit to CLOSED and if it is not in CREATED / RUNNING then do nothing. Though it has some duplicated lines but it is a bit straight-forward.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429696732", "createdAt": "2020-05-25T01:12:42Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -189,38 +190,52 @@ private void prepareClose(final boolean clean) {\n     @Override\n     public void closeClean(final Map<TopicPartition, Long> checkpoint) {\n         Objects.requireNonNull(checkpoint);\n-        close(true);\n+        close(true, false);\n \n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n-        close(false);\n+        close(false, false);\n \n         log.info(\"Closed dirty\");\n     }\n \n-    private void close(final boolean clean) {\n+    @Override\n+    public void closeAndRecycleState() {\n+        prepareClose(true);\n+        close(true, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NzI0Ng==", "bodyText": "Are these new lines intentional?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429697246", "createdAt": "2020-05-25T01:16:23Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -112,6 +103,7 @@ static void closeStateManager(final Logger log,\n         final AtomicReference<ProcessorStateException> firstException = new AtomicReference<>(null);\n         try {\n             if (stateDirectory.lock(id)) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NzU4MQ==", "bodyText": "Ditto here, I'd suggest add cycle as a separate function.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429697581", "createdAt": "2020-05-25T01:19:05Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -482,7 +488,8 @@ public void closeDirty() {\n      * </pre>\n      */\n     private void close(final boolean clean,\n-                       final Map<TopicPartition, Long> checkpoint) {\n+                       final Map<TopicPartition, Long> checkpoint,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5ODQwMQ==", "bodyText": "This will add all current tasks, is that right?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429698401", "createdAt": "2020-05-25T01:25:06Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -272,6 +274,36 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n+        if (taskCloseExceptions.isEmpty()) {\n+            Task oldTask = null;\n+            final Iterator<Task> transitioningTasksIter = tasksToRecycle.iterator();\n+            try {\n+                while (transitioningTasksIter.hasNext()) {\n+                    oldTask = transitioningTasksIter.next();\n+                    final Task newTask;\n+                    if (oldTask.isActive()) {\n+                        final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n+                        newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    } else {\n+                        final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n+                        newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n+                    }\n+                    tasks.remove(oldTask.id());\n+                    addNewTask(newTask);\n+                    transitioningTasksIter.remove();\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(oldTask.id(), e);\n+\n+                dirtyTasks.addAll(tasksToRecycle); // contains the tasks we have not yet tried to transition\n+                dirtyTasks.addAll(tasks.values());         // contains the new tasks we just created", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5ODc1OA==", "bodyText": "nit: I feel it is not necessary to remove the extra reference on the cache and then call context.cache() every time we need it, but I think this is really a very very nit point so your call.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r429698758", "createdAt": "2020-05-25T01:27:56Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "diffHunk": "@@ -46,7 +46,6 @@\n     private final SessionKeySchema keySchema;\n     private final SegmentedCacheFunction cacheFunction;\n     private String cacheName;\n-    private ThreadCache cache;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODM0Njkx", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-418834691", "createdAt": "2020-05-27T03:35:43Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzozNTo0NFrOGa4UPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzozNTo0NFrOGa4UPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzODg0Ng==", "bodyText": "Renamed to unregister and moved to ChangelogRegister interface", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430838846", "createdAt": "2020-05-27T03:35:44Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java", "diffHunk": "@@ -45,12 +44,6 @@\n      */\n     Set<TopicPartition> completedChangelogs();\n \n-    /**\n-     * Removes the passed in partitions from the set of changelogs\n-     * @param revokedPartitions the set of partitions to remove\n-     */\n-    void remove(Collection<TopicPartition> revokedPartitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODQxNzIy", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-418841722", "createdAt": "2020-05-27T04:02:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNDowMjoyNlrOGa4rBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNDowMjoyNlrOGa4rBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng==", "bodyText": "Alright, the situation here is that we need to make sure we toggle bulk loading off for any active restoring tasks that convert to standby. Rather than try and force a direct call to toggleForBulkLoading on the store itself I figured we should just call onRestoreEnd. Technically, restoration is ending. It just happens to be due to type transition, rather than restore completion.\nI figured this might be relevant for users of custom stores, which might do something similar to bulk loading that they wish to turn off for standbys. But since this is only relevant to the underlying store, and doesn't mean we have actually finished restoring a task, we should only call the specific store's listener -- and not the user registered global listener.\nWDYT?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r430844676", "createdAt": "2020-05-27T04:02:26Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5MzY3OTIx", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-419367921", "createdAt": "2020-05-27T15:53:56Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNTo1Mzo1NlrOGbRgyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QyMTo0NzowNVrOGbeUqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI1MTY1Nw==", "bodyText": "When I introduced closeClean and closeDirty, I resisted the urge to inline close(boolean) only to control the LOC of the change. Having more branches and flags is generally more of a liability than a few duplicated statements.\nNow that we have two boolean flags (again) and new branch in the internal close method, I'd be much more inclined to inline it. But we can do this in a follow-on PR, if you prefer.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431251657", "createdAt": "2020-05-27T15:53:56Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -189,38 +190,52 @@ private void prepareClose(final boolean clean) {\n     @Override\n     public void closeClean(final Map<TopicPartition, Long> checkpoint) {\n         Objects.requireNonNull(checkpoint);\n-        close(true);\n+        close(true, false);\n \n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n-        close(false);\n+        close(false, false);\n \n         log.info(\"Closed dirty\");\n     }\n \n-    private void close(final boolean clean) {\n+    @Override\n+    public void closeAndRecycleState() {\n+        prepareClose(true);\n+        close(true, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NjczMg=="}, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyNDU3Mw==", "bodyText": "I think this makes sense. The restore listener / bulk loading interaction is a bit wacky, but it seems reasonable to just work around it for now.\nJust to play devil's advocate briefly, though, is it not true for all listeners that the restore has ended, for exactly the reason you cited above?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431324573", "createdAt": "2020-05-27T17:40:36Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ2MTU0NQ==", "bodyText": "This seems like an odd case. Am I right in reading this as, \"something went wrong, and we don't know what it was, so we're just going to assume the worst and dump all the tasks that we were hoping to recycle\"?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431461545", "createdAt": "2020-05-27T21:47:05Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -272,6 +274,30 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n+        if (taskCloseExceptions.isEmpty()) {\n+            for (final Task oldTask : tasksToRecycle) {\n+                final Task newTask;\n+                try {\n+                    if (oldTask.isActive()) {\n+                        final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n+                        newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    } else {\n+                        final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n+                        newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n+                    }\n+                    tasks.remove(oldTask.id());\n+                    addNewTask(newTask);\n+                } catch (final RuntimeException e) {\n+                    final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n+                    log.error(uncleanMessage, e);\n+                    taskCloseExceptions.put(oldTask.id(), e);\n+                    dirtyTasks.add(oldTask);\n+                }\n+            }\n+        } else {\n+            dirtyTasks.addAll(tasksToRecycle);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NzI5MzI4", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-419729328", "createdAt": "2020-05-28T02:07:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQwMjowNzoxMVrOGbjQMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQwMjowNzoxMVrOGbjQMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MjMyMg==", "bodyText": "@guozhangwang was there a reason for these to be TreeMaps?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r431542322", "createdAt": "2020-05-28T02:07:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -183,12 +180,15 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new TreeMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new TreeMap<>(standbyTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwMzMxODgx", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-420331881", "createdAt": "2020-05-28T17:22:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzoyMjoxM1rOGb_NoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzoyMjoxM1rOGb_NoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAwMDQxNw==", "bodyText": "Yup, I propose to decouple these two and only allow restore callback at the per-store level and restore listener at the global level. We will always open the store with compaction disabled etc when we are transiting to restoring, and after we've done the restoration (for active) we will do a one-off compaction, and then reopen the store with configs overridden.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432000417", "createdAt": "2020-05-28T17:22:13Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -811,17 +812,41 @@ private void prepareChangelogs(final Set<ChangelogMetadata> newPartitionsToResto\n         }\n     }\n \n+    private RuntimeException invokeOnRestoreEnd(final TopicPartition partition,\n+                                                final ChangelogMetadata changelogMetadata) {\n+        // only trigger the store's specific listener to make sure we disable bulk loading before transition to standby\n+        final StateStoreMetadata storeMetadata = changelogMetadata.storeMetadata;\n+        final StateRestoreCallback restoreCallback = storeMetadata.restoreCallback();\n+        final String storeName = storeMetadata.store().name();\n+        if (restoreCallback instanceof StateRestoreListener) {\n+            try {\n+                ((StateRestoreListener) restoreCallback).onRestoreEnd(partition, storeName, changelogMetadata.totalRestored);\n+            } catch (final RuntimeException e) {\n+                return e;\n+            }\n+        }\n+        return null;\n+    }\n+\n     @Override\n-    public void remove(final Collection<TopicPartition> revokedChangelogs) {\n-        // Only changelogs that are initialized that been added to the restore consumer's assignment\n+    public void unregister(final Collection<TopicPartition> revokedChangelogs,\n+                           final boolean triggerOnRestoreEnd) {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        // Only changelogs that are initialized have been added to the restore consumer's assignment\n         final List<TopicPartition> revokedInitializedChangelogs = new ArrayList<>();\n \n         for (final TopicPartition partition : revokedChangelogs) {\n             final ChangelogMetadata changelogMetadata = changelogs.remove(partition);\n             if (changelogMetadata != null) {\n-                if (changelogMetadata.state() != ChangelogState.REGISTERED) {\n+                if (triggerOnRestoreEnd && changelogMetadata.state().equals(ChangelogState.RESTORING)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NDY3Ng=="}, "originalCommit": null, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwNDM4NDk2", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-420438496", "createdAt": "2020-05-28T19:44:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo0NDo1OFrOGcD7bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDo1NDo1MFrOGcGXnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA3NzY3OA==", "bodyText": "I don't think there was a reason besides determinism for debugging, etc.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432077678", "createdAt": "2020-05-28T19:44:58Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -183,12 +180,15 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new TreeMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new TreeMap<>(standbyTasks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MjMyMg=="}, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzcxNQ==", "bodyText": "Why do we need to remove the Rule annotation?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432083715", "createdAt": "2020-05-28T19:52:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -79,25 +88,27 @@\n \n     private static final String APPID = \"restore-test\";\n \n-    @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n+    public final EmbeddedKafkaCluster cluster = new EmbeddedKafkaCluster(NUM_BROKERS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwOTIwNw==", "bodyText": "This isn't threadsafe, but it looks like we're using it from multiple threads during the tests.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432109207", "createdAt": "2020-05-28T20:37:45Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java", "diffHunk": "@@ -1247,4 +1270,33 @@ public void waitForNextStableAssignment(final long maxWaitMs) throws Interrupted\n             );\n         }\n     }\n+\n+    public static class TrackingStateRestoreListener implements StateRestoreListener {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExMzY4NQ==", "bodyText": "I feel like I missed something here. We don't expect the changelog to get unregistered during close anymore?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432113685", "createdAt": "2020-05-28T20:46:53Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1468,9 +1456,6 @@ public void shouldCloseActiveTasksAndIgnoreExceptionsOnUncleanShutdown() {\n \n         resetToStrict(changeLogReader);\n         expect(changeLogReader.completedChangelogs()).andReturn(emptySet());\n-        // make sure we also remove the changelog partitions from the changelog reader\n-        changeLogReader.remove(eq(singletonList(changelog)));\n-        expectLastCall();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExNzY2MA==", "bodyText": "One high-level concern I've developed during this review is whether there's any possibility that something like this could leak into the public API. I.e., is it possible that a Processor, Transformer, or StateStore could have cached some reference from the context that would become invalid when the context gets recycled, similar to the way this recordCollector (which is not public, I know) did.\nWhat's your take on that, @ableegoldman ?", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432117660", "createdAt": "2020-05-28T20:54:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -263,27 +259,28 @@ private void logValue(final Bytes key, final BufferKey bufferKey, final BufferVa\n         final ByteBuffer buffer = value.serialize(sizeOfBufferTime);\n         buffer.putLong(bufferKey.time());\n \n-        collector.send(\n-            changelogTopic,\n-            key,\n-            buffer.array(),\n-            V_2_CHANGELOG_HEADERS,\n-            partition,\n-            null,\n-            KEY_SERIALIZER,\n-            VALUE_SERIALIZER\n+        ((RecordCollector.Supplier) context).recordCollector().send(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25bac9ab42393c80e55bab751f64eb4cdf75a189", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/25bac9ab42393c80e55bab751f64eb4cdf75a189", "committedDate": "2020-05-28T22:18:40Z", "message": "Squashed for rebase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "36b372f2c68956fd914ad01e00ea7d8cb47ec4a8", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/36b372f2c68956fd914ad01e00ea7d8cb47ec4a8", "committedDate": "2020-05-28T22:18:40Z", "message": "fix checkstyle suppresion file merge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d823a4f0c2af99328429b4d3967e5f018f5c185b", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/d823a4f0c2af99328429b4d3967e5f018f5c185b", "committedDate": "2020-05-28T22:18:40Z", "message": "remove moved code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc5f2eb11bde4e1c31eaaf0360f058e1e2960964", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/dc5f2eb11bde4e1c31eaaf0360f058e1e2960964", "committedDate": "2020-05-28T22:18:40Z", "message": "need to re-register flush listener with new cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ac734054fb423d7407581a4062177a54d7d2a6c", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/1ac734054fb423d7407581a4062177a54d7d2a6c", "committedDate": "2020-05-28T22:18:40Z", "message": "fix mock"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84380330f12d93a116c6e3f5fc05a9efb17448e8", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/84380330f12d93a116c6e3f5fc05a9efb17448e8", "committedDate": "2020-05-28T22:18:40Z", "message": "fix suppression buffer record collector reference"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "84380330f12d93a116c6e3f5fc05a9efb17448e8", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/84380330f12d93a116c6e3f5fc05a9efb17448e8", "committedDate": "2020-05-28T22:18:40Z", "message": "fix suppression buffer record collector reference"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28ba380809d0b0d2a60c0381f4b74743bd9741bb", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/28ba380809d0b0d2a60c0381f4b74743bd9741bb", "committedDate": "2020-05-28T23:13:06Z", "message": "fix TrackingStateRestoreListener"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c491bfbff2805e78fd1c5f854b432e7b5954ceb0", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/c491bfbff2805e78fd1c5f854b432e7b5954ceb0", "committedDate": "2020-05-28T23:39:57Z", "message": "clean up RestoreIntegrationTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30ac7b3ccd47063497c17ac148d90f9b29683e82", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/30ac7b3ccd47063497c17ac148d90f9b29683e82", "committedDate": "2020-05-28T23:49:23Z", "message": "split test into separate close/recycle tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMDEzMTA0", "url": "https://github.com/apache/kafka/pull/8248#pullrequestreview-421013104", "createdAt": "2020-05-29T14:34:34Z", "commit": {"oid": "30ac7b3ccd47063497c17ac148d90f9b29683e82"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNDozNDozNVrOGcfepA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNDozNDozNVrOGcfepA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjUyOTA2MA==", "bodyText": "Note for the future: it's not necessary to prefix the temp directory.", "url": "https://github.com/apache/kafka/pull/8248#discussion_r432529060", "createdAt": "2020-05-29T14:34:35Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -77,29 +89,30 @@\n public class RestoreIntegrationTest {\n     private static final int NUM_BROKERS = 1;\n \n-    private static final String APPID = \"restore-test\";\n-\n     @ClassRule\n-    public static final EmbeddedKafkaCluster CLUSTER =\n-            new EmbeddedKafkaCluster(NUM_BROKERS);\n-    private static final String INPUT_STREAM = \"input-stream\";\n-    private static final String INPUT_STREAM_2 = \"input-stream-2\";\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(NUM_BROKERS);\n+\n+    @Rule\n+    public final TestName testName = new TestName();\n+    private String appId;\n+    private String inputStream;\n+\n     private final int numberOfKeys = 10000;\n     private KafkaStreams kafkaStreams;\n \n-    @BeforeClass\n-    public static void createTopics() throws InterruptedException {\n-        CLUSTER.createTopic(INPUT_STREAM, 2, 1);\n-        CLUSTER.createTopic(INPUT_STREAM_2, 2, 1);\n-        CLUSTER.createTopic(APPID + \"-store-changelog\", 2, 1);\n+    @Before\n+    public void createTopics() throws InterruptedException {\n+        appId = safeUniqueTestName(RestoreIntegrationTest.class, testName);\n+        inputStream = appId + \"-input-stream\";\n+        CLUSTER.createTopic(inputStream, 2, 1);\n     }\n \n-    private Properties props(final String applicationId) {\n+    private Properties props() {\n         final Properties streamsConfiguration = new Properties();\n-        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);\n+        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, appId);\n         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n-        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(applicationId).getPath());\n+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(appId).getPath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30ac7b3ccd47063497c17ac148d90f9b29683e82"}, "originalPosition": 99}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 150, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}