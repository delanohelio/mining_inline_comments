{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2ODg5NTUx", "number": 8787, "title": "KAFKA-10085: correctly compute lag for optimized source changelogs", "bodyText": "Split out the optimized source changelogs and fetch the committed offsets rather than the end offset for task lag computation\nMust be cherrypicked to 2.6", "createdAt": "2020-06-03T00:00:18Z", "url": "https://github.com/apache/kafka/pull/8787", "merged": true, "mergeCommit": {"oid": "42f46abb34a2b29993b1a8e6333a400a00227e30"}, "closed": true, "closedAt": "2020-06-11T14:47:49Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcoBgMFgFqTQyNDY1ODY2Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcqPNrWgFqTQyODk3NDI5OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0NjU4NjY2", "url": "https://github.com/apache/kafka/pull/8787#pullrequestreview-424658666", "createdAt": "2020-06-04T17:10:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzoxMDoyNVrOGfPsjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzoxODoyOVrOGfP-kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQxNjIwNA==", "bodyText": "What's the idea of dropping this?", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435416204", "createdAt": "2020-06-04T17:10:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMDgxOQ==", "bodyText": "This seems to be a step backwards, actually. Why wrap it as a StreamsException only just to immediately unwrap it again?", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435420819", "createdAt": "2020-06-04T17:18:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-\n         final Map<TopicPartition, Long> committedOffsets;\n         try {\n-            // those do not have a committed offset would default to 0\n-            committedOffsets =  mainConsumer.committed(partitions).entrySet().stream()\n-                .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue() == null ? 0L : e.getValue().offset()));\n-        } catch (final TimeoutException e) {\n-            // if it timed out we just retry next time.\n-            return Collections.emptyMap();\n-        } catch (final KafkaException e) {\n-            throw new StreamsException(String.format(\"Failed to retrieve end offsets for %s\", partitions), e);\n+            committedOffsets = fetchCommittedOffsets(partitions, mainConsumer);\n+        } catch (final StreamsException e) {\n+            if (e.getCause() instanceof TimeoutException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0OTQ2MDM1", "url": "https://github.com/apache/kafka/pull/8787#pullrequestreview-424946035", "createdAt": "2020-06-05T01:51:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMTo1MToxMVrOGfd0Vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMTo1MToxMVrOGfd0Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0NzU3NQ==", "bodyText": "@vvcephei I've been wondering if maybe we should only  catch the TimeoutException, and interpret a StreamsException as fatal (like IllegalStateException for example). This is how we were using  Consumer#committed in the StoreChangelogReader, and AFAICT that only throws KafkaException on \"unrecoverable errors\" (quoted from javadocs)\nBut I can't tell whether the Admin's listOffsets might throw on transient errors, so I'm leaning towards catching both just to be safe. WDYT?", "url": "https://github.com/apache/kafka/pull/8787#discussion_r435647575", "createdAt": "2020-06-05T01:51:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -763,18 +778,36 @@ private boolean populateClientStatesMap(final Map<UUID, ClientState> clientState\n                     .flatMap(Collection::stream)\n                     .collect(Collectors.toList());\n \n-            final Collection<TopicPartition> allPreexistingChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allPreexistingChangelogPartitions.removeIf(partition -> newlyCreatedChangelogs.contains(partition.topic()));\n+            final Set<TopicPartition> preexistingChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> preexistingSourceChangelogPartitions = new HashSet<>();\n+            final Set<TopicPartition> newlyCreatedChangelogPartitions = new HashSet<>();\n+            for (final TopicPartition changelog : allChangelogPartitions) {\n+                if (newlyCreatedChangelogs.contains(changelog.topic())) {\n+                    newlyCreatedChangelogPartitions.add(changelog);\n+                } else if (optimizedSourceChangelogs.contains(changelog.topic())) {\n+                    preexistingSourceChangelogPartitions.add(changelog);\n+                } else {\n+                    preexistingChangelogPartitions.add(changelog);\n+                }\n+            }\n+\n+            // Make the listOffsets request first so it can  fetch the offsets for non-source changelogs\n+            // asynchronously while we use the blocking Consumer#committed call to fetch source-changelog offsets\n+            final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> endOffsetsFuture =\n+                fetchEndOffsetsFuture(preexistingChangelogPartitions, adminClient);\n \n-            final Collection<TopicPartition> allNewlyCreatedChangelogPartitions = new ArrayList<>(allChangelogPartitions);\n-            allNewlyCreatedChangelogPartitions.removeAll(allPreexistingChangelogPartitions);\n+            final Map<TopicPartition, Long> sourceChangelogEndOffsets =\n+                fetchCommittedOffsets(preexistingSourceChangelogPartitions, taskManager.mainConsumer());\n \n-            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets =\n-                fetchEndOffsets(allPreexistingChangelogPartitions, adminClient);\n+            final Map<TopicPartition, ListOffsetsResultInfo> endOffsets = ClientUtils.getEndOffsets(endOffsetsFuture);\n \n-            allTaskEndOffsetSums = computeEndOffsetSumsByTask(endOffsets, changelogsByStatefulTask, allNewlyCreatedChangelogPartitions);\n+            allTaskEndOffsetSums = computeEndOffsetSumsByTask(\n+                changelogsByStatefulTask,\n+                endOffsets,\n+                sourceChangelogEndOffsets,\n+                newlyCreatedChangelogPartitions);\n             fetchEndOffsetsSuccessful = true;\n-        } catch (final StreamsException e) {\n+        } catch (final StreamsException | TimeoutException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 121}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1NjgxNTA0", "url": "https://github.com/apache/kafka/pull/8787#pullrequestreview-425681504", "createdAt": "2020-06-05T23:43:10Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzo0MzoxMVrOGgACcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzo0MzozOVrOGgACuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwODI0MA==", "bodyText": "Upon retrospect, I'm not sure if this is possible. The javadoc for Future#get indicates that any exception would be wrapped in an ExecutionException.", "url": "https://github.com/apache/kafka/pull/8787#discussion_r436208240", "createdAt": "2020-06-05T23:43:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ClientUtils.java", "diffHunk": "@@ -95,19 +99,65 @@ public static String getTaskProducerClientId(final String threadClientId, final\n         return result;\n     }\n \n-    public static Map<TopicPartition, ListOffsetsResultInfo> fetchEndOffsets(final Collection<TopicPartition> partitions,\n-                                                                             final Admin adminClient) {\n-        final Map<TopicPartition, ListOffsetsResultInfo> endOffsets;\n+    /**\n+     * @throws StreamsException if the consumer throws an exception\n+     * @throws org.apache.kafka.common.errors.TimeoutException if the request times out\n+     */\n+    public static Map<TopicPartition, Long> fetchCommittedOffsets(final Set<TopicPartition> partitions,\n+                                                                  final Consumer<byte[], byte[]> consumer) {\n+        if (partitions.isEmpty()) {\n+            return Collections.emptyMap();\n+        }\n+\n+        final Map<TopicPartition, Long> committedOffsets;\n         try {\n-            final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> future =  adminClient.listOffsets(\n-                partitions.stream().collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest())))\n-                                                                                        .all();\n-            endOffsets = future.get();\n+            // those which do not have a committed offset would default to 0\n+            committedOffsets = consumer.committed(partitions).entrySet().stream()\n+                .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue() == null ? 0L : e.getValue().offset()));\n+        } catch (final TimeoutException e) {\n+            LOG.warn(\"The committed offsets request timed out, try increasing the consumer client's default.api.timeout.ms\", e);\n+            throw e;\n+        } catch (final KafkaException e) {\n+            LOG.warn(\"The committed offsets request failed.\", e);\n+            throw new StreamsException(String.format(\"Failed to retrieve end offsets for %s\", partitions), e);\n+        }\n+\n+        return committedOffsets;\n+    }\n \n+    public static KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> fetchEndOffsetsFuture(final Collection<TopicPartition> partitions,\n+                                                                                                final Admin adminClient) {\n+        return adminClient.listOffsets(\n+            partitions.stream().collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest())))\n+            .all();\n+    }\n+\n+    /**\n+     * A helper method that wraps the {@code Future#get} call and rethrows any thrown exception as a StreamsException\n+     * @throws StreamsException if the admin client request throws an exception\n+     * @throws org.apache.kafka.common.errors.TimeoutException if the request times out\n+     */\n+    public static Map<TopicPartition, ListOffsetsResultInfo> getEndOffsets(final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> endOffsetsFuture) {\n+        try {\n+            return endOffsetsFuture.get();\n+        } catch (final TimeoutException e) {\n+            LOG.warn(\"The listOffsets request timed out, try increasing the admin client's default.api.timeout.ms\", e);\n+            throw e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwODMxMg==", "bodyText": "Ah, now I see it.", "url": "https://github.com/apache/kafka/pull/8787#discussion_r436208312", "createdAt": "2020-06-05T23:43:39Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -562,23 +564,18 @@ private void restoreChangelog(final ChangelogMetadata changelogMetadata) {\n     }\n \n     private Map<TopicPartition, Long> committedOffsetForChangelogs(final Set<TopicPartition> partitions) {\n-        if (partitions.isEmpty())\n-            return Collections.emptyMap();\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQxNjIwNA=="}, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50aadc714d5da05e4c4d384b06bbcac73b1d3d80", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/50aadc714d5da05e4c4d384b06bbcac73b1d3d80", "committedDate": "2020-06-10T16:45:20Z", "message": "WIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "699d8de34709536a83894fdc7a88e13a0caa21c9", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/699d8de34709536a83894fdc7a88e13a0caa21c9", "committedDate": "2020-06-10T16:45:20Z", "message": "refactor ClientUtils and get soruce changelogs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf228fbfcb816f2c32802aa2615bad81ec4a230a", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cf228fbfcb816f2c32802aa2615bad81ec4a230a", "committedDate": "2020-06-10T16:45:20Z", "message": "remove consumer reference"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a50a599cb4545d9b93a09f7f49e732fc6710bf69", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/a50a599cb4545d9b93a09f7f49e732fc6710bf69", "committedDate": "2020-06-10T16:45:20Z", "message": "remove unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecebe9a6cb3cc8d38ca6c50758fc66f4e98b19c8", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/ecebe9a6cb3cc8d38ca6c50758fc66f4e98b19c8", "committedDate": "2020-06-10T16:45:20Z", "message": "fix ClientUtilsTest test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da7ea8f6f09f041155d488079f1ed378d46c5a99", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/da7ea8f6f09f041155d488079f1ed378d46c5a99", "committedDate": "2020-06-10T17:25:50Z", "message": "throw TimeoutException separately"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bbd8b49fc2ec599b2178c93085e033ed6e545953", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/bbd8b49fc2ec599b2178c93085e033ed6e545953", "committedDate": "2020-06-10T17:25:54Z", "message": "unit test new ClientUtils methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f1260b6bf09ec1114c9312c2070bf290ac71be1", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/6f1260b6bf09ec1114c9312c2070bf290ac71be1", "committedDate": "2020-06-10T17:26:18Z", "message": "only catch the timeout e's"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cada478d83846e17d51ca4105008cf5853d60873", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cada478d83846e17d51ca4105008cf5853d60873", "committedDate": "2020-06-10T17:26:20Z", "message": "add SPA test coverage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "85acb837276be63479a8f5662d83cc252c6c0565", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/85acb837276be63479a8f5662d83cc252c6c0565", "committedDate": "2020-06-10T17:26:38Z", "message": "catch StreamsException too"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3863bb2177336be0fd59e75188bb7da28e8ee963", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/3863bb2177336be0fd59e75188bb7da28e8ee963", "committedDate": "2020-06-10T17:26:39Z", "message": "fix merge conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "303a6e7bca36a34ea2a1eee8ba71b410a174311f", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/303a6e7bca36a34ea2a1eee8ba71b410a174311f", "committedDate": "2020-06-10T17:26:39Z", "message": "remove unnecessary line change"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "303a6e7bca36a34ea2a1eee8ba71b410a174311f", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/303a6e7bca36a34ea2a1eee8ba71b410a174311f", "committedDate": "2020-06-10T17:26:39Z", "message": "remove unnecessary line change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d226945f517bd4037a4e9c5dc3711b30a931bd4", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/8d226945f517bd4037a4e9c5dc3711b30a931bd4", "committedDate": "2020-06-10T17:29:56Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0be5bb9e1fa73a4c28f01117466e349a684b639a", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/0be5bb9e1fa73a4c28f01117466e349a684b639a", "committedDate": "2020-06-10T17:32:06Z", "message": "remove no longer relevant test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bce24581523f955b02eb52b9e337742ee4b7a459", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/bce24581523f955b02eb52b9e337742ee4b7a459", "committedDate": "2020-06-10T17:34:31Z", "message": "only catch Streams/Timeout Exceptions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4OTc0Mjk5", "url": "https://github.com/apache/kafka/pull/8787#pullrequestreview-428974299", "createdAt": "2020-06-11T14:34:25Z", "commit": {"oid": "bce24581523f955b02eb52b9e337742ee4b7a459"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 841, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}