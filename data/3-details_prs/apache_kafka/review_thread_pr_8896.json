{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2NjU5MTA4", "number": 8896, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0NDozMVrOEG2fPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxNjoxODoyMFrOEHK5cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjE5NjQ3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0NDozMVrOGl5zUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0NDozMVrOGl5zUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NzUyMw==", "bodyText": "trivial cleanup", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442397523", "createdAt": "2020-06-18T17:44:31Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -415,19 +418,20 @@ public void restore() {\n                 // for restoring active and updating standby we may prefer different poll time\n                 // in order to make sure we call the main consumer#poll in time.\n                 // TODO: once we move ChangelogReader to a separate thread this may no longer be a concern\n-                polledRecords = restoreConsumer.poll(state.equals(ChangelogReaderState.STANDBY_UPDATING) ? Duration.ZERO : pollTime);\n+                polledRecords = restoreConsumer.poll(state == ChangelogReaderState.STANDBY_UPDATING ? Duration.ZERO : pollTime);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjIwMTIyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0NTo0OVrOGl52PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOTowMjo0OFrOGmepyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5ODI2OA==", "bodyText": "Added the exception itself as the \"cause\" of the warning. The actual message of the IOE is actually pretty good at explaining the root cause.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442398268", "createdAt": "2020-06-18T17:45:49Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -415,19 +418,20 @@ public void restore() {\n                 // for restoring active and updating standby we may prefer different poll time\n                 // in order to make sure we call the main consumer#poll in time.\n                 // TODO: once we move ChangelogReader to a separate thread this may no longer be a concern\n-                polledRecords = restoreConsumer.poll(state.equals(ChangelogReaderState.STANDBY_UPDATING) ? Duration.ZERO : pollTime);\n+                polledRecords = restoreConsumer.poll(state == ChangelogReaderState.STANDBY_UPDATING ? Duration.ZERO : pollTime);\n             } catch (final InvalidOffsetException e) {\n-                log.warn(\"Encountered {} fetching records from restore consumer for partitions {}, it is likely that \" +\n+                log.warn(\"Encountered \" + e.getClass().getName() +\n+                    \" fetching records from restore consumer for partitions \" + e.partitions() + \", it is likely that \" +\n                     \"the consumer's position has fallen out of the topic partition offset range because the topic was \" +\n                     \"truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing\" +\n-                    \" it later.\", e.getClass().getName(), e.partitions());\n+                    \" it later.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkyODM1OQ==", "bodyText": "The exception message may not always contain the partitions() list, maybe we should still print that as part of warn log?", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442928359", "createdAt": "2020-06-19T16:11:32Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -415,19 +418,20 @@ public void restore() {\n                 // for restoring active and updating standby we may prefer different poll time\n                 // in order to make sure we call the main consumer#poll in time.\n                 // TODO: once we move ChangelogReader to a separate thread this may no longer be a concern\n-                polledRecords = restoreConsumer.poll(state.equals(ChangelogReaderState.STANDBY_UPDATING) ? Duration.ZERO : pollTime);\n+                polledRecords = restoreConsumer.poll(state == ChangelogReaderState.STANDBY_UPDATING ? Duration.ZERO : pollTime);\n             } catch (final InvalidOffsetException e) {\n-                log.warn(\"Encountered {} fetching records from restore consumer for partitions {}, it is likely that \" +\n+                log.warn(\"Encountered \" + e.getClass().getName() +\n+                    \" fetching records from restore consumer for partitions \" + e.partitions() + \", it is likely that \" +\n                     \"the consumer's position has fallen out of the topic partition offset range because the topic was \" +\n                     \"truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing\" +\n-                    \" it later.\", e.getClass().getName(), e.partitions());\n+                    \" it later.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5ODI2OA=="}, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwMDA3Ng==", "bodyText": "It is still there, on L424.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r443000076", "createdAt": "2020-06-19T18:59:38Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -415,19 +418,20 @@ public void restore() {\n                 // for restoring active and updating standby we may prefer different poll time\n                 // in order to make sure we call the main consumer#poll in time.\n                 // TODO: once we move ChangelogReader to a separate thread this may no longer be a concern\n-                polledRecords = restoreConsumer.poll(state.equals(ChangelogReaderState.STANDBY_UPDATING) ? Duration.ZERO : pollTime);\n+                polledRecords = restoreConsumer.poll(state == ChangelogReaderState.STANDBY_UPDATING ? Duration.ZERO : pollTime);\n             } catch (final InvalidOffsetException e) {\n-                log.warn(\"Encountered {} fetching records from restore consumer for partitions {}, it is likely that \" +\n+                log.warn(\"Encountered \" + e.getClass().getName() +\n+                    \" fetching records from restore consumer for partitions \" + e.partitions() + \", it is likely that \" +\n                     \"the consumer's position has fallen out of the topic partition offset range because the topic was \" +\n                     \"truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing\" +\n-                    \" it later.\", e.getClass().getName(), e.partitions());\n+                    \" it later.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5ODI2OA=="}, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwMTI4OA==", "bodyText": "Ah got it, I'm still think about it as the string template and was overlooking that. SG.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r443001288", "createdAt": "2020-06-19T19:02:48Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -415,19 +418,20 @@ public void restore() {\n                 // for restoring active and updating standby we may prefer different poll time\n                 // in order to make sure we call the main consumer#poll in time.\n                 // TODO: once we move ChangelogReader to a separate thread this may no longer be a concern\n-                polledRecords = restoreConsumer.poll(state.equals(ChangelogReaderState.STANDBY_UPDATING) ? Duration.ZERO : pollTime);\n+                polledRecords = restoreConsumer.poll(state == ChangelogReaderState.STANDBY_UPDATING ? Duration.ZERO : pollTime);\n             } catch (final InvalidOffsetException e) {\n-                log.warn(\"Encountered {} fetching records from restore consumer for partitions {}, it is likely that \" +\n+                log.warn(\"Encountered \" + e.getClass().getName() +\n+                    \" fetching records from restore consumer for partitions \" + e.partitions() + \", it is likely that \" +\n                     \"the consumer's position has fallen out of the topic partition offset range because the topic was \" +\n                     \"truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing\" +\n-                    \" it later.\", e.getClass().getName(), e.partitions());\n+                    \" it later.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5ODI2OA=="}, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjIwMjE3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0NjowNlrOGl524Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0NjowNlrOGl524Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5ODQzMw==", "bodyText": "Also added the cause to the thrown exception.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442398433", "createdAt": "2020-06-18T17:46:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -415,19 +418,20 @@ public void restore() {\n                 // for restoring active and updating standby we may prefer different poll time\n                 // in order to make sure we call the main consumer#poll in time.\n                 // TODO: once we move ChangelogReader to a separate thread this may no longer be a concern\n-                polledRecords = restoreConsumer.poll(state.equals(ChangelogReaderState.STANDBY_UPDATING) ? Duration.ZERO : pollTime);\n+                polledRecords = restoreConsumer.poll(state == ChangelogReaderState.STANDBY_UPDATING ? Duration.ZERO : pollTime);\n             } catch (final InvalidOffsetException e) {\n-                log.warn(\"Encountered {} fetching records from restore consumer for partitions {}, it is likely that \" +\n+                log.warn(\"Encountered \" + e.getClass().getName() +\n+                    \" fetching records from restore consumer for partitions \" + e.partitions() + \", it is likely that \" +\n                     \"the consumer's position has fallen out of the topic partition offset range because the topic was \" +\n                     \"truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing\" +\n-                    \" it later.\", e.getClass().getName(), e.partitions());\n+                    \" it later.\", e);\n \n                 final Map<TaskId, Collection<TopicPartition>> taskWithCorruptedChangelogs = new HashMap<>();\n                 for (final TopicPartition partition : e.partitions()) {\n                     final TaskId taskId = changelogs.get(partition).stateManager.taskId();\n                     taskWithCorruptedChangelogs.computeIfAbsent(taskId, k -> new HashSet<>()).add(partition);\n                 }\n-                throw new TaskCorruptedException(taskWithCorruptedChangelogs);\n+                throw new TaskCorruptedException(taskWithCorruptedChangelogs, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjIwNzYyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0Nzo0N1rOGl56mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzo0Nzo0N1rOGl56mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5OTM4Ng==", "bodyText": "This is the main change. Once every ten seconds, we will log the progress for each active restoring changelog.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442399386", "createdAt": "2020-06-18T17:47:47Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -446,6 +450,38 @@ public void restore() {\n             }\n \n             maybeUpdateLimitOffsetsForStandbyChangelogs();\n+\n+            maybeLogRestorationProgress();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzAxMTIwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjoxMToyM1rOGmB4tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjoxMToyM1rOGmB4tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyOTk3NQ==", "bodyText": "I've rolled back a bunch of accidental formatting changes, but left the ones that are actually code style compliance issues (like using brackets around conditional bodies).", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442529975", "createdAt": "2020-06-18T22:11:23Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -496,8 +539,9 @@ private void bufferChangelogRecords(final ChangelogMetadata changelogMetadata, f\n             } else {\n                 changelogMetadata.bufferedRecords.add(record);\n                 final long offset = record.offset();\n-                if (changelogMetadata.restoreEndOffset == null || offset < changelogMetadata.restoreEndOffset)\n+                if (changelogMetadata.restoreEndOffset == null || offset < changelogMetadata.restoreEndOffset) {\n                     changelogMetadata.bufferedLimitIndex = changelogMetadata.bufferedRecords.size();\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzAxODk5OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjoxNToxN1rOGmB95A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOToxNDozNFrOGme6QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzMTMwMA==", "bodyText": "This is moderately obnoxious... The addition of logging these values means that these tests will get a NullPointerException unless we mock this call, but the mock is irrelevant to the test outcome.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442531300", "createdAt": "2020-06-18T22:15:17Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "diffHunk": "@@ -223,6 +227,7 @@ public void shouldInitializeChangelogAndCheckForCompletion() {\n     @Test\n     public void shouldPollWithRightTimeout() {\n         EasyMock.expect(storeMetadata.offset()).andReturn(null).andReturn(9L).anyTimes();\n+        EasyMock.expect(stateManager.changelogOffsets()).andReturn(singletonMap(tp, 5L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMTkyNA==", "bodyText": "This comment seems worth adding to the code :)", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442931924", "createdAt": "2020-06-19T16:19:38Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "diffHunk": "@@ -223,6 +227,7 @@ public void shouldInitializeChangelogAndCheckForCompletion() {\n     @Test\n     public void shouldPollWithRightTimeout() {\n         EasyMock.expect(storeMetadata.offset()).andReturn(null).andReturn(9L).anyTimes();\n+        EasyMock.expect(stateManager.changelogOffsets()).andReturn(singletonMap(tp, 5L));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzMTMwMA=="}, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwNTUwNA==", "bodyText": "I didn't think to do this... This might be equivocation, but it seems like if I wrote that in a code comment, it may or may not be true in the future. Looking at the tests, there are already like a dozen cryptic, redundant mocks, so I'm not sure justifying this one really makes a material impact on this test's readability, which is already approaching zero.\nAdding a comment like \"this is just to prevent the logger from throwing an NPE\" carries the risk that it can quickly become untrue in two ways:\n\nMaybe we remove or change the log so that it wouldn't need this mock; since it's a \"nice\" mock, we'll never know. In fact, I can't verify this call because the way the logger is configured only to print every ten seconds makes the NPE nondeterministic. Plus, it's not great to verify stuff that is beside the point of the test.\nMaybe we change the implementation so that it actually does exercise this mocked behavior, then the comment will become untrue, but we may not even notice.\n\nTypically, having this many specific and complex mocks in a test indicates that we shouldn't be using easymock, but instead configure the component with \"dummy\" state manager, etc. If we re-wrote this test to use that strategy, then we wouldn't need to make explicit expectations like this.\nAnyway, that's why I'm sort of inclined on just declaring bankruptcy on the comprehensibility of this test.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r443005504", "createdAt": "2020-06-19T19:14:34Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "diffHunk": "@@ -223,6 +227,7 @@ public void shouldInitializeChangelogAndCheckForCompletion() {\n     @Test\n     public void shouldPollWithRightTimeout() {\n         EasyMock.expect(storeMetadata.offset()).andReturn(null).andReturn(9L).anyTimes();\n+        EasyMock.expect(stateManager.changelogOffsets()).andReturn(singletonMap(tp, 5L));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzMTMwMA=="}, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1OTU0MDM1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxNjoxODoyMFrOGmaYnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOToxMDo1OVrOGme1IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMTM1Ng==", "bodyText": "nit: should we have a newline for each partition? Otherwise that ling maybe too long.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r442931356", "createdAt": "2020-06-19T16:18:20Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -458,9 +462,48 @@ public void restore() {\n             }\n \n             maybeUpdateLimitOffsetsForStandbyChangelogs();\n+\n+            maybeLogRestorationProgress();\n+        }\n+    }\n+\n+    private void maybeLogRestorationProgress() {\n+        if (state == ChangelogReaderState.ACTIVE_RESTORING) {\n+            if (time.milliseconds() - lastRestoreLogTime > RESTORE_LOG_INTERVAL_MS) {\n+                final Set<TopicPartition> topicPartitions = activeRestoringChangelogs();\n+                if (!topicPartitions.isEmpty()) {\n+                    final StringBuilder builder = new StringBuilder().append(\"Restoration in progress for \")\n+                                                                     .append(topicPartitions.size())\n+                                                                     .append(\" partitions.\");\n+                    for (final TopicPartition partition : topicPartitions) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6431b199f51feb7a4fc92b446780d85721e56879"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwMTQ0Mg==", "bodyText": "I thought about it; while it does make the logs easier to read, it makes them harder to search (as in grep, since the line that would match the query doesn't contain all the information.\nWe do have other places where we concatenate every topic-partition on a single line, eg in the StreamsPartitionAssignor, so I think if long lines were a problem, people would already be complaining.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r443001442", "createdAt": "2020-06-19T19:03:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -458,9 +462,48 @@ public void restore() {\n             }\n \n             maybeUpdateLimitOffsetsForStandbyChangelogs();\n+\n+            maybeLogRestorationProgress();\n+        }\n+    }\n+\n+    private void maybeLogRestorationProgress() {\n+        if (state == ChangelogReaderState.ACTIVE_RESTORING) {\n+            if (time.milliseconds() - lastRestoreLogTime > RESTORE_LOG_INTERVAL_MS) {\n+                final Set<TopicPartition> topicPartitions = activeRestoringChangelogs();\n+                if (!topicPartitions.isEmpty()) {\n+                    final StringBuilder builder = new StringBuilder().append(\"Restoration in progress for \")\n+                                                                     .append(topicPartitions.size())\n+                                                                     .append(\" partitions.\");\n+                    for (final TopicPartition partition : topicPartitions) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMTM1Ng=="}, "originalCommit": {"oid": "6431b199f51feb7a4fc92b446780d85721e56879"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwNDE5Mg==", "bodyText": "Actually I do have complaints about the StreamsPartitionAssignor log entries haha :)\nAnyways, I think grep a valid reason. My rationale was that when searching for this entry, most people would use \"Restoration in progress for\" and then manually check if the particular interested partition in the following line, but I guess I'm just biased because I'm not a heavy grep user.\nIt is a quite nit comment and I won't feel strong about it. Your call.", "url": "https://github.com/apache/kafka/pull/8896#discussion_r443004192", "createdAt": "2020-06-19T19:10:59Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -458,9 +462,48 @@ public void restore() {\n             }\n \n             maybeUpdateLimitOffsetsForStandbyChangelogs();\n+\n+            maybeLogRestorationProgress();\n+        }\n+    }\n+\n+    private void maybeLogRestorationProgress() {\n+        if (state == ChangelogReaderState.ACTIVE_RESTORING) {\n+            if (time.milliseconds() - lastRestoreLogTime > RESTORE_LOG_INTERVAL_MS) {\n+                final Set<TopicPartition> topicPartitions = activeRestoringChangelogs();\n+                if (!topicPartitions.isEmpty()) {\n+                    final StringBuilder builder = new StringBuilder().append(\"Restoration in progress for \")\n+                                                                     .append(topicPartitions.size())\n+                                                                     .append(\" partitions.\");\n+                    for (final TopicPartition partition : topicPartitions) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMTM1Ng=="}, "originalCommit": {"oid": "6431b199f51feb7a4fc92b446780d85721e56879"}, "originalPosition": 68}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2404, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}