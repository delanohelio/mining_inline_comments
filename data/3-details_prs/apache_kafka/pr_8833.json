{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxMjgxNTky", "number": 8833, "title": "KAFKA-9441: remove prepareClose() to simplify task management", "bodyText": "Removes prepareCloseClean() and prepareCloseDirty().\nRemoves state transition RUNNING -> CLOSED (tasks must be suspended before closing now)\nreplaces suspend() with suspendDirty and suspendAndPrepareCommit()\nDecouples suspending/committing/closing (ie, no redundant code any longer, but enforces \"linear\" order of calls)\n\nCall for review @guozhangwang @abbccdda @vvcephei @ableegoldman @cadonna", "createdAt": "2020-06-08T17:12:21Z", "url": "https://github.com/apache/kafka/pull/8833", "merged": true, "mergeCommit": {"oid": "8e083e1b8879e732522bb9a560c82ff84b176cc9"}, "closed": true, "closedAt": "2020-06-11T00:53:44Z", "author": {"login": "mjsax"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcpTskJgFqTQyNjQ0ODY2OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcqCUiwgFqTQyODQ5Njk5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NDQ4NjY5", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426448669", "createdAt": "2020-06-08T17:13:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxMzo1MFrOGgoGMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxMzo1MFrOGgoGMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg2NDU2MQ==", "bodyText": "When we suspend, we always want to commit.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436864561", "createdAt": "2020-06-08T17:13:50Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -108,13 +107,20 @@ public void completeRestoration() {\n     }\n \n     @Override\n-    public void prepareSuspend() {\n-        log.trace(\"No-op prepareSuspend with state {}\", state());\n+    public void suspendDirty() {\n+        log.trace(\"No-op suspend dirty with state {}\", state());\n+        if (state() == State.RUNNING) {\n+            transitionTo(State.SUSPENDED);\n+        }\n     }\n \n     @Override\n-    public void suspend() {\n-        log.trace(\"No-op suspend with state {}\", state());\n+    public void suspendCleanAndPrepareCommit() {\n+        log.trace(\"No-op suspend clean with state {}\", state());\n+        if (state() == State.RUNNING) {\n+            transitionTo(State.SUSPENDED);\n+        }\n+        prepareCommit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NDQ5NzE0", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426449714", "createdAt": "2020-06-08T17:15:05Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxNTowNVrOGgoJgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxNTowNVrOGgoJgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg2NTQxMA==", "bodyText": "This logic is now followed in suspendCleanAndPrepareCommit() that must be called before a task can be closed.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436865410", "createdAt": "2020-06-08T17:15:05Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -151,69 +157,23 @@ public void postCommit() {\n         }\n     }\n \n-    @Override\n-    public void prepareCloseClean() {\n-        prepareClose(true);\n-\n-        log.info(\"Prepared clean close\");\n-    }\n-\n-    @Override\n-    public void prepareCloseDirty() {\n-        prepareClose(false);\n-\n-        log.info(\"Prepared dirty close\");\n-    }\n-\n-    /**\n-     * 1. commit if we are running and clean close;\n-     * 2. close the state manager.\n-     *\n-     * @throws TaskMigratedException all the task has been migrated\n-     * @throws StreamsException fatal error, should close the thread\n-     */\n-    private void prepareClose(final boolean clean) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NDUwMzcz", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426450373", "createdAt": "2020-06-08T17:15:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxNTo1MVrOGgoLeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxNTo1MVrOGgoLeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg2NTkxMw==", "bodyText": "This is now done via suspendAndPrepareCommit()", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436865913", "createdAt": "2020-06-08T17:15:51Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -246,82 +245,39 @@ public void completeRestoration() {\n         }\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     * </pre>\n-     *\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n     @Override\n-    public void prepareSuspend() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NDUxODk4", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426451898", "createdAt": "2020-06-08T17:17:43Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxNzo0M1rOGgoQFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxNzo0M1rOGgoQFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg2NzA5Mg==", "bodyText": "The \"old\" suspend() was called after committing, the \"new\" suspend() is now called before committing!\nThe old suspend logic is now handled via postCommit and close", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436867092", "createdAt": "2020-06-08T17:17:43Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -246,82 +245,39 @@ public void completeRestoration() {\n         }\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     * </pre>\n-     *\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n     @Override\n-    public void prepareSuspend() {\n-        switch (state()) {\n-            case CREATED:\n-            case SUSPENDED:\n-                // do nothing\n-                log.trace(\"Skip prepare suspending since state is {}\", state());\n-\n-                break;\n-\n-            case RESTORING:\n-                stateMgr.flush();\n-                log.info(\"Prepare suspending restoring\");\n-\n-                break;\n-\n-            case RUNNING:\n-                closeTopology(true);\n-\n-                stateMgr.flush();\n-                recordCollector.flush();\n-\n-                log.info(\"Prepare suspending running\");\n-\n-                break;\n-\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n-        }\n+    public void suspendDirty() {\n+        log.info(\"Suspending dirty\");\n+        suspend(false);\n     }\n \n     @Override\n-    public void suspend() {\n+    public void suspendCleanAndPrepareCommit() {\n+        log.info(\"Suspending clean\");\n+        suspend(true);\n+    }\n+\n+    @SuppressWarnings(\"fallthrough\")\n+    private void suspend(final boolean clean) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NDUzMDA3", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426453007", "createdAt": "2020-06-08T17:19:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxOTowN1rOGgoTbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoxOTowN1rOGgoTbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg2Nzk0OQ==", "bodyText": "Instead of \"blindly\" writing a checkpoint in postCommit(), we only do it if a checkpoint get's scheduled.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436867949", "createdAt": "2020-06-08T17:19:07Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -369,6 +325,8 @@ public void prepareCommit() {\n         switch (state()) {\n             case RUNNING:\n             case RESTORING:\n+            case SUSPENDED:\n+                maybeScheduleCheckpoint();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NDU0MDE2", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426454016", "createdAt": "2020-06-08T17:20:24Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoyMDoyNFrOGgoWlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNzoyMDoyNFrOGgoWlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg2ODc1OQ==", "bodyText": "Mainly rewrite to use switch now -- however, we return a proper non-empty checkpoint on SUSPEND now.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436868759", "createdAt": "2020-06-08T17:20:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -423,39 +377,51 @@ public void postCommit() {\n \n     @Override\n     public Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n-        if (state() == State.CLOSED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 171}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NjAzMTI0", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426603124", "createdAt": "2020-06-08T20:39:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQyMDozOTowMFrOGgvj6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQyMjoxNjowOVrOGgySOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk4Njg1OQ==", "bodyText": "Should update the exception text like while scheduling checkpoint", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436986859", "createdAt": "2020-06-08T20:39:00Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -534,62 +482,38 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     * </pre>\n-     *\n-     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n-     *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if the task producer got fenced (EOS)\n-     */\n-    private void prepareClose(final boolean clean) {\n-        // Reset any previously scheduled checkpoint.\n-        checkpoint = null;\n-\n+    private void maybeScheduleCheckpoint() {\n         switch (state()) {\n-            case CREATED:\n-                // the task is created and not initialized, just re-write the checkpoint file\n-                scheduleCheckpoint(emptyMap());\n+            case RESTORING:\n+                this.checkpoint = checkpointableOffsets();\n+\n                 break;\n \n             case RUNNING:\n-                closeTopology(clean);\n-\n-                if (clean) {\n-                    stateMgr.flush();\n-                    recordCollector.flush();\n-                    scheduleCheckpoint(checkpointableOffsets());\n-                } else {\n-                    executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\", log);\n+                if (!eosEnabled) {\n+                    this.checkpoint = checkpointableOffsets();\n                 }\n \n                 break;\n \n-            case RESTORING:\n-                executeAndMaybeSwallow(clean, stateMgr::flush, \"state manager flush\", log);\n-                scheduleCheckpoint(emptyMap());\n+            case SUSPENDED:\n+                this.checkpoint = Collections.emptyMap();\n \n                 break;\n \n-            case SUSPENDED:\n+            case CREATED:\n             case CLOSED:\n-                // not need to checkpoint, since when suspending we've already committed the state\n-                break;\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 360}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk4NzA4MQ==", "bodyText": "similar here", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436987081", "createdAt": "2020-06-08T20:39:28Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -534,62 +482,38 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     * </pre>\n-     *\n-     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n-     *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if the task producer got fenced (EOS)\n-     */\n-    private void prepareClose(final boolean clean) {\n-        // Reset any previously scheduled checkpoint.\n-        checkpoint = null;\n-\n+    private void maybeScheduleCheckpoint() {\n         switch (state()) {\n-            case CREATED:\n-                // the task is created and not initialized, just re-write the checkpoint file\n-                scheduleCheckpoint(emptyMap());\n+            case RESTORING:\n+                this.checkpoint = checkpointableOffsets();\n+\n                 break;\n \n             case RUNNING:\n-                closeTopology(clean);\n-\n-                if (clean) {\n-                    stateMgr.flush();\n-                    recordCollector.flush();\n-                    scheduleCheckpoint(checkpointableOffsets());\n-                } else {\n-                    executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\", log);\n+                if (!eosEnabled) {\n+                    this.checkpoint = checkpointableOffsets();\n                 }\n \n                 break;\n \n-            case RESTORING:\n-                executeAndMaybeSwallow(clean, stateMgr::flush, \"state manager flush\", log);\n-                scheduleCheckpoint(emptyMap());\n+            case SUSPENDED:\n+                this.checkpoint = Collections.emptyMap();\n \n                 break;\n \n-            case SUSPENDED:\n+            case CREATED:\n             case CLOSED:\n-                // not need to checkpoint, since when suspending we've already committed the state\n-                break;\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);\n \n             default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while prepare closing active task \" + id);\n+                throw new IllegalStateException(\"Unknown state \" + state() + \" while closing active task \" + id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 364}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk4NzM2Mw==", "bodyText": "if commit is needed", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436987363", "createdAt": "2020-06-08T20:40:06Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -534,62 +482,38 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     * </pre>\n-     *\n-     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n-     *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if the task producer got fenced (EOS)\n-     */\n-    private void prepareClose(final boolean clean) {\n-        // Reset any previously scheduled checkpoint.\n-        checkpoint = null;\n-\n+    private void maybeScheduleCheckpoint() {\n         switch (state()) {\n-            case CREATED:\n-                // the task is created and not initialized, just re-write the checkpoint file\n-                scheduleCheckpoint(emptyMap());\n+            case RESTORING:\n+                this.checkpoint = checkpointableOffsets();\n+\n                 break;\n \n             case RUNNING:\n-                closeTopology(clean);\n-\n-                if (clean) {\n-                    stateMgr.flush();\n-                    recordCollector.flush();\n-                    scheduleCheckpoint(checkpointableOffsets());\n-                } else {\n-                    executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\", log);\n+                if (!eosEnabled) {\n+                    this.checkpoint = checkpointableOffsets();\n                 }\n \n                 break;\n \n-            case RESTORING:\n-                executeAndMaybeSwallow(clean, stateMgr::flush, \"state manager flush\", log);\n-                scheduleCheckpoint(emptyMap());\n+            case SUSPENDED:\n+                this.checkpoint = Collections.emptyMap();\n \n                 break;\n \n-            case SUSPENDED:\n+            case CREATED:\n             case CLOSED:\n-                // not need to checkpoint, since when suspending we've already committed the state\n-                break;\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);\n \n             default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while prepare closing active task \" + id);\n+                throw new IllegalStateException(\"Unknown state \" + state() + \" while closing active task \" + id);\n         }\n     }\n \n-    private void scheduleCheckpoint(final Map<TopicPartition, Long> checkpoint) {\n-        this.checkpoint = checkpoint;\n-    }\n-\n     private void writeCheckpointIfNeed() {\n+        if (commitNeeded) {\n+            throw new IllegalStateException(\"A checkpoint should only be written if now commit is needed.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 374}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk4ODAyNw==", "bodyText": "Seems applied to both RUNNING and SUSPENDED?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r436988027", "createdAt": "2020-06-08T20:41:23Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -389,30 +346,27 @@ public void prepareCommit() {\n     @Override\n     public void postCommit() {\n         switch (state()) {\n+            case RESTORING:\n             case RUNNING:\n-                commitNeeded = false;\n+            case SUSPENDED:\n                 commitRequested = false;\n+                commitNeeded = false;\n \n-                if (!eosEnabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n+                if (state() == State.RESTORING) {\n+                    writeCheckpointIfNeed();\n+                } else if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAwMzI5Ng==", "bodyText": "This comment could be removed IMHO.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437003296", "createdAt": "2020-06-08T21:11:07Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -246,82 +245,39 @@ public void completeRestoration() {\n         }\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     * </pre>\n-     *\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n     @Override\n-    public void prepareSuspend() {\n-        switch (state()) {\n-            case CREATED:\n-            case SUSPENDED:\n-                // do nothing\n-                log.trace(\"Skip prepare suspending since state is {}\", state());\n-\n-                break;\n-\n-            case RESTORING:\n-                stateMgr.flush();\n-                log.info(\"Prepare suspending restoring\");\n-\n-                break;\n-\n-            case RUNNING:\n-                closeTopology(true);\n-\n-                stateMgr.flush();\n-                recordCollector.flush();\n-\n-                log.info(\"Prepare suspending running\");\n-\n-                break;\n-\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n-        }\n+    public void suspendDirty() {\n+        log.info(\"Suspending dirty\");\n+        suspend(false);\n     }\n \n     @Override\n-    public void suspend() {\n+    public void suspendCleanAndPrepareCommit() {\n+        log.info(\"Suspending clean\");\n+        suspend(true);\n+    }\n+\n+    @SuppressWarnings(\"fallthrough\")\n+    private void suspend(final boolean clean) {\n         switch (state()) {\n             case CREATED:\n             case SUSPENDED:\n                 // do nothing", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAwNTI0MQ==", "bodyText": "a standby task is never in", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437005241", "createdAt": "2020-06-08T21:15:02Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -260,8 +220,8 @@ private void close(final boolean clean) {\n                 log.trace(\"Skip closing since state is {}\", state());\n                 return;\n \n-            case RESTORING:\n-            case SUSPENDED:\n+            case RESTORING: // a StandbyTask in never in RESTORING state", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAwNjE0OA==", "bodyText": "Seems like we could use a separate illegal state exception here for RESTORING as we should never hit it.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437006148", "createdAt": "2020-06-08T21:16:39Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -260,8 +220,8 @@ private void close(final boolean clean) {\n                 log.trace(\"Skip closing since state is {}\", state());\n                 return;\n \n-            case RESTORING:\n-            case SUSPENDED:\n+            case RESTORING: // a StandbyTask in never in RESTORING state", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAxMDU0NA==", "bodyText": "Although this holds true for now, I'm thinking whether in long term a commit will always follow suspendClean, what's the downside of having these two logics separate?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437010544", "createdAt": "2020-06-08T21:25:47Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -246,82 +245,39 @@ public void completeRestoration() {\n         }\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     * </pre>\n-     *\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n     @Override\n-    public void prepareSuspend() {\n-        switch (state()) {\n-            case CREATED:\n-            case SUSPENDED:\n-                // do nothing\n-                log.trace(\"Skip prepare suspending since state is {}\", state());\n-\n-                break;\n-\n-            case RESTORING:\n-                stateMgr.flush();\n-                log.info(\"Prepare suspending restoring\");\n-\n-                break;\n-\n-            case RUNNING:\n-                closeTopology(true);\n-\n-                stateMgr.flush();\n-                recordCollector.flush();\n-\n-                log.info(\"Prepare suspending running\");\n-\n-                break;\n-\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n-        }\n+    public void suspendDirty() {\n+        log.info(\"Suspending dirty\");\n+        suspend(false);\n     }\n \n     @Override\n-    public void suspend() {\n+    public void suspendCleanAndPrepareCommit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAxMjcyMg==", "bodyText": "Could we merge the two if-else here?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437012722", "createdAt": "2020-06-08T21:30:44Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -389,30 +346,27 @@ public void prepareCommit() {\n     @Override\n     public void postCommit() {\n         switch (state()) {\n+            case RESTORING:\n             case RUNNING:\n-                commitNeeded = false;\n+            case SUSPENDED:\n                 commitRequested = false;\n+                commitNeeded = false;\n \n-                if (!eosEnabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n+                if (state() == State.RESTORING) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyNjcxMA==", "bodyText": "Do we want to throw here if the current state is CLOSED?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437026710", "createdAt": "2020-06-08T22:03:53Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -108,13 +107,20 @@ public void completeRestoration() {\n     }\n \n     @Override\n-    public void prepareSuspend() {\n-        log.trace(\"No-op prepareSuspend with state {}\", state());\n+    public void suspendDirty() {\n+        log.trace(\"No-op suspend dirty with state {}\", state());\n+        if (state() == State.RUNNING) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyNzMzMA==", "bodyText": "Similar here, maybe we could leverage transitionTo to help throw the exception.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437027330", "createdAt": "2020-06-08T22:05:18Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -108,13 +107,20 @@ public void completeRestoration() {\n     }\n \n     @Override\n-    public void prepareSuspend() {\n-        log.trace(\"No-op prepareSuspend with state {}\", state());\n+    public void suspendDirty() {\n+        log.trace(\"No-op suspend dirty with state {}\", state());\n+        if (state() == State.RUNNING) {\n+            transitionTo(State.SUSPENDED);\n+        }\n     }\n \n     @Override\n-    public void suspend() {\n-        log.trace(\"No-op suspend with state {}\", state());\n+    public void suspendCleanAndPrepareCommit() {\n+        log.trace(\"No-op suspend clean with state {}\", state());\n+        if (state() == State.RUNNING) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyODMwMw==", "bodyText": "The partitionGroup.clear and partitionGroup.close are interchangeable right now, should we just consolidate both?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437028303", "createdAt": "2020-06-08T22:07:46Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -389,30 +346,27 @@ public void prepareCommit() {\n     @Override\n     public void postCommit() {\n         switch (state()) {\n+            case RESTORING:\n             case RUNNING:\n-                commitNeeded = false;\n+            case SUSPENDED:\n                 commitRequested = false;\n+                commitNeeded = false;\n \n-                if (!eosEnabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n+                if (state() == State.RESTORING) {\n+                    writeCheckpointIfNeed();\n+                } else if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n+                    writeCheckpointIfNeed();\n                 }\n \n-                log.debug(\"Committed\");\n-\n-                break;\n-\n-            case RESTORING:\n-                commitNeeded = false;\n-                commitRequested = false;\n-\n-                stateMgr.checkpoint(checkpointableOffsets());\n+                if (state() == State.SUSPENDED) {\n+                    partitionGroup.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyOTQ2NA==", "bodyText": "Prefer to throw different illegal state exception here than making comments", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437029464", "createdAt": "2020-06-08T22:10:46Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -506,19 +455,18 @@ public void update(final Set<TopicPartition> topicPartitions, final ProcessorTop\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n-\n+        suspendCleanAndPrepareCommit();\n         writeCheckpointIfNeed();\n \n         switch (state()) {\n             case CREATED:\n-            case RUNNING:\n-            case RESTORING:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n                 break;\n \n+            case RESTORING: // we should have transitioned to `SUSPENDED` already", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 297}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAzMTQ4Mg==", "bodyText": "I could see we are trying to maintain the same behavior, but still why a restoring task won't need to close topology?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437031482", "createdAt": "2020-06-08T22:16:09Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -891,6 +821,9 @@ private void initializeTopology() {\n     }\n \n     private void closeTopology(final boolean clean) {\n+        if (state() != State.RUNNING) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 412}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NzI0NzUw", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-426724750", "createdAt": "2020-06-09T01:17:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMToxNzo1OFrOGg1hJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMTo0MTowOVrOGg14SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NDQ1Mw==", "bodyText": "I'm wondering if we could merge committableOffsetsAndMetadata with prepareCommit as well, letting the latter to return the map? See my other comment aside.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437084453", "createdAt": "2020-06-09T01:17:58Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -125,40 +126,21 @@ public boolean isValidTransition(final State newState) {\n \n     void postCommit();\n \n-    /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA5MDM3Ng==", "bodyText": "It seems to me that the reason we want to have two suspends and also merging the suspendClean with prepareCommit is that for StreamTask, if state SUSPENDED we want to skip prepareCommit. I feel it is a tad cleaner to separate them further into one suspend which does not try to call prepareCommit, and rely on whether prepareCommit should do anything or not based on both state (i.e. only running/restoring/suspended need to commit) and commitNeeded flag.\nWith that we can convert the callers as follows:\n\nsuspendDirty(): just call suspend(), do not call prepareCommit().\nsuspendCleanAndPrepareCommit():\n2.a) from task.closeAndRecycleState: call suspend(), and then call prepareCommit(); the second would check commitNeeded and if it was false, we would not try to flush / commit. Hence if the task just transited from other states to suspended, then commitNeeded should still be true.\n2.b) from taskManager directly: same as above, but for this call we always follow with a committableOffsetsAndMetadata getting the map of offsets, so I'm thinking we can merge prepareCommit with committableOffsetsAndMetadata as well: if the state is right and commitNeeded is set, execute the prepare committing procedure, and accumulate the offsets, otherwise returning null indicating no offsets needed to be committed.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437090376", "createdAt": "2020-06-09T01:41:09Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -125,40 +126,21 @@ public boolean isValidTransition(final State newState) {\n \n     void postCommit();\n \n-    /**\n-     * @throws TaskMigratedException all the task has been migrated\n-     * @throws StreamsException fatal error, should close the thread\n-     */\n-    void prepareSuspend();\n+    void suspendDirty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "139bceb054bfdc49309e177cd9265d97716da196", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/139bceb054bfdc49309e177cd9265d97716da196", "committedDate": "2020-06-09T17:44:32Z", "message": "KAFKA-9441: remove closePrepare() to simplify task management"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1be5d9216ac5f98d3de87a7f86218dca906455f6", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/1be5d9216ac5f98d3de87a7f86218dca906455f6", "committedDate": "2020-06-09T17:48:26Z", "message": "Add missing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54f24e4b8d844d6ee3438648fb9714ecad97e350", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/54f24e4b8d844d6ee3438648fb9714ecad97e350", "committedDate": "2020-06-09T17:48:26Z", "message": "Github comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/120fb8d8134add8aeb4b402c1e0ecd54a5b673a5", "committedDate": "2020-06-09T18:12:29Z", "message": "Rebased"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/120fb8d8134add8aeb4b402c1e0ecd54a5b673a5", "committedDate": "2020-06-09T18:12:29Z", "message": "Rebased"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NTAyNjky", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-427502692", "createdAt": "2020-06-09T20:09:15Z", "commit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMDowOToxNVrOGhaYWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMDoyODo0NVrOGha__g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4ODQwOQ==", "bodyText": "Could we merge RESTORING and SUSPENDED?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437688409", "createdAt": "2020-06-09T20:09:15Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -504,88 +438,66 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n-\n+        suspend();\n+        prepareCommit();\n         writeCheckpointIfNeed();\n \n         switch (state()) {\n             case CREATED:\n-            case RESTORING:\n-            case RUNNING:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n+\n                 break;\n+\n+            case RESTORING: // we should have transitioned to `SUSPENDED` already\n+            case RUNNING: // we should have transitioned to `SUSPENDED` already\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n         }\n \n-        partitionGroup.close();\n+        partitionGroup.clear();\n         closeTaskSensor.record();\n \n         transitionTo(State.CLOSED);\n \n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     * </pre>\n-     *\n-     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n-     *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if the task producer got fenced (EOS)\n-     */\n-    private void prepareClose(final boolean clean) {\n-        // Reset any previously scheduled checkpoint.\n-        checkpoint = null;\n-\n+    private void maybeScheduleCheckpoint() {\n         switch (state()) {\n-            case CREATED:\n-                // the task is created and not initialized, just re-write the checkpoint file\n-                scheduleCheckpoint(emptyMap());\n+            case RESTORING:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 364}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MDY4MA==", "bodyText": "Sg, should we move the log on L811 inside the if statement?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437690680", "createdAt": "2020-06-09T20:13:36Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -891,6 +821,9 @@ private void initializeTopology() {\n     }\n \n     private void closeTopology(final boolean clean) {\n+        if (state() != State.RUNNING) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAzMTQ4Mg=="}, "originalCommit": null, "originalPosition": 412}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MTQyOA==", "bodyText": "add a @return comment for the struct", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437691428", "createdAt": "2020-06-09T20:15:06Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -122,44 +123,23 @@ public boolean isValidTransition(final State newState) {\n     /**\n      * @throws StreamsException fatal error, should close the thread\n      */\n-    void prepareCommit();\n+    Map<TopicPartition, OffsetAndMetadata> prepareCommit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NzY2NQ==", "bodyText": "I could get a follow-up newbie ticket, but it seems that we have a couple of catch and swallow cases in the task manager with clean flag, does it make sense to extract the executeAndMaybeSwallow to TaskManager class and share between cases?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437697665", "createdAt": "2020-06-09T20:26:58Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -623,7 +618,11 @@ private long sumOfChangelogOffsets(final TaskId id, final Map<TopicPartition, Lo\n     }\n \n     private void closeTaskDirty(final Task task) {\n-        task.prepareCloseDirty();\n+        try {\n+            task.suspend();\n+        } catch (final RuntimeException swallow) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5ODU1OA==", "bodyText": "What does this lastCall suggest?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437698558", "createdAt": "2020-06-09T20:28:45Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "diffHunk": "@@ -476,6 +445,26 @@ public void shouldDeleteStateDirOnTaskCreatedAndEosBetaUncleanClose() {\n         assertEquals(Task.State.CLOSED, task.state());\n     }\n \n+    @Test\n+    public void shouldRecycleTask() {\n+        EasyMock.expectLastCall();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 126}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "141c4d018a857b29253c3cc0c114928f5328c558", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/141c4d018a857b29253c3cc0c114928f5328c558", "committedDate": "2020-06-09T23:24:16Z", "message": "Github comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NTkxNTUw", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-427591550", "createdAt": "2020-06-09T22:34:11Z", "commit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "state": "COMMENTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMjozNDoxMVrOGhetTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMzoyODoyM1rOGhfzXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc1OTMwOQ==", "bodyText": "nit: we can throw illegal-state if the state() == RESTORING since it should never happen.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437759309", "createdAt": "2020-06-09T22:34:11Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -108,13 +107,20 @@ public void completeRestoration() {\n     }\n \n     @Override\n-    public void prepareSuspend() {\n-        log.trace(\"No-op prepareSuspend with state {}\", state());\n+    public void suspendDirty() {\n+        log.trace(\"No-op suspend dirty with state {}\", state());\n+        if (state() == State.RUNNING) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyNjcxMA=="}, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2MTAyMg==", "bodyText": "The comment below is not accurate anymore: we do not write checkpoint during recycle actually.\nEDIT: actually, the updated offsetSnapshotSinceLastCommit seems not used since after this function we would create a new StreamTask and in between we do not check if commitNeeded at all. Could we remove line 175 then?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437761022", "createdAt": "2020-06-09T22:39:06Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -151,69 +151,24 @@ public void postCommit() {\n         }\n     }\n \n-    @Override\n-    public void prepareCloseClean() {\n-        prepareClose(true);\n-\n-        log.info(\"Prepared clean close\");\n-    }\n-\n-    @Override\n-    public void prepareCloseDirty() {\n-        prepareClose(false);\n-\n-        log.info(\"Prepared dirty close\");\n-    }\n-\n-    /**\n-     * 1. commit if we are running and clean close;\n-     * 2. close the state manager.\n-     *\n-     * @throws TaskMigratedException all the task has been migrated\n-     * @throws StreamsException fatal error, should close the thread\n-     */\n-    private void prepareClose(final boolean clean) {\n-        switch (state()) {\n-            case CREATED:\n-            case CLOSED:\n-                log.trace(\"Skip prepare closing since state is {}\", state());\n-                return;\n-\n-            case RUNNING:\n-                if (clean) {\n-                    stateMgr.flush();\n-                }\n-\n-                break;\n-\n-            case RESTORING:\n-            case SUSPENDED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing standby task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while closing standby task \" + id);\n-        }\n-    }\n-\n     @Override\n     public void closeClean() {\n         close(true);\n-\n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n         close(false);\n-\n         log.info(\"Closed dirty\");\n     }\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n+        suspend();\n+        prepareCommit();\n \n-        if (state() == State.CREATED || state() == State.RUNNING) {\n+        if (state() == State.CREATED || state() == State.SUSPENDED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2MjQ2OQ==", "bodyText": "Ditto for line 195: we do not need to update the snapshot since we are closing the task already.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437762469", "createdAt": "2020-06-09T22:43:22Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -232,7 +187,7 @@ public void closeAndRecycleState() {\n     private void close(final boolean clean) {\n         switch (state()) {\n             case CREATED:\n-            case RUNNING:\n+            case SUSPENDED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NjI3Mw==", "bodyText": "nit: since this is a private function only called by suspend, we can modify the caller such that we only call this in RUNNING not in RESTORING, and then inside this function we do not need this check anymore.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437766273", "createdAt": "2020-06-09T22:54:59Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -891,6 +821,9 @@ private void initializeTopology() {\n     }\n \n     private void closeTopology(final boolean clean) {\n+        if (state() != State.RUNNING) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAzMTQ4Mg=="}, "originalCommit": null, "originalPosition": 412}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NjcxNg==", "bodyText": "Maybe we can skip calling this if we are in RESTORING; I have another comment below.\nAlso could we add javadoc on top explaining what exception can be thrown?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437766716", "createdAt": "2020-06-09T22:56:14Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -247,82 +246,23 @@ public void completeRestoration() {\n         }\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     * </pre>\n-     *\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n-    @Override\n-    public void prepareSuspend() {\n-        switch (state()) {\n-            case CREATED:\n-            case SUSPENDED:\n-                // do nothing\n-                log.trace(\"Skip prepare suspending since state is {}\", state());\n-\n-                break;\n-\n-            case RESTORING:\n-                stateMgr.flush();\n-                log.info(\"Prepare suspending restoring\");\n-\n-                break;\n-\n-            case RUNNING:\n-                closeTopology(true);\n-\n-                stateMgr.flush();\n-                recordCollector.flush();\n-\n-                log.info(\"Prepare suspending running\");\n-\n-                break;\n-\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n-        }\n-    }\n-\n     @Override\n     public void suspend() {\n         switch (state()) {\n             case CREATED:\n             case SUSPENDED:\n-                // do nothing\n-                log.trace(\"Skip suspending since state is {}\", state());\n-\n-                break;\n-\n-            case RUNNING:\n-                stateMgr.checkpoint(checkpointableOffsets());\n-                partitionGroup.clear();\n-\n-                transitionTo(State.SUSPENDED);\n-                log.info(\"Suspended running\");\n+                log.info(\"Skip suspending since state is {}\", state());\n \n                 break;\n \n             case RESTORING:\n-                // we just checkpoint the position that we've restored up to without\n-                // going through the commit process\n-                stateMgr.checkpoint(emptyMap());\n-\n-                // we should also clear any buffered records of a task when suspending it\n-                partitionGroup.clear();\n-\n-                transitionTo(State.SUSPENDED);\n-                log.info(\"Suspended restoring\");\n+            case RUNNING:\n+                try {\n+                    closeTopology();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NzA1Nw==", "bodyText": "+1, IDEA also suggests it :)", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437767057", "createdAt": "2020-06-09T22:57:20Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -504,88 +438,66 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n-\n+        suspend();\n+        prepareCommit();\n         writeCheckpointIfNeed();\n \n         switch (state()) {\n             case CREATED:\n-            case RESTORING:\n-            case RUNNING:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n+\n                 break;\n+\n+            case RESTORING: // we should have transitioned to `SUSPENDED` already\n+            case RUNNING: // we should have transitioned to `SUSPENDED` already\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n         }\n \n-        partitionGroup.close();\n+        partitionGroup.clear();\n         closeTaskSensor.record();\n \n         transitionTo(State.CLOSED);\n \n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     * </pre>\n-     *\n-     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n-     *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if the task producer got fenced (EOS)\n-     */\n-    private void prepareClose(final boolean clean) {\n-        // Reset any previously scheduled checkpoint.\n-        checkpoint = null;\n-\n+    private void maybeScheduleCheckpoint() {\n         switch (state()) {\n-            case CREATED:\n-                // the task is created and not initialized, just re-write the checkpoint file\n-                scheduleCheckpoint(emptyMap());\n+            case RESTORING:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4ODQwOQ=="}, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 364}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NzkxMA==", "bodyText": "Maybe we can make an optimization by remembering the committableOffsets, and then if the value (both offset and time) does not change we do not need to give it out to consumer to commit.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437767910", "createdAt": "2020-06-09T22:59:43Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -378,42 +320,88 @@ public void prepareCommit() {\n                 break;\n \n             case CREATED:\n-            case SUSPENDED:\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n \n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while preparing active task \" + id + \" for committing\");\n         }\n+\n+        return committableOffsetsAndMetadata();\n     }\n \n-    @Override\n-    public void postCommit() {\n+    private Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n+        final Map<TopicPartition, OffsetAndMetadata> committableOffsets;\n+\n         switch (state()) {\n-            case RUNNING:\n-                commitNeeded = false;\n-                commitRequested = false;\n+            case CREATED:\n+            case RESTORING:\n+                committableOffsets = Collections.emptyMap();\n \n-                if (!eosEnabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n-                }\n+                break;\n \n-                log.debug(\"Committed\");\n+            case RUNNING:\n+            case SUSPENDED:\n+                final Map<TopicPartition, Long> partitionTimes = extractPartitionTimes();\n+\n+                committableOffsets = new HashMap<>(consumedOffsets.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2ODg5Nw==", "bodyText": "Why we need this check?\nAlso nit: how about maybeWriteCheckpoint to align with maybeScheduleCheckpoint.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437768897", "createdAt": "2020-06-09T23:02:25Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -504,88 +438,66 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n-\n+        suspend();\n+        prepareCommit();\n         writeCheckpointIfNeed();\n \n         switch (state()) {\n             case CREATED:\n-            case RESTORING:\n-            case RUNNING:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n+\n                 break;\n+\n+            case RESTORING: // we should have transitioned to `SUSPENDED` already\n+            case RUNNING: // we should have transitioned to `SUSPENDED` already\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n         }\n \n-        partitionGroup.close();\n+        partitionGroup.clear();\n         closeTaskSensor.record();\n \n         transitionTo(State.CLOSED);\n \n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     * </pre>\n-     *\n-     * @param clean    shut down cleanly (ie, incl. flush) if {@code true} --\n-     *                 otherwise, just close open resources\n-     * @throws TaskMigratedException if the task producer got fenced (EOS)\n-     */\n-    private void prepareClose(final boolean clean) {\n-        // Reset any previously scheduled checkpoint.\n-        checkpoint = null;\n-\n+    private void maybeScheduleCheckpoint() {\n         switch (state()) {\n-            case CREATED:\n-                // the task is created and not initialized, just re-write the checkpoint file\n-                scheduleCheckpoint(emptyMap());\n+            case RESTORING:\n+                this.checkpoint = checkpointableOffsets();\n+\n                 break;\n \n             case RUNNING:\n-                closeTopology(clean);\n-\n-                if (clean) {\n-                    stateMgr.flush();\n-                    recordCollector.flush();\n-                    scheduleCheckpoint(checkpointableOffsets());\n-                } else {\n-                    executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\", log);\n+                if (!eosEnabled) {\n+                    this.checkpoint = checkpointableOffsets();\n                 }\n \n                 break;\n \n-            case RESTORING:\n-                executeAndMaybeSwallow(clean, stateMgr::flush, \"state manager flush\", log);\n-                scheduleCheckpoint(emptyMap());\n+            case SUSPENDED:\n+                this.checkpoint = checkpointableOffsets();\n \n                 break;\n \n-            case SUSPENDED:\n+            case CREATED:\n             case CLOSED:\n-                // not need to checkpoint, since when suspending we've already committed the state\n-                break;\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while scheduling checkpoint for active task \" + id);\n \n             default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while prepare closing active task \" + id);\n+                throw new IllegalStateException(\"Unknown state \" + state() + \" while scheduling checkpoint for active task \" + id);\n         }\n     }\n \n-    private void scheduleCheckpoint(final Map<TopicPartition, Long> checkpoint) {\n-        this.checkpoint = checkpoint;\n-    }\n-\n     private void writeCheckpointIfNeed() {\n+        if (commitNeeded) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 410}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2OTY2NA==", "bodyText": "What about 1) move the line 387/388 out of the switch, also line 400 after the switch block, 2) and then make these three states separate branches, so that we can avoid a mix of switch / if-else.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437769664", "createdAt": "2020-06-09T23:04:43Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -378,42 +320,88 @@ public void prepareCommit() {\n                 break;\n \n             case CREATED:\n-            case SUSPENDED:\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n \n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while preparing active task \" + id + \" for committing\");\n         }\n+\n+        return committableOffsetsAndMetadata();\n     }\n \n-    @Override\n-    public void postCommit() {\n+    private Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n+        final Map<TopicPartition, OffsetAndMetadata> committableOffsets;\n+\n         switch (state()) {\n-            case RUNNING:\n-                commitNeeded = false;\n-                commitRequested = false;\n+            case CREATED:\n+            case RESTORING:\n+                committableOffsets = Collections.emptyMap();\n \n-                if (!eosEnabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n-                }\n+                break;\n \n-                log.debug(\"Committed\");\n+            case RUNNING:\n+            case SUSPENDED:\n+                final Map<TopicPartition, Long> partitionTimes = extractPartitionTimes();\n+\n+                committableOffsets = new HashMap<>(consumedOffsets.size());\n+                for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n+                    final TopicPartition partition = entry.getKey();\n+                    Long offset = partitionGroup.headRecordOffset(partition);\n+                    if (offset == null) {\n+                        try {\n+                            offset = mainConsumer.position(partition);\n+                        } catch (final TimeoutException error) {\n+                            // the `consumer.position()` call should never block, because we know that we did process data\n+                            // for the requested partition and thus the consumer should have a valid local position\n+                            // that it can return immediately\n+\n+                            // hence, a `TimeoutException` indicates a bug and thus we rethrow it as fatal `IllegalStateException`\n+                            throw new IllegalStateException(error);\n+                        } catch (final KafkaException fatal) {\n+                            throw new StreamsException(fatal);\n+                        }\n+                    }\n+                    final long partitionTime = partitionTimes.get(partition);\n+                    committableOffsets.put(partition, new OffsetAndMetadata(offset, encodeTimestamp(partitionTime)));\n+                }\n \n                 break;\n \n+            case CLOSED:\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while getting commitable offsets for active task \" + id);\n+\n+            default:\n+                throw new IllegalStateException(\"Unknown state \" + state() + \" while post committing active task \" + id);\n+        }\n+\n+        return committableOffsets;\n+    }\n+\n+    @Override\n+    public void postCommit() {\n+        switch (state()) {\n             case RESTORING:\n-                commitNeeded = false;\n+            case RUNNING:\n+            case SUSPENDED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MTA0Mg==", "bodyText": "Hmm... this reads a bit weird to me. Can we call this in suspend instead? Also in that case we do not need to call this in close and closeAndRecycle.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437771042", "createdAt": "2020-06-09T23:08:58Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -378,42 +320,88 @@ public void prepareCommit() {\n                 break;\n \n             case CREATED:\n-            case SUSPENDED:\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n \n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while preparing active task \" + id + \" for committing\");\n         }\n+\n+        return committableOffsetsAndMetadata();\n     }\n \n-    @Override\n-    public void postCommit() {\n+    private Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n+        final Map<TopicPartition, OffsetAndMetadata> committableOffsets;\n+\n         switch (state()) {\n-            case RUNNING:\n-                commitNeeded = false;\n-                commitRequested = false;\n+            case CREATED:\n+            case RESTORING:\n+                committableOffsets = Collections.emptyMap();\n \n-                if (!eosEnabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n-                }\n+                break;\n \n-                log.debug(\"Committed\");\n+            case RUNNING:\n+            case SUSPENDED:\n+                final Map<TopicPartition, Long> partitionTimes = extractPartitionTimes();\n+\n+                committableOffsets = new HashMap<>(consumedOffsets.size());\n+                for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n+                    final TopicPartition partition = entry.getKey();\n+                    Long offset = partitionGroup.headRecordOffset(partition);\n+                    if (offset == null) {\n+                        try {\n+                            offset = mainConsumer.position(partition);\n+                        } catch (final TimeoutException error) {\n+                            // the `consumer.position()` call should never block, because we know that we did process data\n+                            // for the requested partition and thus the consumer should have a valid local position\n+                            // that it can return immediately\n+\n+                            // hence, a `TimeoutException` indicates a bug and thus we rethrow it as fatal `IllegalStateException`\n+                            throw new IllegalStateException(error);\n+                        } catch (final KafkaException fatal) {\n+                            throw new StreamsException(fatal);\n+                        }\n+                    }\n+                    final long partitionTime = partitionTimes.get(partition);\n+                    committableOffsets.put(partition, new OffsetAndMetadata(offset, encodeTimestamp(partitionTime)));\n+                }\n \n                 break;\n \n+            case CLOSED:\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while getting commitable offsets for active task \" + id);\n+\n+            default:\n+                throw new IllegalStateException(\"Unknown state \" + state() + \" while post committing active task \" + id);\n+        }\n+\n+        return committableOffsets;\n+    }\n+\n+    @Override\n+    public void postCommit() {\n+        switch (state()) {\n             case RESTORING:\n-                commitNeeded = false;\n+            case RUNNING:\n+            case SUSPENDED:\n                 commitRequested = false;\n+                commitNeeded = false;\n+\n+                if (state() == State.RESTORING || state() == State.SUSPENDED) {\n+                    writeCheckpointIfNeed();\n+                } else if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n+                    writeCheckpointIfNeed();\n+                }\n \n-                stateMgr.checkpoint(checkpointableOffsets());\n+                if (state() == State.SUSPENDED) {\n+                    partitionGroup.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MzE1Mw==", "bodyText": "I think we actually do not need to commit (including write-checkpoint) when closeAndRecycle actually, and only need to suspend the task before recycle it. But this is out of the scope and we can discuss about this in another PR (cc @ableegoldman ).", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437773153", "createdAt": "2020-06-09T23:15:15Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -504,88 +438,66 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n-\n+        suspend();\n+        prepareCommit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 313}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MzU4Mg==", "bodyText": "Should we return emptyMap if we are SUSPENDED as well?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437773582", "createdAt": "2020-06-09T23:16:28Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -813,6 +727,10 @@ private void updateProcessorContext(final StampedRecord record, final ProcessorN\n      * Currently only changelog topic offsets need to be checkpointed.\n      */\n     private Map<TopicPartition, Long> checkpointableOffsets() {\n+        if (state() == State.RESTORING) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 444}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3NDYwNg==", "bodyText": "EDIT: actually, I think we need to accumulate the consumed offsets when we just transited to suspend and then called prepareCommit, but if we are already in suspended then it is actually okay to return an emptyMap. However since we do not know if we have just transited to suspended and the below code should not be a big overhead, we can just keep it as-is.\nSo please ignore my previous comment :)", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437774606", "createdAt": "2020-06-09T23:20:01Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -813,6 +727,10 @@ private void updateProcessorContext(final StampedRecord record, final ProcessorN\n      * Currently only changelog topic offsets need to be checkpointed.\n      */\n     private Map<TopicPartition, Long> checkpointableOffsets() {\n+        if (state() == State.RESTORING) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MzU4Mg=="}, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 444}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3NjcwNg==", "bodyText": "question: I cannot remember why we need to commit those still owned tasks during handle-assignment, is that necessary? Or is that just an optimization: since we are going to commit anyways, let's just commit everyone.\nIf that's the case, we can refresh the last-commit timestamp as well.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437776706", "createdAt": "2020-06-09T23:26:34Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -210,9 +214,8 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 tasksToRecycle.add(task);\n             } else {\n                 try {\n-                    task.prepareCloseClean();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task\n-                        .committableOffsetsAndMetadata();\n+                    task.suspend();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3NzI0NQ==", "bodyText": "Even in that case, in line 205 we could check task.commitNeeded() && task.isActive right?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437777245", "createdAt": "2020-06-09T23:28:23Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -210,9 +214,8 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 tasksToRecycle.add(task);\n             } else {\n                 try {\n-                    task.prepareCloseClean();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task\n-                        .committableOffsetsAndMetadata();\n+                    task.suspend();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3NjcwNg=="}, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 20}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4b266602250f63b6c11b8612997aee2f10db871", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/c4b266602250f63b6c11b8612997aee2f10db871", "committedDate": "2020-06-10T01:27:30Z", "message": "Github comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3Njc0MDU1", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-427674055", "createdAt": "2020-06-10T02:46:16Z", "commit": {"oid": "c4b266602250f63b6c11b8612997aee2f10db871"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwMjo0NjoxNlrOGhi_sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNjoyMzo1OFrOGh836g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyOTU1NQ==", "bodyText": "Is this addressed?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r437829555", "createdAt": "2020-06-10T02:46:16Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -247,82 +246,23 @@ public void completeRestoration() {\n         }\n     }\n \n-    /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. first close topology to make sure all cached records in the topology are processed\n-     *  2. then flush the state, send any left changelog records\n-     *  3. then flush the record collector\n-     *  4. then commit the record collector -- for EOS this is the synchronization barrier\n-     *  5. then checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     * </pre>\n-     *\n-     * @throws TaskMigratedException if committing offsets failed (non-EOS)\n-     *                               or if the task producer got fenced (EOS)\n-     */\n-    @Override\n-    public void prepareSuspend() {\n-        switch (state()) {\n-            case CREATED:\n-            case SUSPENDED:\n-                // do nothing\n-                log.trace(\"Skip prepare suspending since state is {}\", state());\n-\n-                break;\n-\n-            case RESTORING:\n-                stateMgr.flush();\n-                log.info(\"Prepare suspending restoring\");\n-\n-                break;\n-\n-            case RUNNING:\n-                closeTopology(true);\n-\n-                stateMgr.flush();\n-                recordCollector.flush();\n-\n-                log.info(\"Prepare suspending running\");\n-\n-                break;\n-\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n-        }\n-    }\n-\n     @Override\n     public void suspend() {\n         switch (state()) {\n             case CREATED:\n             case SUSPENDED:\n-                // do nothing\n-                log.trace(\"Skip suspending since state is {}\", state());\n-\n-                break;\n-\n-            case RUNNING:\n-                stateMgr.checkpoint(checkpointableOffsets());\n-                partitionGroup.clear();\n-\n-                transitionTo(State.SUSPENDED);\n-                log.info(\"Suspended running\");\n+                log.info(\"Skip suspending since state is {}\", state());\n \n                 break;\n \n             case RESTORING:\n-                // we just checkpoint the position that we've restored up to without\n-                // going through the commit process\n-                stateMgr.checkpoint(emptyMap());\n-\n-                // we should also clear any buffered records of a task when suspending it\n-                partitionGroup.clear();\n-\n-                transitionTo(State.SUSPENDED);\n-                log.info(\"Suspended restoring\");\n+            case RUNNING:\n+                try {\n+                    closeTopology();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NjcxNg=="}, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI0Mzc5Nw==", "bodyText": "+1, this seems not really necessary atm.", "url": "https://github.com/apache/kafka/pull/8833#discussion_r438243797", "createdAt": "2020-06-10T16:10:18Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -378,42 +320,88 @@ public void prepareCommit() {\n                 break;\n \n             case CREATED:\n-            case SUSPENDED:\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n \n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while preparing active task \" + id + \" for committing\");\n         }\n+\n+        return committableOffsetsAndMetadata();\n     }\n \n-    @Override\n-    public void postCommit() {\n+    private Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n+        final Map<TopicPartition, OffsetAndMetadata> committableOffsets;\n+\n         switch (state()) {\n-            case RUNNING:\n-                commitNeeded = false;\n-                commitRequested = false;\n+            case CREATED:\n+            case RESTORING:\n+                committableOffsets = Collections.emptyMap();\n \n-                if (!eosEnabled) {\n-                    stateMgr.checkpoint(checkpointableOffsets());\n-                }\n+                break;\n \n-                log.debug(\"Committed\");\n+            case RUNNING:\n+            case SUSPENDED:\n+                final Map<TopicPartition, Long> partitionTimes = extractPartitionTimes();\n+\n+                committableOffsets = new HashMap<>(consumedOffsets.size());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NzkxMA=="}, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI0ODg4Nw==", "bodyText": "Maybe not necessary after a second thought. However, one more question: why not making closeAndRecycleState idempotent as well?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r438248887", "createdAt": "2020-06-10T16:16:56Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -506,19 +455,18 @@ public void update(final Set<TopicPartition> topicPartitions, final ProcessorTop\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n-\n+        suspendCleanAndPrepareCommit();\n         writeCheckpointIfNeed();\n \n         switch (state()) {\n             case CREATED:\n-            case RUNNING:\n-            case RESTORING:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n                 break;\n \n+            case RESTORING: // we should have transitioned to `SUSPENDED` already", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyOTQ2NA=="}, "originalCommit": null, "originalPosition": 297}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI1MjA5Nw==", "bodyText": "Is this logic necessary? I don't think we would populate data in record collector or consumed offsets until we start processing?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r438252097", "createdAt": "2020-06-10T16:21:45Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -813,6 +727,10 @@ private void updateProcessorContext(final StampedRecord record, final ProcessorN\n      * Currently only changelog topic offsets need to be checkpointed.\n      */\n     private Map<TopicPartition, Long> checkpointableOffsets() {\n+        if (state() == State.RESTORING) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MzU4Mg=="}, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 444}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI1MzU0Ng==", "bodyText": "Done", "url": "https://github.com/apache/kafka/pull/8833#discussion_r438253546", "createdAt": "2020-06-10T16:23:58Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -623,7 +618,11 @@ private long sumOfChangelogOffsets(final TaskId id, final Map<TopicPartition, Lo\n     }\n \n     private void closeTaskDirty(final Task task) {\n-        task.prepareCloseDirty();\n+        try {\n+            task.suspend();\n+        } catch (final RuntimeException swallow) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NzY2NQ=="}, "originalCommit": {"oid": "120fb8d8134add8aeb4b402c1e0ecd54a5b673a5"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDE2NjA2", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-428416606", "createdAt": "2020-06-10T20:48:31Z", "commit": {"oid": "c4b266602250f63b6c11b8612997aee2f10db871"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e9859258b2f5bb842262549b362cc6efe85dc5d", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/9e9859258b2f5bb842262549b362cc6efe85dc5d", "committedDate": "2020-06-10T20:53:48Z", "message": "Github comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "329e187a9187c4ea532ce88e0202402cb6f78b86", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/329e187a9187c4ea532ce88e0202402cb6f78b86", "committedDate": "2020-06-10T22:32:54Z", "message": "Github comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDk2OTk2", "url": "https://github.com/apache/kafka/pull/8833#pullrequestreview-428496996", "createdAt": "2020-06-10T23:33:09Z", "commit": {"oid": "329e187a9187c4ea532ce88e0202402cb6f78b86"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMzozMzowOVrOGiJpAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMzozMzowOVrOGiJpAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2MjcyMQ==", "bodyText": "Why call prepareCommit (or suspend for that matter)?", "url": "https://github.com/apache/kafka/pull/8833#discussion_r438462721", "createdAt": "2020-06-10T23:33:09Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -151,73 +158,24 @@ public void postCommit() {\n         }\n     }\n \n-    @Override\n-    public void prepareCloseClean() {\n-        prepareClose(true);\n-\n-        log.info(\"Prepared clean close\");\n-    }\n-\n-    @Override\n-    public void prepareCloseDirty() {\n-        prepareClose(false);\n-\n-        log.info(\"Prepared dirty close\");\n-    }\n-\n-    /**\n-     * 1. commit if we are running and clean close;\n-     * 2. close the state manager.\n-     *\n-     * @throws TaskMigratedException all the task has been migrated\n-     * @throws StreamsException fatal error, should close the thread\n-     */\n-    private void prepareClose(final boolean clean) {\n-        switch (state()) {\n-            case CREATED:\n-            case CLOSED:\n-                log.trace(\"Skip prepare closing since state is {}\", state());\n-                return;\n-\n-            case RUNNING:\n-                if (clean) {\n-                    stateMgr.flush();\n-                }\n-\n-                break;\n-\n-            case RESTORING:\n-            case SUSPENDED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing standby task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while closing standby task \" + id);\n-        }\n-    }\n-\n     @Override\n     public void closeClean() {\n         close(true);\n-\n         log.info(\"Closed clean\");\n     }\n \n     @Override\n     public void closeDirty() {\n         close(false);\n-\n         log.info(\"Closed dirty\");\n     }\n \n     @Override\n     public void closeAndRecycleState() {\n-        prepareClose(true);\n+        suspend();\n+        prepareCommit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329e187a9187c4ea532ce88e0202402cb6f78b86"}, "originalPosition": 141}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 926, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}