{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA0MDMyNzIw", "number": 9441, "title": "KAFKA-10614: Ensure group state (un)load is executed in the submitted order", "bodyText": "Implements the single thread with FIFO approach suggested in https://issues.apache.org/jira/browse/KAFKA-10614", "createdAt": "2020-10-15T11:33:16Z", "url": "https://github.com/apache/kafka/pull/9441", "merged": true, "mergeCommit": {"oid": "c72ce005e70a965dee25d341e5be8fceb1477b37"}, "closed": true, "closedAt": "2021-06-07T13:19:49Z", "author": {"login": "tombentley"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdTF9XcgBqjM4ODYyNDY0MTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABeeV-wQABqjQ4NTAyMzAwODE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "09ccad0a991ebc7f847cdde504aec3f222f7e5c3", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/09ccad0a991ebc7f847cdde504aec3f222f7e5c3", "committedDate": "2020-10-16T12:57:01Z", "message": "fixup"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "07a9eb359ca564594ae99dddb0d0a46939ec96e7", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/07a9eb359ca564594ae99dddb0d0a46939ec96e7", "committedDate": "2020-12-15T11:19:31Z", "message": "Move calls to *Coordinator.onResignation inside ReplicManager.stopReplicas\n\nThis is so that onResignation is called with the replicaStateChangeLock held."}, "afterCommit": {"oid": "a7cde76f9d6362528fa8277b00d92f9170f7b03a", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/a7cde76f9d6362528fa8277b00d92f9170f7b03a", "committedDate": "2020-12-15T11:49:10Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a7cde76f9d6362528fa8277b00d92f9170f7b03a", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/a7cde76f9d6362528fa8277b00d92f9170f7b03a", "committedDate": "2020-12-15T11:49:10Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order"}, "afterCommit": {"oid": "f13bd300503631e8cc3065cd6d4625fc7268e4d8", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/f13bd300503631e8cc3065cd6d4625fc7268e4d8", "committedDate": "2021-01-25T14:20:21Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f13bd300503631e8cc3065cd6d4625fc7268e4d8", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/f13bd300503631e8cc3065cd6d4625fc7268e4d8", "committedDate": "2021-01-25T14:20:21Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order"}, "afterCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e", "committedDate": "2021-02-24T09:56:08Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI4MjEyNzU3", "url": "https://github.com/apache/kafka/pull/9441#pullrequestreview-628212757", "createdAt": "2021-04-05T22:39:00Z", "commit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0wNVQyMjozOTowMVrOJDPMPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0wNVQyMjo0MDoxMVrOJDPN6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzM3NDM5Nw==", "bodyText": "Hmm.. Why remove the epoch after resignation? It seems like it would be useful to keep tracking it. Maybe it's useful to distinguish the case where the replica is to be deleted?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r607374397", "createdAt": "2021-04-05T22:39:01Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +907,33 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {\n+      info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+      epochForPartitionId.remove(offsetTopicPartitionId)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzM3NDgyNg==", "bodyText": "Does this need to be a concurrent collection? It does not look like we can count on a lock protecting onElection and onResignation.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r607374826", "createdAt": "2021-04-05T22:40:11Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -87,6 +87,8 @@ class GroupCoordinator(val brokerId: Int,\n \n   private val isActive = new AtomicBoolean(false)\n \n+  val epochForPartitionId = mutable.Map[Int, Int]()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQxNzE2NTkz", "url": "https://github.com/apache/kafka/pull/9441#pullrequestreview-641716593", "createdAt": "2021-04-22T01:59:40Z", "commit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMTo1OTo0MFrOJNZOGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMjowMzowNFrOJNZWew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNDQ3NA==", "bodyText": "Can we do a CAS update? Otherwise I don't think this is safe.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618024474", "createdAt": "2021-04-22T01:59:40Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,32 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNTAyNg==", "bodyText": "Similarly, we should update epochForPartitionId here with a CAS operation.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618025026", "createdAt": "2021-04-22T02:00:29Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,32 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw==", "bodyText": "It's not clear to me why we moved this in ReplicaManager. Is there some reason we need the replicaStateChangeLock lock?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618026427", "createdAt": "2021-04-22T02:02:45Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjYxOQ==", "bodyText": "Can you fix this?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618026619", "createdAt": "2021-04-22T02:03:04Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala", "diffHunk": "@@ -2891,17 +2898,29 @@ class KafkaApisTest {\n       EasyMock.eq(controllerId),\n       EasyMock.eq(controllerEpoch),\n       EasyMock.eq(brokerEpochInRequest),\n-      EasyMock.eq(stopReplicaRequest.partitionStates().asScala)\n-    )).andStubReturn(\n-      (mutable.Map(\n+      EasyMock.eq(stopReplicaRequest.partitionStates().asScala),\n+      EasyMock.anyObject()\n+    )).andStubAnswer {() =>\n+      val result = (mutable.Map(\n         fooPartition -> Errors.NONE\n       ), Errors.NONE)\n-    )\n+//<<<<<<< HEAD", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ1MTY3OTU2", "url": "https://github.com/apache/kafka/pull/9441#pullrequestreview-645167956", "createdAt": "2021-04-26T22:32:51Z", "commit": {"oid": "4c478d59d7a2c27cc3c2450dc3e2b711d2cbc368"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yNlQyMjozMjo1MVrOJP8P3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yNlQyMjozNzo0MVrOJP8YCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMDY5NTUxNw==", "bodyText": "One final thing I was considering is whether we should push this check into GroupMetadataManager.loadGroupsAndOffsets. That would give us some protection against any assumptions about ordering in KafkaScheduler.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r620695517", "createdAt": "2021-04-26T22:32:51Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,35 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    epochForPartitionId.compute(offsetTopicPartitionId, (_, epoch) => {\n+      val currentEpoch = Option(epoch)\n+      if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c478d59d7a2c27cc3c2450dc3e2b711d2cbc368"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMDY5NzYxMQ==", "bodyText": "I have probably not been doing a good job of being clear. It is useful to bump the epoch whenever we observe a larger value whether it is in onResignation or onElection. This protects us from all potential reorderings.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r620697611", "createdAt": "2021-04-26T22:37:41Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,32 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNTAyNg=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NTQ2OTQ2", "url": "https://github.com/apache/kafka/pull/9441#pullrequestreview-649546946", "createdAt": "2021-04-30T19:17:40Z", "commit": {"oid": "311a6f91a83aa65cec4d9c67258e47fb3cf8ab4a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0zMFQxOToxNzo0MFrOJTN9kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0zMFQxOToxNzo0MFrOJTN9kQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNDEzMTQ3Mw==", "bodyText": "One minor improvement here is to change addLoadingPartition so that it checks whether the partition is already contained in ownedPartitions. If so, we can return false.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r624131473", "createdAt": "2021-04-30T19:17:40Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -526,34 +529,42 @@ class GroupMetadataManager(brokerId: Int,\n   /**\n    * Asynchronously read the partition from the offsets topic and populate the cache\n    */\n-  def scheduleLoadGroupAndOffsets(offsetsPartition: Int, onGroupLoaded: GroupMetadata => Unit): Unit = {\n+  def scheduleLoadGroupAndOffsets(offsetsPartition: Int, coordinatorEpoch: Int, onGroupLoaded: GroupMetadata => Unit): Unit = {\n     val topicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, offsetsPartition)\n-    if (addLoadingPartition(offsetsPartition)) {\n-      info(s\"Scheduling loading of offsets and group metadata from $topicPartition\")\n-      val startTimeMs = time.milliseconds()\n-      scheduler.schedule(topicPartition.toString, () => loadGroupsAndOffsets(topicPartition, onGroupLoaded, startTimeMs))\n-    } else {\n-      info(s\"Already loading offsets and group metadata from $topicPartition\")\n-    }\n+    info(s\"Scheduling loading of offsets and group metadata from $topicPartition for epoch $coordinatorEpoch\")\n+    val startTimeMs = time.milliseconds()\n+    scheduler.schedule(topicPartition.toString, () => loadGroupsAndOffsets(topicPartition, coordinatorEpoch, onGroupLoaded, startTimeMs))\n   }\n \n-  private[group] def loadGroupsAndOffsets(topicPartition: TopicPartition, onGroupLoaded: GroupMetadata => Unit, startTimeMs: java.lang.Long): Unit = {\n-    try {\n-      val schedulerTimeMs = time.milliseconds() - startTimeMs\n-      debug(s\"Started loading offsets and group metadata from $topicPartition\")\n-      doLoadGroupsAndOffsets(topicPartition, onGroupLoaded)\n-      val endTimeMs = time.milliseconds()\n-      val totalLoadingTimeMs = endTimeMs - startTimeMs\n-      partitionLoadSensor.record(totalLoadingTimeMs.toDouble, endTimeMs, false)\n-      info(s\"Finished loading offsets and group metadata from $topicPartition \"\n-        + s\"in $totalLoadingTimeMs milliseconds, of which $schedulerTimeMs milliseconds\"\n-        + s\" was spent in the scheduler.\")\n-    } catch {\n-      case t: Throwable => error(s\"Error loading offsets from $topicPartition\", t)\n-    } finally {\n-      inLock(partitionLock) {\n-        ownedPartitions.add(topicPartition.partition)\n-        loadingPartitions.remove(topicPartition.partition)\n+  private[group] def loadGroupsAndOffsets(\n+    topicPartition: TopicPartition,\n+    coordinatorEpoch: Int,\n+    onGroupLoaded: GroupMetadata => Unit,\n+    startTimeMs: java.lang.Long\n+  ): Unit = {\n+    if (!maybeUpdateCoordinatorEpoch(topicPartition.partition, Some(coordinatorEpoch))) {\n+      info(s\"Not loading offsets and group metadata for $topicPartition \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is ${epochForPartitionId.get(topicPartition.partition)}\")\n+    } else if (!addLoadingPartition(topicPartition.partition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "311a6f91a83aa65cec4d9c67258e47fb3cf8ab4a"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc2Njg2NzE3", "url": "https://github.com/apache/kafka/pull/9441#pullrequestreview-676686717", "createdAt": "2021-06-04T21:45:48Z", "commit": {"oid": "59e539625c3bb83918d95cc17df1332837bf1b90"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "59e539625c3bb83918d95cc17df1332837bf1b90", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/59e539625c3bb83918d95cc17df1332837bf1b90", "committedDate": "2021-05-10T11:33:49Z", "message": "Fix failing tests"}, "afterCommit": {"oid": "b42dbee1314b92d5e6f343a14a43d6e4290b354c", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/b42dbee1314b92d5e6f343a14a43d6e4290b354c", "committedDate": "2021-06-07T08:00:20Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order\n\nCo-authored-by: Jason Gustafson<jason@confluent.io>\nReviewers: Jason Gustafson<jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2dabf355f6ef7110901c6eb1204836683d626db", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/b2dabf355f6ef7110901c6eb1204836683d626db", "committedDate": "2021-06-07T08:02:26Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order\n\nReviewers: Jason Gustafson<jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>\nCo-authored-by: Jason Gustafson<jason@confluent.io>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b42dbee1314b92d5e6f343a14a43d6e4290b354c", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/b42dbee1314b92d5e6f343a14a43d6e4290b354c", "committedDate": "2021-06-07T08:00:20Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order\n\nCo-authored-by: Jason Gustafson<jason@confluent.io>\nReviewers: Jason Gustafson<jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>"}, "afterCommit": {"oid": "b2dabf355f6ef7110901c6eb1204836683d626db", "author": {"user": {"login": "tombentley", "name": "Tom Bentley"}}, "url": "https://github.com/apache/kafka/commit/b2dabf355f6ef7110901c6eb1204836683d626db", "committedDate": "2021-06-07T08:02:26Z", "message": "KAFKA-10614: Ensure group state (un)load is executed in the right order\n\nReviewers: Jason Gustafson<jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>\nCo-authored-by: Jason Gustafson<jason@confluent.io>"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 554, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}