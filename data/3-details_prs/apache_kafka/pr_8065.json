{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyNjQyNTcy", "number": 8065, "title": "KAKFA-9503: Fix TopologyTestDriver output order", "bodyText": "Because of the recursive invocations of pipeRecord, records in recursive topologies\nwould be appended to the TopologyTestDriver output buffer in depth-first order,\nrather than breadth-first, which is how KafkaStreams would do it.\nThis PR changes the processing algorithm from an (implicitly, via recursion) stack-based\none to a queue-based one more similar to how KafkaSteams actually executes.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-02-07T23:23:58Z", "url": "https://github.com/apache/kafka/pull/8065", "merged": true, "mergeCommit": {"oid": "998f1520f9af2dddfec9a9ac072f8dcf9d9004fd"}, "closed": true, "closedAt": "2020-02-12T03:00:18Z", "author": {"login": "vvcephei"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcC1fRBgFqTM1NTY4MDM0NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcDZynXAH2gAyMzcyNjQyNTcyOjlmYzRkYTBlN2YwYTI5ODQ1NzgyYzRmM2E4NjAyODliYmRmYjM1NmE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1NjgwMzQ1", "url": "https://github.com/apache/kafka/pull/8065#pullrequestreview-355680345", "createdAt": "2020-02-10T03:53:05Z", "commit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwMzo1MzowNVrOFnZqXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwNDoyMToxN1rOFnZ6TA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1OTIyOA==", "bodyText": "Not sure why you flip this condition?", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376859228", "createdAt": "2020-02-10T03:53:05Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MDE2MA==", "bodyText": "nit: add a blank line before this one to make it easier to read", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376860160", "createdAt": "2020-02-10T03:59:32Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -547,9 +571,12 @@ private void captureOutputRecords() {\n \n             // Forward back into the topology if the produced record is to an internal or a source topic ...\n             final String outputTopicName = record.topic();\n-            if (internalTopics.contains(outputTopicName) || processorTopology.sourceTopics().contains(outputTopicName)\n-                || globalPartitionsByTopic.containsKey(outputTopicName)) {\n-                pipeRecord(record);\n+            if (internalTopics.contains(outputTopicName)\n+                || processorTopology.sourceTopics().contains(outputTopicName)) {\n+                final TopicPartition topicPartition = getTopicPartition(record.topic());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MTQ3Ng==", "bodyText": "Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376861476", "createdAt": "2020-02-10T04:08:42Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {\n+            processGlobalRecord(topicName, timestamp, key, value, headers);\n         } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+            enqueueTaskRecord(topicName, topicPartition, timestamp, key, value, headers);\n+            processAllProcessableRecords();\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String topicName,\n+                                   final TopicPartition topicPartition,\n+                                   final Long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n+            validateSourceTopicNameRegexPattern(topicName);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MTc2OA==", "bodyText": "Should we add the same check for topicName, timestamp, and headers ?", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376861768", "createdAt": "2020-02-10T04:10:14Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {\n+            processGlobalRecord(topicName, timestamp, key, value, headers);\n         } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+            enqueueTaskRecord(topicName, topicPartition, timestamp, key, value, headers);\n+            processAllProcessableRecords();\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String topicName,\n+                                   final TopicPartition topicPartition,\n+                                   final Long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n+            validateSourceTopicNameRegexPattern(topicName);\n+        }\n+        Objects.requireNonNull(topicPartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MjA2OQ==", "bodyText": "There is already StreamTask#numBuffered() -- should we reuse this one instead of adding a new method?", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376862069", "createdAt": "2020-02-10T04:11:49Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {\n+            processGlobalRecord(topicName, timestamp, key, value, headers);\n         } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+            enqueueTaskRecord(topicName, topicPartition, timestamp, key, value, headers);\n+            processAllProcessableRecords();\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String topicName,\n+                                   final TopicPartition topicPartition,\n+                                   final Long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n+            validateSourceTopicNameRegexPattern(topicName);\n+        }\n+        Objects.requireNonNull(topicPartition);\n+\n+        final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n+        task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n+            topicName,\n+            topicPartition.partition(),\n+            offset,\n+            timestamp,\n+            TimestampType.CREATE_TIME,\n+            (long) ConsumerRecord.NULL_CHECKSUM,\n+            key == null ? ConsumerRecord.NULL_SIZE : key.length,\n+            value == null ? ConsumerRecord.NULL_SIZE : value.length,\n+            key,\n+            value,\n+            headers)));\n+    }\n+\n+    private void processAllProcessableRecords() {\n+        // If the topology only has global tasks, then `task` would be null.\n+        // For this method, it just means there's nothing to do.\n+        if (task != null) {\n+            while (task.hasRecordsQueued()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzIyNQ==", "bodyText": "Why do we not use a List?", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376863225", "createdAt": "2020-02-10T04:20:41Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java", "diffHunk": "@@ -1522,4 +1525,52 @@ public void shouldCreateStateDirectoryForStatefulTopology() {\n         final TaskId taskId = new TaskId(0, 0);\n         assertTrue(new File(appDir, taskId.toString()).exists());\n     }\n+\n+    @Test\n+    public void shouldEnqueueLaterOutputsAfterEarlierOnes() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            // given the topology above, we expect to see the output _first_ echo the input\n+            // and _then_ print it with \"recurse-\" prepended.\n+\n+            in.pipeInput(\"A\", \"alpha\");\n+            final Map<String, String> table = out.readKeyValuesToMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzMwOA==", "bodyText": "Why do we test the same thing twice?", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376863308", "createdAt": "2020-02-10T04:21:17Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java", "diffHunk": "@@ -1522,4 +1525,52 @@ public void shouldCreateStateDirectoryForStatefulTopology() {\n         final TaskId taskId = new TaskId(0, 0);\n         assertTrue(new File(appDir, taskId.toString()).exists());\n     }\n+\n+    @Test\n+    public void shouldEnqueueLaterOutputsAfterEarlierOnes() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            // given the topology above, we expect to see the output _first_ echo the input\n+            // and _then_ print it with \"recurse-\" prepended.\n+\n+            in.pipeInput(\"A\", \"alpha\");\n+            final Map<String, String> table = out.readKeyValuesToMap();\n+            assertThat(table, is(Collections.singletonMap(\"A\", \"recurse-alpha\")));\n+\n+            in.pipeInput(\"B\", \"beta\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb"}, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "feccf8219f65c5a24c570518c43467fc3b975a3c", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/feccf8219f65c5a24c570518c43467fc3b975a3c", "committedDate": "2020-02-10T23:09:21Z", "message": "KAFKA-9503: Fix TopologyTestDriver output order"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MzYwODg5", "url": "https://github.com/apache/kafka/pull/8065#pullrequestreview-356360889", "createdAt": "2020-02-11T00:30:04Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDozMDowNFrOFn6ckQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDozMDowNFrOFn6ckQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5NjM2OQ==", "bodyText": "ProducerRecord is actually guarantee to have a non-null timestamp -- not sure if we need this check -- actually similar for other fields like topic or header", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377396369", "createdAt": "2020-02-11T00:30:04Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -570,13 +570,14 @@ private void captureOutputRecords() {\n \n             // Forward back into the topology if the produced record is to an internal or a source topic ...\n             final String outputTopicName = record.topic();\n-            final long timestamp = Objects.requireNonNull(record.timestamp());\n             if (internalTopics.contains(outputTopicName)\n                 || processorTopology.sourceTopics().contains(outputTopicName)) {\n \n                 final TopicPartition topicPartition = getTopicPartition(record.topic());\n+                final long timestamp = Objects.requireNonNull(record.timestamp());\n                 enqueueTaskRecord(record.topic(), topicPartition, timestamp, record.key(), record.value(), record.headers());\n             } else if (globalPartitionsByTopic.containsKey(outputTopicName)) {\n+                final long timestamp = Objects.requireNonNull(record.timestamp());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MzcxNjAy", "url": "https://github.com/apache/kafka/pull/8065#pullrequestreview-356371602", "createdAt": "2020-02-11T01:05:12Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ae1c3f35e5fc38999a8ad6a4786536231e2dc32", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/0ae1c3f35e5fc38999a8ad6a4786536231e2dc32", "committedDate": "2020-02-11T04:13:38Z", "message": "code review feedback and an extra fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "committedDate": "2020-02-11T04:20:18Z", "message": "porting cleanup from tdd-fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2NDE3NTA1", "url": "https://github.com/apache/kafka/pull/8065#pullrequestreview-356417505", "createdAt": "2020-02-11T04:55:54Z", "commit": {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNDo1NTo1NFrOFn9evQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNDo1ODo1MVrOFn9gYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0NjA3Nw==", "bodyText": "Exposed this again for use in TopologyTestDriver", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377446077", "createdAt": "2020-02-11T04:55:54Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -444,7 +444,7 @@ private void close(final boolean clean) {\n      * An active task is processable if its buffer contains data for all of its input\n      * source topic partitions, or if it is enforced to be processable\n      */\n-    private boolean isProcessable(final long wallClockTime) {\n+    public boolean isProcessable(final long wallClockTime) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0NjIyMw==", "bodyText": "Here's the loop condition I added to let the loop terminate when task idling limits our ability to process enqueued records.", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377446223", "createdAt": "2020-02-11T04:56:51Z", "author": {"login": "vvcephei"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -456,62 +456,88 @@ public void pipeInput(final ConsumerRecord<byte[], byte[]> consumerRecord) {\n             consumerRecord.headers());\n     }\n \n-    private void pipeRecord(final ProducerRecord<byte[], byte[]> record) {\n-        pipeRecord(record.topic(), record.timestamp(), record.key(), record.value(), record.headers());\n-    }\n-\n     private void pipeRecord(final String topicName,\n-                            final Long timestamp,\n+                            final long timestamp,\n                             final byte[] key,\n                             final byte[] value,\n                             final Headers headers) {\n+        final TopicPartition inputTopicOrPatternPartition = getInputTopicOrPatternPartition(topicName);\n+        final TopicPartition globalInputTopicPartition = globalPartitionsByInputTopic.get(topicName);\n \n-        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n-            validateSourceTopicNameRegexPattern(topicName);\n+        if (inputTopicOrPatternPartition == null && globalInputTopicPartition == null) {\n+            throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n         }\n-        final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n-        } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+\n+        if (inputTopicOrPatternPartition != null) {\n+            enqueueTaskRecord(topicName, inputTopicOrPatternPartition, timestamp, key, value, headers);\n+            completeAllProcessableWork();\n+        }\n+\n+        if (globalInputTopicPartition != null) {\n+            processGlobalRecord(globalInputTopicPartition, timestamp, key, value, headers);\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String inputTopic,\n+                                   final TopicPartition topicOrPatternPartition,\n+                                   final long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        task.addRecords(topicOrPatternPartition, Collections.singleton(new ConsumerRecord<>(\n+            inputTopic,\n+            topicOrPatternPartition.partition(),\n+            offsetsByTopicOrPatternPartition.get(topicOrPatternPartition).incrementAndGet() - 1,\n+            timestamp,\n+            TimestampType.CREATE_TIME,\n+            (long) ConsumerRecord.NULL_CHECKSUM,\n+            key == null ? ConsumerRecord.NULL_SIZE : key.length,\n+            value == null ? ConsumerRecord.NULL_SIZE : value.length,\n+            key,\n+            value,\n+            headers)));\n+    }\n+\n+    private void completeAllProcessableWork() {\n+        // for internally triggered processing (like wall-clock punctuations),\n+        // we might have buffered some records to internal topics that need to\n+        // be piped back in to kick-start the processing loop. This is idempotent\n+        // and therefore harmless in the case where all we've done is enqueued an\n+        // input record from the user.\n+        captureOutputsAndReEnqueueInternalResults();\n+\n+        // If the topology only has global tasks, then `task` would be null.\n+        // For this method, it just means there's nothing to do.\n+        if (task != null) {\n+            while (task.hasRecordsQueued() && task.isProcessable(mockWallClockTime.milliseconds())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0NjQ5OA==", "bodyText": "Here's the new test I added to make sure TopologyTestDriver works properly with task idling.", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377446498", "createdAt": "2020-02-11T04:58:51Z", "author": {"login": "vvcephei"}, "path": "streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java", "diffHunk": "@@ -1522,4 +1526,174 @@ public void shouldCreateStateDirectoryForStatefulTopology() {\n         final TaskId taskId = new TaskId(0, 0);\n         assertTrue(new File(appDir, taskId.toString()).exists());\n     }\n+\n+    @Test\n+    public void shouldEnqueueLaterOutputsAfterEarlierOnes() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            // given the topology above, we expect to see the output _first_ echo the input\n+            // and _then_ print it with \"recurse-\" prepended.\n+\n+            in.pipeInput(\"B\", \"beta\");\n+            final List<KeyValue<String, String>> events = out.readKeyValuesToList();\n+            assertThat(\n+                events,\n+                is(Arrays.asList(\n+                    new KeyValue<>(\"B\", \"beta\"),\n+                    new KeyValue<>(\"B\", \"recurse-beta\")\n+                ))\n+            );\n+\n+        }\n+    }\n+\n+    @Test\n+    public void shouldApplyGlobalUpdatesCorrectlyInRecursiveTopologies() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addGlobalStore(\n+            Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore(\"globule-store\"), Serdes.String(), Serdes.String()).withLoggingDisabled(),\n+            \"globuleSource\",\n+            new StringDeserializer(),\n+            new StringDeserializer(),\n+            \"globule-topic\",\n+            \"globuleProcessor\",\n+            () -> new Processor<String, String>() {\n+                private KeyValueStore<String, String> stateStore;\n+\n+                @SuppressWarnings(\"unchecked\")\n+                @Override\n+                public void init(final ProcessorContext context) {\n+                    stateStore = (KeyValueStore<String, String>) context.getStateStore(\"globule-store\");\n+                }\n+\n+                @Override\n+                public void process(final String key, final String value) {\n+                    stateStore.put(key, value);\n+                }\n+\n+                @Override\n+                public void close() {\n+\n+                }\n+            }\n+        );\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                    context().forward(key, value, To.child(\"globuleSink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"globuleSink\", \"globule-topic\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> globalTopic = topologyTestDriver.createOutputTopic(\"globule-topic\", new StringDeserializer(), new StringDeserializer());\n+\n+            in.pipeInput(\"A\", \"alpha\");\n+\n+            // expect the global store to correctly reflect the last update\n+            final KeyValueStore<String, String> keyValueStore = topologyTestDriver.getKeyValueStore(\"globule-store\");\n+            assertThat(keyValueStore, notNullValue());\n+            assertThat(keyValueStore.get(\"A\"), is(\"recurse-alpha\"));\n+\n+            // and also just make sure the test really sent both events to the topic.\n+            final List<KeyValue<String, String>> events = globalTopic.readKeyValuesToList();\n+            assertThat(\n+                events,\n+                is(Arrays.asList(\n+                    new KeyValue<>(\"A\", \"alpha\"),\n+                    new KeyValue<>(\"A\", \"recurse-alpha\")\n+                ))\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldRespectTaskIdling() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        // This is the key to this test. Wall-clock time doesn't advance automatically in TopologyTestDriver,\n+        // so with an idle time specified, TTD can't just expect all enqueued records to be processable.\n+        properties.setProperty(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG, \"1000\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source1\", new StringDeserializer(), new StringDeserializer(), \"input1\");\n+        topology.addSource(\"source2\", new StringDeserializer(), new StringDeserializer(), \"input2\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"source1\", \"source2\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in1 = topologyTestDriver.createInputTopic(\"input1\", new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> in2 = topologyTestDriver.createInputTopic(\"input2\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            in1.pipeInput(\"A\", \"alpha\");\n+            topologyTestDriver.advanceWallClockTime(Duration.ofMillis(1));\n+\n+            // only one input has records, and it's only been one ms\n+            assertThat(out.readKeyValuesToList(), is(Collections.emptyList()));\n+\n+            in2.pipeInput(\"B\", \"beta\");\n+\n+            // because both topics have records, we can process (even though it's only been one ms)\n+            // but after processing A (the earlier record), we now only have one input queued, so\n+            // task idling takes effect again\n+            assertThat(\n+                out.readKeyValuesToList(),\n+                is(Collections.singletonList(\n+                    new KeyValue<>(\"A\", \"alpha\")\n+                ))\n+            );\n+\n+            topologyTestDriver.advanceWallClockTime(Duration.ofSeconds(1));\n+\n+            // now that one second has elapsed, the idle time has expired, and we can process B\n+            assertThat(\n+                out.readKeyValuesToList(),\n+                is(Collections.singletonList(\n+                    new KeyValue<>(\"B\", \"beta\")\n+                ))\n+            );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322"}, "originalPosition": 289}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2ODYyNDQx", "url": "https://github.com/apache/kafka/pull/8065#pullrequestreview-356862441", "createdAt": "2020-02-11T17:45:20Z", "commit": {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0NToyMFrOFoSvJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0NToyMFrOFoSvJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NDM0Mg==", "bodyText": "I think we should add a INFO (or WARN) log statement for the case that there is buffered data that cannot be processed yet and inform the use that they need to advance wall-clock time manually.\nWe might also have an INFO log when the TTD is created if we detect that task.idle is non-zero?\nHow should we handle close()? I think we should internally advance wall-clock time to ensure we drain all output? If we leave it to the user the might not be able to drain all records if there is a loop in the dataflow (what would required to advance wall-clock time after each new enqueue?). We would also have a test for this case if we implement it that way.", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377794342", "createdAt": "2020-02-11T17:45:20Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -456,62 +456,88 @@ public void pipeInput(final ConsumerRecord<byte[], byte[]> consumerRecord) {\n             consumerRecord.headers());\n     }\n \n-    private void pipeRecord(final ProducerRecord<byte[], byte[]> record) {\n-        pipeRecord(record.topic(), record.timestamp(), record.key(), record.value(), record.headers());\n-    }\n-\n     private void pipeRecord(final String topicName,\n-                            final Long timestamp,\n+                            final long timestamp,\n                             final byte[] key,\n                             final byte[] value,\n                             final Headers headers) {\n+        final TopicPartition inputTopicOrPatternPartition = getInputTopicOrPatternPartition(topicName);\n+        final TopicPartition globalInputTopicPartition = globalPartitionsByInputTopic.get(topicName);\n \n-        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n-            validateSourceTopicNameRegexPattern(topicName);\n+        if (inputTopicOrPatternPartition == null && globalInputTopicPartition == null) {\n+            throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n         }\n-        final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n-        } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+\n+        if (inputTopicOrPatternPartition != null) {\n+            enqueueTaskRecord(topicName, inputTopicOrPatternPartition, timestamp, key, value, headers);\n+            completeAllProcessableWork();\n+        }\n+\n+        if (globalInputTopicPartition != null) {\n+            processGlobalRecord(globalInputTopicPartition, timestamp, key, value, headers);\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String inputTopic,\n+                                   final TopicPartition topicOrPatternPartition,\n+                                   final long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        task.addRecords(topicOrPatternPartition, Collections.singleton(new ConsumerRecord<>(\n+            inputTopic,\n+            topicOrPatternPartition.partition(),\n+            offsetsByTopicOrPatternPartition.get(topicOrPatternPartition).incrementAndGet() - 1,\n+            timestamp,\n+            TimestampType.CREATE_TIME,\n+            (long) ConsumerRecord.NULL_CHECKSUM,\n+            key == null ? ConsumerRecord.NULL_SIZE : key.length,\n+            value == null ? ConsumerRecord.NULL_SIZE : value.length,\n+            key,\n+            value,\n+            headers)));\n+    }\n+\n+    private void completeAllProcessableWork() {\n+        // for internally triggered processing (like wall-clock punctuations),\n+        // we might have buffered some records to internal topics that need to\n+        // be piped back in to kick-start the processing loop. This is idempotent\n+        // and therefore harmless in the case where all we've done is enqueued an\n+        // input record from the user.\n+        captureOutputsAndReEnqueueInternalResults();\n+\n+        // If the topology only has global tasks, then `task` would be null.\n+        // For this method, it just means there's nothing to do.\n+        if (task != null) {\n+            while (task.hasRecordsQueued() && task.isProcessable(mockWallClockTime.milliseconds())) {\n+                // Process the record ...\n+                task.process(mockWallClockTime.milliseconds());\n+                task.maybePunctuateStreamTime();\n+                task.commit();\n+                captureOutputsAndReEnqueueInternalResults();\n             }\n-            final long offset = offsetsByTopicPartition.get(globalTopicPartition).incrementAndGet() - 1;\n-            globalStateTask.update(new ConsumerRecord<>(\n-                    globalTopicPartition.topic(),\n-                    globalTopicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers));\n-            globalStateTask.flushState();\n         }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322"}, "originalPosition": 197}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "committedDate": "2020-02-11T21:54:19Z", "message": "add logs when there are unprocessable records in TTD"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc5a693b6ed0ddb4fe76dc78f58a6574b93071f8", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/fc5a693b6ed0ddb4fe76dc78f58a6574b93071f8", "committedDate": "2020-02-11T22:10:23Z", "message": "fix npe in close if task is null"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MDMxMjUw", "url": "https://github.com/apache/kafka/pull/8065#pullrequestreview-357031250", "createdAt": "2020-02-11T22:05:10Z", "commit": {"oid": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMjowNToxMFrOFoa36g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMjoxMDozNFrOFobBRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyNzY1OA==", "bodyText": "nit: add () -> advanceWallClockTime() ?", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377927658", "createdAt": "2020-02-11T22:05:10Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -287,6 +287,14 @@ private TopologyTestDriver(final InternalTopologyBuilder builder,\n                                final Properties config,\n                                final long initialWallClockTimeMs) {\n         final StreamsConfig streamsConfig = new QuietStreamsConfig(config);\n+        final Long taskIdleTime = streamsConfig.getLong(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);\n+        if (taskIdleTime > 0) {\n+            log.info(\"Detected {} config in use with TopologyTestDriver (set to {}ms). This means you might need to\" +\n+                         \" use TopologyTestDriver#advanceWallClockTime or enqueue records on all partitions to allow\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyNzgzMw==", "bodyText": "What does Such occurrences means? Might be good to be more elaborative.", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377927833", "createdAt": "2020-02-11T22:05:32Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -287,6 +287,14 @@ private TopologyTestDriver(final InternalTopologyBuilder builder,\n                                final Properties config,\n                                final long initialWallClockTimeMs) {\n         final StreamsConfig streamsConfig = new QuietStreamsConfig(config);\n+        final Long taskIdleTime = streamsConfig.getLong(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);\n+        if (taskIdleTime > 0) {\n+            log.info(\"Detected {} config in use with TopologyTestDriver (set to {}ms). This means you might need to\" +\n+                         \" use TopologyTestDriver#advanceWallClockTime or enqueue records on all partitions to allow\" +\n+                         \" Steams to make progress. Such occurrences will be logged.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyOTIwMA==", "bodyText": "empty partitions -> should we say empty topics ?\nJust to clarify that some partitions from different topics are empty?", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377929200", "createdAt": "2020-02-11T22:08:37Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -516,6 +524,12 @@ private void completeAllProcessableWork() {\n                 task.commit();\n                 captureOutputsAndReEnqueueInternalResults();\n             }\n+            if (task.hasRecordsQueued()) {\n+                log.info(\"Due to the {} configuration, there are currently some records that can't be processed.\" +\n+                             \" Advancing wall-clock time or enqueuing records on the empty partitions will allow\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkzMDA1Mw==", "bodyText": "can't -> could not ?\nI would remove even though TopologyTestDriver is shutting down (read cleaner this way IMHO.)", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377930053", "createdAt": "2020-02-11T22:10:34Z", "author": {"login": "mjsax"}, "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -1058,6 +1072,11 @@ public void close() {\n             }\n         }\n         completeAllProcessableWork();\n+        if (task.hasRecordsQueued()) {\n+            log.warn(\"Due to the {} configuration, there were some records that can't be processed even\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0"}, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f67afd2ff3bf55f17fbe70c45b5d6aa9fe61892d", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/f67afd2ff3bf55f17fbe70c45b5d6aa9fe61892d", "committedDate": "2020-02-11T22:36:32Z", "message": "code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fc4da0e7f0a29845782c4f3a860289bbdfb356a", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/9fc4da0e7f0a29845782c4f3a860289bbdfb356a", "committedDate": "2020-02-11T22:50:46Z", "message": "decrease method length"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1810, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}