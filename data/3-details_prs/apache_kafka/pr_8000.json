{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY2MDgyNTU5", "number": 8000, "title": "KAFKA-9417: New Integration Test for KIP-447", "bodyText": "This change mainly have 2 components:\n\nextend the existing transactions_test.py to also try out new sendTxnOffsets(groupMetadata) API to make sure we are not introducing any regression or compatibility issue\na. We shrink the time window to 10 seconds for the txn timeout scheduler on broker so that we could trigger expiration earlier than later\ncreate a completely new system test class called group_mode_transactions_test which is more complicated than the existing system test, as we are taking rebalance into consideration and using multiple partitions instead of one. For further breakdown:\na. The message count was done on partition level, instead of global as we need to visualize the per partition order throughout the test. For this sake, we extend ConsoleConsumer to print out the data partition as well to help message copier interpret the per partition data.\nb. The progress count includes the time for completing the pending txn offset expiration\nc. More visibility and feature improvements on TransactionMessageCopier to better work under either standalone or group mode.\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-01-22T21:55:14Z", "url": "https://github.com/apache/kafka/pull/8000", "merged": true, "mergeCommit": {"oid": "07db26c20fcbccbf758591607864f7fd4bd8975f"}, "closed": true, "closedAt": "2020-02-12T20:34:13Z", "author": {"login": "abbccdda"}, "timelineItems": {"totalCount": 60, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb9O5tigBqjI5NzQ4NDU5Nzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcDYW7MgFqTM1Njk5Njg2Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ3NjU2ODQw", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-347656840", "createdAt": "2020-01-23T22:40:17Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo0MDoxN1rOFhPAjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo0MDoxN1rOFhPAjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5MzIyOA==", "bodyText": "During the test, I realized one scenario which is pretty dangerous. If we have a hard failure and the txn offsets get sticky on the broker side, for next consumer it could take over one minute to back-off and wait for the clearance, thus timing out on the fetch. Shrinking to 10 seconds here is just a remediation, but we should pay attention to general usage as well.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r370393228", "createdAt": "2020-01-23T22:40:17Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/tests/core/transactions_test.py", "diffHunk": "@@ -47,6 +47,7 @@ def __init__(self, test_context):\n         self.num_output_partitions = 3\n         self.num_seed_messages = 100000\n         self.transaction_size = 750\n+        self.transaction_timeout = 10000", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNjk4OTkx", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-351698991", "createdAt": "2020-01-31T18:40:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0MDo0MVrOFkUU8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0MDo0MVrOFkUU8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNjA5OQ==", "bodyText": "This timeout was increased from 30 to 60 seconds for the sake of rebalance caused offset wait (if triggered twice, that would be roughly 20~30 seconds)", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373626099", "createdAt": "2020-01-31T18:40:41Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/tests/core/group_mode_transactions_test.py", "diffHunk": "@@ -0,0 +1,312 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from kafkatest.services.zookeeper import ZookeeperService\n+from kafkatest.services.kafka import KafkaService\n+from kafkatest.services.console_consumer import ConsoleConsumer\n+from kafkatest.services.verifiable_producer import VerifiableProducer\n+from kafkatest.services.transactional_message_copier import TransactionalMessageCopier\n+from kafkatest.utils import is_int\n+\n+from ducktape.tests.test import Test\n+from ducktape.mark import matrix\n+from ducktape.mark.resource import cluster\n+from ducktape.utils.util import wait_until\n+\n+\n+class GroupModeTransactionsTest(Test):\n+    \"\"\"This test essentially does the same effort as TransactionsTest by transactionally copying data from a source topic to\n+    a destination topic and killing the copy process as well as the broker randomly through the process.\n+    The major difference is that we choose to work as a collaborated group with same topic subscription\n+    instead of individual consumers.\n+\n+    In the end we verify that the final output\n+    topic contains exactly one committed copy of each message in the input\n+    topic\n+    \"\"\"\n+    def __init__(self, test_context):\n+        \"\"\":type test_context: ducktape.tests.test.TestContext\"\"\"\n+        super(GroupModeTransactionsTest, self).__init__(test_context=test_context)\n+\n+        self.input_topic = \"input-topic\"\n+        self.output_topic = \"output-topic\"\n+\n+        self.num_brokers = 3\n+\n+        # Test parameters\n+        self.num_input_partitions = 9\n+        self.num_output_partitions = 9\n+        self.num_copiers = 3\n+        self.num_seed_messages = 100000\n+        self.transaction_size = 750\n+        self.transaction_timeout = 10000\n+        self.consumer_group = \"grouped-transactions-test-consumer-group\"\n+\n+        self.zk = ZookeeperService(test_context, num_nodes=1)\n+        self.kafka = KafkaService(test_context,\n+                                  num_nodes=self.num_brokers,\n+                                  zk=self.zk)\n+\n+    def setUp(self):\n+        self.zk.start()\n+\n+    def seed_messages(self, topic, num_seed_messages):\n+        seed_timeout_sec = 10000\n+        seed_producer = VerifiableProducer(context=self.test_context,\n+                                           num_nodes=1,\n+                                           kafka=self.kafka,\n+                                           topic=topic,\n+                                           message_validator=is_int,\n+                                           max_messages=num_seed_messages,\n+                                           enable_idempotence=True,\n+                                           repeating_keys=self.num_input_partitions)\n+        seed_producer.start()\n+        wait_until(lambda: seed_producer.num_acked >= num_seed_messages,\n+                   timeout_sec=seed_timeout_sec,\n+                   err_msg=\"Producer failed to produce messages %d in  %ds.\" % \\\n+                           (self.num_seed_messages, seed_timeout_sec))\n+        return seed_producer.acked_by_partition\n+\n+    def get_messages_from_topic(self, topic, num_messages):\n+        consumer = self.start_consumer(topic, group_id=\"verifying_consumer\")\n+        return self.drain_consumer(consumer, num_messages)\n+\n+    def bounce_brokers(self, clean_shutdown):\n+        for node in self.kafka.nodes:\n+            if clean_shutdown:\n+                self.kafka.restart_node(node, clean_shutdown = True)\n+            else:\n+                self.kafka.stop_node(node, clean_shutdown = False)\n+                wait_until(lambda: len(self.kafka.pids(node)) == 0 and not self.kafka.is_registered(node),\n+                           timeout_sec=self.kafka.zk_session_timeout + 5,\n+                           err_msg=\"Failed to see timely deregistration of \\\n+                           hard-killed broker %s\" % str(node.account))\n+                self.kafka.start_node(node)\n+\n+    def create_and_start_message_copier(self, input_topic, output_topic, transactional_id):\n+        message_copier = TransactionalMessageCopier(\n+            context=self.test_context,\n+            num_nodes=1,\n+            kafka=self.kafka,\n+            transactional_id=transactional_id,\n+            consumer_group=self.consumer_group,\n+            input_topic=input_topic,\n+            input_partition=-1,\n+            output_topic=output_topic,\n+            max_messages=-1,\n+            transaction_size=self.transaction_size,\n+            transaction_timeout=self.transaction_timeout,\n+            group_mode=True\n+        )\n+        message_copier.start()\n+        wait_until(lambda: message_copier.alive(message_copier.nodes[0]),\n+                   timeout_sec=10,\n+                   err_msg=\"Message copier failed to start after 10 s\")\n+        return message_copier\n+\n+    def bounce_copiers(self, copiers, clean_shutdown, timeout_sec=60):", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 119}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNzAwODEy", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-351700812", "createdAt": "2020-01-31T18:43:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0Mzo1MVrOFkUajA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0Mzo1MVrOFkUajA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNzUzMg==", "bodyText": "The changes in transactions_test are mainly for compatibility. Nothing should be behaving differently", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373627532", "createdAt": "2020-01-31T18:43:51Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/tests/core/transactions_test.py", "diffHunk": "@@ -218,8 +225,9 @@ def setup_topics(self):\n     @cluster(num_nodes=9)\n     @matrix(failure_mode=[\"hard_bounce\", \"clean_bounce\"],\n             bounce_target=[\"brokers\", \"clients\"],\n-            check_order=[True, False])\n-    def test_transactions(self, failure_mode, bounce_target, check_order):\n+            check_order=[True, False],\n+            use_group_metadata=[True, False])", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNzAyNzc0", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-351702774", "createdAt": "2020-01-31T18:47:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0NzoxM1rOFkUgjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0NzoxM1rOFkUgjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyOTA2OA==", "bodyText": "Only minor style fixes in this class", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373629068", "createdAt": "2020-01-31T18:47:13Z", "author": {"login": "abbccdda"}, "path": "tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java", "diffHunk": "@@ -159,8 +159,9 @@ private boolean isFinished() {\n                     partitionRecords.size(), minOffset, maxOffset));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzMzQ0OTY2", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-353344966", "createdAt": "2020-02-04T22:22:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyMjo0OFrOFlljsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjozODoxOVrOFll97w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1Njk3OA==", "bodyText": "This line is a bit confusing: the delayed rebalance is true / false, what does this mean?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374956978", "createdAt": "2020-02-04T22:22:48Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1033,7 +1034,9 @@ class GroupCoordinator(val brokerId: Int,\n     group.transitionTo(PreparingRebalance)\n \n     info(s\"Preparing to rebalance group ${group.groupId} in state ${group.currentState} with old generation \" +\n-      s\"${group.generationId} (${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) (reason: $reason)\")\n+      s\"${group.generationId} (${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) (reason: $reason), \" +\n+      s\"the remaining instances are ${group.allMembers}, and the delayed rebalance is $startEmpty \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NzIzOA==", "bodyText": "Is this part of the KIP to change the default value?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374957238", "createdAt": "2020-02-04T22:23:25Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala", "diffHunk": "@@ -47,7 +47,7 @@ object TransactionStateManager {\n   // default transaction management config values\n   val DefaultTransactionsMaxTimeoutMs: Int = TimeUnit.MINUTES.toMillis(15).toInt\n   val DefaultTransactionalIdExpirationMs: Int = TimeUnit.DAYS.toMillis(7).toInt\n-  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.MINUTES.toMillis(1).toInt\n+  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.SECONDS.toMillis(10).toInt", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1ODE0MQ==", "bodyText": "Where is this value used?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374958141", "createdAt": "2020-02-04T22:25:39Z", "author": {"login": "guozhangwang"}, "path": "tests/kafkatest/services/console_consumer.py", "diffHunk": "@@ -105,9 +106,11 @@ def __init__(self, context, num_nodes, kafka, topic, group_id=\"test-consumer-gro\n         self.from_beginning = from_beginning\n         self.message_validator = message_validator\n         self.messages_consumed = {idx: [] for idx in range(1, num_nodes + 1)}\n+        self.messages_consumed_by_partition = {idx: [] for idx in range(1, num_nodes + 1)}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1OTAwOQ==", "bodyText": "nit: some = are spaced before / after and some are not.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374959009", "createdAt": "2020-02-04T22:27:47Z", "author": {"login": "guozhangwang"}, "path": "tests/kafkatest/services/transactional_message_copier.py", "diffHunk": "@@ -47,12 +47,13 @@ class TransactionalMessageCopier(KafkaPathResolverMixin, BackgroundThreadService\n \n     def __init__(self, context, num_nodes, kafka, transactional_id, consumer_group,\n                  input_topic, input_partition, output_topic, max_messages = -1,\n-                 transaction_size = 1000, enable_random_aborts=True):\n+                 transaction_size = 1000, transaction_timeout = None, enable_random_aborts=True, use_group_metadata=False, group_mode=False):", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MTc4OQ==", "bodyText": "Why these values need to be atomic?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374961789", "createdAt": "2020-02-04T22:33:59Z", "author": {"login": "guozhangwang"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);\n+        shutdownData.put(\"remaining\", remaining);\n+        shutdownData.put(\"time\", FORMAT.format(new Date()));\n         return toJsonString(shutdownData);\n     }\n \n-    public static void main(String[] args) throws IOException {\n+    public static void main(String[] args) {\n         Namespace parsedArgs = argParser().parseArgsOrFail(args);\n         Integer numMessagesPerTransaction = parsedArgs.getInt(\"messagesPerTransaction\");\n         final String transactionalId = parsedArgs.getString(\"transactionalId\");\n         final String outputTopic = parsedArgs.getString(\"outputTopic\");\n \n         String consumerGroup = parsedArgs.getString(\"consumerGroup\");\n-        TopicPartition inputPartition = new TopicPartition(parsedArgs.getString(\"inputTopic\"), parsedArgs.getInt(\"inputPartition\"));\n \n         final KafkaProducer<String, String> producer = createProducer(parsedArgs);\n         final KafkaConsumer<String, String> consumer = createConsumer(parsedArgs);\n \n-        consumer.assign(singleton(inputPartition));\n+        final AtomicLong messageCap = new AtomicLong(\n+            parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\"));\n+\n+        boolean groupMode = parsedArgs.getBoolean(\"groupMode\");\n+        String topicName = parsedArgs.getString(\"inputTopic\");\n+        final AtomicLong remainingMessages = new AtomicLong(messageCap.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MjE2Ng==", "bodyText": "I had a similar question on the other PR: are we suggesting for manual assignment to use the original overload (hence we would never deprecate it)?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374962166", "createdAt": "2020-02-04T22:34:52Z", "author": {"login": "guozhangwang"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);\n+        shutdownData.put(\"remaining\", remaining);\n+        shutdownData.put(\"time\", FORMAT.format(new Date()));\n         return toJsonString(shutdownData);\n     }\n \n-    public static void main(String[] args) throws IOException {\n+    public static void main(String[] args) {\n         Namespace parsedArgs = argParser().parseArgsOrFail(args);\n         Integer numMessagesPerTransaction = parsedArgs.getInt(\"messagesPerTransaction\");\n         final String transactionalId = parsedArgs.getString(\"transactionalId\");\n         final String outputTopic = parsedArgs.getString(\"outputTopic\");\n \n         String consumerGroup = parsedArgs.getString(\"consumerGroup\");\n-        TopicPartition inputPartition = new TopicPartition(parsedArgs.getString(\"inputTopic\"), parsedArgs.getInt(\"inputPartition\"));\n \n         final KafkaProducer<String, String> producer = createProducer(parsedArgs);\n         final KafkaConsumer<String, String> consumer = createConsumer(parsedArgs);\n \n-        consumer.assign(singleton(inputPartition));\n+        final AtomicLong messageCap = new AtomicLong(\n+            parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\"));\n+\n+        boolean groupMode = parsedArgs.getBoolean(\"groupMode\");\n+        String topicName = parsedArgs.getString(\"inputTopic\");\n+        final AtomicLong remainingMessages = new AtomicLong(messageCap.get());\n+        final AtomicLong numMessagesProcessedSinceLastRebalance = new AtomicLong(0);\n+        final AtomicLong totalMessageProcessed = new AtomicLong(0);\n+        if (groupMode) {\n+            consumer.subscribe(Collections.singleton(topicName), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    messageCap.set(partitions.stream()\n+                        .mapToLong(partition -> messagesRemaining(consumer, partition)).sum());\n+                    log.info(\"Message cap set to {} on rebalance complete\", messageCap);\n+                    numMessagesProcessedSinceLastRebalance.set(0);\n+                    // We use message cap for remaining here as the remainingMessages are not set yet.\n+                    System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                        numMessagesProcessedSinceLastRebalance.get(), messageCap.get(), transactionalId, \"RebalanceComplete\"));\n+                }\n+            });\n+        } else {\n+            TopicPartition inputPartition = new TopicPartition(topicName, parsedArgs.getInt(\"inputPartition\"));\n+            consumer.assign(singleton(inputPartition));\n+            messageCap.set(Math.min(messagesRemaining(consumer, inputPartition), messageCap.get()));\n+            remainingMessages.set(messageCap.get());\n+        }\n \n-        long maxMessages = parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\");\n-        maxMessages = Math.min(messagesRemaining(consumer, inputPartition), maxMessages);\n         final boolean enableRandomAborts = parsedArgs.getBoolean(\"enableRandomAborts\");\n \n         producer.initTransactions();\n \n         final AtomicBoolean isShuttingDown = new AtomicBoolean(false);\n-        final AtomicLong remainingMessages = new AtomicLong(maxMessages);\n-        final AtomicLong numMessagesProcessed = new AtomicLong(0);\n+\n         Exit.addShutdownHook(\"transactional-message-copier-shutdown-hook\", () -> {\n             isShuttingDown.set(true);\n             // Flush any remaining messages\n             producer.close();\n             synchronized (consumer) {\n                 consumer.close();\n             }\n-            System.out.println(shutDownString(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            System.out.println(shutDownString(totalMessageProcessed.get(),\n+                numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId));\n         });\n \n+        final boolean useGroupMetadata = parsedArgs.getBoolean(\"useGroupMetadata\");\n         try {\n             Random random = new Random();\n-            while (0 < remainingMessages.get()) {\n-                System.out.println(statusAsJson(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            while (remainingMessages.get() > 0) {\n+                System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                    numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId, \"ProcessLoop\"));\n                 if (isShuttingDown.get())\n                     break;\n-                int messagesInCurrentTransaction = 0;\n-                long numMessagesForNextTransaction = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                long messagesSentWithinCurrentTxn = 0L;\n+                long messagesNeededForCurrentTxn = remainingMessages.get();\n \n                 try {\n                     producer.beginTransaction();\n-                    while (messagesInCurrentTransaction < numMessagesForNextTransaction) {\n+                    Map<Integer, Integer> partitionCount = new HashMap<>();\n+                    while (messagesSentWithinCurrentTxn < messagesNeededForCurrentTxn) {\n                         ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(200));\n+                        log.info(\"number of consumer records fetched: {}\", records.count());\n+                        if (messageCap.get() <= 0) {\n+                            // We could see no more message needed for processing after poll\n+                            break;\n+                        }\n                         for (ConsumerRecord<String, String> record : records) {\n                             producer.send(producerRecordFromConsumerRecord(outputTopic, record));\n-                            messagesInCurrentTransaction++;\n+                            messagesSentWithinCurrentTxn++;\n+                            partitionCount.put(record.partition(), partitionCount.getOrDefault(record.partition(), 0) + 1);\n                         }\n+                        messagesNeededForCurrentTxn = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                    }\n+\n+                    log.info(\"Messages sent in current transaction: {}\", messagesSentWithinCurrentTxn);\n+                    for (Map.Entry<Integer, Integer> entry : partitionCount.entrySet()) {\n+                        log.info(\"Partition {} has contributed {} records\", entry.getKey(), entry.getValue());\n+                    }\n+\n+                    if (useGroupMetadata) {\n+                        producer.sendOffsetsToTransaction(consumerPositions(consumer), consumer.groupMetadata());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MzY5NQ==", "bodyText": "nit: consumedSinceLastRebalanced.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374963695", "createdAt": "2020-02-04T22:38:19Z", "author": {"login": "guozhangwang"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0MDIwMDky", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-354020092", "createdAt": "2020-02-05T20:31:52Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDozMTo1MlrOFmGNhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDozMTo1MlrOFmGNhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5MTk3NQ==", "bodyText": "Is INFO level appropriate? (If yes, the message read a little too code centric IMHO)", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375491975", "createdAt": "2020-02-05T20:31:52Z", "author": {"login": "mjsax"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala", "diffHunk": "@@ -362,7 +362,10 @@ private[group] class GroupMetadata(val groupId: String, initialState: GroupState\n \n   def notYetRejoinedMembers = members.values.filter(!_.isAwaitingJoin).toList\n \n-  def hasAllMembersJoined = members.size == numMembersAwaitingJoin && pendingMembers.isEmpty\n+  def hasAllMembersJoined = {\n+    info(s\"The join complete condition checking: members => $members, members waiting join count: $numMembersAwaitingJoin, pending members: $pendingMembers\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0MDI4NzY5", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-354028769", "createdAt": "2020-02-05T20:47:01Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDo0NzowMlrOFmGn2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDo0NzowMlrOFmGn2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5ODcxMw==", "bodyText": "If we are in groupMode and enable userGroupMetadata should we use a different transaction.id for each producer instead of the same?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375498713", "createdAt": "2020-02-05T20:47:02Z", "author": {"login": "mjsax"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -137,16 +156,28 @@ private static ArgumentParser argParser() {\n                 .dest(\"enableRandomAborts\")\n                 .help(\"Whether or not to enable random transaction aborts (for system testing)\");\n \n+        parser.addArgument(\"--group-mode\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"GROUP-MODE\")\n+                .dest(\"groupMode\")\n+                .help(\"Whether to let consumer subscribe to the input topic or do manual assign. If we do\" +\n+                          \" subscription based consumption, the input partition shall be ignored\");\n+\n+        parser.addArgument(\"--use-group-metadata\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"USE-GROUP-METADATA\")\n+                .dest(\"useGroupMetadata\")\n+                .help(\"Whether to use the new transactional commit API with group metadata\");\n+\n         return parser;\n     }\n \n     private static KafkaProducer<String, String> createProducer(Namespace parsedArgs) {\n-        String transactionalId = parsedArgs.getString(\"transactionalId\");\n-        String brokerList = parsedArgs.getString(\"brokerList\");\n-\n         Properties props = new Properties();\n-        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);\n-        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, parsedArgs.getString(\"brokerList\"));\n+        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, parsedArgs.getString(\"transactionalId\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0MDMwODE3", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-354030817", "createdAt": "2020-02-05T20:50:29Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDo1MDoyOVrOFmGuBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDo1MDoyOVrOFmGuBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTUwMDI5Mg==", "bodyText": "Do we really need to a a logger instead of just using System.out.println()? And if yes, should we update all outputs to use the logger? It's a little unclear why we would mix both approaches?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375500292", "createdAt": "2020-02-05T20:50:29Z", "author": {"login": "mjsax"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -54,6 +61,9 @@\n  */\n public class TransactionalMessageCopier {\n \n+    private static final Logger log = LoggerFactory.getLogger(TransactionalMessageCopier.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f6d8ab335b286eef84d11afbac80fd72e5df543", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/7f6d8ab335b286eef84d11afbac80fd72e5df543", "committedDate": "2020-02-11T20:37:34Z", "message": "new txn test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40ed3bd5aa91ad358cccfc76e3ddc922a123c7ce", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/40ed3bd5aa91ad358cccfc76e3ddc922a123c7ce", "committedDate": "2020-02-11T20:37:34Z", "message": "tooling class support"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c88e6d0c7f7ffca4db743b533c90126557be321d", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/c88e6d0c7f7ffca4db743b533c90126557be321d", "committedDate": "2020-02-11T20:37:35Z", "message": "tune txn timeout for transactions_test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2395c9d443c3103f95cfebf1a6d7daed6288237", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/b2395c9d443c3103f95cfebf1a6d7daed6288237", "committedDate": "2020-02-11T20:37:35Z", "message": "more fixes on partition output"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "466e66805c17a8e85b8cf3575534fd15a7caff32", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/466e66805c17a8e85b8cf3575534fd15a7caff32", "committedDate": "2020-02-11T20:37:35Z", "message": "debug log for transaction_test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ccb59d6c4daffa13a31009158e9277b392c9699", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/2ccb59d6c4daffa13a31009158e9277b392c9699", "committedDate": "2020-02-11T20:37:35Z", "message": "group mode remaining count fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18bd2dd00a442804d639549bad0be51dcce7e4e8", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/18bd2dd00a442804d639549bad0be51dcce7e4e8", "committedDate": "2020-02-11T20:37:35Z", "message": "adjust result"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b78e0262a76f81a8cc8bb951e9929db756ca041", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/6b78e0262a76f81a8cc8bb951e9929db756ca041", "committedDate": "2020-02-11T20:37:35Z", "message": "debug log"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5aca9b632674c3f67d2cdbd0a8546c4fdf6e4c97", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/5aca9b632674c3f67d2cdbd0a8546c4fdf6e4c97", "committedDate": "2020-02-11T20:37:35Z", "message": "less records"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f824ff4d9365a74dc77eccb9cdee078da6a4167", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/1f824ff4d9365a74dc77eccb9cdee078da6a4167", "committedDate": "2020-02-11T20:37:35Z", "message": "revert to only single test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e15f3037647e4eb531337a687d38d001aaebe2c", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/1e15f3037647e4eb531337a687d38d001aaebe2c", "committedDate": "2020-02-11T20:37:35Z", "message": "debug txn timeout and shorten abort scheduler interval from 1 min to 10 seconds"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e7d0212c76515de4d32e74746411d4f231ba72c", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/2e7d0212c76515de4d32e74746411d4f231ba72c", "committedDate": "2020-02-11T20:37:35Z", "message": "make message cap atomic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b386012c497beac4de83513e792b273f38854b75", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/b386012c497beac4de83513e792b273f38854b75", "committedDate": "2020-02-11T20:37:35Z", "message": "debug group mode"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f7d5783f277a2e1a5d45f18128aef7d2cf3ac438", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/f7d5783f277a2e1a5d45f18128aef7d2cf3ac438", "committedDate": "2020-02-11T20:37:36Z", "message": "good for group test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "297c700cabee3fb7619678bc40c31329a58a1b55", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/297c700cabee3fb7619678bc40c31329a58a1b55", "committedDate": "2020-02-11T20:37:36Z", "message": "remove logging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d244b6c60d14ed57d6561addf582879ebd38bcc0", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/d244b6c60d14ed57d6561addf582879ebd38bcc0", "committedDate": "2020-02-11T20:37:36Z", "message": "add total message tracking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "653bc2758da6c7df5e4b8ea83707dee9b94209db", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/653bc2758da6c7df5e4b8ea83707dee9b94209db", "committedDate": "2020-02-11T20:37:36Z", "message": "more rebalance related debug log"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40cbb2aa2d3eddc4ed2fb5a41ea018c139018710", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/40cbb2aa2d3eddc4ed2fb5a41ea018c139018710", "committedDate": "2020-02-11T20:37:36Z", "message": "add notes for local docker build failure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b78116ea9ca5bcfde65b9e48fd687936c9eb02c", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/1b78116ea9ca5bcfde65b9e48fd687936c9eb02c", "committedDate": "2020-02-11T20:37:36Z", "message": "fix remaining message count change after rebalance"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf30abf7fcc43ceb8a9d91f81b73cf39d3816fab", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/bf30abf7fcc43ceb8a9d91f81b73cf39d3816fab", "committedDate": "2020-02-11T20:37:36Z", "message": "debug logging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0800139fadbb213cae31fa645574d1aa15d08fe", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/e0800139fadbb213cae31fa645574d1aa15d08fe", "committedDate": "2020-02-11T20:37:36Z", "message": "debug info for double counting and delayed join complete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54f1227c8c7db4c41ffaf4ca7f28f577ab2545e5", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/54f1227c8c7db4c41ffaf4ca7f28f577ab2545e5", "committedDate": "2020-02-11T20:37:36Z", "message": "deprecate misleading message cap"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21b8d54ffc564412e89242879f9fe0727dffc0d6", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/21b8d54ffc564412e89242879f9fe0727dffc0d6", "committedDate": "2020-02-11T20:37:36Z", "message": "excessive long timeout for new member join timeout"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6a07e1e5c8a59a51466887bca678cc1f6132af7", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/a6a07e1e5c8a59a51466887bca678cc1f6132af7", "committedDate": "2020-02-11T20:37:36Z", "message": "long expire hb"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34946fc7bd06c9270859094b9c4f10405a386f9b", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/34946fc7bd06c9270859094b9c4f10405a386f9b", "committedDate": "2020-02-11T20:37:37Z", "message": "add 0 check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5f862a8436474aac517e857a0de3db385a7525f", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/f5f862a8436474aac517e857a0de3db385a7525f", "committedDate": "2020-02-11T20:37:37Z", "message": "fix later"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e651db36378181794f6ea1471641979ad3fd09d", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/8e651db36378181794f6ea1471641979ad3fd09d", "committedDate": "2020-02-11T20:37:37Z", "message": "remove record bound"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6902766174f8b6baf44e447f8243712b222c83da", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/6902766174f8b6baf44e447f8243712b222c83da", "committedDate": "2020-02-11T20:37:37Z", "message": "smaller max poll timeout"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b7ffdf9a97368adcc64281f828449efceb17f0e", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/4b7ffdf9a97368adcc64281f828449efceb17f0e", "committedDate": "2020-02-11T20:37:37Z", "message": "expose group metadata"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abd921a275099cdbb729c593a4b2023192812248", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/abd921a275099cdbb729c593a4b2023192812248", "committedDate": "2020-02-11T20:37:37Z", "message": "add timeout to listOffset and let consumer redo the remaining count after rebalance"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "918d57d1f81e271d41f4cae0d7c5f5896fad5bc6", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/918d57d1f81e271d41f4cae0d7c5f5896fad5bc6", "committedDate": "2020-02-11T20:37:37Z", "message": "Revert \"add timeout to listOffset and let consumer redo the remaining count after rebalance\"\n\nThis reverts commit bf94b0af68066aa762a169b08b4c09d74ef0ed0a."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2a8ed5ad92bb64c840b98939d6ed8ea73cc32548", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/2a8ed5ad92bb64c840b98939d6ed8ea73cc32548", "committedDate": "2020-02-11T20:37:37Z", "message": "request update of metadata"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20be486525411a877947dc3c74e82e283bb7b14d", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/20be486525411a877947dc3c74e82e283bb7b14d", "committedDate": "2020-02-11T20:37:37Z", "message": "formal fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51e1809862f52de261ebf961b2b79aa8056b58d7", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/51e1809862f52de261ebf961b2b79aa8056b58d7", "committedDate": "2020-02-11T20:37:37Z", "message": "cleanup log messages"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/17b1ebb275d0bef7744a0215fc2392c4c97285cc", "committedDate": "2020-02-11T21:06:44Z", "message": "add txn timeout to example"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/17b1ebb275d0bef7744a0215fc2392c4c97285cc", "committedDate": "2020-02-11T21:06:44Z", "message": "add txn timeout to example"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2OTk1MjM4", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-356995238", "createdAt": "2020-02-11T21:07:58Z", "commit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowNzo1OFrOFoZH0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowNzo1OFrOFoZH0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5ODk2MA==", "bodyText": "Add a reminder in EOS example to people", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377898960", "createdAt": "2020-02-11T21:07:58Z", "author": {"login": "abbccdda"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -76,8 +76,11 @@ public ExactlyOnceMessageProcessor(final String mode,\n         this.numInstances = numInstances;\n         this.instanceIdx = instanceIdx;\n         this.transactionalId = \"Processor-\" + instanceIdx;\n+        // If we are using the group mode, it is recommended to have a relatively short txn timeout\n+        // in order to clear pending offsets faster.\n+        final int transactionTimeoutMs = this.mode.equals(\"groupMode\") ? 10000 : -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2OTk2ODYz", "url": "https://github.com/apache/kafka/pull/8000#pullrequestreview-356996863", "createdAt": "2020-02-11T21:10:37Z", "commit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMToxMDozN1rOFoZM3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMToxMDozN1rOFoZM3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkwMDI1Mg==", "bodyText": "Need to add synchronized as we are using static date FORMAT which shall triggers spotsbug for concurrent access.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377900252", "createdAt": "2020-02-11T21:10:37Z", "author": {"login": "abbccdda"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,87 +256,124 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static synchronized String statusAsJson(long totalProcessed, long consumedSinceLastRebalanced, long remaining, String transactionalId, String stage) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc"}, "originalPosition": 111}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2106, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}